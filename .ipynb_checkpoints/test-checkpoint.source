<R> <C> [EMPTY] <C> [BOLD] DM id F <C> [BOLD] DM ood F <C> [BOLD] PAS id F <C> [BOLD] PAS ood F <C> [BOLD] PSD id F <C> [BOLD] PSD ood F <C> [BOLD] EDS Smatch F <C> [BOLD] EDS EDM <C> [BOLD] AMR 2015 Smatch F <C> [BOLD] AMR 2017 Smatch F <R> <C> Groschwitz et al. ( 2018 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 70.2 <C> 71.0 <R> <C> Lyu and Titov ( 2018 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 73.7 <C> 74.4 ±0.16 <R> <C> Zhang et al. ( 2019 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 76.3 ±0.1 <R> <C> Peng et al. ( 2017 ) Basic <C> 89.4 <C> 84.5 <C> 92.2 <C> 88.3 <C> 77.6 <C> 75.3 <C> - <C> - <C> - <C> - <R> <C> Dozat and Manning ( 2018 ) <C> 93.7 <C> 88.9 <C> 94.0 <C> 90.8 <C> 81.0 <C> 79.4 <C> - <C> - <C> - <C> - <R> <C> Buys and Blunsom ( 2017 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> 85.5 <C> 85.9 <C> 60.1 <C> - <R> <C> Chen et al. ( 2018 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 90.9, <C> [BOLD] 90.41 <C> - <C> - <R> <C> This paper (GloVe) <C> 90.4 ±0.2 <C> 84.3 ±0.2 <C> 91.4 ±0.1 <C> 86.6 ±0.1 <C> 78.1 ±0.2 <C> 74.5 ±0.2 <C> 87.6 ±0.1 <C> 82.5 ±0.1 <C> 69.2 ±0.4 <C> 70.7 ±0.2 <R> <C> This paper (BERT) <C> [BOLD] 93.9 ±0.1 <C> [BOLD] 90.3 ±0.1 <C> [BOLD] 94.5 ±0.1 <C> [BOLD] 92.5 ±0.1 <C> [BOLD] 82.0 ±0.1 <C> [BOLD] 81.5 ±0.3 <C> 90.1 ±0.1 <C> 84.9 ±0.1 <C> [BOLD] 74.3 ±0.2 <C> 75.3 ±0.2 <R> <C> Peng et al. ( 2017 ) Freda1 <C> 90.0 <C> 84.9 <C> 92.3 <C> 88.3 <C> 78.1 <C> 75.8 <C> - <C> - <C> - <C> - <R> <C> Peng et al. ( 2017 ) Freda3 <C> 90.4 <C> 85.3 <C> 92.7 <C> 89.0 <C> 78.5 <C> 76.4 <C> - <C> - <C> - <C> - <R> <C> This paper, MTL (GloVe) <C> 91.2 ±0.1 <C> 85.7 ±0.0 <C> 92.2 ±0.2 <C> 88.0 ±0.3 <C> 78.9 ±0.3 <C> 76.2 ±0.4 <C> 88.2 ±0.1 <C> 83.3 ±0.1 <C> (70.4) ±0.2 <C> 71.2 ±0.2 <R> <C> This paper, MTL (BERT) <C> [BOLD] 94.1 ±0.1 <C> [BOLD] 90.5 ±0.1 <C> [BOLD] 94.7 ±0.1 <C> [BOLD] 92.8 ±0.1 <C> [BOLD] 82.1 ±0.2 <C> [BOLD] 81.6 ±0.1 <C> 90.4 ±0.1 <C> 85.2 ±0.1 <C> (74.5)3 ±0.1 <C> 75.3 ±0.1 <CAP> Table 1: Semantic parsing accuracies (id = in domain test set; ood = out of domain test set). <COT> Looking at the "This paper (BERT)" row, we can see that the accuracies for DM id F, DM ood F, PAS id F, PAS ood F, PSD id F, and PSD ood F are all higher than the corresponding accuracies in the "This paper (GloVe)" row.
<R> <C> [EMPTY] <C> [BOLD] DM id F <C> [BOLD] DM ood F <C> [BOLD] PAS id F <C> [BOLD] PAS ood F <C> [BOLD] PSD id F <C> [BOLD] PSD ood F <C> [BOLD] EDS Smatch F <C> [BOLD] EDS EDM <C> [BOLD] AMR 2015 Smatch F <C> [BOLD] AMR 2017 Smatch F <R> <C> Groschwitz et al. ( 2018 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 70.2 <C> 71.0 <R> <C> Lyu and Titov ( 2018 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 73.7 <C> 74.4 ±0.16 <R> <C> Zhang et al. ( 2019 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 76.3 ±0.1 <R> <C> Peng et al. ( 2017 ) Basic <C> 89.4 <C> 84.5 <C> 92.2 <C> 88.3 <C> 77.6 <C> 75.3 <C> - <C> - <C> - <C> - <R> <C> Dozat and Manning ( 2018 ) <C> 93.7 <C> 88.9 <C> 94.0 <C> 90.8 <C> 81.0 <C> 79.4 <C> - <C> - <C> - <C> - <R> <C> Buys and Blunsom ( 2017 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> 85.5 <C> 85.9 <C> 60.1 <C> - <R> <C> Chen et al. ( 2018 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 90.9, <C> [BOLD] 90.41 <C> - <C> - <R> <C> This paper (GloVe) <C> 90.4 ±0.2 <C> 84.3 ±0.2 <C> 91.4 ±0.1 <C> 86.6 ±0.1 <C> 78.1 ±0.2 <C> 74.5 ±0.2 <C> 87.6 ±0.1 <C> 82.5 ±0.1 <C> 69.2 ±0.4 <C> 70.7 ±0.2 <R> <C> This paper (BERT) <C> [BOLD] 93.9 ±0.1 <C> [BOLD] 90.3 ±0.1 <C> [BOLD] 94.5 ±0.1 <C> [BOLD] 92.5 ±0.1 <C> [BOLD] 82.0 ±0.1 <C> [BOLD] 81.5 ±0.3 <C> 90.1 ±0.1 <C> 84.9 ±0.1 <C> [BOLD] 74.3 ±0.2 <C> 75.3 ±0.2 <R> <C> Peng et al. ( 2017 ) Freda1 <C> 90.0 <C> 84.9 <C> 92.3 <C> 88.3 <C> 78.1 <C> 75.8 <C> - <C> - <C> - <C> - <R> <C> Peng et al. ( 2017 ) Freda3 <C> 90.4 <C> 85.3 <C> 92.7 <C> 89.0 <C> 78.5 <C> 76.4 <C> - <C> - <C> - <C> - <R> <C> This paper, MTL (GloVe) <C> 91.2 ±0.1 <C> 85.7 ±0.0 <C> 92.2 ±0.2 <C> 88.0 ±0.3 <C> 78.9 ±0.3 <C> 76.2 ±0.4 <C> 88.2 ±0.1 <C> 83.3 ±0.1 <C> (70.4) ±0.2 <C> 71.2 ±0.2 <R> <C> This paper, MTL (BERT) <C> [BOLD] 94.1 ±0.1 <C> [BOLD] 90.5 ±0.1 <C> [BOLD] 94.7 ±0.1 <C> [BOLD] 92.8 ±0.1 <C> [BOLD] 82.1 ±0.2 <C> [BOLD] 81.6 ±0.1 <C> 90.4 ±0.1 <C> 85.2 ±0.1 <C> (74.5)3 ±0.1 <C> 75.3 ±0.1 <CAP> Table 1: Semantic parsing accuracies (id = in domain test set; ood = out of domain test set). <COT> Looking at the "This paper, MTL (BERT)" row, we can see that the accuracies for AMR 2015 Smatch F and AMR 2017 Smatch F are higher than the corresponding accuracies in the "This paper (BERT)" row.
<R> <C> [BOLD] Dataset <C> [BOLD] BERT dev <C> [BOLD] BERT test <C> [BOLD] BioBERT dev <C> [BOLD] BioBERT test <R> <C> MedNLI <C> 79.56 <C> 77.49 <C> 82.15 <C> 79.04 <R> <C> MNLI (M) <C> 83.52 <C> - <C> 81.23 <C> - <R> <C> SNLI (S) <C> 90.39 <C> - <C> 89.10 <C> - <R> <C> M → MedNLI <C> 80.14 <C> [BOLD] 78.62 <C> 82.72 <C> 80.80 <R> <C> S → MedNLI <C> 80.28 <C> 78.19 <C> 83.29 <C> 81.29 <R> <C> M → S → MedNLI <C> 80.43 <C> 78.12 <C> 83.29 <C> 80.30 <R> <C> S → M → MedNLI <C> [BOLD] 81.72 <C> 77.98 <C> [BOLD] 83.51 <C> [BOLD] 82.63 <R> <C> MedNLI (expanded) <C> 79.13 <C> 77.07 <C> [BOLD] 83.87 <C> 79.95 <R> <C> S → M → MedNLI (expanded) <C> [BOLD] 82.15 <C> [BOLD] 79.95 <C> 83.08 <C> [BOLD] 81.85 <CAP> Table 4: All experiment results of transfer learning and abbreviation expansion (top-2 scores marked as bold). MedNLI (expanded) denotes MedNLI with abbreviation expansion. <COT> Looking at the "S → M → MedNLI" row, we can see that the performance of S → M → MedNLI is higher on BioBERT compared to BERT.
<R> <C> Model no-distill <C> Model no-project <C> Test 85.98 <C> [ITALIC] but 78.69 <C> [ITALIC] but or  [ITALIC] neg 80.13 <R> <C> no-distill <C> project <C> 86.54 <C> 83.40 <C> - <R> <C> distill <C> no-project <C> 86.11 <C> 79.04 <C> - <R> <C> distill <C> project <C> 86.62 <C> 83.32 <C> - <R> <C> ELMo <C> no-project <C> 88.89 <C> 86.51 <C> 87.24 <R> <C> ELMo <C> project <C> 88.96 <C> 87.20 <C> - <CAP> Table 2: Average performance (across 100 seeds) of ELMo on the SST2 task. We show performance on A-but-B sentences (“but”), negations (“neg”). <COT> Looking at the "Model no-distill" column and the "Test" row, we can see that the model without distillation achieves a test accuracy of 85.98%.
<R> <C> Model no-distill <C> Model no-project <C> Test 85.98 <C> [ITALIC] but 78.69 <C> [ITALIC] but or  [ITALIC] neg 80.13 <R> <C> no-distill <C> project <C> 86.54 <C> 83.40 <C> - <R> <C> distill <C> no-project <C> 86.11 <C> 79.04 <C> - <R> <C> distill <C> project <C> 86.62 <C> 83.32 <C> - <R> <C> ELMo <C> no-project <C> 88.89 <C> 86.51 <C> 87.24 <R> <C> ELMo <C> project <C> 88.96 <C> 87.20 <C> - <CAP> Table 2: Average performance (across 100 seeds) of ELMo on the SST2 task. We show performance on A-but-B sentences (“but”), negations (“neg”). <COT> Looking at the "ELMo project" row, we can see that the ELMo model with project achieves a test accuracy of 87.20.
<R> <C> Threshold <C> 0.50 <C> 0.66 <C> 0.75 <C> 0.90 <R> <C> Neutral Sentiment <C> 10 <C> 70 <C> 95 <C> 234 <R> <C> Flipped Sentiment <C> 15 <C> 4 <C> 2 <C> 0 <R> <C> Fleiss’ Kappa ( [ITALIC] κ) <C> 0.38 <C> 0.42 <C> 0.44 <C> 0.58 <R> <C> no-distill, no-project <C> 81.32 <C> 83.54 <C> 84.54 <C> 87.55 <R> <C> ELMo, no-project <C> 87.56 <C> 90.00 <C> 91.31 <C> 93.14 <CAP> Table 3: Number of sentences in the crowdsourced study (447 sentences) which got marked as neutral and which got the opposite of their labels in the SST2 dataset, using various thresholds. Inter-annotator agreement is computed using Fleiss’ Kappa. Average accuracies of the baseline and ELMo (over 100 seeds) on non-neutral sentences are also shown. <COT> Looking at the "Threshold" column, we can see that as the threshold increases, the number of sentences marked as neutral decreases.
<R> <C> Threshold <C> 0.50 <C> 0.66 <C> 0.75 <C> 0.90 <R> <C> Neutral Sentiment <C> 10 <C> 70 <C> 95 <C> 234 <R> <C> Flipped Sentiment <C> 15 <C> 4 <C> 2 <C> 0 <R> <C> Fleiss’ Kappa ( [ITALIC] κ) <C> 0.38 <C> 0.42 <C> 0.44 <C> 0.58 <R> <C> no-distill, no-project <C> 81.32 <C> 83.54 <C> 84.54 <C> 87.55 <R> <C> ELMo, no-project <C> 87.56 <C> 90.00 <C> 91.31 <C> 93.14 <CAP> Table 3: Number of sentences in the crowdsourced study (447 sentences) which got marked as neutral and which got the opposite of their labels in the SST2 dataset, using various thresholds. Inter-annotator agreement is computed using Fleiss’ Kappa. Average accuracies of the baseline and ELMo (over 100 seeds) on non-neutral sentences are also shown. <COT> Looking at the "Fleiss’ Kappa (κ)" column, we can see that as the threshold increases, the inter-annotator agreement (Fleiss' Kappa) also increases.
<R> <C> [BOLD] Concept Input →  [BOLD] Embeddings <C> [BOLD] Concept Input →  [BOLD] TF <C> [BOLD] Concept Input →  [BOLD] IDF <C> [BOLD] Label  [BOLD] T <C> [BOLD] Label  [BOLD] P <C> [BOLD] Label  [BOLD] R <C> [BOLD] Label  [BOLD] F <C> [BOLD] Description  [BOLD] T <C> [BOLD] Description  [BOLD] P <C> [BOLD] Description  [BOLD] R <C> [BOLD] Description  [BOLD] F <C> [BOLD] Both  [BOLD] T <C> [BOLD] Both  [BOLD] P <C> [BOLD] Both  [BOLD] R <C> [BOLD] Both  [BOLD] F <R> <C> [BOLD] GloVe <C> [BOLD] - <C> [BOLD] - <C> .635 <C> .750 <C> .818 <C> .783 <C> .720 <C> .754 <C> .891 <C> .817 <C> .735 <C> .765 <C> .945 <C> .846 <R> <C> [BOLD] GloVe <C> [BOLD] + <C> [BOLD] - <C> .640 <C> .891 <C> .745 <C> .812 <C> .700 <C> .831 <C> .891 <C> .860 <C> .690 <C> .813 <C> .945 <C> .874 <R> <C> [BOLD] GloVe <C> [BOLD] - <C> [BOLD] + <C> .600 <C> .738 <C> .873 <C> .800 <C> .670 <C> .746 <C> .909 <C> .820 <C> .755 <C> .865 <C> .818 <C> .841 <R> <C> [BOLD] GloVe <C> [BOLD] + <C> [BOLD] + <C> .605 <C> .904 <C> .855 <C> .879 <C> .665 <C> .857 <C> .873 <C> .865 <C> .715 <C> .923 <C> .873 <C> .897 <R> <C> [BOLD] Google <C> [BOLD] - <C> [BOLD] - <C> .440 <C> .813 <C> .945 <C> .874 <C> .515 <C> .701 <C> .982 <C> .818 <C> .635 <C> .920 <C> .836 <C> .876 <R> <C> [BOLD] Google <C> [BOLD] + <C> [BOLD] - <C> .445 <C> .943 <C> .909 <C> [BOLD] .926 <C> .540 <C> .873 <C> .873 <C> .873 <C> .565 <C> .927 <C> .927 <C> .927 <R> <C> [BOLD] Google <C> [BOLD] - <C> [BOLD] + <C> .435 <C> .839 <C> .945 <C> .889 <C> .520 <C> .732 <C> .945 <C> .825 <C> .590 <C> .877 <C> .909 <C> .893 <R> <C> [BOLD] Google <C> [BOLD] + <C> [BOLD] + <C> .430 <C> .943 <C> .909 <C> [BOLD] .926 <C> .530 <C> .889 <C> .873 <C> [BOLD] .881 <C> .545 <C> .945 <C> .945 <C> [BOLD] .945 <R> <C> [BOLD] fastText <C> [BOLD] - <C> [BOLD] - <C> .440 <C> .781 <C> .909 <C> .840 <C> .555 <C> .708 <C> .927 <C> .803 <C> .615 <C> .778 <C> .891 <C> .831 <R> <C> [BOLD] fastText <C> [BOLD] + <C> [BOLD] - <C> .435 <C> .850 <C> .927 <C> .887 <C> .520 <C> .781 <C> .909 <C> .840 <C> .530 <C> .803 <C> .964 <C> .876 <R> <C> [BOLD] fastText <C> [BOLD] - <C> [BOLD] + <C> .435 <C> .850 <C> .927 <C> .887 <C> .525 <C> .722 <C> .945 <C> .819 <C> .600 <C> .820 <C> .909 <C> .862 <R> <C> [BOLD] fastText <C> [BOLD] + <C> [BOLD] + <C> .420 <C> .895 <C> .927 <C> .911 <C> .505 <C> .803 <C> .891 <C> .845 <C> .520 <C> .833 <C> .909 <C> .870 <CAP> Table 1: Tuning Data Results AVG_COS_SIM. Top F per Concept Input Type in Bold. <COT> Looking at the "Concept Input → Embeddings" column, we can see that the F-score for the "GloVe" embedding without tuning is 0.783.
<R> <C> [BOLD] Concept Input →  [BOLD] Embeddings <C> [BOLD] Concept Input →  [BOLD] TF <C> [BOLD] Concept Input →  [BOLD] IDF <C> [BOLD] Label  [BOLD] T <C> [BOLD] Label  [BOLD] P <C> [BOLD] Label  [BOLD] R <C> [BOLD] Label  [BOLD] F <C> [BOLD] Description  [BOLD] T <C> [BOLD] Description  [BOLD] P <C> [BOLD] Description  [BOLD] R <C> [BOLD] Description  [BOLD] F <C> [BOLD] Both  [BOLD] T <C> [BOLD] Both  [BOLD] P <C> [BOLD] Both  [BOLD] R <C> [BOLD] Both  [BOLD] F <R> <C> [BOLD] GloVe <C> [BOLD] - <C> [BOLD] - <C> .635 <C> .750 <C> .818 <C> .783 <C> .720 <C> .754 <C> .891 <C> .817 <C> .735 <C> .765 <C> .945 <C> .846 <R> <C> [BOLD] GloVe <C> [BOLD] + <C> [BOLD] - <C> .640 <C> .891 <C> .745 <C> .812 <C> .700 <C> .831 <C> .891 <C> .860 <C> .690 <C> .813 <C> .945 <C> .874 <R> <C> [BOLD] GloVe <C> [BOLD] - <C> [BOLD] + <C> .600 <C> .738 <C> .873 <C> .800 <C> .670 <C> .746 <C> .909 <C> .820 <C> .755 <C> .865 <C> .818 <C> .841 <R> <C> [BOLD] GloVe <C> [BOLD] + <C> [BOLD] + <C> .605 <C> .904 <C> .855 <C> .879 <C> .665 <C> .857 <C> .873 <C> .865 <C> .715 <C> .923 <C> .873 <C> .897 <R> <C> [BOLD] Google <C> [BOLD] - <C> [BOLD] - <C> .440 <C> .813 <C> .945 <C> .874 <C> .515 <C> .701 <C> .982 <C> .818 <C> .635 <C> .920 <C> .836 <C> .876 <R> <C> [BOLD] Google <C> [BOLD] + <C> [BOLD] - <C> .445 <C> .943 <C> .909 <C> [BOLD] .926 <C> .540 <C> .873 <C> .873 <C> .873 <C> .565 <C> .927 <C> .927 <C> .927 <R> <C> [BOLD] Google <C> [BOLD] - <C> [BOLD] + <C> .435 <C> .839 <C> .945 <C> .889 <C> .520 <C> .732 <C> .945 <C> .825 <C> .590 <C> .877 <C> .909 <C> .893 <R> <C> [BOLD] Google <C> [BOLD] + <C> [BOLD] + <C> .430 <C> .943 <C> .909 <C> [BOLD] .926 <C> .530 <C> .889 <C> .873 <C> [BOLD] .881 <C> .545 <C> .945 <C> .945 <C> [BOLD] .945 <R> <C> [BOLD] fastText <C> [BOLD] - <C> [BOLD] - <C> .440 <C> .781 <C> .909 <C> .840 <C> .555 <C> .708 <C> .927 <C> .803 <C> .615 <C> .778 <C> .891 <C> .831 <R> <C> [BOLD] fastText <C> [BOLD] + <C> [BOLD] - <C> .435 <C> .850 <C> .927 <C> .887 <C> .520 <C> .781 <C> .909 <C> .840 <C> .530 <C> .803 <C> .964 <C> .876 <R> <C> [BOLD] fastText <C> [BOLD] - <C> [BOLD] + <C> .435 <C> .850 <C> .927 <C> .887 <C> .525 <C> .722 <C> .945 <C> .819 <C> .600 <C> .820 <C> .909 <C> .862 <R> <C> [BOLD] fastText <C> [BOLD] + <C> [BOLD] + <C> .420 <C> .895 <C> .927 <C> .911 <C> .505 <C> .803 <C> .891 <C> .845 <C> .520 <C> .833 <C> .909 <C> .870 <CAP> Table 1: Tuning Data Results AVG_COS_SIM. Top F per Concept Input Type in Bold. <COT> Looking at the "Concept Input → IDF" column, we can see that the precision for the "Google" embedding with tuning is 0.909.
<R> <C> [BOLD] Concept Input →  [BOLD] Embeddings <C> [BOLD] Concept Input →  [BOLD] TF <C> [BOLD] Concept Input →  [BOLD] IDF <C> [BOLD] Label  [BOLD] T/n <C> [BOLD] Label  [BOLD] P <C> [BOLD] Label  [BOLD] R <C> [BOLD] Label  [BOLD] F <C> [BOLD] Description  [BOLD] T/n <C> [BOLD] Description  [BOLD] P <C> [BOLD] Description  [BOLD] R <C> [BOLD] Description  [BOLD] F <C> [BOLD] Both  [BOLD] T/n <C> [BOLD] Both  [BOLD] P <C> [BOLD] Both  [BOLD] R <C> [BOLD] Both  [BOLD] F <R> <C> [BOLD] GloVe <C> [BOLD] + <C> [BOLD] - <C> .365/6 <C> .797 <C> .927 <C> .857 <C> .690/14 <C> .915 <C> .782 <C> .843 <C> .675/16 <C> .836 <C> .927 <C> .879 <R> <C> [BOLD] GloVe <C> [BOLD] - <C> [BOLD] + <C> .300/30 <C> .929 <C> .236 <C> .377 <C> .300/30 <C> .806 <C> .455 <C> .581 <C> .300/30 <C> .778 <C> .636 <C> .700 <R> <C> [BOLD] GloVe <C> [BOLD] + <C> [BOLD] + <C> .330/6 <C> .879 <C> .927 <C> .903 <C> .345/6 <C> .881 <C> .945 <C> [BOLD] .912 <C> .345/6 <C> .895 <C> .927 <C> .911 <R> <C> [BOLD] Google <C> [BOLD] + <C> [BOLD] - <C> .345/22 <C> .981 <C> .927 <C> [BOLD] .953 <C> .480/16 <C> .895 <C> .927 <C> .911 <C> .520/16 <C> .912 <C> .945 <C> .929 <R> <C> [BOLD] Google <C> [BOLD] - <C> [BOLD] + <C> .300/30 <C> 1.00 <C> .345 <C> .514 <C> .300/8 <C> 1.00 <C> .345 <C> .514 <C> .300/30 <C> 1.00 <C> .600 <C> .750 <R> <C> [BOLD] Google <C> [BOLD] + <C> [BOLD] + <C> .300/10 <C> 1.00 <C> .509 <C> .675 <C> .300/14 <C> .972 <C> .636 <C> .769 <C> .350/22 <C> 1.00 <C> .836 <C> .911 <R> <C> [BOLD] fastText <C> [BOLD] + <C> [BOLD] - <C> .415/22 <C> .980 <C> .873 <C> .923 <C> .525/14 <C> .887 <C> .855 <C> .870 <C> .535/20 <C> .869 <C> .964 <C> .914 <R> <C> [BOLD] fastText <C> [BOLD] - <C> [BOLD] + <C> .350/24 <C> 1.00 <C> .309 <C> .472 <C> .300/30 <C> 1.00 <C> .382 <C> .553 <C> .300/28 <C> 1.00 <C> .673 <C> .804 <R> <C> [BOLD] fastText <C> [BOLD] + <C> [BOLD] + <C> .300/20 <C> 1.00 <C> .800 <C> .889 <C> .300/10 <C> .953 <C> .745 <C> .837 <C> .310/14 <C> .963 <C> .945 <C> [BOLD] .954 <CAP> Table 2: Tuning Data Results TOP_n_COS_SIM_AVG. Top F per Concept Input Type in Bold. <COT> Looking at the "GloVe + -" row, we can see that the F-score for the "Concept Input → Embeddings" column is higher than the F-score for the "Concept Input → TF" column.
<R> <C> [BOLD] Concept Input →  [BOLD] Embeddings <C> [BOLD] Concept Input →  [BOLD] TF <C> [BOLD] Concept Input →  [BOLD] IDF <C> [BOLD] Label  [BOLD] T/n <C> [BOLD] Label  [BOLD] P <C> [BOLD] Label  [BOLD] R <C> [BOLD] Label  [BOLD] F <C> [BOLD] Description  [BOLD] T/n <C> [BOLD] Description  [BOLD] P <C> [BOLD] Description  [BOLD] R <C> [BOLD] Description  [BOLD] F <C> [BOLD] Both  [BOLD] T/n <C> [BOLD] Both  [BOLD] P <C> [BOLD] Both  [BOLD] R <C> [BOLD] Both  [BOLD] F <R> <C> [BOLD] GloVe <C> [BOLD] + <C> [BOLD] - <C> .365/6 <C> .797 <C> .927 <C> .857 <C> .690/14 <C> .915 <C> .782 <C> .843 <C> .675/16 <C> .836 <C> .927 <C> .879 <R> <C> [BOLD] GloVe <C> [BOLD] - <C> [BOLD] + <C> .300/30 <C> .929 <C> .236 <C> .377 <C> .300/30 <C> .806 <C> .455 <C> .581 <C> .300/30 <C> .778 <C> .636 <C> .700 <R> <C> [BOLD] GloVe <C> [BOLD] + <C> [BOLD] + <C> .330/6 <C> .879 <C> .927 <C> .903 <C> .345/6 <C> .881 <C> .945 <C> [BOLD] .912 <C> .345/6 <C> .895 <C> .927 <C> .911 <R> <C> [BOLD] Google <C> [BOLD] + <C> [BOLD] - <C> .345/22 <C> .981 <C> .927 <C> [BOLD] .953 <C> .480/16 <C> .895 <C> .927 <C> .911 <C> .520/16 <C> .912 <C> .945 <C> .929 <R> <C> [BOLD] Google <C> [BOLD] - <C> [BOLD] + <C> .300/30 <C> 1.00 <C> .345 <C> .514 <C> .300/8 <C> 1.00 <C> .345 <C> .514 <C> .300/30 <C> 1.00 <C> .600 <C> .750 <R> <C> [BOLD] Google <C> [BOLD] + <C> [BOLD] + <C> .300/10 <C> 1.00 <C> .509 <C> .675 <C> .300/14 <C> .972 <C> .636 <C> .769 <C> .350/22 <C> 1.00 <C> .836 <C> .911 <R> <C> [BOLD] fastText <C> [BOLD] + <C> [BOLD] - <C> .415/22 <C> .980 <C> .873 <C> .923 <C> .525/14 <C> .887 <C> .855 <C> .870 <C> .535/20 <C> .869 <C> .964 <C> .914 <R> <C> [BOLD] fastText <C> [BOLD] - <C> [BOLD] + <C> .350/24 <C> 1.00 <C> .309 <C> .472 <C> .300/30 <C> 1.00 <C> .382 <C> .553 <C> .300/28 <C> 1.00 <C> .673 <C> .804 <R> <C> [BOLD] fastText <C> [BOLD] + <C> [BOLD] + <C> .300/20 <C> 1.00 <C> .800 <C> .889 <C> .300/10 <C> .953 <C> .745 <C> .837 <C> .310/14 <C> .963 <C> .945 <C> [BOLD] .954 <CAP> Table 2: Tuning Data Results TOP_n_COS_SIM_AVG. Top F per Concept Input Type in Bold. <COT> Looking at the "Google + -" row, we can see that the precision for the "Description P" column is higher than the precision for the "Both P" column.
<R> <C> Gong et al.  [BOLD] ( 2018  [BOLD] ) <C> topic_science <C> topic_science <C> topic_science <C> topic_science <C> topic_science <C> [BOLD] P .758 <C> [BOLD] P ±.012 <C> [BOLD] R .885 <C> [BOLD] R ±.071 <C> [BOLD] F .818 <C> [BOLD] F ±.028 <R> <C> Gong et al.  [BOLD] ( 2018  [BOLD] ) <C> topic_wiki <C> topic_wiki <C> topic_wiki <C> topic_wiki <C> topic_wiki <C> .750 <C> ±.009 <C> .842 <C> ±.010 <C> .791 <C> ±.007 <R> <C> [BOLD] Method <C> [BOLD] Embeddings <C> [BOLD] Settings <C> [BOLD] Settings <C> [BOLD] T/n <C> [BOLD] Conc. Input <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] AVG_COS_SIM <C> Google <C> +TF <C> +IDF <C> .515 <C> Label <C> .939 <C> ±.043 <C> .839 <C> ±.067 <C> .884 <C> ±.038 <R> <C> [BOLD] AVG_COS_SIM <C> Google <C> +TF <C> +IDF <C> .520 <C> Description <C> .870 <C> ±.068 <C> .834 <C> ±.048 <C> .849 <C> ±.038 <R> <C> [BOLD] AVG_COS_SIM <C> Google <C> +TF <C> +IDF <C> .545 <C> Both <C> .915 <C> ±.040 <C> .938 <C> ±.047 <C> [BOLD] .926 <C> ±.038 <R> <C> [BOLD] TOP_n_COS_SIM_AVG <C> Google <C> +TF <C> -IDF <C> .345/22 <C> Label <C> .854 <C> ±.077 <C> .861 <C> ±.044 <C> .856 <C> ±.054 <R> <C> [BOLD] TOP_n_COS_SIM_AVG <C> GloVe <C> +TF <C> +IDF <C> .345/6 <C> Description <C> .799 <C> ±.063 <C> .766 <C> ±.094 <C> .780 <C> ±.068 <R> <C> [BOLD] TOP_n_COS_SIM_AVG <C> fastText <C> +TF <C> +IDF <C> .310/14 <C> Both <C> .850 <C> ±.059 <C> .918 <C> ±.049 <C> [BOLD] .881 <C> ±.037 <CAP> Table 3: Test Data Results <COT> Looking at the "Method" cell, "AVG_COS_SIM" cell, and "topic_science" cell, we can see that the average cosine similarity for the method "Google +TF +IDF" is .545 for the topic "topic_science".
<R> <C> Gong et al.  [BOLD] ( 2018  [BOLD] ) <C> topic_science <C> topic_science <C> topic_science <C> topic_science <C> topic_science <C> [BOLD] P .758 <C> [BOLD] P ±.012 <C> [BOLD] R .885 <C> [BOLD] R ±.071 <C> [BOLD] F .818 <C> [BOLD] F ±.028 <R> <C> Gong et al.  [BOLD] ( 2018  [BOLD] ) <C> topic_wiki <C> topic_wiki <C> topic_wiki <C> topic_wiki <C> topic_wiki <C> .750 <C> ±.009 <C> .842 <C> ±.010 <C> .791 <C> ±.007 <R> <C> [BOLD] Method <C> [BOLD] Embeddings <C> [BOLD] Settings <C> [BOLD] Settings <C> [BOLD] T/n <C> [BOLD] Conc. Input <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] AVG_COS_SIM <C> Google <C> +TF <C> +IDF <C> .515 <C> Label <C> .939 <C> ±.043 <C> .839 <C> ±.067 <C> .884 <C> ±.038 <R> <C> [BOLD] AVG_COS_SIM <C> Google <C> +TF <C> +IDF <C> .520 <C> Description <C> .870 <C> ±.068 <C> .834 <C> ±.048 <C> .849 <C> ±.038 <R> <C> [BOLD] AVG_COS_SIM <C> Google <C> +TF <C> +IDF <C> .545 <C> Both <C> .915 <C> ±.040 <C> .938 <C> ±.047 <C> [BOLD] .926 <C> ±.038 <R> <C> [BOLD] TOP_n_COS_SIM_AVG <C> Google <C> +TF <C> -IDF <C> .345/22 <C> Label <C> .854 <C> ±.077 <C> .861 <C> ±.044 <C> .856 <C> ±.054 <R> <C> [BOLD] TOP_n_COS_SIM_AVG <C> GloVe <C> +TF <C> +IDF <C> .345/6 <C> Description <C> .799 <C> ±.063 <C> .766 <C> ±.094 <C> .780 <C> ±.068 <R> <C> [BOLD] TOP_n_COS_SIM_AVG <C> fastText <C> +TF <C> +IDF <C> .310/14 <C> Both <C> .850 <C> ±.059 <C> .918 <C> ±.049 <C> [BOLD] .881 <C> ±.037 <CAP> Table 3: Test Data Results <COT> Looking at the "Gong et al. (2018)" row, "P .758" cell, and "topic_science" cell, we can see that the precision for the method "Gong et al. (2018)" is .758 for the topic "topic_science".
<R> <C> [BOLD] Model <C> [BOLD] CNN  [ITALIC] full <C> [BOLD] CNN  [ITALIC] dict <C> [BOLD] CNN  [ITALIC] list <C> [BOLD] CNN  [ITALIC] k <C> [BOLD] CNN MRR <C> [BOLD] LSTM  [ITALIC] full <C> [BOLD] LSTM  [ITALIC] list <C> [BOLD] LSTM k <C> [BOLD] LSTM MRR <R> <C> Baseline (BL) <C> [BOLD] full: 19.7 dict: 56.0 <C> [BOLD] full: 19.7 dict: 56.0 <C> [BOLD] full: 19.7 dict: 56.0 <C> [BOLD] full: 19.7 dict: 56.0 <C> [BOLD] full: 19.7 dict: 56.0 <C> [BOLD] full: 17.9 <C> [BOLD] full: 17.9 <C> [BOLD] full: 17.9 <C> [BOLD] full: 17.9 <R> <C> BL+ Glove (Jeffrey:14) <C> 22.0 <C> 62.5 <C> 75.8 <C> 7 <C> 44.5 <C> 19.1 <C> 75.3 <C> 4 <C> 78.8 <R> <C> BL+C-LSTM (Chunting:15) <C> 21.4 <C> 61.0 <C> 71.3 <C> 8 <C> 45.6 <C> 18.9 <C> 74.7 <C> 4 <C> 80.7 <R> <C> BL+CNN-RNN (Xingyou:16) <C> 21.7 <C> 61.8 <C> 73.3 <C> 8 <C> 44.5 <C> 19.5 <C> 77.1 <C> 4 <C> 80.9 <R> <C> BL+MVCNN (Wenpeng:16) <C> 21.3 <C> 60.6 <C> 71.9 <C> 8 <C> 44.2 <C> 19.2 <C> 75.8 <C> 4 <C> 78.8 <R> <C> BL+Attentive LSTM (Ming:16) <C> 21.9 <C> 62.4 <C> 74.0 <C> 8 <C> 45.7 <C> 19.1 <C> 71.4 <C> 5 <C> 80.2 <R> <C> BL+fasttext (Armand:17) <C> 21.9 <C> 62.2 <C> 75.4 <C> 7 <C> 44.6 <C> 19.4 <C> 76.1 <C> 4 <C> 80.3 <R> <C> BL+InferSent (Alexis:17) <C> 22.0 <C> 62.5 <C> 75.8 <C> 7 <C> 44.5 <C> 19.4 <C> 76.7 <C> 4 <C> 79.7 <R> <C> BL+USE-T (Daniel:18) <C> 22.0 <C> 62.5 <C> [BOLD] 78.3 <C> 6 <C> 44.7 <C> 19.2 <C> 75.8 <C> 4 <C> 79.5 <R> <C> BL+TWE (Ahmed:18) <C> 22.2 <C> 63.0 <C> 76.3 <C> 7 <C> 44.7 <C> 19.5 <C> 76.7 <C> 4 <C> 80.2 <R> <C> BL+FDCLSTM (ours) <C> 22.3 <C> 63.3 <C> 75.1 <C> 8 <C> 45.0 <C> [BOLD] 20.2 <C> 67.9 <C> 9 <C> 79.8 <R> <C> BL+FDCLSTM [ITALIC] AT (ours) <C> [BOLD] 22.4 <C> 63.7 <C> 75.5 <C> 8 <C> [BOLD] 45.9 <C> 20.1 <C> 67.6 <C> 9 <C> [BOLD] 81.8 <R> <C> BL+FDCLSTM [ITALIC] lexicon (ours) <C> 22.6 <C> 64.3 <C> 76.3 <C> 8 <C> 45.1 <C> 19.4 <C> 76.4 <C> 4 <C> 78.8 <R> <C> BL+FDCLSTM [ITALIC] AT+ [ITALIC] lexicon (ours) <C> 22.6 <C> 64.3 <C> 76.3 <C> 8 <C> 45.1 <C> 19.7 <C> [BOLD] 77.8 <C> 4 <C> 80.4 <CAP> Table 1: Best results after re-ranking using different re-ranker, and different values for k-best hypotheses extracted from the baseline output (%). In addition, to evaluate our re-ranker with MRR we fixed k CNNk=8 LSTMk=4 <COT> Looking at the "Model" column, we can see that the models are evaluated using different re-rankers and different values for k-best hypotheses. 
<R> <C> [BOLD] Model <C> [BOLD] CNN  [ITALIC] full <C> [BOLD] CNN  [ITALIC] dict <C> [BOLD] CNN  [ITALIC] list <C> [BOLD] CNN  [ITALIC] k <C> [BOLD] CNN MRR <C> [BOLD] LSTM  [ITALIC] full <C> [BOLD] LSTM  [ITALIC] list <C> [BOLD] LSTM k <C> [BOLD] LSTM MRR <R> <C> Baseline (BL) <C> [BOLD] full: 19.7 dict: 56.0 <C> [BOLD] full: 19.7 dict: 56.0 <C> [BOLD] full: 19.7 dict: 56.0 <C> [BOLD] full: 19.7 dict: 56.0 <C> [BOLD] full: 19.7 dict: 56.0 <C> [BOLD] full: 17.9 <C> [BOLD] full: 17.9 <C> [BOLD] full: 17.9 <C> [BOLD] full: 17.9 <R> <C> BL+ Glove (Jeffrey:14) <C> 22.0 <C> 62.5 <C> 75.8 <C> 7 <C> 44.5 <C> 19.1 <C> 75.3 <C> 4 <C> 78.8 <R> <C> BL+C-LSTM (Chunting:15) <C> 21.4 <C> 61.0 <C> 71.3 <C> 8 <C> 45.6 <C> 18.9 <C> 74.7 <C> 4 <C> 80.7 <R> <C> BL+CNN-RNN (Xingyou:16) <C> 21.7 <C> 61.8 <C> 73.3 <C> 8 <C> 44.5 <C> 19.5 <C> 77.1 <C> 4 <C> 80.9 <R> <C> BL+MVCNN (Wenpeng:16) <C> 21.3 <C> 60.6 <C> 71.9 <C> 8 <C> 44.2 <C> 19.2 <C> 75.8 <C> 4 <C> 78.8 <R> <C> BL+Attentive LSTM (Ming:16) <C> 21.9 <C> 62.4 <C> 74.0 <C> 8 <C> 45.7 <C> 19.1 <C> 71.4 <C> 5 <C> 80.2 <R> <C> BL+fasttext (Armand:17) <C> 21.9 <C> 62.2 <C> 75.4 <C> 7 <C> 44.6 <C> 19.4 <C> 76.1 <C> 4 <C> 80.3 <R> <C> BL+InferSent (Alexis:17) <C> 22.0 <C> 62.5 <C> 75.8 <C> 7 <C> 44.5 <C> 19.4 <C> 76.7 <C> 4 <C> 79.7 <R> <C> BL+USE-T (Daniel:18) <C> 22.0 <C> 62.5 <C> [BOLD] 78.3 <C> 6 <C> 44.7 <C> 19.2 <C> 75.8 <C> 4 <C> 79.5 <R> <C> BL+TWE (Ahmed:18) <C> 22.2 <C> 63.0 <C> 76.3 <C> 7 <C> 44.7 <C> 19.5 <C> 76.7 <C> 4 <C> 80.2 <R> <C> BL+FDCLSTM (ours) <C> 22.3 <C> 63.3 <C> 75.1 <C> 8 <C> 45.0 <C> [BOLD] 20.2 <C> 67.9 <C> 9 <C> 79.8 <R> <C> BL+FDCLSTM [ITALIC] AT (ours) <C> [BOLD] 22.4 <C> 63.7 <C> 75.5 <C> 8 <C> [BOLD] 45.9 <C> 20.1 <C> 67.6 <C> 9 <C> [BOLD] 81.8 <R> <C> BL+FDCLSTM [ITALIC] lexicon (ours) <C> 22.6 <C> 64.3 <C> 76.3 <C> 8 <C> 45.1 <C> 19.4 <C> 76.4 <C> 4 <C> 78.8 <R> <C> BL+FDCLSTM [ITALIC] AT+ [ITALIC] lexicon (ours) <C> 22.6 <C> 64.3 <C> 76.3 <C> 8 <C> 45.1 <C> 19.7 <C> [BOLD] 77.8 <C> 4 <C> 80.4 <CAP> Table 1: Best results after re-ranking using different re-ranker, and different values for k-best hypotheses extracted from the baseline output (%). In addition, to evaluate our re-ranker with MRR we fixed k CNNk=8 LSTMk=4 <COT> Looking at the "BL+FDCLSTM [ITALIC] AT (ours)" row, we can see that this model achieves the highest MRR score for both CNN and LSTM re-rankers.
<R> <C> [BOLD] Model <C> [BOLD] WMT En-De BLEU <C> [BOLD] WMT En-De Speedup <C> [BOLD] WMT De-En BLEU <C> [BOLD] WMT De-En Speedup <C> [BOLD] IWSLT En-De BLEU <C> [BOLD] IWSLT En-De Speedup <C> [BOLD] WMT En-Fr BLEU <C> [BOLD] WMT En-Fr Speedup <R> <C> Baseline ( [ITALIC] b=1) <C> 25.82 <C> 1.15× <C> 29.83 <C> 1.14× <C> 28.66 <C> 1.16× <C> 39.41 <C> 1.18× <R> <C> Baseline ( [ITALIC] b=4) <C> 26.87 <C> 1.00× <C> 30.73 <C> 1.00× <C> 30.00 <C> 1.00× <C> 40.22 <C> 1.00× <R> <C> SAT ( [ITALIC] k=2) <C> 22.81 <C> 2.05× <C> 26.78 <C> 2.04× <C> 25.48 <C> 2.03× <C> 36.62 <C> 2.14× <R> <C> SAT ( [ITALIC] k=4) <C> 16.44 <C> 3.61× <C> 21.27 <C> 3.58× <C> 20.25 <C> 3.45× <C> 28.07 <C> 3.34× <R> <C> SAT ( [ITALIC] k=6) <C> 12.55 <C> 4.86× <C> 15.23 <C> 4.27× <C> 14.02 <C> 4.39× <C> 24.63 <C> 4.77× <R> <C> LT* <C> 19.8 <C> 3.89× <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> SynST( [ITALIC] k=6) <C> 20.74 <C> 4.86× <C> 25.50 <C> 5.06× <C> 23.82 <C> 3.78× <C> 33.47 <C> 5.32× <CAP> Table 1: Controlled experiments comparing SynST to a baseline Transformer, SAT, and LT on four different datasets (two language pairs) demonstrate speed and BLEU improvements. Wall-clock speedup is measured on a single Nvidia TitanX Pascal by computing the average time taken to decode a single sentence in the dev/test set, averaged over five runs. When beam width b is not specified, we perform greedy decoding (i.e., b=1). Note that the LT results are reported by latentTransformer and not from our own implementation; as such, they are not directly comparable to the other results. <COT> Looking at the "Model" column, we can see that the SAT (k=2) model has the highest WMT En-De BLEU score among all the models.
<R> <C> [BOLD] Model <C> [BOLD] WMT En-De BLEU <C> [BOLD] WMT En-De Speedup <C> [BOLD] WMT De-En BLEU <C> [BOLD] WMT De-En Speedup <C> [BOLD] IWSLT En-De BLEU <C> [BOLD] IWSLT En-De Speedup <C> [BOLD] WMT En-Fr BLEU <C> [BOLD] WMT En-Fr Speedup <R> <C> Baseline ( [ITALIC] b=1) <C> 25.82 <C> 1.15× <C> 29.83 <C> 1.14× <C> 28.66 <C> 1.16× <C> 39.41 <C> 1.18× <R> <C> Baseline ( [ITALIC] b=4) <C> 26.87 <C> 1.00× <C> 30.73 <C> 1.00× <C> 30.00 <C> 1.00× <C> 40.22 <C> 1.00× <R> <C> SAT ( [ITALIC] k=2) <C> 22.81 <C> 2.05× <C> 26.78 <C> 2.04× <C> 25.48 <C> 2.03× <C> 36.62 <C> 2.14× <R> <C> SAT ( [ITALIC] k=4) <C> 16.44 <C> 3.61× <C> 21.27 <C> 3.58× <C> 20.25 <C> 3.45× <C> 28.07 <C> 3.34× <R> <C> SAT ( [ITALIC] k=6) <C> 12.55 <C> 4.86× <C> 15.23 <C> 4.27× <C> 14.02 <C> 4.39× <C> 24.63 <C> 4.77× <R> <C> LT* <C> 19.8 <C> 3.89× <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> SynST( [ITALIC] k=6) <C> 20.74 <C> 4.86× <C> 25.50 <C> 5.06× <C> 23.82 <C> 3.78× <C> 33.47 <C> 5.32× <CAP> Table 1: Controlled experiments comparing SynST to a baseline Transformer, SAT, and LT on four different datasets (two language pairs) demonstrate speed and BLEU improvements. Wall-clock speedup is measured on a single Nvidia TitanX Pascal by computing the average time taken to decode a single sentence in the dev/test set, averaged over five runs. When beam width b is not specified, we perform greedy decoding (i.e., b=1). Note that the LT results are reported by latentTransformer and not from our own implementation; as such, they are not directly comparable to the other results. <COT> Looking at the "WMT En-Fr Speedup" column, we can see that the SynST (k=6) model has a higher speedup compared to the baseline (b=4) model.
<R> <C> [BOLD] Model <C> [BOLD] Time (s) <C> [BOLD] Acc <C> [BOLD] # Param <R> <C> +0 dummy node <C> 56 <C> 81.76 <C> 7,216K <R> <C> +1 dummy node <C> 65 <C> 82.64 <C> 8,768K <R> <C> +2 dummy node <C> 76 <C> 82.24 <C> 10,321K <R> <C> Hidden size 100 <C> 42 <C> 81.75 <C> 4,891K <R> <C> Hidden size 200 <C> 54 <C> 82.04 <C> 6,002K <R> <C> Hidden size 300 <C> 65 <C> 82.64 <C> 8,768K <R> <C> Hidden size 600 <C> 175 <C> 81.84 <C> 17,648K <R> <C> Hidden size 900 <C> 235 <C> 81.66 <C> 33,942K <R> <C> Without ⟨s⟩, ⟨/s⟩ <C> 63 <C> 82.36 <C> 8,768K <R> <C> With ⟨s⟩, ⟨/s⟩ <C> 65 <C> 82.64 <C> 8,768K <CAP> Table 2: Movie review Dev results of S-LSTM <COT> Looking at the "Model" column, we can see that the "Hidden size 300" model has the highest accuracy of 82.64%.
<R> <C> [BOLD] Model <C> [BOLD] Time (s) <C> [BOLD] Acc <C> [BOLD] # Param <R> <C> +0 dummy node <C> 56 <C> 81.76 <C> 7,216K <R> <C> +1 dummy node <C> 65 <C> 82.64 <C> 8,768K <R> <C> +2 dummy node <C> 76 <C> 82.24 <C> 10,321K <R> <C> Hidden size 100 <C> 42 <C> 81.75 <C> 4,891K <R> <C> Hidden size 200 <C> 54 <C> 82.04 <C> 6,002K <R> <C> Hidden size 300 <C> 65 <C> 82.64 <C> 8,768K <R> <C> Hidden size 600 <C> 175 <C> 81.84 <C> 17,648K <R> <C> Hidden size 900 <C> 235 <C> 81.66 <C> 33,942K <R> <C> Without ⟨s⟩, ⟨/s⟩ <C> 63 <C> 82.36 <C> 8,768K <R> <C> With ⟨s⟩, ⟨/s⟩ <C> 65 <C> 82.64 <C> 8,768K <CAP> Table 2: Movie review Dev results of S-LSTM <COT> Looking at the "Time (s)" column, we can see that the models with "+0 dummy node" and "Without ⟨s⟩, ⟨/s⟩" have the shortest training time of 56 seconds.
<R> <C> [BOLD] Model <C> [BOLD] Time (s) <C> [BOLD] Acc <C> [BOLD] # Param <R> <C> LSTM <C> 67 <C> 80.72 <C> 5,977K <R> <C> BiLSTM <C> 106 <C> 81.73 <C> 7,059K <R> <C> 2 stacked BiLSTM <C> 207 <C> 81.97 <C> 9,221K <R> <C> 3 stacked BiLSTM <C> 310 <C> 81.53 <C> 11,383K <R> <C> 4 stacked BiLSTM <C> 411 <C> 81.37 <C> 13,546K <R> <C> S-LSTM <C> 65 <C> 82.64* <C> 8,768K <R> <C> CNN <C> 34 <C> 80.35 <C> 5,637K <R> <C> 2 stacked CNN <C> 40 <C> 80.97 <C> 5,717K <R> <C> 3 stacked CNN <C> 47 <C> 81.46 <C> 5,808K <R> <C> 4 stacked CNN <C> 51 <C> 81.39 <C> 5,855K <R> <C> Transformer (N=6) <C> 138 <C> 81.03 <C> 7,234K <R> <C> Transformer (N=8) <C> 174 <C> 81.86 <C> 7,615K <R> <C> Transformer (N=10) <C> 214 <C> 81.63 <C> 8,004K <R> <C> BiLSTM+Attention <C> 126 <C> 82.37 <C> 7,419K <R> <C> S-LSTM+Attention <C> 87 <C> 83.07* <C> 8,858K <CAP> Table 3: Movie review development results <COT> Looking at the "Model" column, we can see that S-LSTM+Attention has the highest accuracy among all the models.
<R> <C> [BOLD] Model <C> [BOLD] Time (s) <C> [BOLD] Acc <C> [BOLD] # Param <R> <C> LSTM <C> 67 <C> 80.72 <C> 5,977K <R> <C> BiLSTM <C> 106 <C> 81.73 <C> 7,059K <R> <C> 2 stacked BiLSTM <C> 207 <C> 81.97 <C> 9,221K <R> <C> 3 stacked BiLSTM <C> 310 <C> 81.53 <C> 11,383K <R> <C> 4 stacked BiLSTM <C> 411 <C> 81.37 <C> 13,546K <R> <C> S-LSTM <C> 65 <C> 82.64* <C> 8,768K <R> <C> CNN <C> 34 <C> 80.35 <C> 5,637K <R> <C> 2 stacked CNN <C> 40 <C> 80.97 <C> 5,717K <R> <C> 3 stacked CNN <C> 47 <C> 81.46 <C> 5,808K <R> <C> 4 stacked CNN <C> 51 <C> 81.39 <C> 5,855K <R> <C> Transformer (N=6) <C> 138 <C> 81.03 <C> 7,234K <R> <C> Transformer (N=8) <C> 174 <C> 81.86 <C> 7,615K <R> <C> Transformer (N=10) <C> 214 <C> 81.63 <C> 8,004K <R> <C> BiLSTM+Attention <C> 126 <C> 82.37 <C> 7,419K <R> <C> S-LSTM+Attention <C> 87 <C> 83.07* <C> 8,858K <CAP> Table 3: Movie review development results <COT> Looking at the "Time (s)" column, we can see that LSTM has the lowest training time among all the models.
<R> <C> [BOLD] Model <C> [BOLD] Accuracy <C> [BOLD] Train (s) <C> [BOLD] Test (s) <R> <C> socher2011semi <C> 77.70 <C> – <C> – <R> <C> socher2012semantic <C> 79.00 <C> – <C> – <R> <C> kim2014convolutional <C> 81.50 <C> – <C> – <R> <C> qian2016linguistically <C> 81.50 <C> – <C> – <R> <C> BiLSTM <C> 81.61 <C> 51 <C> 1.62 <R> <C> 2 stacked BiLSTM <C> 81.94 <C> 98 <C> 3.18 <R> <C> 3 stacked BiLSTM <C> 81.71 <C> 137 <C> 4.67 <R> <C> 3 stacked CNN <C> 81.59 <C> 31 <C> 1.04 <R> <C> Transformer (N=8) <C> 81.97 <C> 89 <C> 2.75 <R> <C> S-LSTM <C> [BOLD] 82.45* <C> 41 <C> 1.53 <CAP> Table 4: Test set results on movie review dataset (* denotes significance in all tables). <COT> Looking at the "Model" column, we can see that the model "S-LSTM" has the highest accuracy of 82.45.
<R> <C> [BOLD] Model <C> [BOLD] Accuracy <C> [BOLD] Train (s) <C> [BOLD] Test (s) <R> <C> socher2011semi <C> 77.70 <C> – <C> – <R> <C> socher2012semantic <C> 79.00 <C> – <C> – <R> <C> kim2014convolutional <C> 81.50 <C> – <C> – <R> <C> qian2016linguistically <C> 81.50 <C> – <C> – <R> <C> BiLSTM <C> 81.61 <C> 51 <C> 1.62 <R> <C> 2 stacked BiLSTM <C> 81.94 <C> 98 <C> 3.18 <R> <C> 3 stacked BiLSTM <C> 81.71 <C> 137 <C> 4.67 <R> <C> 3 stacked CNN <C> 81.59 <C> 31 <C> 1.04 <R> <C> Transformer (N=8) <C> 81.97 <C> 89 <C> 2.75 <R> <C> S-LSTM <C> [BOLD] 82.45* <C> 41 <C> 1.53 <CAP> Table 4: Test set results on movie review dataset (* denotes significance in all tables). <COT> Looking at the "Train (s)" column, we can see that the model "3 stacked BiLSTM" has the longest training time of 137 seconds.
<R> <C> [BOLD] Dataset Camera <C> [BOLD] SLSTM  [BOLD] 90.02* <C> [BOLD] Time (s) 50 (2.85) <C> [BOLD] BiLSTM 87.05 <C> [BOLD] Time (s) 115 (8.37) <C> [BOLD] 2 BiLSTM 88.07 <C> [BOLD] Time (s) 221 (16.1) <R> <C> Video <C> [BOLD] 86.75* <C> 55 (3.95) <C> 84.73 <C> 140 (12.59) <C> 85.23 <C> 268 (25.86) <R> <C> Health <C> [BOLD] 86.5 <C> 37 (2.17) <C> 85.52 <C> 118 (6.38) <C> 85.89 <C> 227 (11.16) <R> <C> Music <C> [BOLD] 82.04* <C> 52 (3.44) <C> 78.74 <C> 185 (12.27) <C> 80.45 <C> 268 (23.46) <R> <C> Kitchen <C> [BOLD] 84.54* <C> 40 (2.50) <C> 82.22 <C> 118 (10.18) <C> 83.77 <C> 225 (19.77) <R> <C> DVD <C> [BOLD] 85.52* <C> 63 (5.29) <C> 83.71 <C> 166 (15.42) <C> 84.77 <C> 217 (28.31) <R> <C> Toys <C> 85.25 <C> 39 (2.42) <C> 85.72 <C> 119 (7.58) <C> [BOLD] 85.82 <C> 231 (14.83) <R> <C> Baby <C> [BOLD] 86.25* <C> 40 (2.63) <C> 84.51 <C> 125 (8.50) <C> 85.45 <C> 238 (17.73) <R> <C> Books <C> [BOLD] 83.44* <C> 64 (3.64) <C> 82.12 <C> 240 (13.59) <C> 82.77 <C> 458 (28.82) <R> <C> IMDB <C> [BOLD] 87.15* <C> 67 (3.69) <C> 86.02 <C> 248 (13.33) <C> 86.55 <C> 486 (26.22) <R> <C> MR <C> [BOLD] 76.2 <C> 27 (1.25) <C> 75.73 <C> 39 (2.27) <C> 75.98 <C> 72 (4.63) <R> <C> Appeal <C> 85.75 <C> 35 (2.83) <C> 86.05 <C> 119 (11.98) <C> [BOLD] 86.35* <C> 229 (22.76) <R> <C> Magazines <C> [BOLD] 93.75* <C> 51 (2.93) <C> 92.52 <C> 214 (11.06) <C> 92.89 <C> 417 (22.77) <R> <C> Electronics <C> [BOLD] 83.25* <C> 47 (2.55) <C> 82.51 <C> 195 (10.14) <C> 82.33 <C> 356 (19.77) <R> <C> Sports <C> [BOLD] 85.75* <C> 44 (2.64) <C> 84.04 <C> 172 (8.64) <C> 84.78 <C> 328 (16.34) <R> <C> Software <C> [BOLD] 87.75* <C> 54 (2.98) <C> 86.73 <C> 245 (12.38) <C> 86.97 <C> 459 (24.68) <R> <C> [BOLD] Average <C> [BOLD] 85.38* <C> 47.30 (2.98) <C> 84.01 <C> 153.48 (10.29) <C> 84.64 <C> 282.24 (20.2) <CAP> Table 5: Results on the 16 datasets of Liu et al. (2017). Time format: train (test) <COT> Looking at the "Average" row, the "Camera" column, and the "SLSTM" cell, we can see that SLSTM achieves a performance of 90.02 on the Camera dataset.
<R> <C> [BOLD] Dataset Camera <C> [BOLD] SLSTM  [BOLD] 90.02* <C> [BOLD] Time (s) 50 (2.85) <C> [BOLD] BiLSTM 87.05 <C> [BOLD] Time (s) 115 (8.37) <C> [BOLD] 2 BiLSTM 88.07 <C> [BOLD] Time (s) 221 (16.1) <R> <C> Video <C> [BOLD] 86.75* <C> 55 (3.95) <C> 84.73 <C> 140 (12.59) <C> 85.23 <C> 268 (25.86) <R> <C> Health <C> [BOLD] 86.5 <C> 37 (2.17) <C> 85.52 <C> 118 (6.38) <C> 85.89 <C> 227 (11.16) <R> <C> Music <C> [BOLD] 82.04* <C> 52 (3.44) <C> 78.74 <C> 185 (12.27) <C> 80.45 <C> 268 (23.46) <R> <C> Kitchen <C> [BOLD] 84.54* <C> 40 (2.50) <C> 82.22 <C> 118 (10.18) <C> 83.77 <C> 225 (19.77) <R> <C> DVD <C> [BOLD] 85.52* <C> 63 (5.29) <C> 83.71 <C> 166 (15.42) <C> 84.77 <C> 217 (28.31) <R> <C> Toys <C> 85.25 <C> 39 (2.42) <C> 85.72 <C> 119 (7.58) <C> [BOLD] 85.82 <C> 231 (14.83) <R> <C> Baby <C> [BOLD] 86.25* <C> 40 (2.63) <C> 84.51 <C> 125 (8.50) <C> 85.45 <C> 238 (17.73) <R> <C> Books <C> [BOLD] 83.44* <C> 64 (3.64) <C> 82.12 <C> 240 (13.59) <C> 82.77 <C> 458 (28.82) <R> <C> IMDB <C> [BOLD] 87.15* <C> 67 (3.69) <C> 86.02 <C> 248 (13.33) <C> 86.55 <C> 486 (26.22) <R> <C> MR <C> [BOLD] 76.2 <C> 27 (1.25) <C> 75.73 <C> 39 (2.27) <C> 75.98 <C> 72 (4.63) <R> <C> Appeal <C> 85.75 <C> 35 (2.83) <C> 86.05 <C> 119 (11.98) <C> [BOLD] 86.35* <C> 229 (22.76) <R> <C> Magazines <C> [BOLD] 93.75* <C> 51 (2.93) <C> 92.52 <C> 214 (11.06) <C> 92.89 <C> 417 (22.77) <R> <C> Electronics <C> [BOLD] 83.25* <C> 47 (2.55) <C> 82.51 <C> 195 (10.14) <C> 82.33 <C> 356 (19.77) <R> <C> Sports <C> [BOLD] 85.75* <C> 44 (2.64) <C> 84.04 <C> 172 (8.64) <C> 84.78 <C> 328 (16.34) <R> <C> Software <C> [BOLD] 87.75* <C> 54 (2.98) <C> 86.73 <C> 245 (12.38) <C> 86.97 <C> 459 (24.68) <R> <C> [BOLD] Average <C> [BOLD] 85.38* <C> 47.30 (2.98) <C> 84.01 <C> 153.48 (10.29) <C> 84.64 <C> 282.24 (20.2) <CAP> Table 5: Results on the 16 datasets of Liu et al. (2017). Time format: train (test) <COT> Looking at the "Magazines" row, the "BiLSTM" column, and the "Time (s)" cell, we can see that the BiLSTM model takes 214 seconds to train and 11.06 seconds to test on the Magazines dataset.
<R> <C> [BOLD] Model <C> [BOLD] Accuracy <C> [BOLD] Train (s) <C> [BOLD] Test (s) <R> <C> manning2011part <C> 97.28 <C> – <C> – <R> <C> collobert2011natural <C> 97.29 <C> – <C> – <R> <C> sun2014structure <C> 97.36 <C> – <C> – <R> <C> sogaard2011semisupervised <C> 97.50 <C> – <C> – <R> <C> huang2015bidirectional <C> [BOLD] 97.55 <C> – <C> – <R> <C> ma2016end <C> [BOLD] 97.55 <C> – <C> – <R> <C> yang2017transfer <C> [BOLD] 97.55 <C> – <C> – <R> <C> BiLSTM <C> 97.35 <C> 254 <C> 22.50 <R> <C> 2 stacked BiLSTM <C> 97.41 <C> 501 <C> 43.99 <R> <C> 3 stacked BiLSTM <C> 97.40 <C> 746 <C> 64.96 <R> <C> S-LSTM <C> [BOLD] 97.55 <C> 237 <C> 22.16 <CAP> Table 6: Results on PTB (POS tagging) <COT> Looking at the "Model" column, we can see that the models "huang2015bidirectional", "ma2016end", and "yang2017transfer" all have an accuracy of 97.55.
<R> <C> [BOLD] Model <C> [BOLD] Accuracy <C> [BOLD] Train (s) <C> [BOLD] Test (s) <R> <C> manning2011part <C> 97.28 <C> – <C> – <R> <C> collobert2011natural <C> 97.29 <C> – <C> – <R> <C> sun2014structure <C> 97.36 <C> – <C> – <R> <C> sogaard2011semisupervised <C> 97.50 <C> – <C> – <R> <C> huang2015bidirectional <C> [BOLD] 97.55 <C> – <C> – <R> <C> ma2016end <C> [BOLD] 97.55 <C> – <C> – <R> <C> yang2017transfer <C> [BOLD] 97.55 <C> – <C> – <R> <C> BiLSTM <C> 97.35 <C> 254 <C> 22.50 <R> <C> 2 stacked BiLSTM <C> 97.41 <C> 501 <C> 43.99 <R> <C> 3 stacked BiLSTM <C> 97.40 <C> 746 <C> 64.96 <R> <C> S-LSTM <C> [BOLD] 97.55 <C> 237 <C> 22.16 <CAP> Table 6: Results on PTB (POS tagging) <COT> Looking at the "Train (s)" column, we can see that the model "3 stacked BiLSTM" has a training time of 746 seconds.
<R> <C> [BOLD] Model <C> [BOLD] F1 <C> [BOLD] Train (s) <C> [BOLD] Test (s) <R> <C> collobert2011natural <C> 89.59 <C> – <C> – <R> <C> passos2014lexicon <C> 90.90 <C> – <C> – <R> <C> luo2015joint <C> 91.20 <C> – <C> – <R> <C> huang2015bidirectional <C> 90.10 <C> – <C> – <R> <C> lample2016neural <C> 90.94 <C> – <C> – <R> <C> ma2016end <C> 91.21 <C> – <C> – <R> <C> yang2017transfer <C> 91.26 <C> – <C> – <R> <C> rei:2017:Long <C> 86.26 <C> – <C> – <R> <C> peters2017semi <C> [BOLD] 91.93 <C> – <C> – <R> <C> BiLSTM <C> 90.96 <C> 82 <C> 9.89 <R> <C> 2 stacked BiLSTM <C> 91.02 <C> 159 <C> 18.88 <R> <C> 3 stacked BiLSTM <C> 91.06 <C> 235 <C> 30.97 <R> <C> S-LSTM <C> [BOLD] 91.57* <C> 79 <C> 9.78 <CAP> Table 7: Results on CoNLL03 (NER) <COT> Looking at the "Model" column, we can see that the "S-LSTM" model has the highest F1 score of [BOLD] 91.57*.
<R> <C> [BOLD] Model <C> [BOLD] F1 <C> [BOLD] Train (s) <C> [BOLD] Test (s) <R> <C> collobert2011natural <C> 89.59 <C> – <C> – <R> <C> passos2014lexicon <C> 90.90 <C> – <C> – <R> <C> luo2015joint <C> 91.20 <C> – <C> – <R> <C> huang2015bidirectional <C> 90.10 <C> – <C> – <R> <C> lample2016neural <C> 90.94 <C> – <C> – <R> <C> ma2016end <C> 91.21 <C> – <C> – <R> <C> yang2017transfer <C> 91.26 <C> – <C> – <R> <C> rei:2017:Long <C> 86.26 <C> – <C> – <R> <C> peters2017semi <C> [BOLD] 91.93 <C> – <C> – <R> <C> BiLSTM <C> 90.96 <C> 82 <C> 9.89 <R> <C> 2 stacked BiLSTM <C> 91.02 <C> 159 <C> 18.88 <R> <C> 3 stacked BiLSTM <C> 91.06 <C> 235 <C> 30.97 <R> <C> S-LSTM <C> [BOLD] 91.57* <C> 79 <C> 9.78 <CAP> Table 7: Results on CoNLL03 (NER) <COT> Looking at the "Train (s)" column, we can see that the "3 stacked BiLSTM" model takes the longest training time of 235 seconds.
<R> <C> system <C> BLEU <C> ROUGE-L <R> <C> challenge <C> challenge <C> challenge <R> <C> baseline <C> 65.9 <C> 68.5 <R> <C> Thomson Reuters (np 3) <C> [BOLD] 68.1 <C> 69.3 <R> <C> Thomson Reuters (np 4) <C> 67.4 <C> 69.8 <R> <C> HarvardNLP & H. Elder <C> 67.4 <C> [BOLD] 70.8 <R> <C> own <C> own <C> own <R> <C> word <C> 67.8±0.8 <C> 70.4±0.6 <R> <C> character <C> 64.6±6.0 <C> 67.9±4.7 <R> <C> word (best on dev.) <C> 67.8 <C> 70.2 <R> <C> char. (best on dev.) <C> 67.6 <C> 70.4 <CAP> Table 2: E2E test set results. Own results correspond to avg±SD of ten runs and single result of best models on the development set. <COT> Looking at the "system" column, we can see that the baseline system has a BLEU score of 65.9 and a ROUGE-L score of 68.5.
<R> <C> system <C> BLEU <C> ROUGE-L <R> <C> challenge <C> challenge <C> challenge <R> <C> baseline <C> 65.9 <C> 68.5 <R> <C> Thomson Reuters (np 3) <C> [BOLD] 68.1 <C> 69.3 <R> <C> Thomson Reuters (np 4) <C> 67.4 <C> 69.8 <R> <C> HarvardNLP & H. Elder <C> 67.4 <C> [BOLD] 70.8 <R> <C> own <C> own <C> own <R> <C> word <C> 67.8±0.8 <C> 70.4±0.6 <R> <C> character <C> 64.6±6.0 <C> 67.9±4.7 <R> <C> word (best on dev.) <C> 67.8 <C> 70.2 <R> <C> char. (best on dev.) <C> 67.6 <C> 70.4 <CAP> Table 2: E2E test set results. Own results correspond to avg±SD of ten runs and single result of best models on the development set. <COT> Looking at the "Thomson Reuters (np 3)" row, we can see that it has a BLEU score of 68.1.
<R> <C> system <C> BLEU <C> ROUGE-L <R> <C> challenge <C> challenge <C> challenge <R> <C> baseline <C> 32.1 <C> 43.3 <R> <C> Melbourne <C> 43.4 <C> [BOLD] 61.0 <R> <C> Tilburg-SMT <C> 43.1 <C> 58.0 <R> <C> UPF-FORGe <C> 37.5 <C> 58.8 <R> <C> own <C> own <C> own <R> <C> word (best on dev.) <C> [BOLD] 44.2 <C> 60.9 <R> <C> char. (best on dev.) <C> 41.3 <C> 58.4 <R> <C> word <C> 37.0±3.8 <C> 56.3±2.6 <R> <C> character <C> 39.7±1.7 <C> 58.4±0.7 <CAP> Table 3: WebNLG test set results. Own results correspond to single best model on development set and avg±SD of ten runs. <COT> Looking at the "challenge" column, we can see that the baseline system has the lowest scores for both BLEU and ROUGE-L.
<R> <C> system <C> BLEU <C> ROUGE-L <R> <C> challenge <C> challenge <C> challenge <R> <C> baseline <C> 32.1 <C> 43.3 <R> <C> Melbourne <C> 43.4 <C> [BOLD] 61.0 <R> <C> Tilburg-SMT <C> 43.1 <C> 58.0 <R> <C> UPF-FORGe <C> 37.5 <C> 58.8 <R> <C> own <C> own <C> own <R> <C> word (best on dev.) <C> [BOLD] 44.2 <C> 60.9 <R> <C> char. (best on dev.) <C> 41.3 <C> 58.4 <R> <C> word <C> 37.0±3.8 <C> 56.3±2.6 <R> <C> character <C> 39.7±1.7 <C> 58.4±0.7 <CAP> Table 3: WebNLG test set results. Own results correspond to single best model on development set and avg±SD of ten runs. <COT> Looking at the "word (best on dev.)" row, we can see that the system with the highest BLEU score also has the highest ROUGE-L score.
<R> <C> # Layers <C> Max Chunk Size <C> Speedup <C> BLEU <R> <C> 1 <C> [ITALIC] k=6 <C> 3.8× <C> 23.82 <R> <C> 2 <C> [ITALIC] k=6 <C> 2.8× <C> 23.98 <R> <C> 3 <C> [ITALIC] k=6 <C> 2.2× <C> 24.54 <R> <C> 4 <C> [ITALIC] k=6 <C> 1.8× <C> 24.04 <R> <C> 5 <C> [ITALIC] k=6 <C> 1.4× <C> 24.34 <R> <C> 1 <C> [ITALIC] k∈{1…6} <C> 3.1× <C> 25.31 <CAP> Table 4: Increasing the number of layers in SynST’s parse decoder significantly lowers the speedup while marginally impacting BLEU. Randomly sampling k from {1…6} during training boosts BLEU significantly with minimal impact on speedup. <COT> Looking at the "Speedup" column, we can observe that as the number of layers increases, the speedup decreases.
<R> <C> # Layers <C> Max Chunk Size <C> Speedup <C> BLEU <R> <C> 1 <C> [ITALIC] k=6 <C> 3.8× <C> 23.82 <R> <C> 2 <C> [ITALIC] k=6 <C> 2.8× <C> 23.98 <R> <C> 3 <C> [ITALIC] k=6 <C> 2.2× <C> 24.54 <R> <C> 4 <C> [ITALIC] k=6 <C> 1.8× <C> 24.04 <R> <C> 5 <C> [ITALIC] k=6 <C> 1.4× <C> 24.34 <R> <C> 1 <C> [ITALIC] k∈{1…6} <C> 3.1× <C> 25.31 <CAP> Table 4: Increasing the number of layers in SynST’s parse decoder significantly lowers the speedup while marginally impacting BLEU. Randomly sampling k from {1…6} during training boosts BLEU significantly with minimal impact on speedup. <COT> Looking at the "BLEU" column, we can observe that as the number of layers increases, the BLEU score remains relatively stable.
<R> <C> metric <C> human <C> word <C> char. <R> <C> E2E <C> E2E <C> E2E <C> E2E <R> <C> BLEU <C> 55.5±0.7 <C> 68.2±1.4 <C> 65.8±2.6 <R> <C> ROUGE-L <C> 62.0±0.4 <C> 72.1±0.7 <C> 69.8±2.6 <R> <C> WebNLG <C> WebNLG <C> WebNLG <C> WebNLG <R> <C> BLEU <C> 48.3±0.7 <C> 40.6±4.2 <C> 43.7±2.4 <R> <C> ROUGE-L <C> 62.4±0.3 <C> 58.5±3.0 <C> 63.1±0.8 <CAP> Table 4: E2E and WebNLG development set results in the format avg±SD. Human results are averaged over using each human reference as prediction once. <COT> Looking at the "BLEU" column, we can see that for both E2E and WebNLG, the BLEU scores are higher for human references compared to word and character references.
<R> <C> metric <C> human <C> word <C> char. <R> <C> E2E <C> E2E <C> E2E <C> E2E <R> <C> BLEU <C> 55.5±0.7 <C> 68.2±1.4 <C> 65.8±2.6 <R> <C> ROUGE-L <C> 62.0±0.4 <C> 72.1±0.7 <C> 69.8±2.6 <R> <C> WebNLG <C> WebNLG <C> WebNLG <C> WebNLG <R> <C> BLEU <C> 48.3±0.7 <C> 40.6±4.2 <C> 43.7±2.4 <R> <C> ROUGE-L <C> 62.4±0.3 <C> 58.5±3.0 <C> 63.1±0.8 <CAP> Table 4: E2E and WebNLG development set results in the format avg±SD. Human results are averaged over using each human reference as prediction once. <COT> Looking at the "ROUGE-L" column, we can see that for E2E and WebNLG, the ROUGE-L scores are higher for human references compared to word and character references.
<R> <C> [EMPTY] <C> E2E word <C> E2E char. <C> WebNLG word <C> WebNLG char. <R> <C> content errors <C> content errors <C> content errors <C> content errors <C> content errors <R> <C> info. dropped <C> 40.0 <C> 30.0 <C> 42.9 <C> 66.7 <R> <C> info. added <C> 0.0 <C> 0.0 <C> 6.7 <C> 1.9 <R> <C> info. modified <C> 4.4 <C> 0.0 <C> 19.0 <C> 1.9 <R> <C> info. repeated <C> 0.0 <C> 0.0 <C> 15.2 <C> 28.6 <R> <C> linguistic errors <C> linguistic errors <C> linguistic errors <C> linguistic errors <C> linguistic errors <R> <C> punctuation errors <C> 5.6 <C> 5.6 <C> 8.6 <C> 3.8 <R> <C> grammatical errors <C> 13.3 <C> 14.4 <C> 15.2 <C> 12.4 <R> <C> spelling mistakes <C> 0.0 <C> 0.0 <C> 9.5 <C> 5.7 <R> <C> overall correctness <C> overall correctness <C> overall correctness <C> overall correctness <C> overall correctness <R> <C> content correct <C> 55.6 <C> 70.0 <C> 46.7 <C> 31.4 <R> <C> language correct <C> 83.3 <C> 81.1 <C> 69.5 <C> 79.0 <R> <C> all correct <C> 48.9 <C> 61.1 <C> 33.3 <C> 26.7 <CAP> Table 5: Percentage of affected instances in manual error analysis of 15 randomly selected development set instances for each input length. <COT> Looking at the "info. modified" row and the "WebNLG word" column, we can see that the percentage of instances with modified information is higher for WebNLG word compared to E2E word.
<R> <C> [EMPTY] <C> E2E word <C> E2E char. <C> WebNLG word <C> WebNLG char. <R> <C> content errors <C> content errors <C> content errors <C> content errors <C> content errors <R> <C> info. dropped <C> 40.0 <C> 30.0 <C> 42.9 <C> 66.7 <R> <C> info. added <C> 0.0 <C> 0.0 <C> 6.7 <C> 1.9 <R> <C> info. modified <C> 4.4 <C> 0.0 <C> 19.0 <C> 1.9 <R> <C> info. repeated <C> 0.0 <C> 0.0 <C> 15.2 <C> 28.6 <R> <C> linguistic errors <C> linguistic errors <C> linguistic errors <C> linguistic errors <C> linguistic errors <R> <C> punctuation errors <C> 5.6 <C> 5.6 <C> 8.6 <C> 3.8 <R> <C> grammatical errors <C> 13.3 <C> 14.4 <C> 15.2 <C> 12.4 <R> <C> spelling mistakes <C> 0.0 <C> 0.0 <C> 9.5 <C> 5.7 <R> <C> overall correctness <C> overall correctness <C> overall correctness <C> overall correctness <C> overall correctness <R> <C> content correct <C> 55.6 <C> 70.0 <C> 46.7 <C> 31.4 <R> <C> language correct <C> 83.3 <C> 81.1 <C> 69.5 <C> 79.0 <R> <C> all correct <C> 48.9 <C> 61.1 <C> 33.3 <C> 26.7 <CAP> Table 5: Percentage of affected instances in manual error analysis of 15 randomly selected development set instances for each input length. <COT> Looking at the "overall correctness" row and the "content correct" column, we can see that the percentage of instances with correct content is higher compared to overall correctness for all datasets.
<R> <C> [EMPTY] <C> E2E human <C> E2E word <C> E2E character <C> WebNLG human <C> WebNLG word <C> WebNLG character <R> <C> unique sents. <C> 866.3±16.5 <C> 203.5±30.6 <C> 366.8±60.0 <C> 1,185.0±12.6 <C> 603.7±144.3 <C> 875.4±30.2 <R> <C> unique words <C> 419.7±16.7 <C> 64.4±2.3 <C> 73.1±7.2 <C> 1447.3±7.4 <C> 620.3±35.5 <C> 881.5±26.0 <R> <C> word E <C> 6.5±0.0 <C> 5.1±0.0 <C> 5.5±0.0 <C> 7.1±0.0 <C> 6.3±0.0 <C> 6.6±0.0 <R> <C> 1-3-grams E <C> 10.4±0.0 <C> 7.7±0.1 <C> 8.2±0.1 <C> 11.6±0.0 <C> 10.1±0.1 <C> 10.5±0.1 <R> <C> % new texts <C> 99.7±0.2 <C> 98.2±0.3 <C> 98.8±0.2 <C> 91.1±0.3 <C> 69.8±4.8 <C> 87.5±0.6 <R> <C> % new sents. <C> 85.1±1.1 <C> 61.8±6.4 <C> 71.4±4.7 <C> 87.4±0.4 <C> 57.2±5.8 <C> 82.1±1.2 <CAP> Table 6: Linguistic diversity of development set references and generated texts as avg±SD. ‘% new’ denotes the share of generated texts or sentences that do not appear in training references. Higher indicates more diversity for all measures. <COT> Looking at the "WebNLG human" cell and "WebNLG word" cell, we can see that the average number of unique words in WebNLG human-generated texts is higher than in WebNLG word-generated texts.
<R> <C> [EMPTY] <C> E2E human <C> E2E word <C> E2E character <C> WebNLG human <C> WebNLG word <C> WebNLG character <R> <C> unique sents. <C> 866.3±16.5 <C> 203.5±30.6 <C> 366.8±60.0 <C> 1,185.0±12.6 <C> 603.7±144.3 <C> 875.4±30.2 <R> <C> unique words <C> 419.7±16.7 <C> 64.4±2.3 <C> 73.1±7.2 <C> 1447.3±7.4 <C> 620.3±35.5 <C> 881.5±26.0 <R> <C> word E <C> 6.5±0.0 <C> 5.1±0.0 <C> 5.5±0.0 <C> 7.1±0.0 <C> 6.3±0.0 <C> 6.6±0.0 <R> <C> 1-3-grams E <C> 10.4±0.0 <C> 7.7±0.1 <C> 8.2±0.1 <C> 11.6±0.0 <C> 10.1±0.1 <C> 10.5±0.1 <R> <C> % new texts <C> 99.7±0.2 <C> 98.2±0.3 <C> 98.8±0.2 <C> 91.1±0.3 <C> 69.8±4.8 <C> 87.5±0.6 <R> <C> % new sents. <C> 85.1±1.1 <C> 61.8±6.4 <C> 71.4±4.7 <C> 87.4±0.4 <C> 57.2±5.8 <C> 82.1±1.2 <CAP> Table 6: Linguistic diversity of development set references and generated texts as avg±SD. ‘% new’ denotes the share of generated texts or sentences that do not appear in training references. Higher indicates more diversity for all measures. <COT> Looking at the "% new sents." cell, we can see that the percentage of new sentences in E2E human-generated texts is higher than in WebNLG human-generated texts.
<R> <C> [EMPTY] <C> c@1 <C> c@2 <C> c@5 <C> c@30 <R> <C> template 1 <C> 0.8 <C> 0.8 <C> 0.9 <C> 1.7 <R> <C> template 2 <C> 1.0 <C> 1.2 <C> 1.3 <C> 1.9 <R> <C> template 1+2 <C> 0.9 <C> 1.6 <C> 2.2 <C> 3.3 <R> <C> + reranker <C> 0.9 <C> 1.9 <C> 2.7 <C> 3.3 <CAP> Table 7: Manual evaluation of generated texts for 10 random test instances of a word-based model trained with synthetic training data from two templates. c@n: avg. number of correct texts (with respect to content and language) among the top n hypotheses. <COT> Looking at the table, we can see that the average number of correct texts (c@1, c@2, c@5, c@30) increases when the model is trained with more templates.
<R> <C> [EMPTY] <C> c@1 <C> c@2 <C> c@5 <C> c@30 <R> <C> template 1 <C> 0.8 <C> 0.8 <C> 0.9 <C> 1.7 <R> <C> template 2 <C> 1.0 <C> 1.2 <C> 1.3 <C> 1.9 <R> <C> template 1+2 <C> 0.9 <C> 1.6 <C> 2.2 <C> 3.3 <R> <C> + reranker <C> 0.9 <C> 1.9 <C> 2.7 <C> 3.3 <CAP> Table 7: Manual evaluation of generated texts for 10 random test instances of a word-based model trained with synthetic training data from two templates. c@n: avg. number of correct texts (with respect to content and language) among the top n hypotheses. <COT> Looking at the table, we can see that the average number of correct texts (c@1, c@2, c@5, c@30) increases when a reranker is applied to the model.
<R> <C> Model <C> R1 <C> R2 <C> RL <R> <C> Lead-75C <C> 23.69 <C> 7.93 <C> 21.5 <R> <C> Lead-8 <C> 21.30 <C> 7.34 <C> 19.94 <R> <C> Schumann ( 2018 ) <C> 22.19 <C> 4.56 <C> 19.88 <R> <C> Wang and Lee ( 2018 ) <C> 27.09 <C> 9.86 <C> 24.97 <R> <C> Contextual Match <C> 26.48 <C> 10.05 <C> 24.41 <R> <C> Cao et al. ( 2018 ) <C> 37.04 <C> 19.03 <C> 34.46 <R> <C> seq2seq <C> 33.50 <C> 15.85 <C> 31.44 <R> <C> Contextual Oracle <C> 37.03 <C> 15.46 <C> 33.23 <CAP> Table 1: Experimental results of abstractive summarization on Gigaword test set with ROUGE metric. The top section is prefix baselines, the second section is recent unsupervised methods and ours, the third section is state-of-the-art supervised method along with our implementation of a seq-to-seq model with attention, and the bottom section is our model’s oracle performance. Wang and Lee (2018) is by author correspondence (scores differ because of evaluation setup). For another unsupervised work Fevry and Phang (2018), we attempted to replicate on our test set, but were unable to obtain results better than the baselines. <COT> Looking at the "Model" column, "Contextual Oracle" cell, and "R2" cell, we can see that the "Contextual Oracle" model has an R2 score of 15.46.
<R> <C> Model <C> R1 <C> R2 <C> RL <R> <C> Lead-75C <C> 23.69 <C> 7.93 <C> 21.5 <R> <C> Lead-8 <C> 21.30 <C> 7.34 <C> 19.94 <R> <C> Schumann ( 2018 ) <C> 22.19 <C> 4.56 <C> 19.88 <R> <C> Wang and Lee ( 2018 ) <C> 27.09 <C> 9.86 <C> 24.97 <R> <C> Contextual Match <C> 26.48 <C> 10.05 <C> 24.41 <R> <C> Cao et al. ( 2018 ) <C> 37.04 <C> 19.03 <C> 34.46 <R> <C> seq2seq <C> 33.50 <C> 15.85 <C> 31.44 <R> <C> Contextual Oracle <C> 37.03 <C> 15.46 <C> 33.23 <CAP> Table 1: Experimental results of abstractive summarization on Gigaword test set with ROUGE metric. The top section is prefix baselines, the second section is recent unsupervised methods and ours, the third section is state-of-the-art supervised method along with our implementation of a seq-to-seq model with attention, and the bottom section is our model’s oracle performance. Wang and Lee (2018) is by author correspondence (scores differ because of evaluation setup). For another unsupervised work Fevry and Phang (2018), we attempted to replicate on our test set, but were unable to obtain results better than the baselines. <COT> Looking at the "Model" column, "seq2seq" cell, and "RL" cell, we can see that the "seq2seq" model has an RL score of 31.44.
<R> <C> Model <C> F1 <C> CR <R> <C> F&A Unsupervised <C> 52.3 <C> - <R> <C> Contextual Match <C> 60.90 <C> 0.38 <R> <C> Filippova et al. ( 2015 ) <C> 82.0 <C> 0.38 <R> <C> Zhao et al. ( 2018 ) <C> 85.1 <C> 0.39 <R> <C> Contextual Oracle <C> 82.1 <C> 0.39 <CAP> Table 2: Experimental results of extractive summarization on Google data set. F1 is the token overlapping score, and CR is the compression rate. F&A is an unsupervised baseline used in Filippova and Altun (2013), and the middle section is supervised results. <COT> Looking at the "Model" column, we can see that there are four different models mentioned: F&A Unsupervised, Contextual Match, Filippova et al. (2015), and Zhao et al. (2018).
<R> <C> Model <C> F1 <C> CR <R> <C> F&A Unsupervised <C> 52.3 <C> - <R> <C> Contextual Match <C> 60.90 <C> 0.38 <R> <C> Filippova et al. ( 2015 ) <C> 82.0 <C> 0.38 <R> <C> Zhao et al. ( 2018 ) <C> 85.1 <C> 0.39 <R> <C> Contextual Oracle <C> 82.1 <C> 0.39 <CAP> Table 2: Experimental results of extractive summarization on Google data set. F1 is the token overlapping score, and CR is the compression rate. F&A is an unsupervised baseline used in Filippova and Altun (2013), and the middle section is supervised results. <COT> Looking at the "CR" column, we can see that the compression rate for all models is either 0.38 or 0.39.
<R> <C> Models <C> abstractive R1 <C> abstractive R2 <C> abstractive RL <C> extractive F1 <C> extractive CR <R> <C> CS + cat <C> 26.48 <C> 10.05 <C> 24.41 <C> 60.90 <C> 0.38 <R> <C> CS + avg <C> 26.34 <C> 9.79 <C> 24.23 <C> 60.09 <C> 0.38 <R> <C> CS + top <C> 26.21 <C> 9.69 <C> 24.14 <C> 62.18 <C> 0.34 <R> <C> CS + mid <C> 25.46 <C> 9.39 <C> 23.34 <C> 59.32 <C> 0.40 <R> <C> CS + bot <C> 15.29 <C> 3.95 <C> 14.06 <C> 21.14 <C> 0.23 <R> <C> TEMP5 + cat <C> 26.31 <C> 9.38 <C> 23.60 <C> 52.10 <C> 0.43 <R> <C> TEMP10 + cat <C> 25.63 <C> 8.82 <C> 22.86 <C> 42.33 <C> 0.47 <R> <C> NA + cat <C> 24.81 <C> 8.89 <C> 22.87 <C> 49.80 <C> 0.32 <CAP> Table 3: Comparison of different model choices. The top section evaluates the effects of contextual representation in the matching model, and the bottom section evaluates the effects of different smoothing methods in the fluency model. <COT> Looking at the table caption, it mentions that the top section evaluates the effects of contextual representation in the matching model.
<R> <C> Models <C> abstractive R1 <C> abstractive R2 <C> abstractive RL <C> extractive F1 <C> extractive CR <R> <C> CS + cat <C> 26.48 <C> 10.05 <C> 24.41 <C> 60.90 <C> 0.38 <R> <C> CS + avg <C> 26.34 <C> 9.79 <C> 24.23 <C> 60.09 <C> 0.38 <R> <C> CS + top <C> 26.21 <C> 9.69 <C> 24.14 <C> 62.18 <C> 0.34 <R> <C> CS + mid <C> 25.46 <C> 9.39 <C> 23.34 <C> 59.32 <C> 0.40 <R> <C> CS + bot <C> 15.29 <C> 3.95 <C> 14.06 <C> 21.14 <C> 0.23 <R> <C> TEMP5 + cat <C> 26.31 <C> 9.38 <C> 23.60 <C> 52.10 <C> 0.43 <R> <C> TEMP10 + cat <C> 25.63 <C> 8.82 <C> 22.86 <C> 42.33 <C> 0.47 <R> <C> NA + cat <C> 24.81 <C> 8.89 <C> 22.87 <C> 49.80 <C> 0.32 <CAP> Table 3: Comparison of different model choices. The top section evaluates the effects of contextual representation in the matching model, and the bottom section evaluates the effects of different smoothing methods in the fluency model. <COT> Looking at the "abstractive R1" column, the values are decreasing as we move from "CS + cat" to "CS + bot".
<R> <C> [BOLD] Sub-task <C> [BOLD] 1 Ext. <C> [BOLD] 1 Class. <C> [BOLD] 2 Ext. <C> [BOLD] 2 Class. <C> [ITALIC]  [BOLD] mv Ext. <C> [ITALIC]  [BOLD] mv Class. <R> <C> 1.1 <C> - <C> 72.1 <C> - <C> 74.7 <C> - <C> [BOLD] 76.7 <R> <C> 1.2 <C> - <C> [BOLD] 83.2 <C> - <C> 82.9 <C> - <C> 80.1 <R> <C> 2 <C> [BOLD] 37.4 <C> [BOLD] 33.6 <C> 36.5 <C> 28.8 <C> 35.6 <C> 28.3 <CAP> Table 3: Official evaluation results of the submitted runs on the test set. <COT> Looking at the "Sub-task" column, we can see that there are two sub-tasks: 1.1 and 1.2. 
<R> <C> [BOLD] Sub-task <C> [BOLD] 1 Ext. <C> [BOLD] 1 Class. <C> [BOLD] 2 Ext. <C> [BOLD] 2 Class. <C> [ITALIC]  [BOLD] mv Ext. <C> [ITALIC]  [BOLD] mv Class. <R> <C> 1.1 <C> - <C> 72.1 <C> - <C> 74.7 <C> - <C> [BOLD] 76.7 <R> <C> 1.2 <C> - <C> [BOLD] 83.2 <C> - <C> 82.9 <C> - <C> 80.1 <R> <C> 2 <C> [BOLD] 37.4 <C> [BOLD] 33.6 <C> 36.5 <C> 28.8 <C> 35.6 <C> 28.3 <CAP> Table 3: Official evaluation results of the submitted runs on the test set. <COT> Looking at the "2 Ext." column, we can see that the scores for sub-task 2 are higher than the scores for sub-task 1.2 in both the "Ext." and "Class." categories.
<R> <C> [EMPTY] <C> [BOLD]  Predicted parse vs. Gold parse (separate)  <C> [BOLD]  Predicted parse vs. Gold parse (joint)  <C> [BOLD]  Parsed prediction vs. Gold parse  <C> [BOLD]  Parsed prediction vs. Predicted parse  <R> <C> F1 <C> 65.48 <C> 69.64 <C> 79.16 <C> 89.90 <R> <C> Exact match <C> 4.23% <C> 5.24% <C> 5.94% <C> 43.10% <CAP> Table 3: F1 and exact match comparisons of predicted chunk sequences (from the parse decoder), ground-truth chunk sequences (from an external parser in the target language), and chunk sequences obtained after parsing the translation produced by the token decoder. First two columns show the improvement obtained by jointly training the two decoders. The third column shows that when the token decoder deviates from the predicted chunk sequence, it usually results in a translation that is closer to the ground-truth target syntax, while the fourth column shows that the token decoder closely follows the predicted chunk sequence. <COT> Looking at the table caption, we can see that the table shows F1 and exact match comparisons of predicted chunk sequences and ground-truth chunk sequences, as well as comparisons between parsed predictions and gold parses.
<R> <C> [BOLD] Decoder <C> [BOLD] SICK-R <C> [BOLD] SICK-E <C> [BOLD] STS14 <C> [BOLD] MSRP (Acc/F1) <C> [BOLD] SST <C> [BOLD] TREC <R> <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <R> <C> Teacher-Forcing <C> 0.8530 <C> 82.6 <C> 0.51/0.50 <C> 74.1 / 81.7 <C> 82.5 <C> 88.2 <R> <C> Always Sampling <C> 0.8576 <C> 83.2 <C> 0.55/0.53 <C> 74.7 / 81.3 <C> 80.6 <C> 87.0 <R> <C> Uniform Sampling <C> 0.8559 <C> 82.9 <C> 0.54/0.53 <C> 74.0 / 81.8 <C> 81.0 <C> 87.4 <R> <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <R> <C> Teacher-Forcing <C> 0.8510 <C> 82.8 <C> 0.49/0.48 <C> 74.7 / 82.8 <C> 81.4 <C> 82.6 <R> <C> Always Sampling <C> 0.8535 <C> 83.3 <C> 0.53/0.52 <C> 75.0 / 81.7 <C> 81.4 <C> 87.6 <R> <C> Uniform Sampling <C> 0.8568 <C> 83.4 <C> 0.56/0.54 <C> 74.7 / 81.4 <C> 83.0 <C> 88.4 <R> <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <R> <C> RNN <C> 0.8508 <C> 82.8 <C> 0.58/0.55 <C> 74.2 / 82.8 <C> 81.6 <C> 88.8 <R> <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <R> <C> CNN <C> 0.8530 <C> 82.6 <C> [BOLD] 0.58/ [BOLD] 0.56 <C> [BOLD] 75.6 / 82.9 <C> 82.8 <C> 89.2 <R> <C> CNN-MaxOnly <C> 0.8465 <C> 82.6 <C> 0.50/0.47 <C> 73.3 / 81.5 <C> 79.1 <C> 82.2 <R> <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <R> <C> CNN <C> [BOLD] 0.8631 <C> [BOLD] 83.9 <C> [BOLD] 0.58/0.55 <C> 74.7 /  [BOLD] 83.1 <C> [BOLD] 83.4 <C> [BOLD] 90.2 <R> <C> CNN-MaxOnly <C> 0.8485 <C> 83.2 <C> 0.47/0.44 <C> 72.9 / 80.8 <C> 82.2 <C> 86.6 <CAP> Table 1: The models here all have a bi-directional GRU as the encoder (dimensionality 300 in each direction). The default way of producing the representation is a concatenation of outputs from a global mean-pooling and a global max-pooling, while “⋅-MaxOnly” refers to the model with only global max-pooling. Bold numbers are the best results among all presented models. We found that 1) inputting correct words to an autoregressive decoder is not necessary; 2) predict-all-words decoders work roughly the same as autoregressive decoders; 3) mean+max pooling provides stronger transferability than the max-pooling alone does. The table supports our choice of the predict-all-words CNN decoder and the way of producing vector representations from the bi-directional RNN encoder. <COT> Looking at the "Decoder" column, we can see that the "predict-all-words CNN as decoder" consistently performs better than other decoder types across all evaluation metrics.
<R> <C> [BOLD] Decoder <C> [BOLD] SICK-R <C> [BOLD] SICK-E <C> [BOLD] STS14 <C> [BOLD] MSRP (Acc/F1) <C> [BOLD] SST <C> [BOLD] TREC <R> <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <C> [BOLD] auto-regressive RNN as decoder <R> <C> Teacher-Forcing <C> 0.8530 <C> 82.6 <C> 0.51/0.50 <C> 74.1 / 81.7 <C> 82.5 <C> 88.2 <R> <C> Always Sampling <C> 0.8576 <C> 83.2 <C> 0.55/0.53 <C> 74.7 / 81.3 <C> 80.6 <C> 87.0 <R> <C> Uniform Sampling <C> 0.8559 <C> 82.9 <C> 0.54/0.53 <C> 74.0 / 81.8 <C> 81.0 <C> 87.4 <R> <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <C> [BOLD] auto-regressive CNN as decoder <R> <C> Teacher-Forcing <C> 0.8510 <C> 82.8 <C> 0.49/0.48 <C> 74.7 / 82.8 <C> 81.4 <C> 82.6 <R> <C> Always Sampling <C> 0.8535 <C> 83.3 <C> 0.53/0.52 <C> 75.0 / 81.7 <C> 81.4 <C> 87.6 <R> <C> Uniform Sampling <C> 0.8568 <C> 83.4 <C> 0.56/0.54 <C> 74.7 / 81.4 <C> 83.0 <C> 88.4 <R> <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <C> [BOLD] predict-all-words RNN as decoder <R> <C> RNN <C> 0.8508 <C> 82.8 <C> 0.58/0.55 <C> 74.2 / 82.8 <C> 81.6 <C> 88.8 <R> <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <C> [BOLD] predict-all-words CNN as decoder <R> <C> CNN <C> 0.8530 <C> 82.6 <C> [BOLD] 0.58/ [BOLD] 0.56 <C> [BOLD] 75.6 / 82.9 <C> 82.8 <C> 89.2 <R> <C> CNN-MaxOnly <C> 0.8465 <C> 82.6 <C> 0.50/0.47 <C> 73.3 / 81.5 <C> 79.1 <C> 82.2 <R> <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <C> Double-sized RNN Encoder <R> <C> CNN <C> [BOLD] 0.8631 <C> [BOLD] 83.9 <C> [BOLD] 0.58/0.55 <C> 74.7 /  [BOLD] 83.1 <C> [BOLD] 83.4 <C> [BOLD] 90.2 <R> <C> CNN-MaxOnly <C> 0.8485 <C> 83.2 <C> 0.47/0.44 <C> 72.9 / 80.8 <C> 82.2 <C> 86.6 <CAP> Table 1: The models here all have a bi-directional GRU as the encoder (dimensionality 300 in each direction). The default way of producing the representation is a concatenation of outputs from a global mean-pooling and a global max-pooling, while “⋅-MaxOnly” refers to the model with only global max-pooling. Bold numbers are the best results among all presented models. We found that 1) inputting correct words to an autoregressive decoder is not necessary; 2) predict-all-words decoders work roughly the same as autoregressive decoders; 3) mean+max pooling provides stronger transferability than the max-pooling alone does. The table supports our choice of the predict-all-words CNN decoder and the way of producing vector representations from the bi-directional RNN encoder. <COT> Looking at the "SICK-E" column, we can see that the "predict-all-words CNN as decoder" achieves the highest F1 score of 0.58.
<R> <C> [BOLD] Encoder type <C> [BOLD] Encoder dim <C> [BOLD] Decoder type <C> [BOLD] Decoder dim <C> [BOLD] Hrs <C> [BOLD] SICK-R <C> [BOLD] SICK-E <C> [BOLD] STS14 <C> [BOLD] MSRP (Acc/F1) <C> [BOLD] SST <C> [BOLD] TREC <R> <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <R> <C> RNN <C> 2x300 <C> CNN <C> 600-1200-300 <C> 20 <C> 0.8530 <C> 82.6 <C> 0.58/0.56 <C> [BOLD] 75.6/ [BOLD] 82.9 <C> 82.8 <C> [BOLD] 89.2 <R> <C> RNN <C> 2x300 <C> CNN† <C> 600-1200-300 <C> 21 <C> 0.8515 <C> 82.7 <C> 0.58/0.56 <C> 75.3/82.5 <C> [BOLD] 82.9 <C> 85.2 <R> <C> RNN <C> 2x300 <C> CNN(10) <C> 600-1200-300 <C> 11 <C> 0.8474 <C> 82.9 <C> 0.57/0.55 <C> 74.2/81.6 <C> 82.8 <C> 88.0 <R> <C> RNN <C> 2x300 <C> CNN(50) <C> 600-1200-300 <C> 27 <C> 0.8533 <C> 82.5 <C> 0.57/0.55 <C> 74.7/82.2 <C> 81.5 <C> 86.2 <R> <C> RNN <C> 2x300 <C> RNN <C> 600 <C> 26 <C> 0.8530 <C> 82.6 <C> 0.51/0.50 <C> 74.1/81.7 <C> 81.0 <C> 89.0 <R> <C> CNN <C> 4x300 \lx @ [ITALIC] sectionsign <C> CNN <C> 600-1200-300 <C> 8 <C> 0.8117 <C> 80.5 <C> 0.44/0.42 <C> 72.7/80.7 <C> 78.4 <C> 85.0 <R> <C> RNN <C> 2x300 <C> CNN <C> 600-1200-2400-300 <C> 28 <C> [BOLD] 0.8570 <C> [BOLD] 84.0 <C> 0.58/0.56 <C> 74.3/81.5 <C> 82.8 <C> 88.2 <R> <C> RNN <C> 2x300 <C> CNN <C> 1200-2400-300 <C> 27 <C> 0.8541 <C> 83.0 <C> [BOLD] 0.59/ [BOLD] 0.57 <C> 74.3/82.2 <C> [BOLD] 82.9 <C> 89.0 <R> <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <R> <C> RNN <C> 2x600 <C> CNN <C> 600-1200-300 <C> 25 <C> 0.8631 <C> 83.9 <C> [BOLD] 0.58/ [BOLD] 0.55 <C> [BOLD] 74.7/ [BOLD] 83.1 <C> 83.4 <C> [BOLD] 90.2 <R> <C> RNN <C> 2x600 <C> RNN <C> 600 <C> 32 <C> [BOLD] 0.8647 <C> [BOLD] 84.2 <C> 0.52/0.51 <C> 74.0/81.2 <C> [BOLD] 84.2 <C> 87.6 <R> <C> CNN <C> 3x800‡ <C> RNN <C> 600 <C> 8 <C> 0.8132 <C> - <C> - <C> 71.9/81.9 <C> - <C> 86.6 <R> <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <R> <C> RNN <C> 2x1200 <C> CNN <C> 600-1200-300 <C> 34 <C> [BOLD] 0.8698 <C> [BOLD] 85.2 <C> [BOLD] 0.59/ [BOLD] 0.57 <C> [BOLD] 75.1/ [BOLD] 83.2 <C> [BOLD] 84.1 <C> [BOLD] 92.2 <R> <C> Skip-thought (Kiros et al.,  2015 ) <C> Skip-thought (Kiros et al.,  2015 ) <C> Skip-thought (Kiros et al.,  2015 ) <C> Skip-thought (Kiros et al.,  2015 ) <C> 336 <C> 0.8584 <C> 82.3 <C> 0.29/0.35 <C> 73.0/82.0 <C> 82.0 <C> [BOLD] 92.2 <R> <C> Skip-thought+LN (Ba et al.,  2016 ) <C> Skip-thought+LN (Ba et al.,  2016 ) <C> Skip-thought+LN (Ba et al.,  2016 ) <C> Skip-thought+LN (Ba et al.,  2016 ) <C> 720 <C> 0.8580 <C> 79.5 <C> 0.44/0.45 <C> - <C> 82.9 <C> 88.4 <CAP> Table 4: Architecture Comparison. As shown in the table, our designed asymmetric RNN-CNN model (row 1,9, and 12) works better than other asymmetric models (CNN-LSTM, row 11), and models with symmetric structure (RNN-RNN, row 5 and 10). <COT> Looking at the "CAP" tag, we can see that Table 4 is comparing different architectures.
<R> <C> [BOLD] Encoder type <C> [BOLD] Encoder dim <C> [BOLD] Decoder type <C> [BOLD] Decoder dim <C> [BOLD] Hrs <C> [BOLD] SICK-R <C> [BOLD] SICK-E <C> [BOLD] STS14 <C> [BOLD] MSRP (Acc/F1) <C> [BOLD] SST <C> [BOLD] TREC <R> <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <C> [BOLD] Dimension of Sentence Representation: 1200 <R> <C> RNN <C> 2x300 <C> CNN <C> 600-1200-300 <C> 20 <C> 0.8530 <C> 82.6 <C> 0.58/0.56 <C> [BOLD] 75.6/ [BOLD] 82.9 <C> 82.8 <C> [BOLD] 89.2 <R> <C> RNN <C> 2x300 <C> CNN† <C> 600-1200-300 <C> 21 <C> 0.8515 <C> 82.7 <C> 0.58/0.56 <C> 75.3/82.5 <C> [BOLD] 82.9 <C> 85.2 <R> <C> RNN <C> 2x300 <C> CNN(10) <C> 600-1200-300 <C> 11 <C> 0.8474 <C> 82.9 <C> 0.57/0.55 <C> 74.2/81.6 <C> 82.8 <C> 88.0 <R> <C> RNN <C> 2x300 <C> CNN(50) <C> 600-1200-300 <C> 27 <C> 0.8533 <C> 82.5 <C> 0.57/0.55 <C> 74.7/82.2 <C> 81.5 <C> 86.2 <R> <C> RNN <C> 2x300 <C> RNN <C> 600 <C> 26 <C> 0.8530 <C> 82.6 <C> 0.51/0.50 <C> 74.1/81.7 <C> 81.0 <C> 89.0 <R> <C> CNN <C> 4x300 \lx @ [ITALIC] sectionsign <C> CNN <C> 600-1200-300 <C> 8 <C> 0.8117 <C> 80.5 <C> 0.44/0.42 <C> 72.7/80.7 <C> 78.4 <C> 85.0 <R> <C> RNN <C> 2x300 <C> CNN <C> 600-1200-2400-300 <C> 28 <C> [BOLD] 0.8570 <C> [BOLD] 84.0 <C> 0.58/0.56 <C> 74.3/81.5 <C> 82.8 <C> 88.2 <R> <C> RNN <C> 2x300 <C> CNN <C> 1200-2400-300 <C> 27 <C> 0.8541 <C> 83.0 <C> [BOLD] 0.59/ [BOLD] 0.57 <C> 74.3/82.2 <C> [BOLD] 82.9 <C> 89.0 <R> <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <C> [BOLD] Dimension of Sentence Representation: 2400 <R> <C> RNN <C> 2x600 <C> CNN <C> 600-1200-300 <C> 25 <C> 0.8631 <C> 83.9 <C> [BOLD] 0.58/ [BOLD] 0.55 <C> [BOLD] 74.7/ [BOLD] 83.1 <C> 83.4 <C> [BOLD] 90.2 <R> <C> RNN <C> 2x600 <C> RNN <C> 600 <C> 32 <C> [BOLD] 0.8647 <C> [BOLD] 84.2 <C> 0.52/0.51 <C> 74.0/81.2 <C> [BOLD] 84.2 <C> 87.6 <R> <C> CNN <C> 3x800‡ <C> RNN <C> 600 <C> 8 <C> 0.8132 <C> - <C> - <C> 71.9/81.9 <C> - <C> 86.6 <R> <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <C> [BOLD] Dimension of Sentence Representation: 4800 <R> <C> RNN <C> 2x1200 <C> CNN <C> 600-1200-300 <C> 34 <C> [BOLD] 0.8698 <C> [BOLD] 85.2 <C> [BOLD] 0.59/ [BOLD] 0.57 <C> [BOLD] 75.1/ [BOLD] 83.2 <C> [BOLD] 84.1 <C> [BOLD] 92.2 <R> <C> Skip-thought (Kiros et al.,  2015 ) <C> Skip-thought (Kiros et al.,  2015 ) <C> Skip-thought (Kiros et al.,  2015 ) <C> Skip-thought (Kiros et al.,  2015 ) <C> 336 <C> 0.8584 <C> 82.3 <C> 0.29/0.35 <C> 73.0/82.0 <C> 82.0 <C> [BOLD] 92.2 <R> <C> Skip-thought+LN (Ba et al.,  2016 ) <C> Skip-thought+LN (Ba et al.,  2016 ) <C> Skip-thought+LN (Ba et al.,  2016 ) <C> Skip-thought+LN (Ba et al.,  2016 ) <C> 720 <C> 0.8580 <C> 79.5 <C> 0.44/0.45 <C> - <C> 82.9 <C> 88.4 <CAP> Table 4: Architecture Comparison. As shown in the table, our designed asymmetric RNN-CNN model (row 1,9, and 12) works better than other asymmetric models (CNN-LSTM, row 11), and models with symmetric structure (RNN-RNN, row 5 and 10). <COT> Looking at the "Hrs" column, we can see that different models have different training times.
<R> <C> [EMPTY] <C> WMT DE-EN <C> WMT EN-FI <C> WMT RO-EN <C> IWSLT EN-FR <C> IWSLT CS-EN <R> <C> Words 50K <C> 31.6 <C> 12.6 <C> 27.1 <C> 33.6 <C> 21.0 <R> <C> BPE 32K <C> [BOLD] 33.5 <C> [BOLD] 14.7 <C> [BOLD] 27.8 <C> 34.5 <C> 22.6 <R> <C> BPE 16K <C> 33.1 <C> [BOLD] 14.7 <C> [BOLD] 27.8 <C> [BOLD] 34.8 <C> [BOLD] 23.0 <CAP> Table 2: BLEU scores for training NMT models with full word and byte pair encoded vocabularies. Full word models limit vocabulary size to 50K. All models are trained with annealing Adam and scores are averaged over 3 optimizer runs. <COT> Looking at the "BPE 32K" row, we can see that the BLEU scores for WMT DE-EN, WMT EN-FI, and WMT RO-EN are bolded, indicating that these scores are the highest among the three vocabulary sizes.
<R> <C> [EMPTY] <C> WMT DE-EN <C> WMT EN-FI <C> WMT RO-EN <C> IWSLT EN-FR <C> IWSLT CS-EN <R> <C> Words 50K <C> 31.6 <C> 12.6 <C> 27.1 <C> 33.6 <C> 21.0 <R> <C> BPE 32K <C> [BOLD] 33.5 <C> [BOLD] 14.7 <C> [BOLD] 27.8 <C> 34.5 <C> 22.6 <R> <C> BPE 16K <C> 33.1 <C> [BOLD] 14.7 <C> [BOLD] 27.8 <C> [BOLD] 34.8 <C> [BOLD] 23.0 <CAP> Table 2: BLEU scores for training NMT models with full word and byte pair encoded vocabularies. Full word models limit vocabulary size to 50K. All models are trained with annealing Adam and scores are averaged over 3 optimizer runs. <COT> Looking at the "IWSLT CS-EN" column, we can see that the BLEU score increases as the vocabulary size decreases from 50K to 16K.
<R> <C> Model LSTM-Word <C> PPL 88.0 <C> Model Char-CNN <C> PPL 92.3 <R> <C> Syl-LSTM <C> 88.7 <C> Syl-Avg <C> 88.5 <R> <C> Syl-CNN-2 <C> 86.6 <C> Syl-Avg-A <C> 91.4 <R> <C> Syl-CNN-3 <C> [BOLD] 84.6 <C> Syl-Avg-B <C> 88.5 <R> <C> Syl-CNN-4 <C> 86.8 <C> Syl-Concat <C> [BOLD] 83.7 <R> <C> Syl-Sum <C> [BOLD] 84.6 <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: Pre-selection results. PPL stands for test set perplexity, all models have ≈5M parameters. <COT> Looking at the "Syl-CNN-3" cell, "[BOLD] 84.6" cell, "Syl-Concat" cell, and "[BOLD] 83.7" cell, we can see that the Syl-CNN-3 model performs better than the Syl-Concat model.
<R> <C> Model LSTM-Word <C> PPL 88.0 <C> Model Char-CNN <C> PPL 92.3 <R> <C> Syl-LSTM <C> 88.7 <C> Syl-Avg <C> 88.5 <R> <C> Syl-CNN-2 <C> 86.6 <C> Syl-Avg-A <C> 91.4 <R> <C> Syl-CNN-3 <C> [BOLD] 84.6 <C> Syl-Avg-B <C> 88.5 <R> <C> Syl-CNN-4 <C> 86.8 <C> Syl-Concat <C> [BOLD] 83.7 <R> <C> Syl-Sum <C> [BOLD] 84.6 <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: Pre-selection results. PPL stands for test set perplexity, all models have ≈5M parameters. <COT> Looking at the "Model LSTM-Word" cell, "PPL 88.0" cell, "Model Char-CNN" cell, and "PPL 92.3" cell, we can see that the LSTM-Word model has a lower perplexity than the Char-CNN model.
<R> <C> Model <C> EN <C> FR <C> ES <C> DE <C> CS <C> RU <C> [EMPTY] <R> <C> Char-CNN <C> [BOLD] 78.9 <C> [BOLD] 184 <C> [BOLD] 165 <C> [BOLD] 239 <C> [BOLD] 371 <C> [BOLD] 261 <C> DATA-S <R> <C> Syl-CNN <C> 80.5 <C> 191 <C> 172 <C> 239 <C> 374 <C> 269 <C> DATA-S <R> <C> Syl-Sum <C> 80.3 <C> 193 <C> 170 <C> 243 <C> 389 <C> 273 <C> DATA-S <R> <C> Syl-Concat <C> 79.4 <C> 188 <C> 168 <C> 244 <C> 383 <C> 265 <C> DATA-S <R> <C> Char-CNN <C> [BOLD] 160 <C> [BOLD] 124 <C> [BOLD] 118 <C> [BOLD] 198 <C> [BOLD] 392 <C> [BOLD] 190 <C> DATA-L <R> <C> Syl-CNN <C> – <C> – <C> – <C> – <C> – <C> – <C> DATA-L <R> <C> Syl-Sum <C> 170 <C> 141 <C> 129 <C> 212 <C> 451 <C> 233 <C> DATA-L <R> <C> Syl-Concat <C> 176 <C> 139 <C> 129 <C> 225 <C> 449 <C> 225 <C> DATA-L <CAP> Table 3: Evaluation of the syllable-aware models against Char-CNN. In each case the smallest model, Syl-Concat, has 18%–33% less parameters than Char-CNN and is trained 1.2–2.2 times faster (Appendix C). <COT> Looking at the table, we can see that the Syl-Concat model consistently has fewer parameters and faster training times compared to the Char-CNN model.
<R> <C> Model <C> depth <C> [ITALIC] dLM <C> Size <C> PPL <R> <C> RHN-Char-CNN <C> 8 <C> 650 <C> 20M <C> 67.6 <R> <C> RHN-Syl-Concat <C> 8 <C> 439 <C> 13M <C> 72.0 <R> <C> RHN-Syl-Concat <C> 8 <C> 650 <C> 20M <C> 69.4 <CAP> Table 5: Replacing LSTM with Variational RHN. <COT> Looking at the "Model" column, we can see that there are three different models mentioned: RHN-Char-CNN, RHN-Syl-Concat, and RHN-Syl-Concat. 
<R> <C> Model <C> depth <C> [ITALIC] dLM <C> Size <C> PPL <R> <C> RHN-Char-CNN <C> 8 <C> 650 <C> 20M <C> 67.6 <R> <C> RHN-Syl-Concat <C> 8 <C> 439 <C> 13M <C> 72.0 <R> <C> RHN-Syl-Concat <C> 8 <C> 650 <C> 20M <C> 69.4 <CAP> Table 5: Replacing LSTM with Variational RHN. <COT> Looking at the "Size" column, we can see that the sizes of the models mentioned are 20M and 13M.
<R> <C> Problem <C> Support Accuracy <C> Support Kappa <C> Claim Accuracy <C> Claim Kappa <R> <C> ESIM on FEVER One <C> .760 <C> .260 <C> .517 <C> .297 <R> <C> ESIM on FEVER Title One <C> .846 <C> .394 <C> .639 <C> .433 <R> <C> Transformer on FEVER Title One <C> .958 <C> .660 <C> .823 <C> .622 <CAP> Table 1: Effect of adding titles to premises. <COT> Looking at the "Problem" column, we can see that the table shows the effect of adding titles to premises in the FEVER dataset.
<R> <C> Problem <C> Support Accuracy <C> Support Kappa <C> Claim Accuracy <C> Claim Kappa <R> <C> ESIM on FEVER One <C> .760 <C> .260 <C> .517 <C> .297 <R> <C> ESIM on FEVER Title One <C> .846 <C> .394 <C> .639 <C> .433 <R> <C> Transformer on FEVER Title One <C> .958 <C> .660 <C> .823 <C> .622 <CAP> Table 1: Effect of adding titles to premises. <COT> Looking at the "Claim Accuracy" column, we can see that the accuracy of the claim is higher when using the Transformer model on FEVER Title One compared to ESIM on FEVER Title One.
<R> <C> Problem <C> Support Accuracy <C> Support Kappa <C> Claim Accuracy <C> Claim Kappa <R> <C> ESIM on FEVER Title Five Oracle <C> [EMPTY] <C> [EMPTY] <C> .591 <C> .388 <R> <C> ESIM on FEVER Title Five <C> [EMPTY] <C> [EMPTY] <C> .573 <C> .110 <R> <C> ESIM on FEVER Title One <C> .846 <C> .394 <C> .639 <C> .433 <R> <C> Transformer on FEVER Title Five Oracle <C> [EMPTY] <C> [EMPTY] <C> .673 <C> .511 <R> <C> Transformer on FEVER Title Five <C> [EMPTY] <C> [EMPTY] <C> .801 <C> .609 <R> <C> Transformer on FEVER Title One <C> .958 <C> .660 <C> .823 <C> .622 <CAP> Table 2: Concatenating evidence or not. <COT> Looking at the "ESIM on FEVER Title Five Oracle" row, we can see that the Support Accuracy is not provided.
<R> <C> Problem <C> Support Accuracy <C> Support Kappa <C> Claim Accuracy <C> Claim Kappa <R> <C> ESIM on FEVER Title Five Oracle <C> [EMPTY] <C> [EMPTY] <C> .591 <C> .388 <R> <C> ESIM on FEVER Title Five <C> [EMPTY] <C> [EMPTY] <C> .573 <C> .110 <R> <C> ESIM on FEVER Title One <C> .846 <C> .394 <C> .639 <C> .433 <R> <C> Transformer on FEVER Title Five Oracle <C> [EMPTY] <C> [EMPTY] <C> .673 <C> .511 <R> <C> Transformer on FEVER Title Five <C> [EMPTY] <C> [EMPTY] <C> .801 <C> .609 <R> <C> Transformer on FEVER Title One <C> .958 <C> .660 <C> .823 <C> .622 <CAP> Table 2: Concatenating evidence or not. <COT> Looking at the "Transformer on FEVER Title One" row, we can see that the Claim Accuracy is higher than the Support Accuracy.
<R> <C> System <C> Retrieval <R> <C> FEVER Baseline (TFIDF) <C> 66.1% <R> <C> + Titles in TFIDF <C> 68.3% <R> <C> + Titles + NE <C> 80.8% <R> <C> + Titles + NE + Film <C> 81.2% <R> <C> Entire Articles + NE + Film <C> 90.1% <CAP> Table 3: Percentage of evidence retrieved from first half of development set. Single-evidence claims only. <COT> Looking at the "System" column, we can see that the FEVER Baseline (TFIDF) retrieves 66.1% of evidence from the first half of the development set.
<R> <C> System <C> Retrieval <R> <C> FEVER Baseline (TFIDF) <C> 66.1% <R> <C> + Titles in TFIDF <C> 68.3% <R> <C> + Titles + NE <C> 80.8% <R> <C> + Titles + NE + Film <C> 81.2% <R> <C> Entire Articles + NE + Film <C> 90.1% <CAP> Table 3: Percentage of evidence retrieved from first half of development set. Single-evidence claims only. <COT> Looking at the progression of the percentages in the "Retrieval" column, we can observe that adding Titles in TFIDF increases the percentage to 68.3%, adding Titles + NE further increases it to 80.8%, and adding Titles + NE + Film increases it to 81.2%.
<R> <C> System <C> Development <C> Test <R> <C> FEVER Title Five Oracle <C> .5289 <C> — <R> <C> FEVER Title Five <C> .5553 <C> — <R> <C> FEVER Title One <C> .5617 <C> .5539 <R> <C> FEVER Title One (Narrow Evidence) <C> .5550 <C> — <R> <C> FEVER Title One (Entire Articles) <C> .5844 <C> .5736 <CAP> Table 4: FEVER Score of various systems. All use NE+Film retrieval. <COT> Looking at the "FEVER Title One (Narrow Evidence)" row, the "Development" column has a value of .5550, and the "Test" column is empty. This indicates that the FEVER Title One (Narrow Evidence) system achieved a score of .5550 during development, but there is no available score for the test set.
<R> <C> System <C> Development <C> Test <R> <C> FEVER Title Five Oracle <C> .5289 <C> — <R> <C> FEVER Title Five <C> .5553 <C> — <R> <C> FEVER Title One <C> .5617 <C> .5539 <R> <C> FEVER Title One (Narrow Evidence) <C> .5550 <C> — <R> <C> FEVER Title One (Entire Articles) <C> .5844 <C> .5736 <CAP> Table 4: FEVER Score of various systems. All use NE+Film retrieval. <COT> Looking at the "FEVER Title One (Entire Articles)" row, both the "Development" and "Test" columns have non-empty values. The "Development" column has a value of .5844, and the "Test" column has a value of .5736. This indicates that the FEVER Title One (Entire Articles) system achieved a score of .5844 during development and a score of .5736 on the test set.
<R> <C> Pairs (size) <C> @1 <C> @5 <C> @10 <R> <C> All (38) <C> 44.7 <C> 73.7 <C> 84.2 <R> <C> New (7) <C> 14.3 <C> 28.6 <C> 42.9 <CAP> Table 2: Projection accuracy for the isolated example experiment mapping from 2000 → 2001. <COT> Looking at the "Pairs (size)" cell, "All (38)" cell, and "@1" cell, we can see that the projection accuracy for all pairs in the experiment is 44.7% at the top 1.
<R> <C> Pairs (size) <C> @1 <C> @5 <C> @10 <R> <C> All (38) <C> 44.7 <C> 73.7 <C> 84.2 <R> <C> New (7) <C> 14.3 <C> 28.6 <C> 42.9 <CAP> Table 2: Projection accuracy for the isolated example experiment mapping from 2000 → 2001. <COT> Looking at the "Pairs (size)" cell, "New (7)" cell, and "@10" cell, we can see that the projection accuracy for new pairs in the experiment is 42.9% at the top 10.
<R> <C> [EMPTY] <C> [BOLD] Only in-vocabulary pairs  [BOLD] up-to-now <C> [BOLD] Only in-vocabulary pairs  [BOLD] up-to-now <C> [BOLD] Only in-vocabulary pairs  [BOLD] up-to-now <C> [BOLD] Only in-vocabulary pairs  [BOLD] previous <C> [BOLD] Only in-vocabulary pairs  [BOLD] previous <C> [BOLD] Only in-vocabulary pairs  [BOLD] previous <C> [BOLD] All pairs, including OOV  [BOLD] up-to-now <C> [BOLD] All pairs, including OOV  [BOLD] up-to-now <C> [BOLD] All pairs, including OOV  [BOLD] up-to-now <C> [BOLD] All pairs, including OOV  [BOLD] previous <C> [BOLD] All pairs, including OOV  [BOLD] previous <C> [BOLD] All pairs, including OOV  [BOLD] previous <R> <C> [EMPTY] <C> @1 <C> @5 <C> @10 <C> @1 <C> @5 <C> @10 <C> @1 <C> @5 <C> @10 <C> @1 <C> @5 <C> @10 <R> <C> [BOLD] Separate <C> 0.0 <C> 0.7 <C> 2.1 <C> 0.5 <C> 1.1 <C> 2.4 <C> 0.0 <C> 0.5 <C> 1.6 <C> 0.4 <C> 0.8 <C> 1.8 <R> <C> [BOLD] Cumulative <C> 1.7 <C> 8.3 <C> 13.8 <C> 2.9 <C> 9.6 <C> 15.2 <C> 1.5 <C> 7.4 <C> 12.2 <C> 2.5 <C> 8.5 <C> 13.4 <R> <C> [BOLD] Incr. static <C> 54.9 <C> 82.8 <C> 90.1 <C> 60.4 <C> 79.6 <C> 84.8 <C> 20.8 <C> 31.5 <C> 34.2 <C> 23.0 <C> 30.3 <C> 32.2 <R> <C> [BOLD] Incr. dynamic <C> 32.5 <C> 64.5 <C> 72.2 <C> 42.6 <C> 64.8 <C> 71.5 <C> 28.1 <C> 56.1 <C> [BOLD] 62.9 <C> [BOLD] 37.3 <C> [BOLD] 56.7 <C> 62.6 <CAP> Table 3: Average accuracies of predicting next-year insurgents on the basis of locations, using projections trained on the conflicts from all the preceding years (up-to-now) or the preceding year only (previous). Results for 3 baselines are shown along with the proposed incremental dynamic approach. <COT> Looking at the "Cumulative" row, "15.2" cell, and "All pairs, including OOV previous" cell, we can see that the average accuracy of predicting next-year insurgents based on locations using the cumulative approach with all pairs, including OOV, is 15.2.
<R> <C> [EMPTY] <C> [BOLD] Only in-vocabulary pairs  [BOLD] up-to-now <C> [BOLD] Only in-vocabulary pairs  [BOLD] up-to-now <C> [BOLD] Only in-vocabulary pairs  [BOLD] up-to-now <C> [BOLD] Only in-vocabulary pairs  [BOLD] previous <C> [BOLD] Only in-vocabulary pairs  [BOLD] previous <C> [BOLD] Only in-vocabulary pairs  [BOLD] previous <C> [BOLD] All pairs, including OOV  [BOLD] up-to-now <C> [BOLD] All pairs, including OOV  [BOLD] up-to-now <C> [BOLD] All pairs, including OOV  [BOLD] up-to-now <C> [BOLD] All pairs, including OOV  [BOLD] previous <C> [BOLD] All pairs, including OOV  [BOLD] previous <C> [BOLD] All pairs, including OOV  [BOLD] previous <R> <C> [EMPTY] <C> @1 <C> @5 <C> @10 <C> @1 <C> @5 <C> @10 <C> @1 <C> @5 <C> @10 <C> @1 <C> @5 <C> @10 <R> <C> [BOLD] Separate <C> 0.0 <C> 0.7 <C> 2.1 <C> 0.5 <C> 1.1 <C> 2.4 <C> 0.0 <C> 0.5 <C> 1.6 <C> 0.4 <C> 0.8 <C> 1.8 <R> <C> [BOLD] Cumulative <C> 1.7 <C> 8.3 <C> 13.8 <C> 2.9 <C> 9.6 <C> 15.2 <C> 1.5 <C> 7.4 <C> 12.2 <C> 2.5 <C> 8.5 <C> 13.4 <R> <C> [BOLD] Incr. static <C> 54.9 <C> 82.8 <C> 90.1 <C> 60.4 <C> 79.6 <C> 84.8 <C> 20.8 <C> 31.5 <C> 34.2 <C> 23.0 <C> 30.3 <C> 32.2 <R> <C> [BOLD] Incr. dynamic <C> 32.5 <C> 64.5 <C> 72.2 <C> 42.6 <C> 64.8 <C> 71.5 <C> 28.1 <C> 56.1 <C> [BOLD] 62.9 <C> [BOLD] 37.3 <C> [BOLD] 56.7 <C> 62.6 <CAP> Table 3: Average accuracies of predicting next-year insurgents on the basis of locations, using projections trained on the conflicts from all the preceding years (up-to-now) or the preceding year only (previous). Results for 3 baselines are shown along with the proposed incremental dynamic approach. <COT> Looking at the "Incr. dynamic" row, "62.9" cell, and "All pairs, including OOV up-to-now" cell, we can see that the average accuracy of predicting next-year insurgents based on locations using the incremental dynamic approach with all pairs, including OOV, up to now is 62.9.
<R> <C> [BOLD] Model <C> [BOLD] TE3‡  [BOLD] P <C> [BOLD] TE3‡  [BOLD] R <C> [BOLD] TE3‡  [BOLD] F <C> [BOLD] TD‡  [BOLD] P <C> [BOLD] TD‡  [BOLD] R <C> [BOLD] TD‡  [BOLD] F <R> <C> [ITALIC] Indirect:  [ITALIC] O( [ITALIC] n2) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> TL2RTL ( [ITALIC] Lτ) <C> 53.5 <C> 51.1 <C> 52.3 <C> 59.1 <C> 61.2 <C> 60.1 <R> <C> TL2RTL ( [ITALIC] Lτce) <C> 53.9 <C> 51.7 <C> [BOLD] 52.8 <C> 61.2 <C> 60.7 <C> 60.9 <R> <C> TL2RTL ( [ITALIC] Lτh) <C> 52.8 <C> 51.1 <C> 51.9 <C> 57.9 <C> 60.6 <C> 59.2 <R> <C> TL2RTL ( [ITALIC] L∗) <C> 52.6 <C> 52.0 <C> 52.3 <C> 62.3 <C> 62.3 <C> [BOLD] 62.3 <R> <C> [ITALIC] Direct:  [ITALIC] O( [ITALIC] n) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> S-TLM ( [ITALIC] Lτ) <C> 50.1 <C> 50.4 <C> 50.2 <C> 57.8 <C> 59.5 <C> [BOLD] 58.6 <R> <C> S-TLM ( [ITALIC] Lτce) <C> 50.1 <C> 50.0 <C> 50.1 <C> 53.4 <C> 53.5 <C> 53.5 <R> <C> S-TLM ( [ITALIC] Lτh) <C> 51.5 <C> 51.7 <C> [BOLD] 51.6 <C> 55.1 <C> 56.4 <C> 55.7 <R> <C> S-TLM ( [ITALIC] L∗) <C> 50.9 <C> 51.0 <C> 51.0 <C> 56.5 <C> 55.3 <C> 55.9 <R> <C> C-TLM ( [ITALIC] Lτ) <C> 56.2 <C> 56.1 <C> [BOLD] 56.1 <C> 57.1 <C> 59.7 <C> [BOLD] 58.4 <R> <C> C-TLM ( [ITALIC] Lτce) <C> 54.4 <C> 55.4 <C> 54.9 <C> 52.4 <C> 57.3 <C> 54.7 <R> <C> C-TLM ( [ITALIC] Lτh) <C> 55.7 <C> 55.5 <C> 55.6 <C> 55.3 <C> 54.9 <C> 55.1 <R> <C> C-TLM ( [ITALIC] L∗) <C> 54.0 <C> 54.3 <C> 54.1 <C> 54.6 <C> 53.5 <C> 54.1 <CAP> Table 4: Evaluation of relative time-lines for each model and loss function, where L∗ indicates the (unweighted) sum of Lτ, Lτce, and Lτh. <COT> Looking at the "Model" column, we can see that TL2RTL (L∗) has the highest TE3‡ P score and the highest TE3‡ F score among all models.
<R> <C> [BOLD] Model <C> [BOLD] TE3‡  [BOLD] P <C> [BOLD] TE3‡  [BOLD] R <C> [BOLD] TE3‡  [BOLD] F <C> [BOLD] TD‡  [BOLD] P <C> [BOLD] TD‡  [BOLD] R <C> [BOLD] TD‡  [BOLD] F <R> <C> [ITALIC] Indirect:  [ITALIC] O( [ITALIC] n2) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> TL2RTL ( [ITALIC] Lτ) <C> 53.5 <C> 51.1 <C> 52.3 <C> 59.1 <C> 61.2 <C> 60.1 <R> <C> TL2RTL ( [ITALIC] Lτce) <C> 53.9 <C> 51.7 <C> [BOLD] 52.8 <C> 61.2 <C> 60.7 <C> 60.9 <R> <C> TL2RTL ( [ITALIC] Lτh) <C> 52.8 <C> 51.1 <C> 51.9 <C> 57.9 <C> 60.6 <C> 59.2 <R> <C> TL2RTL ( [ITALIC] L∗) <C> 52.6 <C> 52.0 <C> 52.3 <C> 62.3 <C> 62.3 <C> [BOLD] 62.3 <R> <C> [ITALIC] Direct:  [ITALIC] O( [ITALIC] n) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> S-TLM ( [ITALIC] Lτ) <C> 50.1 <C> 50.4 <C> 50.2 <C> 57.8 <C> 59.5 <C> [BOLD] 58.6 <R> <C> S-TLM ( [ITALIC] Lτce) <C> 50.1 <C> 50.0 <C> 50.1 <C> 53.4 <C> 53.5 <C> 53.5 <R> <C> S-TLM ( [ITALIC] Lτh) <C> 51.5 <C> 51.7 <C> [BOLD] 51.6 <C> 55.1 <C> 56.4 <C> 55.7 <R> <C> S-TLM ( [ITALIC] L∗) <C> 50.9 <C> 51.0 <C> 51.0 <C> 56.5 <C> 55.3 <C> 55.9 <R> <C> C-TLM ( [ITALIC] Lτ) <C> 56.2 <C> 56.1 <C> [BOLD] 56.1 <C> 57.1 <C> 59.7 <C> [BOLD] 58.4 <R> <C> C-TLM ( [ITALIC] Lτce) <C> 54.4 <C> 55.4 <C> 54.9 <C> 52.4 <C> 57.3 <C> 54.7 <R> <C> C-TLM ( [ITALIC] Lτh) <C> 55.7 <C> 55.5 <C> 55.6 <C> 55.3 <C> 54.9 <C> 55.1 <R> <C> C-TLM ( [ITALIC] L∗) <C> 54.0 <C> 54.3 <C> 54.1 <C> 54.6 <C> 53.5 <C> 54.1 <CAP> Table 4: Evaluation of relative time-lines for each model and loss function, where L∗ indicates the (unweighted) sum of Lτ, Lτce, and Lτh. <COT> Looking at the "TL2RTL (Lτce)" row, we can see that it has a higher TE3‡ R score and a higher TE3‡ F score compared to TL2RTL (Lτ) and TL2RTL (Lτh).
<R> <C> [EMPTY] <C> [EMPTY] <C> All Onion all <C> All Onion half 1 <C> All Onion half 2 <C> eBay all <C> eBay half 1 <C> eBay half 2 <C> Illegal Onion all <C> Illegal Onion half 1 <C> Illegal Onion half 2 <C> Legal Onion all <C> Legal Onion half 1 <C> Legal Onion half 2 <R> <C> [EMPTY] <C> all <C> [EMPTY] <C> 0.23 <C> 0.25 <C> 0.60 <C> 0.61 <C> 0.61 <C> 0.33 <C> 0.39 <C> 0.41 <C> 0.35 <C> 0.41 <C> 0.42 <R> <C> All Onion <C> half 1 <C> 0.23 <C> [EMPTY] <C> 0.43 <C> 0.60 <C> 0.62 <C> 0.62 <C> 0.37 <C> 0.33 <C> 0.50 <C> 0.40 <C> 0.36 <C> 0.52 <R> <C> [EMPTY] <C> half 2 <C> 0.25 <C> 0.43 <C> [EMPTY] <C> 0.61 <C> 0.62 <C> 0.62 <C> 0.39 <C> 0.50 <C> 0.35 <C> 0.39 <C> 0.51 <C> 0.35 <R> <C> [EMPTY] <C> all <C> 0.60 <C> 0.60 <C> 0.61 <C> [EMPTY] <C> 0.23 <C> 0.25 <C> 0.59 <C> 0.60 <C> 0.60 <C> 0.66 <C> 0.67 <C> 0.67 <R> <C> eBay <C> half 1 <C> 0.61 <C> 0.62 <C> 0.62 <C> 0.23 <C> [EMPTY] <C> 0.43 <C> 0.60 <C> 0.61 <C> 0.61 <C> 0.67 <C> 0.67 <C> 0.68 <R> <C> [EMPTY] <C> half 2 <C> 0.61 <C> 0.62 <C> 0.62 <C> 0.25 <C> 0.43 <C> [EMPTY] <C> 0.60 <C> 0.61 <C> 0.61 <C> 0.67 <C> 0.68 <C> 0.68 <R> <C> [EMPTY] <C> all <C> 0.33 <C> 0.37 <C> 0.39 <C> 0.59 <C> 0.60 <C> 0.60 <C> [EMPTY] <C> 0.23 <C> 0.27 <C> 0.61 <C> 0.62 <C> 0.62 <R> <C> Illegal Onion <C> half 1 <C> 0.39 <C> 0.33 <C> 0.50 <C> 0.60 <C> 0.61 <C> 0.61 <C> 0.23 <C> [EMPTY] <C> 0.45 <C> 0.62 <C> 0.63 <C> 0.62 <R> <C> [EMPTY] <C> half 2 <C> 0.41 <C> 0.50 <C> 0.35 <C> 0.60 <C> 0.61 <C> 0.61 <C> 0.27 <C> 0.45 <C> [EMPTY] <C> 0.62 <C> 0.63 <C> 0.63 <R> <C> [EMPTY] <C> all <C> 0.35 <C> 0.40 <C> 0.39 <C> 0.66 <C> 0.67 <C> 0.67 <C> 0.61 <C> 0.62 <C> 0.62 <C> [EMPTY] <C> 0.26 <C> 0.26 <R> <C> Legal onion <C> half 1 <C> 0.41 <C> 0.36 <C> 0.51 <C> 0.67 <C> 0.67 <C> 0.68 <C> 0.62 <C> 0.63 <C> 0.63 <C> 0.26 <C> [EMPTY] <C> 0.47 <R> <C> [EMPTY] <C> half 2 <C> 0.42 <C> 0.52 <C> 0.35 <C> 0.67 <C> 0.68 <C> 0.68 <C> 0.62 <C> 0.62 <C> 0.63 <C> 0.26 <C> 0.47 <C> [EMPTY] <CAP> Table 1: Jensen-Shannon divergence between word distribution in all Onion drug sites, Legal and Illegal Onion drug sites, and eBay sites. Each domain was also split in half for within-domain comparison. <COT> Looking at the "All Onion all" cell, "0.61" cell, and "Legal Onion half 2" cell, we can see that the Jensen-Shannon divergence between word distribution in all Onion drug sites and Legal Onion drug sites is 0.61.
<R> <C> [EMPTY] <C> [EMPTY] <C> All Onion all <C> All Onion half 1 <C> All Onion half 2 <C> eBay all <C> eBay half 1 <C> eBay half 2 <C> Illegal Onion all <C> Illegal Onion half 1 <C> Illegal Onion half 2 <C> Legal Onion all <C> Legal Onion half 1 <C> Legal Onion half 2 <R> <C> [EMPTY] <C> all <C> [EMPTY] <C> 0.23 <C> 0.25 <C> 0.60 <C> 0.61 <C> 0.61 <C> 0.33 <C> 0.39 <C> 0.41 <C> 0.35 <C> 0.41 <C> 0.42 <R> <C> All Onion <C> half 1 <C> 0.23 <C> [EMPTY] <C> 0.43 <C> 0.60 <C> 0.62 <C> 0.62 <C> 0.37 <C> 0.33 <C> 0.50 <C> 0.40 <C> 0.36 <C> 0.52 <R> <C> [EMPTY] <C> half 2 <C> 0.25 <C> 0.43 <C> [EMPTY] <C> 0.61 <C> 0.62 <C> 0.62 <C> 0.39 <C> 0.50 <C> 0.35 <C> 0.39 <C> 0.51 <C> 0.35 <R> <C> [EMPTY] <C> all <C> 0.60 <C> 0.60 <C> 0.61 <C> [EMPTY] <C> 0.23 <C> 0.25 <C> 0.59 <C> 0.60 <C> 0.60 <C> 0.66 <C> 0.67 <C> 0.67 <R> <C> eBay <C> half 1 <C> 0.61 <C> 0.62 <C> 0.62 <C> 0.23 <C> [EMPTY] <C> 0.43 <C> 0.60 <C> 0.61 <C> 0.61 <C> 0.67 <C> 0.67 <C> 0.68 <R> <C> [EMPTY] <C> half 2 <C> 0.61 <C> 0.62 <C> 0.62 <C> 0.25 <C> 0.43 <C> [EMPTY] <C> 0.60 <C> 0.61 <C> 0.61 <C> 0.67 <C> 0.68 <C> 0.68 <R> <C> [EMPTY] <C> all <C> 0.33 <C> 0.37 <C> 0.39 <C> 0.59 <C> 0.60 <C> 0.60 <C> [EMPTY] <C> 0.23 <C> 0.27 <C> 0.61 <C> 0.62 <C> 0.62 <R> <C> Illegal Onion <C> half 1 <C> 0.39 <C> 0.33 <C> 0.50 <C> 0.60 <C> 0.61 <C> 0.61 <C> 0.23 <C> [EMPTY] <C> 0.45 <C> 0.62 <C> 0.63 <C> 0.62 <R> <C> [EMPTY] <C> half 2 <C> 0.41 <C> 0.50 <C> 0.35 <C> 0.60 <C> 0.61 <C> 0.61 <C> 0.27 <C> 0.45 <C> [EMPTY] <C> 0.62 <C> 0.63 <C> 0.63 <R> <C> [EMPTY] <C> all <C> 0.35 <C> 0.40 <C> 0.39 <C> 0.66 <C> 0.67 <C> 0.67 <C> 0.61 <C> 0.62 <C> 0.62 <C> [EMPTY] <C> 0.26 <C> 0.26 <R> <C> Legal onion <C> half 1 <C> 0.41 <C> 0.36 <C> 0.51 <C> 0.67 <C> 0.67 <C> 0.68 <C> 0.62 <C> 0.63 <C> 0.63 <C> 0.26 <C> [EMPTY] <C> 0.47 <R> <C> [EMPTY] <C> half 2 <C> 0.42 <C> 0.52 <C> 0.35 <C> 0.67 <C> 0.68 <C> 0.68 <C> 0.62 <C> 0.62 <C> 0.63 <C> 0.26 <C> 0.47 <C> [EMPTY] <CAP> Table 1: Jensen-Shannon divergence between word distribution in all Onion drug sites, Legal and Illegal Onion drug sites, and eBay sites. Each domain was also split in half for within-domain comparison. <COT> Looking at the "eBay all" cell, "0.61" cell, and "Illegal Onion half 1" cell, we can see that the Jensen-Shannon divergence between word distribution in eBay sites and Illegal Onion drug sites is 0.61.
<R> <C> [EMPTY] <C> % Wikifiable <R> <C> eBay <C> 38.6±2.00 <R> <C> Illegal Onion <C> 32.5±1.35 <R> <C> Legal Onion <C> 50.8±2.31 <CAP> Table 2: Average percentage of wikifiable named entities in a website per domain, with standard error. <COT> Looking at the table, we can see that the average percentage of wikifiable named entities is higher in Legal Onion websites compared to eBay and Illegal Onion websites.
<R> <C> [EMPTY] <C> % Wikifiable <R> <C> eBay <C> 38.6±2.00 <R> <C> Illegal Onion <C> 32.5±1.35 <R> <C> Legal Onion <C> 50.8±2.31 <CAP> Table 2: Average percentage of wikifiable named entities in a website per domain, with standard error. <COT> Looking at the table, we can see that the standard error is the highest for Legal Onion websites compared to eBay and Illegal Onion websites.
<R> <C> [EMPTY] <C> All <C> S2 <C> S3 <C> S07 <C> S13 <C> S15 <R> <C> UKB (this work) <C> [BOLD] 67.3 <C> 68.8 <C> 66.1 <C> 53.0 <C> [BOLD] 68.8 <C> [BOLD] 70.3 <R> <C> UKB (elsewhere)†‡ <C> 57.5 <C> 60.6 <C> 54.1 <C> 42.0 <C> 59.0 <C> 61.2 <R> <C> Chaplot and Sakajhutdinov ( 2018 ) ‡ <C> 66.9 <C> [BOLD] 69.0 <C> [BOLD] 66.9 <C> 55.6 <C> 65.3 <C> 69.6 <R> <C> Babelfy Moro et al. ( 2014 )† <C> 65.5 <C> 67.0 <C> 63.5 <C> 51.6 <C> 66.4 <C> 70.3 <R> <C> MFS <C> 65.2 <C> 66.8 <C> 66.2 <C> 55.2 <C> 63.0 <C> 67.8 <R> <C> Basile et al. ( 2014 )† <C> 63.7 <C> 63.0 <C> 63.7 <C> [BOLD] 56.7 <C> 66.2 <C> 64.6 <R> <C> Banerjee and Pedersen ( 2003 )† <C> 48.7 <C> 50.6 <C> 44.5 <C> 32.0 <C> 53.6 <C> 51.0 <CAP> Table 1: F1 results for knowledge-based systems on the Raganato et al. (2017a) dataset. Top rows show conflicting results for UKB. † for results reported in Raganato et al. (2017a), ‡ for results reported in Chaplot and Sakajhutdinov (2018). Best results in bold. S2 stands for Senseval-2, S3 for Senseval-3, S07 for Semeval-2007, S13 for Semeval-2013 and S15 for Semeval-2015. <COT> Looking at the "Basile et al. (2014)" row, we can see that the F1 result for S07 dataset is the lowest among all the datasets.
<R> <C> [EMPTY] <C> All <C> S2 <C> S3 <C> S07 <C> S13 <C> S15 <R> <C> Yuan et al. ( 2016 ) <C> [BOLD] 71.5 <C> [BOLD] 73.8 <C> [BOLD] 71.8 <C> 63.5 <C> [BOLD] 69.5 <C> [BOLD] 72.6 <R> <C> Raganato et al. ( 2017b ) <C> 69.9 <C> 72.0 <C> 69.1 <C> [BOLD] 64.8 <C> 66.9 <C> 71.5 <R> <C> Iacobacci et al. ( 2016 )† <C> 69.7 <C> 73.3 <C> 69.6 <C> 61.1 <C> 66.7 <C> 70.4 <R> <C> Melamud et al. ( 2016 )† <C> 69.4 <C> 72.3 <C> 68.2 <C> 61.5 <C> 67.2 <C> 71.7 <R> <C> IMS Zhong and Ng ( 2010 )† <C> 68.8 <C> 72.8 <C> 69.2 <C> 60.0 <C> 65.0 <C> 69.3 <CAP> Table 2: F1 results for supervised systems on the Raganato et al. (2017a) dataset. † for results reported in Raganato et al. (2017a). Best results in bold. Note that Raganato et al. (2017b) used S07 for development. <COT> Looking at the "CAP" tag, we can see that Table 2 shows the F1 results for supervised systems on the Raganato et al. (2017a) dataset.
<R> <C> [EMPTY] <C> All <C> S2 <C> S3 <C> S07 <C> S13 <C> S15 <R> <C> Yuan et al. ( 2016 ) <C> [BOLD] 71.5 <C> [BOLD] 73.8 <C> [BOLD] 71.8 <C> 63.5 <C> [BOLD] 69.5 <C> [BOLD] 72.6 <R> <C> Raganato et al. ( 2017b ) <C> 69.9 <C> 72.0 <C> 69.1 <C> [BOLD] 64.8 <C> 66.9 <C> 71.5 <R> <C> Iacobacci et al. ( 2016 )† <C> 69.7 <C> 73.3 <C> 69.6 <C> 61.1 <C> 66.7 <C> 70.4 <R> <C> Melamud et al. ( 2016 )† <C> 69.4 <C> 72.3 <C> 68.2 <C> 61.5 <C> 67.2 <C> 71.7 <R> <C> IMS Zhong and Ng ( 2010 )† <C> 68.8 <C> 72.8 <C> 69.2 <C> 60.0 <C> 65.0 <C> 69.3 <CAP> Table 2: F1 results for supervised systems on the Raganato et al. (2017a) dataset. † for results reported in Raganato et al. (2017a). Best results in bold. Note that Raganato et al. (2017b) used S07 for development. <COT> Looking at the "Yuan et al. (2016)" row, we can see that the highest F1 scores are bolded in the "All", "S2", "S3", "S15" columns.
<R> <C> [EMPTY] <C> All <C> S2 <C> S3 <C> S07 <C> S13 <C> S15 <R> <C> Single context sentence <C> Single context sentence <C> Single context sentence <C> Single context sentence <C> Single context sentence <C> Single context sentence <C> Single context sentence <R> <C> ppr_w2w <C> 66.9 <C> [BOLD] 69.0 <C> 65.7 <C> 53.9 <C> 67.1 <C> 69.9 <R> <C> dfs_ppr <C> 65.2 <C> 67.5 <C> 65.6 <C> 53.6 <C> 62.7 <C> 68.2 <R> <C> ppr <C> 65.5 <C> 67.5 <C> [BOLD] 66.5 <C> [BOLD] 54.7 <C> 63.3 <C> 67.4 <R> <C> ppr_w2wnf <C> 60.2 <C> 63.7 <C> 55.1 <C> 42.2 <C> 63.5 <C> 63.8 <R> <C> pprnf <C> 57.1 <C> 60.5 <C> 53.8 <C> 41.3 <C> 58.0 <C> 61.4 <R> <C> dfsnf <C> 58.7 <C> 63.3 <C> 52.8 <C> 40.4 <C> 61.6 <C> 62.5 <R> <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <R> <C> ppr_w2w <C> [BOLD] 67.3 <C> 68.8 <C> 66.1 <C> 53.0 <C> [BOLD] 68.8 <C> [BOLD] 70.3 <R> <C> ppr <C> 65.6 <C> 67.5 <C> 66.4 <C> 54.1 <C> 64.0 <C> 67.8 <R> <C> dfs <C> 65.7 <C> 67.9 <C> 65.9 <C> 54.5 <C> 64.2 <C> 68.1 <R> <C> ppr_w2wnf <C> 60.4 <C> 64.2 <C> 54.8 <C> 40.0 <C> 64.5 <C> 64.5 <R> <C> pprnf <C> 58.6 <C> 61.3 <C> 54.9 <C> 42.2 <C> 60.9 <C> 62.9 <R> <C> dfsnf <C> 59.1 <C> 62.7 <C> 54.4 <C> 39.3 <C> 62.8 <C> 62.2 <CAP> Table 3: Additional results on other settings of UKB. nf subscript stands for “no sense frequency”. Top rows use a single sentence as context, while the bottom rows correspond to extended context (cf. Sect. 3). Best results in bold. <COT> looking at the "CAP" tag, finding that Table 3 provides additional results on other settings of UKB.
<R> <C> [EMPTY] <C> All <C> S2 <C> S3 <C> S07 <C> S13 <C> S15 <R> <C> Single context sentence <C> Single context sentence <C> Single context sentence <C> Single context sentence <C> Single context sentence <C> Single context sentence <C> Single context sentence <R> <C> ppr_w2w <C> 66.9 <C> [BOLD] 69.0 <C> 65.7 <C> 53.9 <C> 67.1 <C> 69.9 <R> <C> dfs_ppr <C> 65.2 <C> 67.5 <C> 65.6 <C> 53.6 <C> 62.7 <C> 68.2 <R> <C> ppr <C> 65.5 <C> 67.5 <C> [BOLD] 66.5 <C> [BOLD] 54.7 <C> 63.3 <C> 67.4 <R> <C> ppr_w2wnf <C> 60.2 <C> 63.7 <C> 55.1 <C> 42.2 <C> 63.5 <C> 63.8 <R> <C> pprnf <C> 57.1 <C> 60.5 <C> 53.8 <C> 41.3 <C> 58.0 <C> 61.4 <R> <C> dfsnf <C> 58.7 <C> 63.3 <C> 52.8 <C> 40.4 <C> 61.6 <C> 62.5 <R> <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <C> One or more context sentences (# [ITALIC] words≥20) <R> <C> ppr_w2w <C> [BOLD] 67.3 <C> 68.8 <C> 66.1 <C> 53.0 <C> [BOLD] 68.8 <C> [BOLD] 70.3 <R> <C> ppr <C> 65.6 <C> 67.5 <C> 66.4 <C> 54.1 <C> 64.0 <C> 67.8 <R> <C> dfs <C> 65.7 <C> 67.9 <C> 65.9 <C> 54.5 <C> 64.2 <C> 68.1 <R> <C> ppr_w2wnf <C> 60.4 <C> 64.2 <C> 54.8 <C> 40.0 <C> 64.5 <C> 64.5 <R> <C> pprnf <C> 58.6 <C> 61.3 <C> 54.9 <C> 42.2 <C> 60.9 <C> 62.9 <R> <C> dfsnf <C> 59.1 <C> 62.7 <C> 54.4 <C> 39.3 <C> 62.8 <C> 62.2 <CAP> Table 3: Additional results on other settings of UKB. nf subscript stands for “no sense frequency”. Top rows use a single sentence as context, while the bottom rows correspond to extended context (cf. Sect. 3). Best results in bold. <COT> looking at the "One or more context sentences (# [ITALIC] words≥20)" cell, "[BOLD] 70.3" cell, "S15" cell, finding that the ppr_w2w model performs the best on the S15 dataset when there is one or more context sentences with words greater than or equal to 20.
<R> <C> [BOLD] Models <C> [BOLD] SWBD <C> [BOLD] SWBD2 <R> <C> BoW + Logsitic <C> 78.95 <C> 87.76 <R> <C> BoW + SVM <C> 73.68 <C> [BOLD] 90.82 <R> <C> Bigram + SVM <C> 52.63 <C> 79.59 <R> <C> BoW + TF-IDF + Logistic <C> 52.63 <C> 81.63 <R> <C> nGram + Logistic <C> 52.63 <C> 78.57 <R> <C> nGram + TF-IDF + Logistic <C> 57.89 <C> 87.76 <R> <C> Bag of Means + Logistic <C> 78.95 <C> 87.76 <R> <C> Avg. Skipgram + Logistic <C> 26.32 <C> 59.18 <R> <C> Doc2Vec + SVM <C> 73.68 <C> 86.73 <R> <C> HN <C> 31.58 <C> 54.08 <R> <C> HN-ATT Yang et al. ( 2016 ) <C> 73.68 <C> 85.71 <R> <C> CNN Kim ( 2014 ) <C> 84.21 <C> 93.87 <R> <C> HN-SA (our model) <C> [BOLD] 89.47 <C> [BOLD] 95.92 <CAP> Table 2: Accuracy (in %) of our model and other text classification models on both versions of SWBD. <COT> Looking at the "Models" column, we can see that the "HN-SA (our model)" has the highest accuracy for both SWBD and SWBD2.
<R> <C> [BOLD] Models <C> [BOLD] SWBD <C> [BOLD] SWBD2 <R> <C> BoW + Logsitic <C> 78.95 <C> 87.76 <R> <C> BoW + SVM <C> 73.68 <C> [BOLD] 90.82 <R> <C> Bigram + SVM <C> 52.63 <C> 79.59 <R> <C> BoW + TF-IDF + Logistic <C> 52.63 <C> 81.63 <R> <C> nGram + Logistic <C> 52.63 <C> 78.57 <R> <C> nGram + TF-IDF + Logistic <C> 57.89 <C> 87.76 <R> <C> Bag of Means + Logistic <C> 78.95 <C> 87.76 <R> <C> Avg. Skipgram + Logistic <C> 26.32 <C> 59.18 <R> <C> Doc2Vec + SVM <C> 73.68 <C> 86.73 <R> <C> HN <C> 31.58 <C> 54.08 <R> <C> HN-ATT Yang et al. ( 2016 ) <C> 73.68 <C> 85.71 <R> <C> CNN Kim ( 2014 ) <C> 84.21 <C> 93.87 <R> <C> HN-SA (our model) <C> [BOLD] 89.47 <C> [BOLD] 95.92 <CAP> Table 2: Accuracy (in %) of our model and other text classification models on both versions of SWBD. <COT> Looking at the "SWBD2" column, we can see that the "BoW + SVM" model has the highest accuracy for SWBD2.
<R> <C> [BOLD] Approach <C> [BOLD] Dev <C> [BOLD] Test <R> <C> Seq2Seq <C> 1.9% <C> 3.7% <R> <C> Seq2Seq + Attention <C> 1.8% <C> 4.8% <R> <C> Seq2Seq + Copying <C> 4.1% <C> 5.3% <R> <C> TypeSQL <C> 8.0% <C> 8.2% <R> <C> SQLNet <C> 10.9% <C> 12.4% <R> <C> SyntaxSQLNet <C> 18.9% <C> 19.7% <R> <C> SyntaxSQLNet(augment) <C> 24.8% <C> 27.2% <R> <C> [BOLD] IRNet <C> [BOLD] 53.2% <C> [BOLD] 46.7% <R> <C> [BOLD] BERT <C> [EMPTY] <C> [EMPTY] <R> <C> SyntaxSQLNet(BERT) <C> 25.0% <C> 25.4% <R> <C> [BOLD] IRNet(BERT) <C> [BOLD] 61.9% <C> [BOLD] 54.7% <CAP> Table 1: Exact matching accuracy on SQL queries. <COT> Looking at the "Approach" column, we can see that the accuracy of IRNet is higher than the accuracy of SyntaxSQLNet and SyntaxSQLNet(augment) on both the development and test sets.
<R> <C> [BOLD] Approach <C> [BOLD] Dev <C> [BOLD] Test <R> <C> Seq2Seq <C> 1.9% <C> 3.7% <R> <C> Seq2Seq + Attention <C> 1.8% <C> 4.8% <R> <C> Seq2Seq + Copying <C> 4.1% <C> 5.3% <R> <C> TypeSQL <C> 8.0% <C> 8.2% <R> <C> SQLNet <C> 10.9% <C> 12.4% <R> <C> SyntaxSQLNet <C> 18.9% <C> 19.7% <R> <C> SyntaxSQLNet(augment) <C> 24.8% <C> 27.2% <R> <C> [BOLD] IRNet <C> [BOLD] 53.2% <C> [BOLD] 46.7% <R> <C> [BOLD] BERT <C> [EMPTY] <C> [EMPTY] <R> <C> SyntaxSQLNet(BERT) <C> 25.0% <C> 25.4% <R> <C> [BOLD] IRNet(BERT) <C> [BOLD] 61.9% <C> [BOLD] 54.7% <CAP> Table 1: Exact matching accuracy on SQL queries. <COT> Looking at the "IRNet(BERT)" row, we can see that the accuracy of IRNet(BERT) is higher than the accuracy of IRNet on both the development and test sets.
<R> <C> [BOLD] Approach <C> [BOLD] Easy <C> [BOLD] Medium <C> [BOLD] Hard <C> [BOLD] Extra <R> <C> [BOLD] Approach <C> [BOLD] Easy <C> [BOLD] Medium <C> [BOLD] Hard <C> [BOLD] Hard <R> <C> SyntaxSQLNet <C> 38.6% <C> 17.6% <C> 16.3% <C> 4.9% <R> <C> SyntaxSQLNet <C> 42.9% <C> 24.9% <C> 21.9% <C> 8.6% <R> <C> (BERT) <C> 42.9% <C> 24.9% <C> 21.9% <C> 8.6% <R> <C> [BOLD] IRNet <C> [BOLD] 70.1% <C> [BOLD] 49.2% <C> [BOLD] 39.5% <C> [BOLD] 19.1% <R> <C> [BOLD] IRNet(BERT) <C> [BOLD] 77.2% <C> [BOLD] 58.7% <C> [BOLD] 48.1% <C> [BOLD] 25.3% <CAP> Table 2: Exact matching accuracy of SyntaxSQLNet, SyntaxSQLNet(BERT), IRNet and IRNet(BERT) on test set by hardness level. <COT> Looking at the "CAP" tag, we can see that Table 2 provides the exact matching accuracy of SyntaxSQLNet, SyntaxSQLNet(BERT), IRNet, and IRNet(BERT) on the test set by hardness level.
<R> <C> [BOLD] Approach <C> [BOLD] Easy <C> [BOLD] Medium <C> [BOLD] Hard <C> [BOLD] Extra <R> <C> [BOLD] Approach <C> [BOLD] Easy <C> [BOLD] Medium <C> [BOLD] Hard <C> [BOLD] Hard <R> <C> SyntaxSQLNet <C> 38.6% <C> 17.6% <C> 16.3% <C> 4.9% <R> <C> SyntaxSQLNet <C> 42.9% <C> 24.9% <C> 21.9% <C> 8.6% <R> <C> (BERT) <C> 42.9% <C> 24.9% <C> 21.9% <C> 8.6% <R> <C> [BOLD] IRNet <C> [BOLD] 70.1% <C> [BOLD] 49.2% <C> [BOLD] 39.5% <C> [BOLD] 19.1% <R> <C> [BOLD] IRNet(BERT) <C> [BOLD] 77.2% <C> [BOLD] 58.7% <C> [BOLD] 48.1% <C> [BOLD] 25.3% <CAP> Table 2: Exact matching accuracy of SyntaxSQLNet, SyntaxSQLNet(BERT), IRNet and IRNet(BERT) on test set by hardness level. <COT> Looking at the "IRNet(BERT)" row, we can see that the accuracy percentages for the different hardness levels are higher for IRNet(BERT) compared to IRNet.
<R> <C> [BOLD] Approach <C> [BOLD] SQL <C> [BOLD] SemQL <R> <C> Seq2Seq <C> 1.9% <C> 11.4%( [BOLD] +9.5) <R> <C> Seq2Seq + Attention <C> 1.8% <C> 14.7%( [BOLD] +12.9) <R> <C> Seq2Seq + Copying <C> 4.1% <C> 18.5%( [BOLD] +14.1) <R> <C> TypeSQL <C> 8.0% <C> 14.4%( [BOLD] +6.4) <R> <C> SQLNet <C> 10.9% <C> 17.5%( [BOLD] +6.6) <R> <C> SyntaxSQLNet <C> 18.9% <C> 27.5%( [BOLD] +8.6) <R> <C> [BOLD] BERT <C> [EMPTY] <C> [EMPTY] <R> <C> SyntaxSQLNet(BERT) <C> 25.0% <C> 35.8%( [BOLD] +10.8) <CAP> Table 3: Exact matching accuracy on development set. The header ‘SQL’ means that the approaches are learned to generate SQL, while the header ‘SemQL’ indicates that they are learned to generate SemQL queries. <COT> Looking at the "Approach" column, we can see that the "SyntaxSQLNet(BERT)" approach has the highest accuracy for generating SemQL queries.
<R> <C> [BOLD] Approach <C> [BOLD] SQL <C> [BOLD] SemQL <R> <C> Seq2Seq <C> 1.9% <C> 11.4%( [BOLD] +9.5) <R> <C> Seq2Seq + Attention <C> 1.8% <C> 14.7%( [BOLD] +12.9) <R> <C> Seq2Seq + Copying <C> 4.1% <C> 18.5%( [BOLD] +14.1) <R> <C> TypeSQL <C> 8.0% <C> 14.4%( [BOLD] +6.4) <R> <C> SQLNet <C> 10.9% <C> 17.5%( [BOLD] +6.6) <R> <C> SyntaxSQLNet <C> 18.9% <C> 27.5%( [BOLD] +8.6) <R> <C> [BOLD] BERT <C> [EMPTY] <C> [EMPTY] <R> <C> SyntaxSQLNet(BERT) <C> 25.0% <C> 35.8%( [BOLD] +10.8) <CAP> Table 3: Exact matching accuracy on development set. The header ‘SQL’ means that the approaches are learned to generate SQL, while the header ‘SemQL’ indicates that they are learned to generate SemQL queries. <COT> Looking at the "SemQL" column, we can see that the accuracy of the "Seq2Seq + Copying" approach is 18.5%, which is 14.1% higher than the accuracy of the "Seq2Seq" approach.
<R> <C> Model <C> FEVER Dev base <C> FEVER Dev r.w <C> Generated base <C> Generated r.w <R> <C> NSMN <C> 81.8 <C> - <C> 58.7 <C> - <R> <C> ESIM <C> 80.8 <C> 76.0 <C> 55.9 <C> 59.3 <R> <C> BERT <C> [BOLD] 86.2 <C> 84.6 <C> 58.3 <C> [BOLD] 61.6 <CAP> Table 3: Classifiers’ accuracy on the Supports and Refutes cases from the FEVER Dev set and on the Generated pairs for the Symmetric Test Set in the setting of without (base) and with (r.w) re-weight. <COT> Looking at the "Model" column, we can see that the BERT model has the highest accuracy on both the FEVER Dev base and Generated r.w cases.
<R> <C> Model <C> FEVER Dev base <C> FEVER Dev r.w <C> Generated base <C> Generated r.w <R> <C> NSMN <C> 81.8 <C> - <C> 58.7 <C> - <R> <C> ESIM <C> 80.8 <C> 76.0 <C> 55.9 <C> 59.3 <R> <C> BERT <C> [BOLD] 86.2 <C> 84.6 <C> 58.3 <C> [BOLD] 61.6 <CAP> Table 3: Classifiers’ accuracy on the Supports and Refutes cases from the FEVER Dev set and on the Generated pairs for the Symmetric Test Set in the setting of without (base) and with (r.w) re-weight. <COT> Looking at the "Generated r.w" column, we can see that the BERT model has a higher accuracy than the ESIM model.
<R> <C> [BOLD] Bigram <C> [BOLD] Train  [BOLD] LMI ⋅10−6 <C> [BOLD] Train  [ITALIC] p( [ITALIC] l| [ITALIC] w) <C> [BOLD] Development  [BOLD] LMI ⋅10−6 <C> [BOLD] Development  [ITALIC] p( [ITALIC] l| [ITALIC] w) <R> <C> united states <C> 271 <C> 0.64 <C> 268 <C> 0.44 <R> <C> least one <C> 269 <C> 0.90 <C> 267 <C> 0.77 <R> <C> at least <C> 256 <C> 0.72 <C> 163 <C> 0.48 <R> <C> person who <C> 162 <C> 0.90 <C> 135 <C> 0.61 <R> <C> stars actor <C> 143 <C> 0.86 <C> 111 <C> 0.71 <R> <C> won award <C> 133 <C> 0.80 <C> 50 <C> 0.56 <R> <C> american actor <C> 126 <C> 0.79 <C> 55 <C> 0.45 <R> <C> starred movie <C> 100 <C> 0.88 <C> 34 <C> 0.80 <R> <C> from united <C> 100 <C> 0.82 <C> 108 <C> 0.67 <R> <C> from america <C> 96 <C> 0.89 <C> 108 <C> 0.74 <CAP> Table 7: Top 10 LMI-ranked bigrams in the train set of FEVER for Support. <COT> Looking at the "Train LMI ⋅10−6" column, we can see that the bigram "united states" has a higher LMI score than the bigram "from america" in the train set.
<R> <C> System <C> EM <C> F1 <R> <C> [ITALIC] Mnewsqa <C> 46.3 <C> 60.8 <R> <C> [ITALIC] Mnewsqa +  [ITALIC] Snet <C> 47.9 <C> 61.5 <CAP> Table 3: NewsQA to SQuAD. Exact match (EM) and span F1 results on SQuAD development set of a NewsQA BIDAF model baseline vs. one finetuned on SQuAD using the data generated by a 2-stage SynNet (Snet). <COT> looking at the "System" column, we can see that there are two systems mentioned: "Mnewsqa" and "Mnewsqa + Snet".
<R> <C> System <C> EM <C> F1 <R> <C> [ITALIC] Mnewsqa <C> 46.3 <C> 60.8 <R> <C> [ITALIC] Mnewsqa +  [ITALIC] Snet <C> 47.9 <C> 61.5 <CAP> Table 3: NewsQA to SQuAD. Exact match (EM) and span F1 results on SQuAD development set of a NewsQA BIDAF model baseline vs. one finetuned on SQuAD using the data generated by a 2-stage SynNet (Snet). <COT> looking at the "EM" and "F1" columns, we can see that the values for "Mnewsqa + Snet" are higher than those for "Mnewsqa".
<R> <C> Model <C> Dataset 1  [ITALIC] vs. Simulator <C> Dataset 1  [ITALIC] vs. Simulator <C> Dataset 1  [ITALIC] vs. Human <C> Dataset 1  [ITALIC] vs. Human <C> Dataset 2  [ITALIC] vs. Simulator <C> Dataset 2  [ITALIC] vs. Simulator <C> Dataset 2  [ITALIC] vs. Human <C> Dataset 2  [ITALIC] vs. Human <R> <C> [EMPTY] <C> % Achieved <C> # Turns <C> % Achieved <C> # Turns <C> % Achieved <C> # Turns <C> % Achieved <C> # Turns <R> <C> Seq2Seq(goal) <C> 76.00 <C> 4.74 <C> 67.74 <C> 7.87 <C> 67.10 <C> 7.38 <C> 54.1 <C> 7.56 <R> <C> Seq2Seq(goal+state) <C> 79.41 <C> 4.74 <C> 70.97 <C> 6.35 <C> 67.37 <C> 7.42 <C> 58.1 <C> 8.04 <R> <C> Seq2Seq(goal+look) <C> 80.64 <C> 6.54 <C> 74.19 <C> 5.41 <C> 83.54 <C> [BOLD] 5.82 <C> 60.3 <C> [BOLD] 6.94 <R> <C> Seq2Seq(goal+look+state) <C> [BOLD] 85.07 <C> [BOLD] 4.10 <C> [BOLD] 77.42 <C> [BOLD] 5.02 <C> [BOLD] 83.58 <C> 6.36 <C> [BOLD] 61.2 <C> 7.30 <CAP> Table 3: Performance on two datasets against the user simulator and human. <COT> Looking at the "Dataset 1 vs. Simulator" column, we can see that the percentage achieved by Seq2Seq(goal+look+state) is higher than Seq2Seq(goal+look) and Seq2Seq(goal+state).
<R> <C> Model <C> Dataset 1  [ITALIC] vs. Simulator <C> Dataset 1  [ITALIC] vs. Simulator <C> Dataset 1  [ITALIC] vs. Human <C> Dataset 1  [ITALIC] vs. Human <C> Dataset 2  [ITALIC] vs. Simulator <C> Dataset 2  [ITALIC] vs. Simulator <C> Dataset 2  [ITALIC] vs. Human <C> Dataset 2  [ITALIC] vs. Human <R> <C> [EMPTY] <C> % Achieved <C> # Turns <C> % Achieved <C> # Turns <C> % Achieved <C> # Turns <C> % Achieved <C> # Turns <R> <C> Seq2Seq(goal) <C> 76.00 <C> 4.74 <C> 67.74 <C> 7.87 <C> 67.10 <C> 7.38 <C> 54.1 <C> 7.56 <R> <C> Seq2Seq(goal+state) <C> 79.41 <C> 4.74 <C> 70.97 <C> 6.35 <C> 67.37 <C> 7.42 <C> 58.1 <C> 8.04 <R> <C> Seq2Seq(goal+look) <C> 80.64 <C> 6.54 <C> 74.19 <C> 5.41 <C> 83.54 <C> [BOLD] 5.82 <C> 60.3 <C> [BOLD] 6.94 <R> <C> Seq2Seq(goal+look+state) <C> [BOLD] 85.07 <C> [BOLD] 4.10 <C> [BOLD] 77.42 <C> [BOLD] 5.02 <C> [BOLD] 83.58 <C> 6.36 <C> [BOLD] 61.2 <C> 7.30 <CAP> Table 3: Performance on two datasets against the user simulator and human. <COT> Looking at the "Dataset 2 vs. Human" column, we can see that the number of turns for Seq2Seq(goal+look+state) is higher than Seq2Seq(goal+look) and Seq2Seq(goal+state).
<R> <C> [BOLD] Type  [BOLD] POS Tags <C> [BOLD] Type Noun <C> [BOLD] Zh⇒En  [BOLD] 21.0% <C> [BOLD] En⇒Fr 1.9% <C> [BOLD] En⇒Ja 0.7% <R> <C> [BOLD] POS Tags <C> Verb <C> 0.3% <C> [BOLD] 25.0% <C> 0.3% <R> <C> [BOLD] POS Tags <C> Adj. <C> 0.4% <C> 9.3% <C> 0.7% <R> <C> [BOLD] POS Tags <C> Prep. <C> 1.3% <C> 4.5% <C> [BOLD] 26.7% <R> <C> [BOLD] POS Tags <C> Dete. <C> 3.0% <C> 5.7% <C> 2.1% <R> <C> [BOLD] POS Tags <C> Punc. <C> 3.5% <C> [BOLD] 18.3% <C> [BOLD] 30.5% <R> <C> [BOLD] POS Tags <C> Others <C> 0.5% <C> 1.2% <C> 4.7% <R> <C> [BOLD] Fertility <C> ≥2 <C> [BOLD] 50.2% <C> [BOLD] 21.4% <C> [BOLD] 21.7% <R> <C> [BOLD] Fertility <C> 1 <C> [BOLD] 15.4% <C> 7.0% <C> 3.1% <R> <C> [BOLD] Fertility <C> (0,1) <C> 2.5% <C> 0.4% <C> 3.0% <R> <C> [BOLD] Fertility <C> 0 <C> 0.0% <C> 1.9% <C> 3.8% <R> <C> [BOLD] Syntactic <C> Low <C> 1.6% <C> 2.5% <C> 1.2% <R> <C> [BOLD] Syntactic <C> Middle <C> 0.3% <C> 0.8% <C> 1.4% <R> <C> [BOLD] Syntactic <C> High <C> 0.0% <C> 0.1% <C> 0.1% <CAP> Table 2: Correlation between Attribution word importance with POS tags, Fertility, and Syntactic Depth. Fertility can be categorized into 4 types: one-to-many (“≥2”), one-to-one (“1”), many-to-one (“(0,1)”), and null-aligned (“0”). Syntactic depth shows the depth of a word in the dependency tree. A lower tree depth indicates closer to the root node in the dependency tree, which might indicate a more important word. <COT> Looking at the "Type Noun" column, "Zh⇒En" cell, and "En⇒Fr" cell, we can see that the percentage of Type Noun in the Zh⇒En translation is higher than in the En⇒Fr translation.
<R> <C> [BOLD] Type  [BOLD] POS Tags <C> [BOLD] Type Noun <C> [BOLD] Zh⇒En  [BOLD] 21.0% <C> [BOLD] En⇒Fr 1.9% <C> [BOLD] En⇒Ja 0.7% <R> <C> [BOLD] POS Tags <C> Verb <C> 0.3% <C> [BOLD] 25.0% <C> 0.3% <R> <C> [BOLD] POS Tags <C> Adj. <C> 0.4% <C> 9.3% <C> 0.7% <R> <C> [BOLD] POS Tags <C> Prep. <C> 1.3% <C> 4.5% <C> [BOLD] 26.7% <R> <C> [BOLD] POS Tags <C> Dete. <C> 3.0% <C> 5.7% <C> 2.1% <R> <C> [BOLD] POS Tags <C> Punc. <C> 3.5% <C> [BOLD] 18.3% <C> [BOLD] 30.5% <R> <C> [BOLD] POS Tags <C> Others <C> 0.5% <C> 1.2% <C> 4.7% <R> <C> [BOLD] Fertility <C> ≥2 <C> [BOLD] 50.2% <C> [BOLD] 21.4% <C> [BOLD] 21.7% <R> <C> [BOLD] Fertility <C> 1 <C> [BOLD] 15.4% <C> 7.0% <C> 3.1% <R> <C> [BOLD] Fertility <C> (0,1) <C> 2.5% <C> 0.4% <C> 3.0% <R> <C> [BOLD] Fertility <C> 0 <C> 0.0% <C> 1.9% <C> 3.8% <R> <C> [BOLD] Syntactic <C> Low <C> 1.6% <C> 2.5% <C> 1.2% <R> <C> [BOLD] Syntactic <C> Middle <C> 0.3% <C> 0.8% <C> 1.4% <R> <C> [BOLD] Syntactic <C> High <C> 0.0% <C> 0.1% <C> 0.1% <CAP> Table 2: Correlation between Attribution word importance with POS tags, Fertility, and Syntactic Depth. Fertility can be categorized into 4 types: one-to-many (“≥2”), one-to-one (“1”), many-to-one (“(0,1)”), and null-aligned (“0”). Syntactic depth shows the depth of a word in the dependency tree. A lower tree depth indicates closer to the root node in the dependency tree, which might indicate a more important word. <COT> Looking at the "POS Tags" column, "Punc." row, and "En⇒Fr" cell, we can see that the percentage of Punc. in the En⇒Fr translation is higher than in the other translations.
<R> <C> Data <C> Method <C> SemEval-15 Acc. <C> SemEval-15 Macro-F1 <C> SemEval-16 Acc. <C> SemEval-16 Macro-F1 <R> <C> A <C> TDLSTM+ATT Tang et al. ( 2016a ) <C> 77.10 <C> 59.46 <C> 83.11 <C> 57.53 <R> <C> A <C> ATAE-LSTM Wang et al. ( 2016 ) <C> 78.48 <C> 62.84 <C> 83.77 <C> 61.71 <R> <C> A <C> MM Tang et al. ( 2016b ) <C> 77.89 <C> 59.52 <C> 83.04 <C> 57.91 <R> <C> A <C> RAM Chen et al. ( 2017 ) <C> 79.98 <C> 60.57 <C> 83.88 <C> 62.14 <R> <C> A <C> LSTM+SynATT+TarRep He et al. ( 2018a ) <C> 81.67 <C> 66.05 <C> 84.61 <C> 67.45 <R> <C> S+A <C> Semisupervised He et al. ( 2018b ) <C> 81.30 <C> [BOLD] 68.74 <C> 85.58 <C> 69.76 <R> <C> S <C> BiLSTM-104 Sentence Training <C> 80.24 ± 1.64 <C> 61.89 ± 0.94 <C> 80.89 ± 2.79 <C> 61.40 ± 2.49 <R> <C> S+A <C> BiLSTM-104 Sentence Training →Aspect Based Finetuning <C> 77.75 ± 2.09 <C> 60.83 ± 4.53 <C> 84.87± 0.31 <C> 61.87 ± 5.44 <R> <C> N <C> BiLSTM-XR-Dev Estimation <C> [BOLD] 83.31∗± 0.62 <C> 62.24 ± 0.66 <C> [BOLD] 87.68∗± 0.47 <C> 63.23 ± 1.81 <R> <C> N <C> BiLSTM-XR <C> [BOLD] 83.31∗± 0.77 <C> 64.42 ± 2.78 <C> [BOLD] 88.12∗± 0.24 <C> 68.60 ± 1.79 <R> <C> N+A <C> BiLSTM-XR →Aspect Based Finetuning <C> [BOLD] 83.44∗± 0.74 <C> [BOLD] 67.23 ± 1.42 <C> [BOLD] 87.66∗± 0.28 <C> [BOLD] 71.19†± 1.40 <CAP> Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p<0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from He et al. (2018a). Numbers for Semisupervised are from He et al. (2018b). <COT> Looking at the "SemEval-15 Acc." column, we can see that the BiLSTM-XR-Dev Estimation method has the highest accuracy with a score of [BOLD] 83.31∗± 0.62.
<R> <C> Data <C> Method <C> SemEval-15 Acc. <C> SemEval-15 Macro-F1 <C> SemEval-16 Acc. <C> SemEval-16 Macro-F1 <R> <C> A <C> TDLSTM+ATT Tang et al. ( 2016a ) <C> 77.10 <C> 59.46 <C> 83.11 <C> 57.53 <R> <C> A <C> ATAE-LSTM Wang et al. ( 2016 ) <C> 78.48 <C> 62.84 <C> 83.77 <C> 61.71 <R> <C> A <C> MM Tang et al. ( 2016b ) <C> 77.89 <C> 59.52 <C> 83.04 <C> 57.91 <R> <C> A <C> RAM Chen et al. ( 2017 ) <C> 79.98 <C> 60.57 <C> 83.88 <C> 62.14 <R> <C> A <C> LSTM+SynATT+TarRep He et al. ( 2018a ) <C> 81.67 <C> 66.05 <C> 84.61 <C> 67.45 <R> <C> S+A <C> Semisupervised He et al. ( 2018b ) <C> 81.30 <C> [BOLD] 68.74 <C> 85.58 <C> 69.76 <R> <C> S <C> BiLSTM-104 Sentence Training <C> 80.24 ± 1.64 <C> 61.89 ± 0.94 <C> 80.89 ± 2.79 <C> 61.40 ± 2.49 <R> <C> S+A <C> BiLSTM-104 Sentence Training →Aspect Based Finetuning <C> 77.75 ± 2.09 <C> 60.83 ± 4.53 <C> 84.87± 0.31 <C> 61.87 ± 5.44 <R> <C> N <C> BiLSTM-XR-Dev Estimation <C> [BOLD] 83.31∗± 0.62 <C> 62.24 ± 0.66 <C> [BOLD] 87.68∗± 0.47 <C> 63.23 ± 1.81 <R> <C> N <C> BiLSTM-XR <C> [BOLD] 83.31∗± 0.77 <C> 64.42 ± 2.78 <C> [BOLD] 88.12∗± 0.24 <C> 68.60 ± 1.79 <R> <C> N+A <C> BiLSTM-XR →Aspect Based Finetuning <C> [BOLD] 83.44∗± 0.74 <C> [BOLD] 67.23 ± 1.42 <C> [BOLD] 87.66∗± 0.28 <C> [BOLD] 71.19†± 1.40 <CAP> Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p<0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from He et al. (2018a). Numbers for Semisupervised are from He et al. (2018b). <COT> Looking at the "SemEval-15 Macro-F1" column, we can see that the Semisupervised method has the highest Macro-F1 score with a score of [BOLD] 68.74.
<R> <C> [BOLD] A) <C> EM <C> F1 <C> [BOLD] B) <C> EM <C> F1 <R> <C> k=0 <C> 27.2 <C> 40.5 <C> 2s +  [ITALIC] Aner <C> 22.8 <C> 36.1 <R> <C> k=2 <C> 29.8 <C> 43.9 <C> all +  [ITALIC] Aner <C> 27.2 <C> 40.5 <R> <C> k=4 <C> 30.4 <C> 44.3 <C> 2s +  [ITALIC] Aoracle <C> 31.3 <C> 45.2 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> all +  [ITALIC] Aoracle <C> 32.5 <C> 46.8 <CAP> Table 4: Ablation Studies. Exact match (EM) and span F1 results on NewsQA test set of a BIDAF model finetuned with a 2-stage SynNet. In study A, we vary k, the number of mini-batches from SQuAD for every batch in NewsQA. In study B, we set k=0, and vary the answer type and how much of the paragraph we use for question synthesis. 2−sent refers to using two sentences before answer span, while all refers to using the entire paragraph. Aner refers to using an NER system and Aor refers to using the human-annotated answers to generate questions. <COT> Looking at the "Table 4" caption, we can see that it provides results on the NewsQA test set for an ablation study on a BIDAF model. The study includes two different experiments, A and B, where different parameters are varied.
<R> <C> [BOLD] A) <C> EM <C> F1 <C> [BOLD] B) <C> EM <C> F1 <R> <C> k=0 <C> 27.2 <C> 40.5 <C> 2s +  [ITALIC] Aner <C> 22.8 <C> 36.1 <R> <C> k=2 <C> 29.8 <C> 43.9 <C> all +  [ITALIC] Aner <C> 27.2 <C> 40.5 <R> <C> k=4 <C> 30.4 <C> 44.3 <C> 2s +  [ITALIC] Aoracle <C> 31.3 <C> 45.2 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> all +  [ITALIC] Aoracle <C> 32.5 <C> 46.8 <CAP> Table 4: Ablation Studies. Exact match (EM) and span F1 results on NewsQA test set of a BIDAF model finetuned with a 2-stage SynNet. In study A, we vary k, the number of mini-batches from SQuAD for every batch in NewsQA. In study B, we set k=0, and vary the answer type and how much of the paragraph we use for question synthesis. 2−sent refers to using two sentences before answer span, while all refers to using the entire paragraph. Aner refers to using an NER system and Aor refers to using the human-annotated answers to generate questions. <COT> Looking at the "k=4" row, we can see that the values for EM and F1 are higher when using "all + Aoracle" compared to "2s + Aoracle".
<R> <C> [BOLD] Transfer learning schedule <C> [BOLD] es2en  [BOLD] Khresmoi <C> [BOLD] es2en  [BOLD] Health <C> [BOLD] es2en  [BOLD] Bio <C> [BOLD] en2es  [BOLD] Khresmoi <C> [BOLD] en2es  [BOLD] Health <C> [BOLD] en2es  [BOLD] Bio <R> <C> Health <C> 45.1 <C> 35.7 <C> 34.0 <C> 41.2 <C> 34.7 <C> 36.1 <R> <C> All-biomed <C> 49.8 <C> 35.4 <C> 35.7 <C> 43.4 <C> 33.9 <C> 37.5 <R> <C> All-biomed → Health <C> 48.9 <C> 36.4 <C> 35.9 <C> 43.0 <C> 35.2 <C> 38.0 <R> <C> All-biomed → Bio <C> 48.0 <C> 34.6 <C> 37.2 <C> 43.2 <C> 34.1 <C> 40.5 <R> <C> Health → All-biomed <C> [BOLD] 52.1 <C> 36.7 <C> 37.0 <C> 44.2 <C> 35.0 <C> 39.0 <R> <C> Health → All-biomed → Health <C> 51.1 <C> [BOLD] 37.0 <C> 37.2 <C> 44.0 <C> [BOLD] 36.3 <C> 39.5 <R> <C> Health → All-biomed → Bio <C> 50.6 <C> 36.0 <C> [BOLD] 38.0 <C> [BOLD] 45.2 <C> 35.3 <C> [BOLD] 41.3 <CAP> Table 2: Validation BLEU for English-Spanish models with transfer learning. We use the final three models in our submission. <COT> Looking at the "Health -> All-biomed" row, we can see that the BLEU score for the "Health -> All-biomed" transfer learning schedule is higher than the BLEU scores for the "es2en Health" and "en2es Health" transfer learning schedules.
<R> <C> [BOLD] Transfer learning schedule <C> [BOLD] es2en  [BOLD] Khresmoi <C> [BOLD] es2en  [BOLD] Health <C> [BOLD] es2en  [BOLD] Bio <C> [BOLD] en2es  [BOLD] Khresmoi <C> [BOLD] en2es  [BOLD] Health <C> [BOLD] en2es  [BOLD] Bio <R> <C> Health <C> 45.1 <C> 35.7 <C> 34.0 <C> 41.2 <C> 34.7 <C> 36.1 <R> <C> All-biomed <C> 49.8 <C> 35.4 <C> 35.7 <C> 43.4 <C> 33.9 <C> 37.5 <R> <C> All-biomed → Health <C> 48.9 <C> 36.4 <C> 35.9 <C> 43.0 <C> 35.2 <C> 38.0 <R> <C> All-biomed → Bio <C> 48.0 <C> 34.6 <C> 37.2 <C> 43.2 <C> 34.1 <C> 40.5 <R> <C> Health → All-biomed <C> [BOLD] 52.1 <C> 36.7 <C> 37.0 <C> 44.2 <C> 35.0 <C> 39.0 <R> <C> Health → All-biomed → Health <C> 51.1 <C> [BOLD] 37.0 <C> 37.2 <C> 44.0 <C> [BOLD] 36.3 <C> 39.5 <R> <C> Health → All-biomed → Bio <C> 50.6 <C> 36.0 <C> [BOLD] 38.0 <C> [BOLD] 45.2 <C> 35.3 <C> [BOLD] 41.3 <CAP> Table 2: Validation BLEU for English-Spanish models with transfer learning. We use the final three models in our submission. <COT> Looking at the "All-biomed -> Bio" column, we can see that the BLEU score for the "All-biomed -> Bio" transfer learning schedule is higher than the BLEU scores for the "es2en Bio" and "en2es Bio" transfer learning schedules.
<R> <C> [EMPTY] <C> [BOLD] es2en  [BOLD] Khresmoi <C> [BOLD] es2en  [BOLD] Health <C> [BOLD] es2en  [BOLD] Bio <C> [BOLD] es2en  [BOLD] Test <C> [BOLD] en2es  [BOLD] Khresmoi <C> [BOLD] en2es  [BOLD] Health <C> [BOLD] en2es  [BOLD] Bio <C> [BOLD] en2es  [BOLD] Test <R> <C> Health → All-biomed <C> 52.1 <C> 36.7 <C> 37.0 <C> 42.4 <C> 44.2 <C> 35.0 <C> 39.0 <C> 44.9 <R> <C> Health → All-biomed → Health <C> 51.1 <C> 37.0 <C> 37.2 <C> - <C> 44.0 <C> 36.3 <C> 39.5 <C> - <R> <C> Health → All-biomed → Bio <C> 50.6 <C> 36.0 <C> 38.0 <C> - <C> 45.2 <C> 35.3 <C> 41.3 <C> - <R> <C> Uniform ensemble <C> [BOLD] 52.2 <C> 36.9 <C> 37.9 <C> [BOLD] 43.0 <C> [BOLD] 45.1 <C> 35.6 <C> 40.2 <C> 45.4 <R> <C> BI ensemble ( [ITALIC] α=0.5) <C> 52.1 <C> [BOLD] 37.0 <C> [BOLD] 38.1 <C> 42.9 <C> 44.5 <C> [BOLD] 35.7 <C> [BOLD] 41.2 <C> [BOLD] 45.6 <CAP> Table 3: Validation and test BLEU for models used in English-Spanish language pair submissions. <COT> Looking at the table, we can see that the "Uniform ensemble" performs better than "Health → All-biomed" on the "es2en Khresmoi" dataset.
<R> <C> [EMPTY] <C> [BOLD] es2en  [BOLD] Khresmoi <C> [BOLD] es2en  [BOLD] Health <C> [BOLD] es2en  [BOLD] Bio <C> [BOLD] es2en  [BOLD] Test <C> [BOLD] en2es  [BOLD] Khresmoi <C> [BOLD] en2es  [BOLD] Health <C> [BOLD] en2es  [BOLD] Bio <C> [BOLD] en2es  [BOLD] Test <R> <C> Health → All-biomed <C> 52.1 <C> 36.7 <C> 37.0 <C> 42.4 <C> 44.2 <C> 35.0 <C> 39.0 <C> 44.9 <R> <C> Health → All-biomed → Health <C> 51.1 <C> 37.0 <C> 37.2 <C> - <C> 44.0 <C> 36.3 <C> 39.5 <C> - <R> <C> Health → All-biomed → Bio <C> 50.6 <C> 36.0 <C> 38.0 <C> - <C> 45.2 <C> 35.3 <C> 41.3 <C> - <R> <C> Uniform ensemble <C> [BOLD] 52.2 <C> 36.9 <C> 37.9 <C> [BOLD] 43.0 <C> [BOLD] 45.1 <C> 35.6 <C> 40.2 <C> 45.4 <R> <C> BI ensemble ( [ITALIC] α=0.5) <C> 52.1 <C> [BOLD] 37.0 <C> [BOLD] 38.1 <C> 42.9 <C> 44.5 <C> [BOLD] 35.7 <C> [BOLD] 41.2 <C> [BOLD] 45.6 <CAP> Table 3: Validation and test BLEU for models used in English-Spanish language pair submissions. <COT> Looking at the table, we can see that the "en2es Test" dataset has the highest BLEU score for the "BI ensemble (α=0.5)" model.
<R> <C> [EMPTY] <C> [BOLD] de2en  [BOLD] Khresmoi <C> [BOLD] de2en  [BOLD] Cochrane <C> [BOLD] de2en  [BOLD] Test <C> [BOLD] en2de  [BOLD] Khresmoi <C> [BOLD] en2de  [BOLD] Cochrane <C> [BOLD] en2de  [BOLD] Test <R> <C> News <C> 43.8 <C> 46.8 <C> - <C> 30.4 <C> 40.7 <C> - <R> <C> News → All-biomed <C> 44.5 <C> 47.6 <C> 27.4 <C> 31.1 <C> 39.5 <C> 26.5 <R> <C> Uniform ensemble <C> 45.3 <C> 48.4 <C> [BOLD] 28.6 <C> [BOLD] 32.6 <C> 42.9 <C> [BOLD] 27.2 <R> <C> BI ensemble ( [ITALIC] α=0.5) <C> [BOLD] 45.4 <C> [BOLD] 48.8 <C> 28.5 <C> 32.4 <C> [BOLD] 43.1 <C> 26.4 <CAP> Table 4: Validation and test BLEU for models used in English-German language pair submissions. <COT> Looking at the "Uniform ensemble" row, "28.6" cell, "de2en Khresmoi" cell, and "de2en Test" cell, we can see that the uniform ensemble model performs better on the Khresmoi dataset compared to the Test dataset for the de2en translation.
<R> <C> [EMPTY] <C> [BOLD] de2en  [BOLD] Khresmoi <C> [BOLD] de2en  [BOLD] Cochrane <C> [BOLD] de2en  [BOLD] Test <C> [BOLD] en2de  [BOLD] Khresmoi <C> [BOLD] en2de  [BOLD] Cochrane <C> [BOLD] en2de  [BOLD] Test <R> <C> News <C> 43.8 <C> 46.8 <C> - <C> 30.4 <C> 40.7 <C> - <R> <C> News → All-biomed <C> 44.5 <C> 47.6 <C> 27.4 <C> 31.1 <C> 39.5 <C> 26.5 <R> <C> Uniform ensemble <C> 45.3 <C> 48.4 <C> [BOLD] 28.6 <C> [BOLD] 32.6 <C> 42.9 <C> [BOLD] 27.2 <R> <C> BI ensemble ( [ITALIC] α=0.5) <C> [BOLD] 45.4 <C> [BOLD] 48.8 <C> 28.5 <C> 32.4 <C> [BOLD] 43.1 <C> 26.4 <CAP> Table 4: Validation and test BLEU for models used in English-German language pair submissions. <COT> Looking at the "BI ensemble (α=0.5)" row, "45.4" cell, "48.8" cell, and "de2en Khresmoi" cell, we can see that the BI ensemble model achieves higher BLEU scores on both the Khresmoi dataset and the Test dataset compared to the uniform ensemble model for the de2en translation.
<R> <C> [EMPTY] <C> [BOLD] es2en <C> [BOLD] en2es <C> [BOLD] de2en <C> [BOLD] en2de <R> <C> Uniform <C> [BOLD] 43.2 <C> 45.3 <C> 28.3 <C> 25.9 <R> <C> BI ( [ITALIC] α=0.5) <C> 43.0 <C> [BOLD] 45.5 <C> 28.2 <C> 25.2 <R> <C> BI ( [ITALIC] α=0.1) <C> [BOLD] 43.2 <C> [BOLD] 45.5 <C> [BOLD] 28.5 <C> [BOLD] 26.0 <CAP> Table 5: Comparing uniform ensembles and BI with varying smoothing factor on the WMT19 test data. Small deviations from official test scores on submitted runs are due to tokenization differences. α=0.5 was chosen for submission based on results on available development data. <COT> Looking at the "Table 5" caption, we can see that the table compares uniform ensembles and BI with varying smoothing factor on the WMT19 test data.
<R> <C> [EMPTY] <C> [BOLD] es2en <C> [BOLD] en2es <C> [BOLD] de2en <C> [BOLD] en2de <R> <C> Uniform <C> [BOLD] 43.2 <C> 45.3 <C> 28.3 <C> 25.9 <R> <C> BI ( [ITALIC] α=0.5) <C> 43.0 <C> [BOLD] 45.5 <C> 28.2 <C> 25.2 <R> <C> BI ( [ITALIC] α=0.1) <C> [BOLD] 43.2 <C> [BOLD] 45.5 <C> [BOLD] 28.5 <C> [BOLD] 26.0 <CAP> Table 5: Comparing uniform ensembles and BI with varying smoothing factor on the WMT19 test data. Small deviations from official test scores on submitted runs are due to tokenization differences. α=0.5 was chosen for submission based on results on available development data. <COT> Looking at the "BI (α=0.1)" row, we can see that the en2es and en2de translation scores are all bolded, indicating that BI with α=0.1 performs better than other methods in these translation directions.
<R> <C> dev <C> train <C> eScape <R> <C> 77.15 <C> 77.42 <C> 37.68 <CAP> Table 1: BLEU Scores of Data Sets <COT> Looking at the "dev" column, we can see that the BLEU score for the "dev" dataset is 77.15.
<R> <C> dev <C> train <C> eScape <R> <C> 77.15 <C> 77.42 <C> 37.68 <CAP> Table 1: BLEU Scores of Data Sets <COT> Looking at the "eScape" column, we can see that the BLEU score for the "eScape" dataset is 37.68.
<R> <C> Models <C> TER <C> BLEU <R> <C> MT as PE <C> 16.84 <C> 74.73 <R> <C> Gaussian <C> 16.79 <C> 75.03 <R> <C> Uniform <C> 16.80 <C> 75.03 <R> <C> Ensemble x5 <C> 16.77 <C> 75.03 <CAP> Table 3: Results on the Test Set <COT> Looking at the "Models" column, we can see that the "Ensemble x5" model has the lowest TER score.
<R> <C> Models <C> TER <C> BLEU <R> <C> MT as PE <C> 16.84 <C> 74.73 <R> <C> Gaussian <C> 16.79 <C> 75.03 <R> <C> Uniform <C> 16.80 <C> 75.03 <R> <C> Ensemble x5 <C> 16.77 <C> 75.03 <CAP> Table 3: Results on the Test Set <COT> Looking at the "BLEU" column, we can see that the "Gaussian" and "Uniform" models have the same BLEU score.
<R> <C> Models <C> BLEU <R> <C> MT as PE <C> 76.76 <R> <C> Processed MT <C> 76.61 <R> <C> Base <C> 76.91 ∼ 77.13 <R> <C> Gaussian <C> 76.94 ∼ 77.08 <R> <C> Uniform <C> 77.01 ∼ 77.10 <R> <C> Ensemble x5 <C> [BOLD] 77.22 <CAP> Table 2: BLEU Scores on the Development Set <COT> Looking at the "Models" column and the "BLEU" column, we can observe that the "Ensemble x5" model has the highest BLEU score.
<R> <C> Models <C> BLEU <R> <C> MT as PE <C> 76.76 <R> <C> Processed MT <C> 76.61 <R> <C> Base <C> 76.91 ∼ 77.13 <R> <C> Gaussian <C> 76.94 ∼ 77.08 <R> <C> Uniform <C> 77.01 ∼ 77.10 <R> <C> Ensemble x5 <C> [BOLD] 77.22 <CAP> Table 2: BLEU Scores on the Development Set <COT> Looking at the "Base" row and the "BLEU" column, we can see that the BLEU score ranges from 76.91 to 77.13.
<R> <C> Condition <C> Accuracy (%) Corr <C> Accuracy (%) Gram <C> Time (s) <C> Diversity Distinct <C> Diversity PINC <R> <C> Baseline <C> 74 <C> 97 <C> 36 <C> 99 <C> 68 <R> <C> Lexical Examples <C> [BOLD] 90† <C> 98 <C> [BOLD] 27 <C> [BOLD] 93 <C> [BOLD] 55† <R> <C> Mixed Examples <C> [BOLD] 89† <C> 96 <C> 36 <C> [BOLD] 87† <C> [BOLD] 58† <R> <C> No Examples <C> 84 <C> 96 <C> 30 <C> 95 <C> 63 <R> <C> Novelty Bonus <C> 72 <C> 96 <C> 30 <C> 99 <C> 69 <R> <C> No Bonus <C> 78 <C> 94 <C> 28 <C> 99 <C> 66 <R> <C> One Paraphrase <C> 82 <C> [BOLD] 89 <C> 38 <C> 96 <C> 65 <R> <C> Chain <C> 68 <C> 94 <C> [BOLD] 25 <C> 98 <C> [BOLD] 74 <R> <C> Answers <C> 80 <C> 94 <C> [BOLD] 29 <C> 96 <C> 65 <R> <C> advising <C> 78 <C> 94 <C> 31 <C> 97 <C> 70 <R> <C> geoquery <C> 77 <C> [BOLD] 85† <C> [BOLD] 25† <C> [BOLD] 94 <C> [BOLD] 63 <R> <C> wsj <C> 68 <C> [BOLD] 90 <C> [BOLD] 61† <C> [BOLD] 94† <C> [BOLD] 38† <R> <C> ubuntu <C> [BOLD] 56† <C> 92 <C> 44 <C> 97 <C> 67 <CAP> Table 1: Variation across conditions for a range of metrics (defined in § 3.4). Bold indicates a statistically significant difference compared to the baseline at the 0.05 level, and a † indicates significance at the 0.01 level, both after applying the Holm–-Bonferroni method across each row Holm (1979). <COT> Looking at the "Accuracy (%)" column, we can see that the accuracy of the "Lexical Examples" condition is higher than the accuracy of the "Mixed Examples" condition.
<R> <C> Condition <C> Accuracy (%) Corr <C> Accuracy (%) Gram <C> Time (s) <C> Diversity Distinct <C> Diversity PINC <R> <C> Baseline <C> 74 <C> 97 <C> 36 <C> 99 <C> 68 <R> <C> Lexical Examples <C> [BOLD] 90† <C> 98 <C> [BOLD] 27 <C> [BOLD] 93 <C> [BOLD] 55† <R> <C> Mixed Examples <C> [BOLD] 89† <C> 96 <C> 36 <C> [BOLD] 87† <C> [BOLD] 58† <R> <C> No Examples <C> 84 <C> 96 <C> 30 <C> 95 <C> 63 <R> <C> Novelty Bonus <C> 72 <C> 96 <C> 30 <C> 99 <C> 69 <R> <C> No Bonus <C> 78 <C> 94 <C> 28 <C> 99 <C> 66 <R> <C> One Paraphrase <C> 82 <C> [BOLD] 89 <C> 38 <C> 96 <C> 65 <R> <C> Chain <C> 68 <C> 94 <C> [BOLD] 25 <C> 98 <C> [BOLD] 74 <R> <C> Answers <C> 80 <C> 94 <C> [BOLD] 29 <C> 96 <C> 65 <R> <C> advising <C> 78 <C> 94 <C> 31 <C> 97 <C> 70 <R> <C> geoquery <C> 77 <C> [BOLD] 85† <C> [BOLD] 25† <C> [BOLD] 94 <C> [BOLD] 63 <R> <C> wsj <C> 68 <C> [BOLD] 90 <C> [BOLD] 61† <C> [BOLD] 94† <C> [BOLD] 38† <R> <C> ubuntu <C> [BOLD] 56† <C> 92 <C> 44 <C> 97 <C> 67 <CAP> Table 1: Variation across conditions for a range of metrics (defined in § 3.4). Bold indicates a statistically significant difference compared to the baseline at the 0.05 level, and a † indicates significance at the 0.01 level, both after applying the Holm–-Bonferroni method across each row Holm (1979). <COT> Looking at the "Diversity Distinct" column, we can see that the "Chain" condition has a higher diversity distinct score compared to the "Baseline" condition.
<R> <C> [BOLD] Method <C> [BOLD] Verb  [BOLD] nmPU <C> [BOLD] Verb  [BOLD] niPU <C> [BOLD] Verb  [BOLD] F1 <C> [BOLD] Subject  [BOLD] nmPU <C> [BOLD] Subject  [BOLD] niPU <C> [BOLD] Subject  [BOLD] F1 <C> [BOLD] Object  [BOLD] nmPU <C> [BOLD] Object  [BOLD] niPU <C> [BOLD] Object  [BOLD] F1 <C> [BOLD] Frame  [BOLD] nmPU <C> [BOLD] Frame  [BOLD] niPU <C> [BOLD] Frame  [BOLD] F1 <R> <C> Triframes Watset <C> 42.84 <C> 88.35 <C> [BOLD] 57.70 <C> 54.22 <C> 81.40 <C> 65.09 <C> 53.04 <C> 83.25 <C> 64.80 <C> 55.19 <C> 60.81 <C> [BOLD] 57.87 <R> <C> HOSG Cotterell et al. ( 2017 ) <C> 44.41 <C> 68.43 <C> 53.86 <C> 52.84 <C> 74.53 <C> 61.83 <C> 54.73 <C> 74.05 <C> 62.94 <C> 55.74 <C> 50.45 <C> 52.96 <R> <C> NOAC Egurnov et al. ( 2017 ) <C> 20.73 <C> 88.38 <C> 33.58 <C> 57.00 <C> 80.11 <C> [BOLD] 66.61 <C> 57.32 <C> 81.13 <C> [BOLD] 67.18 <C> 44.01 <C> 63.21 <C> 51.89 <R> <C> Triadic Spectral <C> 49.62 <C> 24.90 <C> 33.15 <C> 50.07 <C> 41.07 <C> 45.13 <C> 50.50 <C> 41.82 <C> 45.75 <C> 52.05 <C> 28.60 <C> 36.91 <R> <C> Triadic  [ITALIC] k-Means <C> [BOLD] 63.87 <C> 23.16 <C> 33.99 <C> [BOLD] 63.15 <C> 38.20 <C> 47.60 <C> [BOLD] 63.98 <C> 37.43 <C> 47.23 <C> [BOLD] 63.64 <C> 24.11 <C> 34.97 <R> <C> LDA-Frames Materna ( 2013 ) <C> 26.11 <C> 66.92 <C> 37.56 <C> 17.28 <C> 83.26 <C> 28.62 <C> 20.80 <C> 90.33 <C> 33.81 <C> 18.80 <C> 71.17 <C> 29.75 <R> <C> Triframes CW <C> 7.75 <C> 6.48 <C> 7.06 <C> 3.70 <C> 14.07 <C> 5.86 <C> 51.91 <C> 76.92 <C> 61.99 <C> 21.67 <C> 26.50 <C> 23.84 <R> <C> Singletons <C> 0.00 <C> 25.23 <C> 0.00 <C> 0.00 <C> 25.68 <C> 0.00 <C> 0.00 <C> 20.80 <C> 0.00 <C> 32.34 <C> 22.15 <C> 26.29 <R> <C> Whole <C> 3.62 <C> [BOLD] 100.0 <C> 6.98 <C> 2.41 <C> [BOLD] 98.41 <C> 4.70 <C> 2.38 <C> [BOLD] 100.0 <C> 4.64 <C> 2.63 <C> [BOLD] 99.55 <C> 5.12 <CAP> Table 3: Frame evaluation results on the triples from the FrameNet 1.7 corpus Baker et al. (1998). The results are sorted by the descending order of the Frame F1-score. Best results are boldfaced. <COT> Looking at the "Method" column, we can see different frame evaluation methods used in the experiment. The F1 scores for each method are provided in the corresponding columns.
<R> <C> [BOLD] Method <C> [BOLD] Verb  [BOLD] nmPU <C> [BOLD] Verb  [BOLD] niPU <C> [BOLD] Verb  [BOLD] F1 <C> [BOLD] Subject  [BOLD] nmPU <C> [BOLD] Subject  [BOLD] niPU <C> [BOLD] Subject  [BOLD] F1 <C> [BOLD] Object  [BOLD] nmPU <C> [BOLD] Object  [BOLD] niPU <C> [BOLD] Object  [BOLD] F1 <C> [BOLD] Frame  [BOLD] nmPU <C> [BOLD] Frame  [BOLD] niPU <C> [BOLD] Frame  [BOLD] F1 <R> <C> Triframes Watset <C> 42.84 <C> 88.35 <C> [BOLD] 57.70 <C> 54.22 <C> 81.40 <C> 65.09 <C> 53.04 <C> 83.25 <C> 64.80 <C> 55.19 <C> 60.81 <C> [BOLD] 57.87 <R> <C> HOSG Cotterell et al. ( 2017 ) <C> 44.41 <C> 68.43 <C> 53.86 <C> 52.84 <C> 74.53 <C> 61.83 <C> 54.73 <C> 74.05 <C> 62.94 <C> 55.74 <C> 50.45 <C> 52.96 <R> <C> NOAC Egurnov et al. ( 2017 ) <C> 20.73 <C> 88.38 <C> 33.58 <C> 57.00 <C> 80.11 <C> [BOLD] 66.61 <C> 57.32 <C> 81.13 <C> [BOLD] 67.18 <C> 44.01 <C> 63.21 <C> 51.89 <R> <C> Triadic Spectral <C> 49.62 <C> 24.90 <C> 33.15 <C> 50.07 <C> 41.07 <C> 45.13 <C> 50.50 <C> 41.82 <C> 45.75 <C> 52.05 <C> 28.60 <C> 36.91 <R> <C> Triadic  [ITALIC] k-Means <C> [BOLD] 63.87 <C> 23.16 <C> 33.99 <C> [BOLD] 63.15 <C> 38.20 <C> 47.60 <C> [BOLD] 63.98 <C> 37.43 <C> 47.23 <C> [BOLD] 63.64 <C> 24.11 <C> 34.97 <R> <C> LDA-Frames Materna ( 2013 ) <C> 26.11 <C> 66.92 <C> 37.56 <C> 17.28 <C> 83.26 <C> 28.62 <C> 20.80 <C> 90.33 <C> 33.81 <C> 18.80 <C> 71.17 <C> 29.75 <R> <C> Triframes CW <C> 7.75 <C> 6.48 <C> 7.06 <C> 3.70 <C> 14.07 <C> 5.86 <C> 51.91 <C> 76.92 <C> 61.99 <C> 21.67 <C> 26.50 <C> 23.84 <R> <C> Singletons <C> 0.00 <C> 25.23 <C> 0.00 <C> 0.00 <C> 25.68 <C> 0.00 <C> 0.00 <C> 20.80 <C> 0.00 <C> 32.34 <C> 22.15 <C> 26.29 <R> <C> Whole <C> 3.62 <C> [BOLD] 100.0 <C> 6.98 <C> 2.41 <C> [BOLD] 98.41 <C> 4.70 <C> 2.38 <C> [BOLD] 100.0 <C> 4.64 <C> 2.63 <C> [BOLD] 99.55 <C> 5.12 <CAP> Table 3: Frame evaluation results on the triples from the FrameNet 1.7 corpus Baker et al. (1998). The results are sorted by the descending order of the Frame F1-score. Best results are boldfaced. <COT> Looking at the "Whole" row, we can see that the F1 scores for the "Whole" method are consistently high across all categories (Verb, Subject, Object, and Frame).
<R> <C> [BOLD] Method <C> [BOLD] nmPU <C> [BOLD] niPU <C> [BOLD] F1 <R> <C> LDA-Frames <C> [BOLD] 52.60 <C> 45.84 <C> [BOLD] 48.98 <R> <C> Triframes Watset <C> 40.05 <C> 62.09 <C> 48.69 <R> <C> NOAC <C> 37.19 <C> 64.09 <C> 47.07 <R> <C> HOSG <C> 38.22 <C> 43.76 <C> 40.80 <R> <C> Triadic Spectral <C> 35.76 <C> 38.96 <C> 36.86 <R> <C> Triadic  [ITALIC] k-Means <C> 52.22 <C> 27.43 <C> 35.96 <R> <C> Triframes CW <C> 18.05 <C> 12.72 <C> 14.92 <R> <C> Whole <C> 24.14 <C> [BOLD] 79.09 <C> 36.99 <R> <C> Singletons <C> 0.00 <C> 27.21 <C> 0.00 <CAP> Table 4: Evaluation results on the dataset of polysemous verb classes by Korhonen et al. (2003). <COT> Looking at the "F1" column, we can see that the highest F1 score is achieved by the "LDA-Frames" method.
<R> <C> [BOLD] Method <C> [BOLD] nmPU <C> [BOLD] niPU <C> [BOLD] F1 <R> <C> LDA-Frames <C> [BOLD] 52.60 <C> 45.84 <C> [BOLD] 48.98 <R> <C> Triframes Watset <C> 40.05 <C> 62.09 <C> 48.69 <R> <C> NOAC <C> 37.19 <C> 64.09 <C> 47.07 <R> <C> HOSG <C> 38.22 <C> 43.76 <C> 40.80 <R> <C> Triadic Spectral <C> 35.76 <C> 38.96 <C> 36.86 <R> <C> Triadic  [ITALIC] k-Means <C> 52.22 <C> 27.43 <C> 35.96 <R> <C> Triframes CW <C> 18.05 <C> 12.72 <C> 14.92 <R> <C> Whole <C> 24.14 <C> [BOLD] 79.09 <C> 36.99 <R> <C> Singletons <C> 0.00 <C> 27.21 <C> 0.00 <CAP> Table 4: Evaluation results on the dataset of polysemous verb classes by Korhonen et al. (2003). <COT> Looking at the "niPU" column, we can see that the highest value is achieved by the "NOAC" method.
<R> <C> Method <C> Max F1 <C> 11-point IAP <R> <C> Baseline <C> 54.92 <C> 50.99 <R> <C> RR <C> 62.94 <C> 59.62 <R> <C> RR_FR_1step <C> 68.35 <C> 64.42 <R> <C> RR_FR_2step <C> 69.72 <C> 67.29 <CAP> Table 1: French-English Performance. Baseline indicates current state of the art performance. <COT> Looking at the "Method" column, we can see that there are four different methods mentioned: Baseline, RR, RR_FR_1step, and RR_FR_2step.
<R> <C> Method <C> Max F1 <C> 11-point IAP <R> <C> Baseline <C> 54.92 <C> 50.99 <R> <C> RR <C> 62.94 <C> 59.62 <R> <C> RR_FR_1step <C> 68.35 <C> 64.42 <R> <C> RR_FR_2step <C> 69.72 <C> 67.29 <CAP> Table 1: French-English Performance. Baseline indicates current state of the art performance. <COT> Looking at the "11-point IAP" column, we can see that the values increase as we move from Baseline to RR_FR_2step.
<R> <C> Method <C> Max F1 <C> 11-point IAP <R> <C> Baseline <C> 55.08 <C> 51.35 <R> <C> RR <C> 60.88 <C> 58.79 <R> <C> RR_FR_1step <C> 65.87 <C> 63.55 <R> <C> RR_FR_2step <C> 65.76 <C> 65.26 <CAP> Table 4: French-English Performance (large data). Baseline indicates state of the art performance. <COT> Looking at the "11-point IAP" column, we can see that the "RR_FR_2step" method has the highest score.
<R> <C> [BOLD] Data set <C> [BOLD] Size <C> [BOLD] Entity Types <C> [BOLD] Description <R> <C> CADEC <C> 120,341 <C> Adverse Drug Event, Disease, Drug, Finding, Symptom <C> Posts taken from AskaPatient, which is a forum where consumers can discuss their experiences with medications. <R> <C> CoNLL2003 <C> 301,418 <C> Person, Organization, Location, Miscellany <C> Newswire from the Reuters RCV1 corpus. <R> <C> CRAFT <C> 561,015 <C> Cell, Chemical entity, Biological taxa, Protein, Biomacromolecular sequence, Entrez gene, Biological process and molecular function, Cellular component <C> Full-length, open-access journal articles about biology. <R> <C> JNLPBA <C> 593,590 <C> Protein, DNA, RNA, Cell line and Cell type <C> Abstract of journal articles about biology. <R> <C> ScienceIE <C> 99,555 <C> Process (including methods, equipment), Task and Material (including corpora, physical materials) <C> Journal articles about Computer Science, Material Sciences and Physics. <R> <C> Wetlab <C> 220,618 <C> Action, 9 object-based (Amount, Concentration, Device, Location, Method, Reagent, Speed, Temperature, Time) entity types, 5 measure-based (Numerical, Generic-Measure, Size, pH, Measure-Type) and 3 other (Mention, Modifier, Seal) types <C> Protocols written by researchers about conducting biology and chemistry experiments. <CAP> Table 2: List of the target NER data sets and their specifications. Size is shown in number of tokens. <COT> Looking at the "Description" column, we can see that the CADEC dataset consists of posts taken from AskaPatient, a forum where consumers discuss their experiences with medications.
<R> <C> [BOLD] Data set <C> [BOLD] Size <C> [BOLD] Entity Types <C> [BOLD] Description <R> <C> CADEC <C> 120,341 <C> Adverse Drug Event, Disease, Drug, Finding, Symptom <C> Posts taken from AskaPatient, which is a forum where consumers can discuss their experiences with medications. <R> <C> CoNLL2003 <C> 301,418 <C> Person, Organization, Location, Miscellany <C> Newswire from the Reuters RCV1 corpus. <R> <C> CRAFT <C> 561,015 <C> Cell, Chemical entity, Biological taxa, Protein, Biomacromolecular sequence, Entrez gene, Biological process and molecular function, Cellular component <C> Full-length, open-access journal articles about biology. <R> <C> JNLPBA <C> 593,590 <C> Protein, DNA, RNA, Cell line and Cell type <C> Abstract of journal articles about biology. <R> <C> ScienceIE <C> 99,555 <C> Process (including methods, equipment), Task and Material (including corpora, physical materials) <C> Journal articles about Computer Science, Material Sciences and Physics. <R> <C> Wetlab <C> 220,618 <C> Action, 9 object-based (Amount, Concentration, Device, Location, Method, Reagent, Speed, Temperature, Time) entity types, 5 measure-based (Numerical, Generic-Measure, Size, pH, Measure-Type) and 3 other (Mention, Modifier, Seal) types <C> Protocols written by researchers about conducting biology and chemistry experiments. <CAP> Table 2: List of the target NER data sets and their specifications. Size is shown in number of tokens. <COT> Looking at the "Entity Types" column, we can see that the CRAFT dataset includes various entity types such as Cell, Chemical entity, Biological taxa, Protein, Biomacromolecular sequence, Entrez gene, Biological process and molecular function, and Cellular component.
<R> <C> [EMPTY] <C> [BOLD] Word vectors <C> [BOLD] LMs <R> <C> TVC <C> 00.454 <C> 00.666 <R> <C> TVcC <C> 00.469 <C> 00.739 <R> <C> PPL <C> -0.398 <C> -0.618 <R> <C> WVV <C> -0.406 <C> -0.747 <CAP> Table 4: Correlation coefficients between similarity measures and the effectiveness of pretrained models. The coefficients vary between -1 (negative correlation) and 1 (positive correlation). Zero means no correlation. <COT> Looking at the "TVC" row and "Word vectors" column, we can see that the correlation coefficient between TVC and Word vectors is 0.454.
<R> <C> [EMPTY] <C> [BOLD] Word vectors <C> [BOLD] LMs <R> <C> TVC <C> 00.454 <C> 00.666 <R> <C> TVcC <C> 00.469 <C> 00.739 <R> <C> PPL <C> -0.398 <C> -0.618 <R> <C> WVV <C> -0.406 <C> -0.747 <CAP> Table 4: Correlation coefficients between similarity measures and the effectiveness of pretrained models. The coefficients vary between -1 (negative correlation) and 1 (positive correlation). Zero means no correlation. <COT> Looking at the "PPL" row and "LMs" column, we can see that the correlation coefficient between PPL and LMs is -0.618.
<R> <C> [EMPTY] <C> [BOLD] Word vectors GloVe <C> [BOLD] Word vectors Ours <C> [BOLD] LMs ELMo <C> [BOLD] LMs Ours <R> <C> CADEC <C> [BOLD] 70.30 <C> 70.27 <C> [BOLD] 71.91 <C> 70.46 <R> <C> CoNLL2003 <C> [BOLD] 90.25 <C> 86.36 <C> [BOLD] 91.34 <C> 89.78 <R> <C> CRAFT <C> 74.22 <C> [BOLD] 75.45 <C> [BOLD] 75.77 <C> 75.45 <R> <C> JNLPBA <C> 73.19 <C> [BOLD] 73.24 <C> 73.65 <C> [BOLD] 74.29 <R> <C> ScienceIE <C> 37.10 <C> [BOLD] 37.91 <C> 41.15 <C> [BOLD] 42.07 <R> <C> WetLab <C> [BOLD] 79.15 <C> 78.93 <C> 79.57 <C> [BOLD] 79.62 <CAP> Table 5: Comparison between our best performance pretrained models and the publicly available ones, which are pretrained on much larger corpora. <COT> Looking at the "Table 5" caption, we can see that the table compares the performance of "our best performance pretrained models" with "publicly available ones" on different datasets.
<R> <C> [EMPTY] <C> [BOLD] Word vectors GloVe <C> [BOLD] Word vectors Ours <C> [BOLD] LMs ELMo <C> [BOLD] LMs Ours <R> <C> CADEC <C> [BOLD] 70.30 <C> 70.27 <C> [BOLD] 71.91 <C> 70.46 <R> <C> CoNLL2003 <C> [BOLD] 90.25 <C> 86.36 <C> [BOLD] 91.34 <C> 89.78 <R> <C> CRAFT <C> 74.22 <C> [BOLD] 75.45 <C> [BOLD] 75.77 <C> 75.45 <R> <C> JNLPBA <C> 73.19 <C> [BOLD] 73.24 <C> 73.65 <C> [BOLD] 74.29 <R> <C> ScienceIE <C> 37.10 <C> [BOLD] 37.91 <C> 41.15 <C> [BOLD] 42.07 <R> <C> WetLab <C> [BOLD] 79.15 <C> 78.93 <C> 79.57 <C> [BOLD] 79.62 <CAP> Table 5: Comparison between our best performance pretrained models and the publicly available ones, which are pretrained on much larger corpora. <COT> Looking at the "Word vectors GloVe" column, we can see that the performance of our models is better than GloVe word vectors on all datasets.
<R> <C> [EMPTY] <C> [BOLD] ScienceIE  [BOLD] Def <C> [BOLD] ScienceIE  [BOLD] Opt <C> [BOLD] WetLab  [BOLD] Def <C> [BOLD] WetLab  [BOLD] Opt <R> <C> 1BWB <C> 34.40 <C> 34.57 <C> 78.66 <C> 79.12 <R> <C> MIMIC <C> 31.23 <C> 34.14 <C> 78.68 <C> 78.65 <R> <C> PubMed <C> [BOLD] 37.91 <C> [BOLD] 38.86 <C> [BOLD] 78.93 <C> [BOLD] 79.28 <R> <C> Wiki <C> 36.15 <C> 35.63 <C> 78.45 <C> 78.99 <R> <C> Yelp <C> 33.92 <C> 34.25 <C> 78.48 <C> 78.78 <CAP> Table 6: Impact of hyper-parameter setting on the effectiveness of pretrained word vectors. ‘Opt’ is hyper-parameter setting proposed in (Chiu et al., 2016), whereas ‘Def’ is the default setting in word2vec. <COT> Looking at the values in the "PubMed" row, we can see that the performance of PubMed with the "Def" hyper-parameter setting is higher than the performance with the "Opt" hyper-parameter setting.
<R> <C> [BOLD] type of discrepancy <C> [BOLD] frequency <R> <C> T-V distinction <C> 67% <R> <C> speaker/addressee gender: <C> [EMPTY] <R> <C> same speaker <C> 22% <R> <C> different speaker <C> 09% <R> <C> other <C> 02% <CAP> Table 3: Types of discrepancy in context-agnostic translation caused by deixis (excluding anaphora) <COT> Looking at the "T-V distinction" row, the frequency of discrepancy caused by the T-V distinction is 67%.
<R> <C> Model <C> EE <C> TE <C> 0-100 <C> 100-500 <C> 500+ <C> All <R> <C> RC (random initializations) <C> 44.5 <C> 64.4 <C> 40.5 <C> 57.8 <C> 63.4 <C> 53.4 <R> <C> RC (SG initializations) <C> 49.5 <C> 68.6 <C> 44.1 <C> 62.5 <C> 67.0 <C> 57.3 <R> <C> RC (SG fixed) <C> 48.9 <C> [BOLD] 68.7 <C> 44.1 <C> 62.7 <C> 67.3 <C> 57.6 <R> <C> RC + SG <C> 51.6 <C> 67.4 <C> [BOLD] 46.4 <C> 62.5 <C> 66.8 <C> 58.2 <R> <C> RC + SGLR <C> [BOLD] 51.7 <C> 68.5 <C> 45.3 <C> [BOLD] 63.0 <C> [BOLD] 68.1 <C> [BOLD] 58.4 <CAP> Table 1: Evaluation on subsets of THYME Dev (in F-measure). The subsets of Event×Event (EE) and Timex3×Event (TE) relation pairs are of sizes 3.3k and 2.7k respectively. The intervals 0-100, 100-500 and 500+ are subsets reflecting average argument token frequency in the training data (of sizes 2.2k, 2.2k and 1.8k respectively). <COT> Looking at the "Model" column, we can see that the RC + SGLR model has the highest F-measure scores for both EE and TE relation pairs.
<R> <C> Model <C> EE <C> TE <C> 0-100 <C> 100-500 <C> 500+ <C> All <R> <C> RC (random initializations) <C> 44.5 <C> 64.4 <C> 40.5 <C> 57.8 <C> 63.4 <C> 53.4 <R> <C> RC (SG initializations) <C> 49.5 <C> 68.6 <C> 44.1 <C> 62.5 <C> 67.0 <C> 57.3 <R> <C> RC (SG fixed) <C> 48.9 <C> [BOLD] 68.7 <C> 44.1 <C> 62.7 <C> 67.3 <C> 57.6 <R> <C> RC + SG <C> 51.6 <C> 67.4 <C> [BOLD] 46.4 <C> 62.5 <C> 66.8 <C> 58.2 <R> <C> RC + SGLR <C> [BOLD] 51.7 <C> 68.5 <C> 45.3 <C> [BOLD] 63.0 <C> [BOLD] 68.1 <C> [BOLD] 58.4 <CAP> Table 1: Evaluation on subsets of THYME Dev (in F-measure). The subsets of Event×Event (EE) and Timex3×Event (TE) relation pairs are of sizes 3.3k and 2.7k respectively. The intervals 0-100, 100-500 and 500+ are subsets reflecting average argument token frequency in the training data (of sizes 2.2k, 2.2k and 1.8k respectively). <COT> Looking at the "RC (SG fixed)" row, we can see that the F-measure score for the 500+ interval is 67.3.
<R> <C> Model <C> P <C> R <C> F <R> <C> [ITALIC] With specialized resources: <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Best Clinical TempEval (2016) <C> 58.8 <C> 55.9 <C> 57.3 <R> <C> Lin et al. (2016) <C> 66.9 <C> 53.4 <C> 59.4 <R> <C> Leeuwenberg et al. (2017) <C> - <C> - <C> 60.8 <R> <C> Tourille et al. (2017) <C> 65.7 <C> 57.5 <C> 61.3 <R> <C> Lin et al. (2017) <C> 66.2 <C> 58.5 <C> 62.1 <R> <C> [ITALIC] No specialized resources: <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> RC (random initialization) <C> 67.9 <C> 52.1 <C> 58.9±0.2 <R> <C> RC (SG initialization) <C> [BOLD] 71.2 <C> 52.0 <C> 60.0±1.2 <R> <C> RC (SG fixed) <C> 68.9 <C> 54.6 <C> 60.9±0.8 <R> <C> RC + SG <C> 66.2 <C> [BOLD] 59.7 <C> [BOLD] 62.8±0.2 <R> <C> RC + SGLR <C> 68.7 <C> 57.5 <C> 62.5±0.3 <CAP> Table 2: THYME test set results, reporting precision (P), recall (R) and F-measure (F), macro-averaged over three runs. The standard deviation for F is also given. <COT> Looking at the "RC (SG initialization)" row, the "P" value is "[BOLD] 71.2" and the "F" value is "60.0±1.2". Comparing these values with the other rows in the "No specialized resources" section, it can be observed that RC (SG initialization) achieves the highest precision and a relatively high F-measure.
<R> <C> Model <C> P <C> R <C> F <R> <C> [ITALIC] With specialized resources: <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Best Clinical TempEval (2016) <C> 58.8 <C> 55.9 <C> 57.3 <R> <C> Lin et al. (2016) <C> 66.9 <C> 53.4 <C> 59.4 <R> <C> Leeuwenberg et al. (2017) <C> - <C> - <C> 60.8 <R> <C> Tourille et al. (2017) <C> 65.7 <C> 57.5 <C> 61.3 <R> <C> Lin et al. (2017) <C> 66.2 <C> 58.5 <C> 62.1 <R> <C> [ITALIC] No specialized resources: <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> RC (random initialization) <C> 67.9 <C> 52.1 <C> 58.9±0.2 <R> <C> RC (SG initialization) <C> [BOLD] 71.2 <C> 52.0 <C> 60.0±1.2 <R> <C> RC (SG fixed) <C> 68.9 <C> 54.6 <C> 60.9±0.8 <R> <C> RC + SG <C> 66.2 <C> [BOLD] 59.7 <C> [BOLD] 62.8±0.2 <R> <C> RC + SGLR <C> 68.7 <C> 57.5 <C> 62.5±0.3 <CAP> Table 2: THYME test set results, reporting precision (P), recall (R) and F-measure (F), macro-averaged over three runs. The standard deviation for F is also given. <COT> Looking at the "Best Clinical TempEval (2016)" row, the "R" value is "55.9" and the "F" value is "57.3". Comparing these values with the other rows in the table, it can be observed that the Best Clinical TempEval (2016) model has the lowest recall and F-measure among all the models.
<R> <C> Error Type <C> RC + SG <C> RC (SG fixed) <C> RC (SG init.) <R> <C> Cross-Clause Relations (CCR) <C> 42 <C> 39 <C> 36 <R> <C> Infrequent Arguments (<10) <C> 11 <C> 15 <C> 26 <R> <C> Frequent Arguments (>250) <C> 37 <C> 50 <C> 40 <R> <C> Mistake in Ground-Truth <C> 10 <C> 8 <C> 5 <R> <C> Other <C> 21 <C> 15 <C> 28 <CAP> Table 3: Error analysis on 50 FP and 50 FN (random from test) for different settings. Clause boundaries are: newlines and sub-clause or sentence boundaries. Error categories are not mutually exclusive. <COT> Looking at the "Error Type" column, we can see that the number of errors in the "Cross-Clause Relations (CCR)" category is decreasing as we move from "RC + SG" to "RC (SG fixed)" to "RC (SG init.)".
<R> <C> Error Type <C> RC + SG <C> RC (SG fixed) <C> RC (SG init.) <R> <C> Cross-Clause Relations (CCR) <C> 42 <C> 39 <C> 36 <R> <C> Infrequent Arguments (<10) <C> 11 <C> 15 <C> 26 <R> <C> Frequent Arguments (>250) <C> 37 <C> 50 <C> 40 <R> <C> Mistake in Ground-Truth <C> 10 <C> 8 <C> 5 <R> <C> Other <C> 21 <C> 15 <C> 28 <CAP> Table 3: Error analysis on 50 FP and 50 FN (random from test) for different settings. Clause boundaries are: newlines and sub-clause or sentence boundaries. Error categories are not mutually exclusive. <COT> Looking at the "Error Type" column, we can see that the number of errors in the "Infrequent Arguments (<10)" category is increasing as we move from "RC + SG" to "RC (SG fixed)" to "RC (SG init.)".
<R> <C> [BOLD] Model <C> [BOLD] Sentiment <C> [BOLD] Stance <C> [BOLD] Hate <R> <C> Frequency <C> 0.332 <C> 0.397 <C> 0.057 <R> <C> LING <C> 0.676 <C> 0.569 <C> 0.624 <R> <C> LING+random <C> 0.657 <C> 0.571 <C> 0.600 <R> <C> LING+PV <C> 0.671 <C> 0.601∗ <C> 0.667∗ <R> <C> LING+N2V <C> 0.672 <C> 0.629∗⋄ <C> 0.656∗ <R> <C> LING+GAT <C> 0.666 <C> 0.640∗⋄† <C> 0.674∗⋄† <CAP> Table 2: Results for all the models on the three datasets in our experiment. Marked with ∗ are the results which significantly improve over LING and LING+random (p<0.05, also for the following results); ⋄ indicates a significant improvement over LING+PV; † a significant improvement over LING+N2V. <COT> Looking at the "Model" column, we can see that the LING+GAT model has the highest sentiment score and stance score among all the models.
<R> <C> [BOLD] Model <C> [BOLD] Sentiment <C> [BOLD] Stance <C> [BOLD] Hate <R> <C> Frequency <C> 0.332 <C> 0.397 <C> 0.057 <R> <C> LING <C> 0.676 <C> 0.569 <C> 0.624 <R> <C> LING+random <C> 0.657 <C> 0.571 <C> 0.600 <R> <C> LING+PV <C> 0.671 <C> 0.601∗ <C> 0.667∗ <R> <C> LING+N2V <C> 0.672 <C> 0.629∗⋄ <C> 0.656∗ <R> <C> LING+GAT <C> 0.666 <C> 0.640∗⋄† <C> 0.674∗⋄† <CAP> Table 2: Results for all the models on the three datasets in our experiment. Marked with ∗ are the results which significantly improve over LING and LING+random (p<0.05, also for the following results); ⋄ indicates a significant improvement over LING+PV; † a significant improvement over LING+N2V. <COT> Looking at the "Hate" column, we can see that the LING model has the lowest hate score among all the models.
<R> <C> [BOLD] Languages  [BOLD] EN–BG <C> [BOLD] # sents 306,380 <C> [BOLD] Languages  [BOLD] EN–IT <C> [BOLD] # sents 1,297,635 <R> <C> [BOLD] EN–CS <C> 491,848 <C> [BOLD] EN–LT <C> 481,570 <R> <C> [BOLD] EN–DA <C> 1,421,197 <C> [BOLD] EN–LV <C> 487,287 <R> <C> [BOLD] EN–DE <C> 1,296,843 <C> [BOLD] EN–NL <C> 1,419,359 <R> <C> [BOLD] EN–EL <C> 921,540 <C> [BOLD] EN–PL <C> 478,008 <R> <C> [BOLD] EN–ES <C> 1,419,507 <C> [BOLD] EN–PT <C> 1,426,043 <R> <C> [BOLD] EN–ET <C> 494,645 <C> [BOLD] EN–RO <C> 303,396 <R> <C> [BOLD] EN–FI <C> 1,393,572 <C> [BOLD] EN–SK <C> 488,351 <R> <C> [BOLD] EN–FR <C> 1,440,620 <C> [BOLD] EN–SL <C> 479,313 <R> <C> [BOLD] EN–HU <C> 251,833 <C> [BOLD] EN–SV <C> 1,349,472 <CAP> Table 1: Overview of annotated parallel sentences per language pair <COT> Looking at the "Languages" column, we can see that the language pairs are listed in alphabetical order.
<R> <C> [BOLD] Languages  [BOLD] EN–BG <C> [BOLD] # sents 306,380 <C> [BOLD] Languages  [BOLD] EN–IT <C> [BOLD] # sents 1,297,635 <R> <C> [BOLD] EN–CS <C> 491,848 <C> [BOLD] EN–LT <C> 481,570 <R> <C> [BOLD] EN–DA <C> 1,421,197 <C> [BOLD] EN–LV <C> 487,287 <R> <C> [BOLD] EN–DE <C> 1,296,843 <C> [BOLD] EN–NL <C> 1,419,359 <R> <C> [BOLD] EN–EL <C> 921,540 <C> [BOLD] EN–PL <C> 478,008 <R> <C> [BOLD] EN–ES <C> 1,419,507 <C> [BOLD] EN–PT <C> 1,426,043 <R> <C> [BOLD] EN–ET <C> 494,645 <C> [BOLD] EN–RO <C> 303,396 <R> <C> [BOLD] EN–FI <C> 1,393,572 <C> [BOLD] EN–SK <C> 488,351 <R> <C> [BOLD] EN–FR <C> 1,440,620 <C> [BOLD] EN–SL <C> 479,313 <R> <C> [BOLD] EN–HU <C> 251,833 <C> [BOLD] EN–SV <C> 1,349,472 <CAP> Table 1: Overview of annotated parallel sentences per language pair <COT> Looking at the "# sents" column, we can see that the number of annotated parallel sentences varies for each language pair.
<R> <C> [BOLD] type of discrepancy <C> [BOLD] frequency <R> <C> wrong morphological form <C> 66% <R> <C> wrong verb (VP-ellipsis) <C> 20% <R> <C> other error <C> 14% <CAP> Table 4: Types of discrepancy in context-agnostic translation caused by ellipsis <COT> Looking at the "type of discrepancy" column, we can see that the most frequent type of discrepancy in context-agnostic translation caused by ellipsis is the wrong morphological form with a frequency of 66%.
<R> <C> [BOLD] type of discrepancy <C> [BOLD] frequency <R> <C> wrong morphological form <C> 66% <R> <C> wrong verb (VP-ellipsis) <C> 20% <R> <C> other error <C> 14% <CAP> Table 4: Types of discrepancy in context-agnostic translation caused by ellipsis <COT> Adding up the frequencies of all types of discrepancies, we can see that the total frequency is 100%.
<R> <C> [BOLD] Model <C> [BOLD] Dev Acc. <C> [BOLD] F - M <R> <C> [BOLD] M.1 (BoW+LogReg) <C> 0.827 <C> 0.035** <R> <C> [BOLD] M.2 (BiLSTM) <C> 0.841 <C> 0.077** <R> <C> [BOLD] M.3 (BERT) <C> 0.930 <C> -0.040** <CAP> Table 1: Results. Dev Acc. represents accuracy on SST-2 dev set. F - M represents difference between means of predicted positive class probabilities for sentences with female nouns and sentences with male nouns. ** denotes statistical significance with p<0.01 (after applying Bonferroni correction). <COT> Looking at the "Model" column and the corresponding values in the "Dev Acc." column, we can see that Model M.3 (BERT) has the highest accuracy on the SST-2 dev set.
<R> <C> [BOLD] Model <C> [BOLD] Dev Acc. <C> [BOLD] F - M <R> <C> [BOLD] M.1 (BoW+LogReg) <C> 0.827 <C> 0.035** <R> <C> [BOLD] M.2 (BiLSTM) <C> 0.841 <C> 0.077** <R> <C> [BOLD] M.3 (BERT) <C> 0.930 <C> -0.040** <CAP> Table 1: Results. Dev Acc. represents accuracy on SST-2 dev set. F - M represents difference between means of predicted positive class probabilities for sentences with female nouns and sentences with male nouns. ** denotes statistical significance with p<0.01 (after applying Bonferroni correction). <COT> Looking at the "Model" column and the corresponding values in the "F - M" column, we can observe that Model M.2 (BiLSTM) has the highest difference between means of predicted positive class probabilities for sentences with female nouns and sentences with male nouns.
<R> <C> Test Condition <C> [BOLD] Cosine Similarity ± SD  (↑)  [BOLD] RGB <C> [BOLD] Cosine Similarity ± SD  (↑)  [BOLD] WM18 <C> [BOLD] Cosine Similarity ± SD  (↑)  [BOLD] HSV <C> [BOLD] Cosine Similarity ± SD  (↑)  [BOLD] Ensemble <C> [BOLD] Cosine Similarity ± SD  (↑)  [BOLD] WM18∗ <R> <C> Seen Pairings <C> 0.954±0.001 <C> 0.953±0.000 <C> 0.934±0.089 <C> [BOLD] 0.954±0.0 <C> 0.68 <R> <C> Unseen Pairings <C> 0.799±0.044 <C> 0.771±0.032 <C> [BOLD] 0.843±0.144 <C> 0.797±0.0 <C> 0.68 <R> <C> Unseen Ref. Color <C> 0.781±0.015 <C> 0.767±0.010 <C> [BOLD] 0.945±0.019 <C> 0.804±0.0 <C> 0.40 <R> <C> Unseen Modifiers <C> 0.633±0.042 <C> 0.637±0.032 <C> [BOLD] 0.724±0.131 <C> 0.629±0.0 <C> 0.41 <R> <C> Fully Unseen <C> 0.370±0.029 <C> 0.358±0.038 <C> [BOLD] 0.919±0.026 <C> 0.445±0.0 <C> -0.21 <R> <C> Overall <C> 0.858±0.006 <C> 0.856±0.003 <C> [BOLD] 0.911±0.057 <C> 0.868±0.0 <C> 0.65 <R> <C> [EMPTY] <C> [BOLD] Delta-E ± SD  (↓) <C> [BOLD] Delta-E ± SD  (↓) <C> [BOLD] Delta-E ± SD  (↓) <C> [BOLD] Delta-E ± SD  (↓) <C> [BOLD] Delta-E ± SD  (↓) <R> <C> Test Condition <C> RGB <C> WM18 <C> HSV <C> Ensemble <C> WM18∗ <R> <C> Seen Pairings <C> [BOLD] 3.121±0.027 <C> 3.188±0.062 <C> 5.380±4.846 <C> 4.093±0.1 <C> 6.1 <R> <C> Unseen Pairings <C> 6.454±0.233 <C> 6.825±0.093 <C> 11.701±3.358 <C> [BOLD] 5.873±0.0 <C> 7.9 <R> <C> Unseen Ref. Color <C> 7.456±0.184 <C> 7.658±0.363 <C> 10.429±2.523 <C> [BOLD] 7.171±0.0 <C> 11.4 <R> <C> Unseen Modifiers <C> 13.288±1.082 <C> 13.891±1.077 <C> 14.183±5.175 <C> 10.927±0.0 <C> [BOLD] 10.5 <R> <C> Fully Unseen <C> 13.859±0.874 <C> 14.516±0.587 <C> [BOLD] 12.432±2.170 <C> 13.448±0.0 <C> 15.9 <R> <C> Overall <C> [BOLD] 5.412±0.169 <C> 5.595±0.128 <C> 7.487±3.940 <C> 5.777±0.0 <C> 6.8 <CAP> Table 1: Average cosine similarity score and Delta-E distance over 5 runs. A smaller Delta-E distance means a less significant difference between two colors. Bold: best performance. Hard: the hard ensemble model. WM18∗: the performance from WM18 paper. See Supplementary Material for example outputs and ensemble analysis. <COT> Looking at the "Overall" row, the highest cosine similarity score is achieved by the HSV model.
<R> <C> Test Condition <C> [BOLD] Cosine Similarity ± SD  (↑)  [BOLD] RGB <C> [BOLD] Cosine Similarity ± SD  (↑)  [BOLD] WM18 <C> [BOLD] Cosine Similarity ± SD  (↑)  [BOLD] HSV <C> [BOLD] Cosine Similarity ± SD  (↑)  [BOLD] Ensemble <C> [BOLD] Cosine Similarity ± SD  (↑)  [BOLD] WM18∗ <R> <C> Seen Pairings <C> 0.954±0.001 <C> 0.953±0.000 <C> 0.934±0.089 <C> [BOLD] 0.954±0.0 <C> 0.68 <R> <C> Unseen Pairings <C> 0.799±0.044 <C> 0.771±0.032 <C> [BOLD] 0.843±0.144 <C> 0.797±0.0 <C> 0.68 <R> <C> Unseen Ref. Color <C> 0.781±0.015 <C> 0.767±0.010 <C> [BOLD] 0.945±0.019 <C> 0.804±0.0 <C> 0.40 <R> <C> Unseen Modifiers <C> 0.633±0.042 <C> 0.637±0.032 <C> [BOLD] 0.724±0.131 <C> 0.629±0.0 <C> 0.41 <R> <C> Fully Unseen <C> 0.370±0.029 <C> 0.358±0.038 <C> [BOLD] 0.919±0.026 <C> 0.445±0.0 <C> -0.21 <R> <C> Overall <C> 0.858±0.006 <C> 0.856±0.003 <C> [BOLD] 0.911±0.057 <C> 0.868±0.0 <C> 0.65 <R> <C> [EMPTY] <C> [BOLD] Delta-E ± SD  (↓) <C> [BOLD] Delta-E ± SD  (↓) <C> [BOLD] Delta-E ± SD  (↓) <C> [BOLD] Delta-E ± SD  (↓) <C> [BOLD] Delta-E ± SD  (↓) <R> <C> Test Condition <C> RGB <C> WM18 <C> HSV <C> Ensemble <C> WM18∗ <R> <C> Seen Pairings <C> [BOLD] 3.121±0.027 <C> 3.188±0.062 <C> 5.380±4.846 <C> 4.093±0.1 <C> 6.1 <R> <C> Unseen Pairings <C> 6.454±0.233 <C> 6.825±0.093 <C> 11.701±3.358 <C> [BOLD] 5.873±0.0 <C> 7.9 <R> <C> Unseen Ref. Color <C> 7.456±0.184 <C> 7.658±0.363 <C> 10.429±2.523 <C> [BOLD] 7.171±0.0 <C> 11.4 <R> <C> Unseen Modifiers <C> 13.288±1.082 <C> 13.891±1.077 <C> 14.183±5.175 <C> 10.927±0.0 <C> [BOLD] 10.5 <R> <C> Fully Unseen <C> 13.859±0.874 <C> 14.516±0.587 <C> [BOLD] 12.432±2.170 <C> 13.448±0.0 <C> 15.9 <R> <C> Overall <C> [BOLD] 5.412±0.169 <C> 5.595±0.128 <C> 7.487±3.940 <C> 5.777±0.0 <C> 6.8 <CAP> Table 1: Average cosine similarity score and Delta-E distance over 5 runs. A smaller Delta-E distance means a less significant difference between two colors. Bold: best performance. Hard: the hard ensemble model. WM18∗: the performance from WM18 paper. See Supplementary Material for example outputs and ensemble analysis. <COT> Looking at the "Fully Unseen" row, the WM18 model achieves the lowest Delta-E distance.
<R> <C> [BOLD] model <C> [BOLD] BLEU <R> <C> baseline (1.5m) <C> 29.10 <R> <C> baseline (6m) <C> [BOLD] 32.40 <R> <C> concat <C> 31.56 <R> <C> s-hier-to-2.tied <C> 26.68 <R> <C> CADec <C> [BOLD] 32.38 <CAP> Table 6: BLEU scores. CADec trained with p=0.5. Scores for CADec are not statistically different from the baseline (6m). <COT> Looking at the "model" column, we can see that the "CADec" model has a BLEU score of 32.38, which is higher than the baseline (1.5m) score of 29.10.
<R> <C> [BOLD] model <C> [BOLD] BLEU <R> <C> baseline (1.5m) <C> 29.10 <R> <C> baseline (6m) <C> [BOLD] 32.40 <R> <C> concat <C> 31.56 <R> <C> s-hier-to-2.tied <C> 26.68 <R> <C> CADec <C> [BOLD] 32.38 <CAP> Table 6: BLEU scores. CADec trained with p=0.5. Scores for CADec are not statistically different from the baseline (6m). <COT> Looking at the "model" and "BLEU" columns, we can see that the "baseline (6m)" model has the highest BLEU score of 32.40, which is higher than all other models.
<R> <C> All Labels <C> Orig F&C <C> Train 587 <C> Dev 5,418 <C> Test 6,007 <C> All 12,012 <R> <C> All Labels <C> No-Leak F&C <C> 712 <C> 3,000 <C> 4,497 <C> 8,209 <R> <C> Subset Labels <C> Orig F&C <C> 361 <C> 3,311 <C> 3,650 <C> 7,322 <R> <C> Subset Labels <C> Clean F&C <C> 173 <C> 1,268 <C> 1,523 <C> 2,964 <CAP> Table 2: F&C dataset size. All Labels represent the original dataset with all the labels. Subset Labels are the subset labels which are inferable by the resource. <COT> Looking at the "All Labels" row and the "Orig F&C" cell, we can see that the original dataset with all the labels has a size of 587 for training, 5,418 for development, and 6,007 for testing.
<R> <C> All Labels <C> Orig F&C <C> Train 587 <C> Dev 5,418 <C> Test 6,007 <C> All 12,012 <R> <C> All Labels <C> No-Leak F&C <C> 712 <C> 3,000 <C> 4,497 <C> 8,209 <R> <C> Subset Labels <C> Orig F&C <C> 361 <C> 3,311 <C> 3,650 <C> 7,322 <R> <C> Subset Labels <C> Clean F&C <C> 173 <C> 1,268 <C> 1,523 <C> 2,964 <CAP> Table 2: F&C dataset size. All Labels represent the original dataset with all the labels. Subset Labels are the subset labels which are inferable by the resource. <COT> Looking at the "Subset Labels" row and the "Clean F&C" cell, we can see that the subset labels which are inferable by the resource have a size of 173 for training, 1,268 for development, and 1,523 for testing.
<R> <C> Model/Dataset <C> F&C Clean Dev <C> F&C Clean Test <C> New Data Dev <C> New Data Test <R> <C> Majority <C> 0.54 <C> 0.57 <C> 0.51 <C> 0.50 <R> <C> Yang et al. (PCE LSTM) <C> [BOLD] 0.86 <C> [BOLD] 0.87 <C> 0.60 <C> 0.57 <R> <C> DoQ <C> 0.78 <C> 0.77 <C> [BOLD] 0.62 <C> [BOLD] 0.62 <R> <C> DoQ + 10-distance <C> 0.78 <C> 0.77 <C> [BOLD] 0.62 <C> [BOLD] 0.62 <R> <C> DoQ + 3-distance <C> 0.81 <C> 0.80 <C> [BOLD] 0.62 <C> 0.61 <CAP> Table 4: Results on the noun comparison datasets. <COT> Looking at the "CAP" tag, we can see that Table 4 shows the results on the noun comparison datasets.
<R> <C> Model/Dataset <C> F&C Clean Dev <C> F&C Clean Test <C> New Data Dev <C> New Data Test <R> <C> Majority <C> 0.54 <C> 0.57 <C> 0.51 <C> 0.50 <R> <C> Yang et al. (PCE LSTM) <C> [BOLD] 0.86 <C> [BOLD] 0.87 <C> 0.60 <C> 0.57 <R> <C> DoQ <C> 0.78 <C> 0.77 <C> [BOLD] 0.62 <C> [BOLD] 0.62 <R> <C> DoQ + 10-distance <C> 0.78 <C> 0.77 <C> [BOLD] 0.62 <C> [BOLD] 0.62 <R> <C> DoQ + 3-distance <C> 0.81 <C> 0.80 <C> [BOLD] 0.62 <C> 0.61 <CAP> Table 4: Results on the noun comparison datasets. <COT> Looking at the "DoQ + 3-distance" row and the "New Data Test" column, we can see that the model achieves a F&C score of 0.61 on the new data test set.
<R> <C> Model <C> Accuracy <R> <C> Chance <C> 0.5 <R> <C> Bagherinezhad et al. <C> 0.835 <R> <C> Yang et al. (Transfer) <C> 0.858 <R> <C> DoQ <C> 0.872 <R> <C> DoQ + 10-distance <C> [BOLD] 0.877 <R> <C> DoQ + 3-distance <C> 0.858 <CAP> Table 5: Results on the Relative dataset. Yang et al. (2018) result was achieved by running their model on their training set, and using it as a transfer method on Relative. Finally, we present our own predictions, with different thresholds, which surpass previous work. <COT> Looking at the "Model" column, we can see that the model "DoQ + 10-distance" has the highest accuracy of 0.877.
<R> <C> Model <C> Accuracy <R> <C> Chance <C> 0.5 <R> <C> Bagherinezhad et al. <C> 0.835 <R> <C> Yang et al. (Transfer) <C> 0.858 <R> <C> DoQ <C> 0.872 <R> <C> DoQ + 10-distance <C> [BOLD] 0.877 <R> <C> DoQ + 3-distance <C> 0.858 <CAP> Table 5: Results on the Relative dataset. Yang et al. (2018) result was achieved by running their model on their training set, and using it as a transfer method on Relative. Finally, we present our own predictions, with different thresholds, which surpass previous work. <COT> Looking at the "Chance" column, we can see that the chance accuracy is 0.5.
<R> <C> Method/Data <C> Mass <C> Length <C> Speed <C> Currency <C> All <R> <C> Indian Annotators <C> 0.61 <C> 0.79 <C> 0.77 <C> 0.58 <C> 0.69 <R> <C> US Annotators <C> - <C> - <C> - <C> 0.76 <C> - <CAP> Table 7: Intrinsic Evaluation. Accuracy of the number of objects which our proposed median fall into range of the object, given the dimension. <COT> Looking at the "Method/Data" column, we can see that the table provides intrinsic evaluation results for different dimensions (Mass, Length, Speed, Currency) using Indian Annotators and US Annotators.
<R> <C> Method/Data <C> Mass <C> Length <C> Speed <C> Currency <C> All <R> <C> Indian Annotators <C> 0.61 <C> 0.79 <C> 0.77 <C> 0.58 <C> 0.69 <R> <C> US Annotators <C> - <C> - <C> - <C> 0.76 <C> - <CAP> Table 7: Intrinsic Evaluation. Accuracy of the number of objects which our proposed median fall into range of the object, given the dimension. <COT> Looking at the "Currency" column, we can see that the accuracy for US Annotators is 0.76.
<R> <C> Satire associated <C> PCA Component  [BOLD] RC19 <C> Description First person singular pronoun incidence <C> estimate 1.80 <C> std.error 0.41 <C> statistic 4.38 <C> *** <R> <C> Satire associated <C> [BOLD] RC5 <C> Sentence length, number of words <C> 0.66 <C> 0.18 <C> 3.68 <C> *** <R> <C> Satire associated <C> [BOLD] RC15 <C> Estimates of hypernymy for nouns <C> 0.61 <C> 0.19 <C> 3.18 <C> ** <R> <C> Satire associated <C> [BOLD] RC49 <C> Word Concreteness <C> 0.54 <C> 0.17 <C> 3.18 <C> ** <R> <C> Satire associated <C> [BOLD] RC35 <C> Ratio of casual particles to causal verbs <C> 0.56 <C> 0.18 <C> 3.10 <C> ** <R> <C> Satire associated <C> [BOLD] RC91 <C> Text Easability PC Referential cohesion <C> 0.45 <C> 0.16 <C> 2.89 <C> ** <R> <C> Satire associated <C> [BOLD] RC20 <C> Incidence score of gerunds <C> 0.43 <C> 0.16 <C> 2.77 <C> ** <R> <C> Satire associated <C> RC32 <C> Expanded temporal connectives incidence <C> 0.44 <C> 0.16 <C> 2.75 <C> ** <R> <C> Satire associated <C> [BOLD] RC9 <C> Third person singular pronoun incidence <C> 0.44 <C> 0.16 <C> 2.67 <C> ** <R> <C> Satire associated <C> RC43 <C> Word length, number of letters <C> 0.45 <C> 0.20 <C> 2.27 <C> * <R> <C> Satire associated <C> RC46 <C> Verb phrase density <C> 0.37 <C> 0.16 <C> 2.25 <C> * <R> <C> Satire associated <C> [BOLD] RC97 <C> Coh-Metrix L2 Readability <C> 0.34 <C> 0.16 <C> 2.16 <C> * <R> <C> Satire associated <C> [BOLD] RC61 <C> Average word frequency for all words <C> 0.50 <C> 0.24 <C> 2.13 <C> * <R> <C> Satire associated <C> RC84 <C> The average givenness of each sentence <C> 0.37 <C> 0.18 <C> 2.11 <C> * <R> <C> Satire associated <C> RC65 <C> Text Easability PC Syntactic simplicity <C> 0.38 <C> 0.18 <C> 2.08 <C> * <R> <C> Satire associated <C> RC50 <C> Lexical diversity <C> 0.37 <C> 0.18 <C> 2.05 <C> * <R> <C> Fake news associated <C> [BOLD] RC30 <C> Agentless passive voice density <C> -1.05 <C> 0.21 <C> -4.96 <C> *** <R> <C> Fake news associated <C> [BOLD] RC73 <C> Average word frequency for content words <C> -0.72 <C> 0.20 <C> -3.68 <C> *** <R> <C> Fake news associated <C> [BOLD] RC59 <C> Adverb incidence <C> -0.62 <C> 0.18 <C> -3.43 <C> *** <R> <C> Fake news associated <C> [BOLD] RC55 <C> Number of sentences <C> -0.79 <C> 0.26 <C> -3.09 <C> ** <R> <C> Fake news associated <C> RC62 <C> Causal and intentional connectives <C> -0.42 <C> 0.15 <C> -2.72 <C> ** <R> <C> Fake news associated <C> [BOLD] RC34 <C> LSA overlap between verbs <C> -0.35 <C> 0.16 <C> -2.22 <C> * <R> <C> Fake news associated <C> [BOLD] RC44 <C> LSA overlap, adjacent sentences <C> -0.36 <C> 0.16 <C> -2.16 <C> * <R> <C> Fake news associated <C> RC47 <C> Sentence length, number of words <C> -0.36 <C> 0.18 <C> -2.03 <C> * <R> <C> Fake news associated <C> RC89 <C> LSA overlap, all sentences in paragraph <C> -0.34 <C> 0.17 <C> -1.97 <C> * <R> <C> [EMPTY] <C> (Intercept) <C> [EMPTY] <C> -0.54 <C> 0.19 <C> -2.91 <C> [EMPTY] <CAP> Table 1: Significant components of our logistic regression model using the Coh-Metrix features. Variables are also separated by their association with either satire or fake news. Bold: the remaining features following the step-wise backward elimination. Note: *** p < 0.001, ** p < 0.01, * p < 0.05. <COT> Looking at the "Satire associated" row and the "[BOLD] RC19" cell, we can see that the estimate for the incidence of the first person singular pronoun is 1.80, which is statistically significant (statistic = 4.38, ***). 
<R> <C> Satire associated <C> PCA Component  [BOLD] RC19 <C> Description First person singular pronoun incidence <C> estimate 1.80 <C> std.error 0.41 <C> statistic 4.38 <C> *** <R> <C> Satire associated <C> [BOLD] RC5 <C> Sentence length, number of words <C> 0.66 <C> 0.18 <C> 3.68 <C> *** <R> <C> Satire associated <C> [BOLD] RC15 <C> Estimates of hypernymy for nouns <C> 0.61 <C> 0.19 <C> 3.18 <C> ** <R> <C> Satire associated <C> [BOLD] RC49 <C> Word Concreteness <C> 0.54 <C> 0.17 <C> 3.18 <C> ** <R> <C> Satire associated <C> [BOLD] RC35 <C> Ratio of casual particles to causal verbs <C> 0.56 <C> 0.18 <C> 3.10 <C> ** <R> <C> Satire associated <C> [BOLD] RC91 <C> Text Easability PC Referential cohesion <C> 0.45 <C> 0.16 <C> 2.89 <C> ** <R> <C> Satire associated <C> [BOLD] RC20 <C> Incidence score of gerunds <C> 0.43 <C> 0.16 <C> 2.77 <C> ** <R> <C> Satire associated <C> RC32 <C> Expanded temporal connectives incidence <C> 0.44 <C> 0.16 <C> 2.75 <C> ** <R> <C> Satire associated <C> [BOLD] RC9 <C> Third person singular pronoun incidence <C> 0.44 <C> 0.16 <C> 2.67 <C> ** <R> <C> Satire associated <C> RC43 <C> Word length, number of letters <C> 0.45 <C> 0.20 <C> 2.27 <C> * <R> <C> Satire associated <C> RC46 <C> Verb phrase density <C> 0.37 <C> 0.16 <C> 2.25 <C> * <R> <C> Satire associated <C> [BOLD] RC97 <C> Coh-Metrix L2 Readability <C> 0.34 <C> 0.16 <C> 2.16 <C> * <R> <C> Satire associated <C> [BOLD] RC61 <C> Average word frequency for all words <C> 0.50 <C> 0.24 <C> 2.13 <C> * <R> <C> Satire associated <C> RC84 <C> The average givenness of each sentence <C> 0.37 <C> 0.18 <C> 2.11 <C> * <R> <C> Satire associated <C> RC65 <C> Text Easability PC Syntactic simplicity <C> 0.38 <C> 0.18 <C> 2.08 <C> * <R> <C> Satire associated <C> RC50 <C> Lexical diversity <C> 0.37 <C> 0.18 <C> 2.05 <C> * <R> <C> Fake news associated <C> [BOLD] RC30 <C> Agentless passive voice density <C> -1.05 <C> 0.21 <C> -4.96 <C> *** <R> <C> Fake news associated <C> [BOLD] RC73 <C> Average word frequency for content words <C> -0.72 <C> 0.20 <C> -3.68 <C> *** <R> <C> Fake news associated <C> [BOLD] RC59 <C> Adverb incidence <C> -0.62 <C> 0.18 <C> -3.43 <C> *** <R> <C> Fake news associated <C> [BOLD] RC55 <C> Number of sentences <C> -0.79 <C> 0.26 <C> -3.09 <C> ** <R> <C> Fake news associated <C> RC62 <C> Causal and intentional connectives <C> -0.42 <C> 0.15 <C> -2.72 <C> ** <R> <C> Fake news associated <C> [BOLD] RC34 <C> LSA overlap between verbs <C> -0.35 <C> 0.16 <C> -2.22 <C> * <R> <C> Fake news associated <C> [BOLD] RC44 <C> LSA overlap, adjacent sentences <C> -0.36 <C> 0.16 <C> -2.16 <C> * <R> <C> Fake news associated <C> RC47 <C> Sentence length, number of words <C> -0.36 <C> 0.18 <C> -2.03 <C> * <R> <C> Fake news associated <C> RC89 <C> LSA overlap, all sentences in paragraph <C> -0.34 <C> 0.17 <C> -1.97 <C> * <R> <C> [EMPTY] <C> (Intercept) <C> [EMPTY] <C> -0.54 <C> 0.19 <C> -2.91 <C> [EMPTY] <CAP> Table 1: Significant components of our logistic regression model using the Coh-Metrix features. Variables are also separated by their association with either satire or fake news. Bold: the remaining features following the step-wise backward elimination. Note: *** p < 0.001, ** p < 0.01, * p < 0.05. <COT> Looking at the "Fake news associated" row and the "[BOLD] RC30" cell, we can see that the estimate for the agentless passive voice density is -1.05, which is statistically significant (statistic = -4.96, ***).
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Headline only <C> 0.46 <C> 0.89 <C> 0.61 <R> <C> Text body only <C> 0.78 <C> 0.52 <C> 0.62 <R> <C> Headline + text body <C> [BOLD] 0.81 <C> [BOLD] 0.75 <C> [BOLD] 0.78 <CAP> Table 2: Results of classification between fake news and satire articles using BERT pre-trained models, based on the headline, body and full text. Bold: best performing model. P: Precision, and R: Recall <COT> Looking at the "Model" column, we can see that the "Headline + text body" model has the highest F1 score of 0.78.
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Headline only <C> 0.46 <C> 0.89 <C> 0.61 <R> <C> Text body only <C> 0.78 <C> 0.52 <C> 0.62 <R> <C> Headline + text body <C> [BOLD] 0.81 <C> [BOLD] 0.75 <C> [BOLD] 0.78 <CAP> Table 2: Results of classification between fake news and satire articles using BERT pre-trained models, based on the headline, body and full text. Bold: best performing model. P: Precision, and R: Recall <COT> Looking at the "Model" column, we can see that the "Text body only" model has a precision of 0.78.
<R> <C> [EMPTY] <C> total <C> latest relevant context 1st <C> latest relevant context 2nd <C> latest relevant context 3rd <R> <C> [BOLD] deixis <C> [BOLD] deixis <C> [BOLD] deixis <C> [EMPTY] <C> [EMPTY] <R> <C> baseline <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <R> <C> concat <C> [BOLD] 83.5 <C> [BOLD] 88.8 <C> [BOLD] 85.6 <C> [BOLD] 76.4 <R> <C> s-hier-to-2.tied <C> 60.9 <C> 83.0 <C> 50.1 <C> 50.0 <R> <C> CADec <C> 81.6 <C> 84.6 <C> 84.4 <C> 75.9 <R> <C> [BOLD] lexical cohesion <C> [BOLD] lexical cohesion <C> [BOLD] lexical cohesion <C> [EMPTY] <C> [EMPTY] <R> <C> baseline <C> 45.9 <C> 46.1 <C> 45.9 <C> 45.4 <R> <C> concat <C> 47.5 <C> 48.6 <C> 46.7 <C> 46.7 <R> <C> s-hier-to-2.tied <C> 48.9 <C> 53.0 <C> 46.1 <C> 45.4 <R> <C> CADec <C> [BOLD] 58.1 <C> [BOLD] 63.2 <C> [BOLD] 52.0 <C> [BOLD] 56.7 <CAP> Table 7: Accuracy for deixis and lexical cohesion. <COT> Looking at the "deixis" column, we can see that the "concat" method has the highest accuracy for all three contexts (latest relevant context 1st, latest relevant context 2nd, and latest relevant context 3rd).
<R> <C> [EMPTY] <C> total <C> latest relevant context 1st <C> latest relevant context 2nd <C> latest relevant context 3rd <R> <C> [BOLD] deixis <C> [BOLD] deixis <C> [BOLD] deixis <C> [EMPTY] <C> [EMPTY] <R> <C> baseline <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <R> <C> concat <C> [BOLD] 83.5 <C> [BOLD] 88.8 <C> [BOLD] 85.6 <C> [BOLD] 76.4 <R> <C> s-hier-to-2.tied <C> 60.9 <C> 83.0 <C> 50.1 <C> 50.0 <R> <C> CADec <C> 81.6 <C> 84.6 <C> 84.4 <C> 75.9 <R> <C> [BOLD] lexical cohesion <C> [BOLD] lexical cohesion <C> [BOLD] lexical cohesion <C> [EMPTY] <C> [EMPTY] <R> <C> baseline <C> 45.9 <C> 46.1 <C> 45.9 <C> 45.4 <R> <C> concat <C> 47.5 <C> 48.6 <C> 46.7 <C> 46.7 <R> <C> s-hier-to-2.tied <C> 48.9 <C> 53.0 <C> 46.1 <C> 45.4 <R> <C> CADec <C> [BOLD] 58.1 <C> [BOLD] 63.2 <C> [BOLD] 52.0 <C> [BOLD] 56.7 <CAP> Table 7: Accuracy for deixis and lexical cohesion. <COT> Looking at the "lexical cohesion" column, we can see that the "CADec" method has the highest accuracy for all three contexts (latest relevant context 1st, latest relevant context 2nd, and latest relevant context 3rd).
<R> <C> [BOLD] Method <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Baseline <C> 0.70 <C> 0.64 <C> 0.67 <R> <C> Coh-Metrix <C> 0.72 <C> 0.66 <C> 0.74* <R> <C> Pre-trained BERT <C> [BOLD] 0.81 <C> [BOLD] 0.75 <C> [BOLD] 0.78* <CAP> Table 3: Summary of results of classification between fake news and satire articles using the baseline Multinomial Naive Bayes method, the linguistic cues of text coherence and semantic representation with a pre-trained BERT model. Statistically significant differences with the baseline are marked with ’*’. Bold: best performing model. P: Precision, and R: Recall. For Coh-Metrix, we report the mean Precision, Recall, and F1 on the test set. <COT> Looking at the "Method" column, we can see that the Pre-trained BERT model has the highest F1 score compared to the Baseline and Coh-Metrix.
<R> <C> [BOLD] Method <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Baseline <C> 0.70 <C> 0.64 <C> 0.67 <R> <C> Coh-Metrix <C> 0.72 <C> 0.66 <C> 0.74* <R> <C> Pre-trained BERT <C> [BOLD] 0.81 <C> [BOLD] 0.75 <C> [BOLD] 0.78* <CAP> Table 3: Summary of results of classification between fake news and satire articles using the baseline Multinomial Naive Bayes method, the linguistic cues of text coherence and semantic representation with a pre-trained BERT model. Statistically significant differences with the baseline are marked with ’*’. Bold: best performing model. P: Precision, and R: Recall. For Coh-Metrix, we report the mean Precision, Recall, and F1 on the test set. <COT> Looking at the "F1" column, we can see that the F1 score for the Pre-trained BERT model is marked with a bold font, indicating it is the best performing model.
<R> <C> Methods <C> Aida-B <R> <C> Q15-1011 <C> 88.7 <R> <C> guorobust <C> 89.0 <R> <C> P16-1059 <C> 91.0 <R> <C> K16-1025 <C> 91.5 <R> <C> D17-1276 <C> 92.22±0.14 <R> <C> rel-norm <C> 92.41±0.19 <R> <C> ment-norm <C> [BOLD] 93.07±0.27 <R> <C> ment-norm ( [ITALIC] K=1) <C> 92.89±0.21 <R> <C> ment-norm (no pad) <C> 92.37±0.26 <CAP> Table 1: F1 scores on AIDA-B (test set). <COT> Looking at the "Methods" column and the F1 scores, we can see that the "ment-norm" method has the highest F1 score of 93.07±0.27.
<R> <C> Methods <C> Aida-B <R> <C> Q15-1011 <C> 88.7 <R> <C> guorobust <C> 89.0 <R> <C> P16-1059 <C> 91.0 <R> <C> K16-1025 <C> 91.5 <R> <C> D17-1276 <C> 92.22±0.14 <R> <C> rel-norm <C> 92.41±0.19 <R> <C> ment-norm <C> [BOLD] 93.07±0.27 <R> <C> ment-norm ( [ITALIC] K=1) <C> 92.89±0.21 <R> <C> ment-norm (no pad) <C> 92.37±0.26 <CAP> Table 1: F1 scores on AIDA-B (test set). <COT> Looking at the F1 scores of the different methods, we can see that the "ment-norm" method consistently outperforms the other methods.
<R> <C> Methods <C> MSNBC <C> AQUAINT <C> ACE2004 <C> CWEB <C> WIKI <C> Avg <R> <C> milne2008learning <C> 78 <C> 85 <C> 81 <C> 64.1 <C> 81.7 <C> 77.96 <R> <C> D11-1072 <C> 79 <C> 56 <C> 80 <C> 58.6 <C> 63 <C> 67.32 <R> <C> P11-1138 <C> 75 <C> 83 <C> 82 <C> 56.2 <C> 67.2 <C> 72.68 <R> <C> cheng-roth:2013:EMNLP <C> 90 <C> [BOLD] 90 <C> 86 <C> 67.5 <C> 73.4 <C> 81.38 <R> <C> guorobust <C> 92 <C> 87 <C> 88 <C> 77 <C> [BOLD] 84.5 <C> [BOLD] 85.7 <R> <C> D17-1276 <C> 93.7 ± 0.1 <C> 88.5 ± 0.4 <C> 88.5 ± 0.3 <C> [BOLD] 77.9 ± 0.1 <C> 77.5 ± 0.1 <C> 85.22 <R> <C> rel-norm <C> 92.2 ± 0.3 <C> 86.7 ± 0.7 <C> 87.9 ± 0.3 <C> 75.2 ± 0.5 <C> 76.4 ± 0.3 <C> 83.67 <R> <C> ment-norm <C> [BOLD] 93.9 ± 0.2 <C> 88.3 ± 0.6 <C> [BOLD] 89.9 ± 0.8 <C> 77.5 ± 0.1 <C> 78.0 ± 0.1 <C> 85.51 <R> <C> ment-norm ( [ITALIC] K=1) <C> 93.2 ± 0.3 <C> 88.4 ± 0.4 <C> 88.9 ± 1.0 <C> 77.0 ± 0.2 <C> 77.2 ± 0.1 <C> 84.94 <R> <C> ment-norm (no pad) <C> 93.6 ± 0.3 <C> 87.8 ± 0.5 <C> [BOLD] 90.0 ± 0.3 <C> 77.0 ± 0.2 <C> 77.3 ± 0.3 <C> 85.13 <CAP> Table 2: F1 scores on five out-domain test sets. Underlined scores show cases where the corresponding model outperforms the baseline. <COT> Looking at the "Methods" column and the "ACE2004" row, we can see that the "guorobust" method achieves a score of 88 on the ACE2004 test set.
<R> <C> Methods <C> MSNBC <C> AQUAINT <C> ACE2004 <C> CWEB <C> WIKI <C> Avg <R> <C> milne2008learning <C> 78 <C> 85 <C> 81 <C> 64.1 <C> 81.7 <C> 77.96 <R> <C> D11-1072 <C> 79 <C> 56 <C> 80 <C> 58.6 <C> 63 <C> 67.32 <R> <C> P11-1138 <C> 75 <C> 83 <C> 82 <C> 56.2 <C> 67.2 <C> 72.68 <R> <C> cheng-roth:2013:EMNLP <C> 90 <C> [BOLD] 90 <C> 86 <C> 67.5 <C> 73.4 <C> 81.38 <R> <C> guorobust <C> 92 <C> 87 <C> 88 <C> 77 <C> [BOLD] 84.5 <C> [BOLD] 85.7 <R> <C> D17-1276 <C> 93.7 ± 0.1 <C> 88.5 ± 0.4 <C> 88.5 ± 0.3 <C> [BOLD] 77.9 ± 0.1 <C> 77.5 ± 0.1 <C> 85.22 <R> <C> rel-norm <C> 92.2 ± 0.3 <C> 86.7 ± 0.7 <C> 87.9 ± 0.3 <C> 75.2 ± 0.5 <C> 76.4 ± 0.3 <C> 83.67 <R> <C> ment-norm <C> [BOLD] 93.9 ± 0.2 <C> 88.3 ± 0.6 <C> [BOLD] 89.9 ± 0.8 <C> 77.5 ± 0.1 <C> 78.0 ± 0.1 <C> 85.51 <R> <C> ment-norm ( [ITALIC] K=1) <C> 93.2 ± 0.3 <C> 88.4 ± 0.4 <C> 88.9 ± 1.0 <C> 77.0 ± 0.2 <C> 77.2 ± 0.1 <C> 84.94 <R> <C> ment-norm (no pad) <C> 93.6 ± 0.3 <C> 87.8 ± 0.5 <C> [BOLD] 90.0 ± 0.3 <C> 77.0 ± 0.2 <C> 77.3 ± 0.3 <C> 85.13 <CAP> Table 2: F1 scores on five out-domain test sets. Underlined scores show cases where the corresponding model outperforms the baseline. <COT> Looking at the "Methods" column and the "WIKI" row, we can see that the "ment-norm" method achieves an average score of 78.0 on the WIKI test set.
<R> <C> [EMPTY] <C> [ITALIC] UAR <C> [ITALIC] κ <C> [ITALIC] ρ <C> [ITALIC] eA <C> [ITALIC] Ep. <R> <C> LSTM <C> 0.78 <C> 0.85 <C> 0.91 <C> [BOLD] 0.99 <C> 101 <R> <C> BiLSTM <C> [BOLD] 0.78 <C> [BOLD] 0.85 <C> [BOLD] 0.92 <C> [BOLD] 0.99 <C> 100 <R> <C> LSTM+att <C> 0.74 <C> 0.82 <C> 0.91 <C> [BOLD] 0.99 <C> 101 <R> <C> BiLSTM+att <C> 0.75 <C> 0.83 <C> 0.91 <C> [BOLD] 0.99 <C> 93 <R> <C> Rach et al. ( 2017 ) <C> 0.55 <C> 0.68 <C> 0.83 <C> 0.94 <C> - <R> <C> Ultes et al. ( 2015 ) <C> 0.55 <C> - <C> - <C> 0.89 <C> - <CAP> Table 2: Performance of the proposed LSTM-based variants with the traditional cross-validation setup. Due to overlapping sub-dialogues in the train and test sets, the performance of the LSTM-based models achieve unrealistically high performance. <COT> Looking at the "Table 2" caption, we can see that the table shows the performance of different LSTM-based variants with the traditional cross-validation setup.
<R> <C> [EMPTY] <C> [ITALIC] UAR <C> [ITALIC] κ <C> [ITALIC] ρ <C> [ITALIC] eA <C> [ITALIC] Ep. <R> <C> LSTM <C> 0.78 <C> 0.85 <C> 0.91 <C> [BOLD] 0.99 <C> 101 <R> <C> BiLSTM <C> [BOLD] 0.78 <C> [BOLD] 0.85 <C> [BOLD] 0.92 <C> [BOLD] 0.99 <C> 100 <R> <C> LSTM+att <C> 0.74 <C> 0.82 <C> 0.91 <C> [BOLD] 0.99 <C> 101 <R> <C> BiLSTM+att <C> 0.75 <C> 0.83 <C> 0.91 <C> [BOLD] 0.99 <C> 93 <R> <C> Rach et al. ( 2017 ) <C> 0.55 <C> 0.68 <C> 0.83 <C> 0.94 <C> - <R> <C> Ultes et al. ( 2015 ) <C> 0.55 <C> - <C> - <C> 0.89 <C> - <CAP> Table 2: Performance of the proposed LSTM-based variants with the traditional cross-validation setup. Due to overlapping sub-dialogues in the train and test sets, the performance of the LSTM-based models achieve unrealistically high performance. <COT> Looking at the "BiLSTM" row, we can see that the UAR, κ, ρ, and eA values are all bolded, indicating that they are the highest values in their respective columns.
<R> <C> [EMPTY] <C> [BOLD] ellipsis (infl.) <C> [BOLD] ellipsis (VP) <R> <C> baseline <C> 53.0 <C> 28.4 <R> <C> concat <C> [BOLD] 76.2 <C> 76.6 <R> <C> s-hier-to-2.tied <C> 66.4 <C> 65.6 <R> <C> CADec <C> 72.2 <C> [BOLD] 80.0 <CAP> Table 8: Accuracy on ellipsis test set. <COT> Looking at the "CAP" tag, we can see that the table shows the accuracy on the ellipsis test set for different models.
<R> <C> [EMPTY] <C> [BOLD] ellipsis (infl.) <C> [BOLD] ellipsis (VP) <R> <C> baseline <C> 53.0 <C> 28.4 <R> <C> concat <C> [BOLD] 76.2 <C> 76.6 <R> <C> s-hier-to-2.tied <C> 66.4 <C> 65.6 <R> <C> CADec <C> 72.2 <C> [BOLD] 80.0 <CAP> Table 8: Accuracy on ellipsis test set. <COT> Looking at the "ellipsis (infl.)" column, we can see that the "concat" model has the highest accuracy.
<R> <C> [EMPTY] <C> [ITALIC] UAR <C> [ITALIC] κ <C> [ITALIC] ρ <C> [ITALIC] eA <C> [ITALIC] Ep. <R> <C> LSTM <C> 0.51 <C> 0.63 <C> 0.78 <C> 0.93 <C> 8 <R> <C> BiLSTM <C> 0.53 <C> 0.63 <C> 0.78 <C> 0.93 <C> 8 <R> <C> LSTM+att <C> 0.52 <C> 0.63 <C> 0.79 <C> 0.92 <C> 40 <R> <C> BiLSTM+att <C> [BOLD] 0.54 <C> [BOLD] 0.65 <C> [BOLD] 0.81 <C> [BOLD] 0.94 <C> 40 <R> <C> Rach et al. ( 2017 ) <C> 0.45 <C> 0.58 <C> 0.79 <C> 0.88 <C> 82 <R> <C> Ultes et al. ( 2015 ) <C> 0.44 <C> 0.53 <C> 0.69 <C> 0.86 <C> - <CAP> Table 3: Performance of the proposed LSTM-based variants with the dialogue-wise cross-validation setup. The models by Rach et al. (2017) and Ultes et al. (2015) have been re-implemented. The BiLSTM with attention mechanism performs best in all evaluation metrics. <COT> Looking at the "Table 3" caption, we can see that the table shows the performance of LSTM-based variants with the dialogue-wise cross-validation setup. The BiLSTM with attention mechanism performs best in all evaluation metrics.
<R> <C> [EMPTY] <C> [ITALIC] UAR <C> [ITALIC] κ <C> [ITALIC] ρ <C> [ITALIC] eA <C> [ITALIC] Ep. <R> <C> LSTM <C> 0.51 <C> 0.63 <C> 0.78 <C> 0.93 <C> 8 <R> <C> BiLSTM <C> 0.53 <C> 0.63 <C> 0.78 <C> 0.93 <C> 8 <R> <C> LSTM+att <C> 0.52 <C> 0.63 <C> 0.79 <C> 0.92 <C> 40 <R> <C> BiLSTM+att <C> [BOLD] 0.54 <C> [BOLD] 0.65 <C> [BOLD] 0.81 <C> [BOLD] 0.94 <C> 40 <R> <C> Rach et al. ( 2017 ) <C> 0.45 <C> 0.58 <C> 0.79 <C> 0.88 <C> 82 <R> <C> Ultes et al. ( 2015 ) <C> 0.44 <C> 0.53 <C> 0.69 <C> 0.86 <C> - <CAP> Table 3: Performance of the proposed LSTM-based variants with the dialogue-wise cross-validation setup. The models by Rach et al. (2017) and Ultes et al. (2015) have been re-implemented. The BiLSTM with attention mechanism performs best in all evaluation metrics. <COT> Looking at the "BiLSTM+att" row, we can see that the values in the "UAR", "κ", "ρ", and "eA" columns are all higher than the corresponding values in other rows.
<R> <C> This work Model <C> This work Mean F1-measure <R> <C> ner <C> 81.07 <R> <C> joint1 <C> 81.28 <R> <C> joint2 <C> 81.84 <R> <C> j_multi <C> [BOLD] 83.21 <R> <C> Previous work <C> Previous work <R> <C> ext_m_feat <C> [BOLD] 83.47 <CAP> Table 1: Evaluation of our models for NER performance with our dataset. We report F1-measure results over the test portion of our dataset averaged over 10 replications of the training with the same hyper parameters. <COT> Looking at the "This work Model" column, we can see that the "j_multi" model has the highest F1-measure of [BOLD] 83.21.
<R> <C> This work Model <C> This work Mean F1-measure <R> <C> ner <C> 81.07 <R> <C> joint1 <C> 81.28 <R> <C> joint2 <C> 81.84 <R> <C> j_multi <C> [BOLD] 83.21 <R> <C> Previous work <C> Previous work <R> <C> ext_m_feat <C> [BOLD] 83.47 <CAP> Table 1: Evaluation of our models for NER performance with our dataset. We report F1-measure results over the test portion of our dataset averaged over 10 replications of the training with the same hyper parameters. <COT> Looking at the "Previous work" column, we can see that the "ext_m_feat" model achieves the highest F1-measure of [BOLD] 83.47.
<R> <C> This work Model <C> This work Mean Accuracy <R> <C> md <C> 88.61 <R> <C> joint1 <C> 88.17 <R> <C> joint2 <C> 86.86 <R> <C> j_multi <C> 88.05 <R> <C> Previous work <C> Previous work <R> <C> yuret2006decision_lists <C> 89.55 <R> <C> shen2016role <C> [BOLD] 91.03 <CAP> Table 2: Evaluation of our models for MD performance. As in the NER evaluation, we report accuracies over the test dataset averaged over 10 replications of the training. <COT> Looking at the "This work Model" column, we can see that the model "md" has the highest mean accuracy of 88.61.
<R> <C> This work Model <C> This work Mean Accuracy <R> <C> md <C> 88.61 <R> <C> joint1 <C> 88.17 <R> <C> joint2 <C> 86.86 <R> <C> j_multi <C> 88.05 <R> <C> Previous work <C> Previous work <R> <C> yuret2006decision_lists <C> 89.55 <R> <C> shen2016role <C> [BOLD] 91.03 <CAP> Table 2: Evaluation of our models for MD performance. As in the NER evaluation, we report accuracies over the test dataset averaged over 10 replications of the training. <COT> Looking at the "Previous work" column, we can see that the model "shen2016role" has a mean accuracy of 91.03, which is marked as bold.
<R> <C> Model <C> Accuracy <R> <C> fastText <C> 68.7% <R> <C> GloVe <C> 73.1% <R> <C> [ITALIC] BioELMo Jin et al. ( 2019 ) <C> 78.2% <R> <C> [ITALIC] ESIMw/ [ITALIC] K Lu et al. ( 2019 ) <C> 77.8% <R> <C> fastText w/ KG+Sentiment <C> 73.67% <R> <C> GloVe w/ KG+Sentiment <C> 74.46% <R> <C> BioELMo w/ KG <C> 78.76% <R> <C> [BOLD] BioELMo w/ KG+Sentiment <C> [BOLD] 79.04% <CAP> Table 1: Performance of our models (bottom four) along with the state-of-the-art baseline models (top four). Baseline results for fastText, GloVe are obtained from Romanov and Shivade (2018). Adding knowledge graph information to the base models showed an absolute improvement of 4.97% in case of fastText and 1.36% in case of GloVe. The baseline model utilizing BioELMo as base embeddings Jin et al. (2019) showed an accuracy of 78.2%. On adding knowledge graph information, we were able to improve these results to 78.76% and on further addition of sentiment information, the accuracy rose to 79.04% <COT> Looking at the "Model" column, we can see that the model "BioELMo w/ KG+Sentiment" has an accuracy of "[BOLD] 79.04%".
<R> <C> Model <C> Accuracy <R> <C> fastText <C> 68.7% <R> <C> GloVe <C> 73.1% <R> <C> [ITALIC] BioELMo Jin et al. ( 2019 ) <C> 78.2% <R> <C> [ITALIC] ESIMw/ [ITALIC] K Lu et al. ( 2019 ) <C> 77.8% <R> <C> fastText w/ KG+Sentiment <C> 73.67% <R> <C> GloVe w/ KG+Sentiment <C> 74.46% <R> <C> BioELMo w/ KG <C> 78.76% <R> <C> [BOLD] BioELMo w/ KG+Sentiment <C> [BOLD] 79.04% <CAP> Table 1: Performance of our models (bottom four) along with the state-of-the-art baseline models (top four). Baseline results for fastText, GloVe are obtained from Romanov and Shivade (2018). Adding knowledge graph information to the base models showed an absolute improvement of 4.97% in case of fastText and 1.36% in case of GloVe. The baseline model utilizing BioELMo as base embeddings Jin et al. (2019) showed an accuracy of 78.2%. On adding knowledge graph information, we were able to improve these results to 78.76% and on further addition of sentiment information, the accuracy rose to 79.04% <COT> Looking at the "fastText w/ KG+Sentiment" row, we can see that it has an accuracy of "73.67%".
<R> <C> [ITALIC] p <C> [BOLD] BLEU <C> [BOLD] deixis <C> [BOLD] lex. c. <C> [BOLD] ellipsis <R> <C> [ITALIC] p=0 <C> 32.34 <C> 84.1 <C> 48.7 <C> 65 / 75 <R> <C> [ITALIC] p=0.25 <C> 32.31 <C> 83.3 <C> 52.4 <C> 67 / 78 <R> <C> [ITALIC] p=0.5 <C> 32.38 <C> 81.6 <C> 58.1 <C> 72 / 80 <R> <C> [ITALIC] p=0.75 <C> 32.45 <C> 80.0 <C> 65.0 <C> 70 / 80 <CAP> Table 9: Results for different probabilities of using corrupted reference at training time. BLEU for 3 context sentences. For ellipsis, we show inflection/VP scores. <COT> Looking at the "Table 9" caption, we can see that the table presents results for different probabilities of using corrupted reference at training time.
<R> <C> [ITALIC] p <C> [BOLD] BLEU <C> [BOLD] deixis <C> [BOLD] lex. c. <C> [BOLD] ellipsis <R> <C> [ITALIC] p=0 <C> 32.34 <C> 84.1 <C> 48.7 <C> 65 / 75 <R> <C> [ITALIC] p=0.25 <C> 32.31 <C> 83.3 <C> 52.4 <C> 67 / 78 <R> <C> [ITALIC] p=0.5 <C> 32.38 <C> 81.6 <C> 58.1 <C> 72 / 80 <R> <C> [ITALIC] p=0.75 <C> 32.45 <C> 80.0 <C> 65.0 <C> 70 / 80 <CAP> Table 9: Results for different probabilities of using corrupted reference at training time. BLEU for 3 context sentences. For ellipsis, we show inflection/VP scores. <COT> Looking at the "lex. c." column, we can see that the scores for lexicon coverage increase as the value of p increases.
<R> <C> Model <C> Total P <C> Total R <C> Total F1 <C> General P <C> General R <C> General F1 <C> Fine P <C> Fine R <C> Fine F1 <C> Ultra-Fine P <C> Ultra-Fine R <C> Ultra-Fine F1 <R> <C> Ours + GloVe w/o augmentation <C> 46.4 <C> 23.3 <C> 31.0 <C> 57.7 <C> 65.5 <C> 61.4 <C> 41.3 <C> 31.3 <C> 35.6 <C> 42.4 <C> 9.2 <C> 15.1 <R> <C> Ours + ELMo w/o augmentation <C> [BOLD] 55.6 <C> 28.1 <C> 37.3 <C> [BOLD] 69.3 <C> 77.3 <C> 73.0 <C> [BOLD] 47.9 <C> 35.4 <C> 40.7 <C> [BOLD] 48.9 <C> 12.6 <C> 20.0 <R> <C> Ours + ELMo w augmentation <C> 55.2 <C> 26.4 <C> 35.7 <C> 69.4 <C> 72.0 <C> 70.7 <C> 46.6 <C> 38.5 <C> 42.2 <C> 48.7 <C> 10.3 <C> 17.1 <R> <C> Ours + ELMo w augmentation <C> 50.7 <C> [BOLD] 33.1 <C> [BOLD] 40.1 <C> 66.9 <C> [BOLD] 80.7 <C> 73.2 <C> 41.7 <C> 46.2 <C> 43.8 <C> 45.6 <C> [BOLD] 17.4 <C> [BOLD] 25.2 <R> <C> + filter & relabel <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT-Base, Uncased <C> 51.6 <C> 32.8 <C> [BOLD] 40.1 <C> 67.4 <C> 80.6 <C> [BOLD] 73.4 <C> 41.6 <C> [BOLD] 54.7 <C> [BOLD] 47.3 <C> 46.3 <C> 15.6 <C> 23.4 <R> <C> Choi et al. ( 2018 ) w augmentation <C> 48.1 <C> 23.2 <C> 31.3 <C> 60.3 <C> 61.6 <C> 61.0 <C> 40.4 <C> 38.4 <C> 39.4 <C> 42.8 <C> 8.8 <C> 14.6 <CAP> Table 1: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et al. (2018) comparing various systems. ELMo gives a substantial improvement over baselines. Over an ELMo-equipped model, data augmentation using the method of Choi et al. (2018) gives no benefit. However, our denoising technique allow us to effectively incorporate distant data, matching the results of a BERT model on this task Devlin et al. (2018). <COT> Looking at the "Model" column, we can see that the "Ours + ELMo w/o augmentation" model has a higher Total P, Total R, and Total F1 compared to the "Ours + GloVe w/o augmentation" model.
<R> <C> Model <C> Total P <C> Total R <C> Total F1 <C> General P <C> General R <C> General F1 <C> Fine P <C> Fine R <C> Fine F1 <C> Ultra-Fine P <C> Ultra-Fine R <C> Ultra-Fine F1 <R> <C> Ours + GloVe w/o augmentation <C> 46.4 <C> 23.3 <C> 31.0 <C> 57.7 <C> 65.5 <C> 61.4 <C> 41.3 <C> 31.3 <C> 35.6 <C> 42.4 <C> 9.2 <C> 15.1 <R> <C> Ours + ELMo w/o augmentation <C> [BOLD] 55.6 <C> 28.1 <C> 37.3 <C> [BOLD] 69.3 <C> 77.3 <C> 73.0 <C> [BOLD] 47.9 <C> 35.4 <C> 40.7 <C> [BOLD] 48.9 <C> 12.6 <C> 20.0 <R> <C> Ours + ELMo w augmentation <C> 55.2 <C> 26.4 <C> 35.7 <C> 69.4 <C> 72.0 <C> 70.7 <C> 46.6 <C> 38.5 <C> 42.2 <C> 48.7 <C> 10.3 <C> 17.1 <R> <C> Ours + ELMo w augmentation <C> 50.7 <C> [BOLD] 33.1 <C> [BOLD] 40.1 <C> 66.9 <C> [BOLD] 80.7 <C> 73.2 <C> 41.7 <C> 46.2 <C> 43.8 <C> 45.6 <C> [BOLD] 17.4 <C> [BOLD] 25.2 <R> <C> + filter & relabel <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT-Base, Uncased <C> 51.6 <C> 32.8 <C> [BOLD] 40.1 <C> 67.4 <C> 80.6 <C> [BOLD] 73.4 <C> 41.6 <C> [BOLD] 54.7 <C> [BOLD] 47.3 <C> 46.3 <C> 15.6 <C> 23.4 <R> <C> Choi et al. ( 2018 ) w augmentation <C> 48.1 <C> 23.2 <C> 31.3 <C> 60.3 <C> 61.6 <C> 61.0 <C> 40.4 <C> 38.4 <C> 39.4 <C> 42.8 <C> 8.8 <C> 14.6 <CAP> Table 1: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et al. (2018) comparing various systems. ELMo gives a substantial improvement over baselines. Over an ELMo-equipped model, data augmentation using the method of Choi et al. (2018) gives no benefit. However, our denoising technique allow us to effectively incorporate distant data, matching the results of a BERT model on this task Devlin et al. (2018). <COT> Looking at the "Fine F1" column, we can see that the "BERT-Base, Uncased" model has a higher Fine F1 score compared to the "Choi et al. (2018) w augmentation" model.
<R> <C> Model <C> P <C> R <C> F1 <R> <C> Ours + GloVe w/o augmentation <C> 47.6 <C> 23.3 <C> 31.3 <R> <C> Ours + ELMo w/o augmentation <C> [BOLD] 55.8 <C> 27.7 <C> 37.0 <R> <C> Ours + ELMo w augmentation <C> 55.5 <C> 26.3 <C> 35.7 <R> <C> Ours + ELMo w augmentation <C> 51.5 <C> [BOLD] 33.0 <C> [BOLD] 40.2 <R> <C> + filter & relabel <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT-Base, Uncased <C> 51.6 <C> [BOLD] 33.0 <C> [BOLD] 40.2 <R> <C> Choi et al. ( 2018 ) w augmentation <C> 47.1 <C> 24.2 <C> 32.0 <R> <C> LabelGCN Xiong et al. ( 2019 ) <C> 50.3 <C> 29.2 <C> 36.9 <CAP> Table 2: Macro-averaged P/R/F1 on the test set for the entity typing task of Choi et al. (2018). Our denoising approach gives substantial gains over naive augmentation and matches the performance of a BERT model. <COT> Looking at the "Model" column, we can see that "Ours + ELMo w/o augmentation" has a higher F1 score than "Ours + GloVe w/o augmentation".
<R> <C> Model <C> P <C> R <C> F1 <R> <C> Ours + GloVe w/o augmentation <C> 47.6 <C> 23.3 <C> 31.3 <R> <C> Ours + ELMo w/o augmentation <C> [BOLD] 55.8 <C> 27.7 <C> 37.0 <R> <C> Ours + ELMo w augmentation <C> 55.5 <C> 26.3 <C> 35.7 <R> <C> Ours + ELMo w augmentation <C> 51.5 <C> [BOLD] 33.0 <C> [BOLD] 40.2 <R> <C> + filter & relabel <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT-Base, Uncased <C> 51.6 <C> [BOLD] 33.0 <C> [BOLD] 40.2 <R> <C> Choi et al. ( 2018 ) w augmentation <C> 47.1 <C> 24.2 <C> 32.0 <R> <C> LabelGCN Xiong et al. ( 2019 ) <C> 50.3 <C> 29.2 <C> 36.9 <CAP> Table 2: Macro-averaged P/R/F1 on the test set for the entity typing task of Choi et al. (2018). Our denoising approach gives substantial gains over naive augmentation and matches the performance of a BERT model. <COT> Looking at the "Ours + ELMo w augmentation" row, we can see that it has the same F1 score as "BERT-Base, Uncased".
<R> <C> Type <C> Denoising Method <C> EL&HEAD P <C> EL&HEAD R <C> EL&HEAD F1 <C> EL P <C> EL R <C> EL F1 <C> HEAD P <C> HEAD R <C> HEAD F1 <R> <C> [EMPTY] <C> Raw Data <C> [BOLD] 55.2 <C> 26.4 <C> 35.7 <C> 52.3 <C> 26.1 <C> 34.8 <C> [BOLD] 52.8 <C> 28.4 <C> 36.9 <R> <C> Heuristic Baselines <C> Synonyms&Hypernyms <C> 43.0 <C> 30.0 <C> 35.3 <C> 47.5 <C> 26.3 <C> 33.9 <C> 44.8 <C> 31.7 <C> 37.1 <R> <C> [EMPTY] <C> Pair <C> 50.2 <C> 29.0 <C> 36.8 <C> 49.6 <C> 27.0 <C> 35.0 <C> 50.6 <C> 31.2 <C> 38.6 <R> <C> [EMPTY] <C> Overlap <C> 50.0 <C> 32.3 <C> 39.2 <C> 49.5 <C> [BOLD] 30.8 <C> 38.0 <C> 50.6 <C> 31.4 <C> 38.7 <R> <C> Proposed Approach <C> Filter <C> 53.1 <C> 28.2 <C> 36.8 <C> 51.9 <C> 26.5 <C> 35.1 <C> 51.2 <C> 31.2 <C> 38.7 <R> <C> [EMPTY] <C> Relabel <C> 52.1 <C> 32.2 <C> 39.8 <C> 50.2 <C> 31.4 <C> 38.6 <C> 50.2 <C> 31.8 <C> 38.9 <R> <C> [EMPTY] <C> Filter&Relabel <C> 50.7 <C> [BOLD] 33.1 <C> [BOLD] 40.1 <C> [BOLD] 52.7 <C> 30.5 <C> [BOLD] 38.7 <C> 50.7 <C> [BOLD] 32.1 <C> [BOLD] 39.3 <R> <C> [EMPTY] <C> Choi et al. ( 2018 ) <C> 48.1 <C> 23.2 <C> 31.3 <C> 50.3 <C> 19.6 <C> 28.2 <C> 48.4 <C> 22.3 <C> 30.6 <CAP> Table 3: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et al. (2018) with various types of augmentation added. The customized loss from Choi et al. (2018) actually causes a decrease in performance from adding any of the datasets. Heuristics can improve incorporation of this data: a relabeling heuristic (Pair) helps on HEAD and a filtering heuristic (Overlap) is helpful in both settings. However, our trainable filtering and relabeling models outperform both of these techniques. <COT> Looking at the "Type" column, the "Proposed Approach" row, and the "EL&HEAD F1" cell, we can see that the EL&HEAD F1 score for the Proposed Approach is 36.8.
<R> <C> Type <C> Denoising Method <C> EL&HEAD P <C> EL&HEAD R <C> EL&HEAD F1 <C> EL P <C> EL R <C> EL F1 <C> HEAD P <C> HEAD R <C> HEAD F1 <R> <C> [EMPTY] <C> Raw Data <C> [BOLD] 55.2 <C> 26.4 <C> 35.7 <C> 52.3 <C> 26.1 <C> 34.8 <C> [BOLD] 52.8 <C> 28.4 <C> 36.9 <R> <C> Heuristic Baselines <C> Synonyms&Hypernyms <C> 43.0 <C> 30.0 <C> 35.3 <C> 47.5 <C> 26.3 <C> 33.9 <C> 44.8 <C> 31.7 <C> 37.1 <R> <C> [EMPTY] <C> Pair <C> 50.2 <C> 29.0 <C> 36.8 <C> 49.6 <C> 27.0 <C> 35.0 <C> 50.6 <C> 31.2 <C> 38.6 <R> <C> [EMPTY] <C> Overlap <C> 50.0 <C> 32.3 <C> 39.2 <C> 49.5 <C> [BOLD] 30.8 <C> 38.0 <C> 50.6 <C> 31.4 <C> 38.7 <R> <C> Proposed Approach <C> Filter <C> 53.1 <C> 28.2 <C> 36.8 <C> 51.9 <C> 26.5 <C> 35.1 <C> 51.2 <C> 31.2 <C> 38.7 <R> <C> [EMPTY] <C> Relabel <C> 52.1 <C> 32.2 <C> 39.8 <C> 50.2 <C> 31.4 <C> 38.6 <C> 50.2 <C> 31.8 <C> 38.9 <R> <C> [EMPTY] <C> Filter&Relabel <C> 50.7 <C> [BOLD] 33.1 <C> [BOLD] 40.1 <C> [BOLD] 52.7 <C> 30.5 <C> [BOLD] 38.7 <C> 50.7 <C> [BOLD] 32.1 <C> [BOLD] 39.3 <R> <C> [EMPTY] <C> Choi et al. ( 2018 ) <C> 48.1 <C> 23.2 <C> 31.3 <C> 50.3 <C> 19.6 <C> 28.2 <C> 48.4 <C> 22.3 <C> 30.6 <CAP> Table 3: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et al. (2018) with various types of augmentation added. The customized loss from Choi et al. (2018) actually causes a decrease in performance from adding any of the datasets. Heuristics can improve incorporation of this data: a relabeling heuristic (Pair) helps on HEAD and a filtering heuristic (Overlap) is helpful in both settings. However, our trainable filtering and relabeling models outperform both of these techniques. <COT> Looking at the "Denoising Method" column, the "Filter&Relabel" row, and the "EL&HEAD F1" cell, we can see that the EL&HEAD F1 score for the Filter&Relabel denoising method is 40.1.
<R> <C> Model <C> Acc. <C> Ma-F1 <C> Mi-F1 <R> <C> Ours + ELMo w/o augmentation <C> 42.7 <C> 72.7 <C> 66.7 <R> <C> Ours + ELMo w augmentation <C> 59.3 <C> 76.5 <C> 70.7 <R> <C> Ours + ELMo w augmentation <C> 63.9 <C> [BOLD] 84.5 <C> 78.9 <R> <C> + filter & relabel <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Ours + ELMo w augmentation <C> [BOLD] 64.9 <C> [BOLD] 84.5 <C> [BOLD] 79.2 <R> <C> by Choi et al. ( 2018 ) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT-Base, Uncased <C> 51.8 <C> 76.6 <C> 69.1 <R> <C> Shimaoka et al. ( 2017 ) <C> 51.7 <C> 70.9 <C> 64.9 <R> <C> AFET Ren et al. ( 2016a ) <C> 55.1 <C> 71.1 <C> 64.7 <R> <C> PLE Ren et al. ( 2016b ) <C> 57.2 <C> 71.5 <C> 66.1 <R> <C> Choi et al. ( 2018 ) <C> 59.5 <C> 76.8 <C> 71.8 <R> <C> LabelGCN Xiong et al. ( 2019 ) <C> 59.6 <C> 77.8 <C> 72.2 <CAP> Table 4: Test results on OntoNotes. Denoising helps substantially even in this reduced setting. Using fewer distant examples, we nearly match the performance using the data from Choi et al. (2018) (see text). <COT> Looking at the "Model" column, "Ours + ELMo w augmentation" row, and the "Acc." column, we can see that the accuracy of the "Ours + ELMo w augmentation" model is 63.9.
<R> <C> Model <C> Acc. <C> Ma-F1 <C> Mi-F1 <R> <C> Ours + ELMo w/o augmentation <C> 42.7 <C> 72.7 <C> 66.7 <R> <C> Ours + ELMo w augmentation <C> 59.3 <C> 76.5 <C> 70.7 <R> <C> Ours + ELMo w augmentation <C> 63.9 <C> [BOLD] 84.5 <C> 78.9 <R> <C> + filter & relabel <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Ours + ELMo w augmentation <C> [BOLD] 64.9 <C> [BOLD] 84.5 <C> [BOLD] 79.2 <R> <C> by Choi et al. ( 2018 ) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT-Base, Uncased <C> 51.8 <C> 76.6 <C> 69.1 <R> <C> Shimaoka et al. ( 2017 ) <C> 51.7 <C> 70.9 <C> 64.9 <R> <C> AFET Ren et al. ( 2016a ) <C> 55.1 <C> 71.1 <C> 64.7 <R> <C> PLE Ren et al. ( 2016b ) <C> 57.2 <C> 71.5 <C> 66.1 <R> <C> Choi et al. ( 2018 ) <C> 59.5 <C> 76.8 <C> 71.8 <R> <C> LabelGCN Xiong et al. ( 2019 ) <C> 59.6 <C> 77.8 <C> 72.2 <CAP> Table 4: Test results on OntoNotes. Denoising helps substantially even in this reduced setting. Using fewer distant examples, we nearly match the performance using the data from Choi et al. (2018) (see text). <COT> Looking at the "Ma-F1" column, "Ours + ELMo w augmentation" row, and the "[BOLD] 84.5" cell, we can see that the macro F1 score of the "Ours + ELMo w augmentation" model is 84.5.
<R> <C> Data <C> General Add <C> General Del <C> Fine Add <C> Fine Del <C> Ultra-Fine Add <C> Ultra-Fine Del <C> Filter (%) <R> <C> EL <C> 0.87 <C> 0.01 <C> 0.36 <C> 0.17 <C> 2.03 <C> 0.12 <C> 9.4 <R> <C> HEAD <C> 1.18 <C> 0.00 <C> 0.51 <C> 0.01 <C> 1.15 <C> 0.16 <C> 10.0 <CAP> Table 5: The average number of types added or deleted by the relabeling function per example. The right-most column shows that the rate of examples discarded by the filtering function. <COT> Looking at the "CAP" tag, we can see that Table 5 provides information about the average number of types added or deleted by the relabeling function per example, as well as the rate of examples discarded by the filtering function.
<R> <C> Data <C> General Add <C> General Del <C> Fine Add <C> Fine Del <C> Ultra-Fine Add <C> Ultra-Fine Del <C> Filter (%) <R> <C> EL <C> 0.87 <C> 0.01 <C> 0.36 <C> 0.17 <C> 2.03 <C> 0.12 <C> 9.4 <R> <C> HEAD <C> 1.18 <C> 0.00 <C> 0.51 <C> 0.01 <C> 1.15 <C> 0.16 <C> 10.0 <CAP> Table 5: The average number of types added or deleted by the relabeling function per example. The right-most column shows that the rate of examples discarded by the filtering function. <COT> Looking at the "Filter (%)" column, we can see that the filtering function discards 9.4% of the examples for the "EL" data and 10.0% of the examples for the "HEAD" data.
<R> <C> [BOLD] Dataset <C> [BOLD] # Samples Train <C> [BOLD] # Samples Val. <C> [BOLD] # Samples Test <C> [BOLD] Message (Avg.) # tokens <C> [BOLD] Message (Avg.) # groups <C> [BOLD] Message (Avg.) # tokens /group <C> [BOLD] Response (Avg.) # tokens <C> [BOLD] Response (Avg.) # groups <C> [BOLD] Response (Avg.) # tokens /group <R> <C> Ubuntu-v1 <C> 1M <C> 35,609 <C> 35,517 <C> 162.47 ±132.47 <C> 8.43 ±6.32 <C> 20.14 ±18.41 <C> 14.44 ±13.93 <C> 1 <C> - <R> <C> Ubuntu-v2 <C> 1M <C> 19,560 <C> 18,920 <C> 85.92 ±74.71 <C> 4.95 ±2.98 <C> 20.73 ±20.19 <C> 17.01 ±16.41 <C> 1 <C> - <R> <C> Samsung QA <C> 163,616 <C> 10,000 <C> 10,000 <C> 12.84 ±6.42 <C> 1 <C> - <C> 173.48 ±192.12 <C> 6.09 ±5.58 <C> 29.28 ±31.91 <CAP> Table 1: Properties of the Ubuntu and Samsung QA dataset. The message and response are {context}, {response} in Ubuntu and {question}, {answer} in the Samsung QA dataset. <COT> Looking at the "Samsung QA" row, the "Message (Avg.) # tokens" column, and the "Response (Avg.) # tokens" column, we can see that the average number of tokens in the message is smaller than the average number of tokens in the response.
<R> <C> [BOLD] Model <C> [BOLD] Ubuntu-v1 1 in 2R@1 <C> [BOLD] Ubuntu-v1 1 in 10R@1 <C> [BOLD] Ubuntu-v1 1 in 10R@2 <C> [BOLD] Ubuntu-v1 1 in 10R@5 <R> <C> TF-IDF [1] <C> 0.659 <C> 0.410 <C> 0.545 <C> 0.708 <R> <C> CNN [2] <C> 0.848 <C> 0.549 <C> 0.684 <C> 0.896 <R> <C> LSTM [2] <C> 0.901 <C> 0.638 <C> 0.784 <C> 0.949 <R> <C> CompAgg [3] <C> 0.884 <C> 0.631 <C> 0.753 <C> 0.927 <R> <C> BiMPM [4] <C> 0.897 <C> 0.665 <C> 0.786 <C> 0.938 <R> <C> RDE <C> 0.898 ±0.002 <C> 0.643 ±0.009 <C> 0.784 ±0.007 <C> 0.945 ±0.002 <R> <C> RDE-LTC <C> 0.903 ±0.001 <C> 0.656 ±0.003 <C> 0.794 ±0.003 <C> 0.948 ±0.001 <R> <C> HRDE <C> 0.915 ±0.001 <C> 0.681 ±0.001 <C> 0.820 ±0.001 <C> 0.959 ±0.001 <R> <C> HRDE-LTC <C> [BOLD] 0.916 ±0.001 <C> [BOLD] 0.684 ±0.001 <C> [BOLD] 0.822 ±0.001 <C> [BOLD] 0.960 ±0.001 <CAP> Table 3: Model performance results for the Ubuntu-v1 dataset. Models [1-4] are from Lowe et al. (2015); Kadlec et al. (2015); Wang and Jiang (2016); Wang et al. (2017), respectively. <COT> Looking at the "HRDE-LTC" cell, "[BOLD] 0.916 ±0.001" cell, and "Ubuntu-v1 1 in 10R@1" cell, we can see that the HRDE-LTC model has the highest performance on the Ubuntu-v1 dataset when considering the top-1 response.
<R> <C> [BOLD] Model <C> [BOLD] Ubuntu-v1 1 in 2R@1 <C> [BOLD] Ubuntu-v1 1 in 10R@1 <C> [BOLD] Ubuntu-v1 1 in 10R@2 <C> [BOLD] Ubuntu-v1 1 in 10R@5 <R> <C> TF-IDF [1] <C> 0.659 <C> 0.410 <C> 0.545 <C> 0.708 <R> <C> CNN [2] <C> 0.848 <C> 0.549 <C> 0.684 <C> 0.896 <R> <C> LSTM [2] <C> 0.901 <C> 0.638 <C> 0.784 <C> 0.949 <R> <C> CompAgg [3] <C> 0.884 <C> 0.631 <C> 0.753 <C> 0.927 <R> <C> BiMPM [4] <C> 0.897 <C> 0.665 <C> 0.786 <C> 0.938 <R> <C> RDE <C> 0.898 ±0.002 <C> 0.643 ±0.009 <C> 0.784 ±0.007 <C> 0.945 ±0.002 <R> <C> RDE-LTC <C> 0.903 ±0.001 <C> 0.656 ±0.003 <C> 0.794 ±0.003 <C> 0.948 ±0.001 <R> <C> HRDE <C> 0.915 ±0.001 <C> 0.681 ±0.001 <C> 0.820 ±0.001 <C> 0.959 ±0.001 <R> <C> HRDE-LTC <C> [BOLD] 0.916 ±0.001 <C> [BOLD] 0.684 ±0.001 <C> [BOLD] 0.822 ±0.001 <C> [BOLD] 0.960 ±0.001 <CAP> Table 3: Model performance results for the Ubuntu-v1 dataset. Models [1-4] are from Lowe et al. (2015); Kadlec et al. (2015); Wang and Jiang (2016); Wang et al. (2017), respectively. <COT> Looking at the "RDE" cell, "0.898 ±0.002" cell, and "Ubuntu-v1 1 in 10R@5" cell, we can observe that the RDE model achieves a performance of 0.898 ±0.002 on the Ubuntu-v1 dataset when considering the top-5 response.
<R> <C> [BOLD] Model <C> [BOLD] Ubuntu-v2 1 in 2R@1 <C> [BOLD] Ubuntu-v2 1 in 10R@1 <C> [BOLD] Ubuntu-v2 1 in 10R@2 <C> [BOLD] Ubuntu-v2 1 in 10R@5 <R> <C> LSTM [1] <C> 0.869 <C> 0.552 <C> 0.721 <C> 0.924 <R> <C> RNN [5] <C> 0.907 ±0.002 <C> 0.664 ±0.004 <C> 0.799 ±0.004 <C> 0.951 ±0.001 <R> <C> CNN [5] <C> 0.863 ±0.003 <C> 0.587 ±0.004 <C> 0.721 ±0.005 <C> 0.907 ±0.003 <R> <C> RNN-CNN [5] <C> 0.911 ±0.001 <C> [BOLD] 0.672 ±0.002 <C> 0.809 ±0.002 <C> 0.956 ±0.001 <R> <C> Attention \scriptsize{[6]}(RNN-CNN) <C> 0.903 ±0.002 <C> 0.653 ±0.005 <C> 0.788 ±0.005 <C> 0.945 ±0.002 <R> <C> CompAgg [3] <C> 0.895 <C> 0.641 <C> 0.776 <C> 0.937 <R> <C> BiMPM [4] <C> 0.877 <C> 0.611 <C> 0.747 <C> 0.921 <R> <C> RDE <C> 0.894 ±0.002 <C> 0.610 ±0.008 <C> 0.776 ±0.006 <C> 0.947 ±0.002 <R> <C> RDE-LTC <C> 0.899 ±0.002 <C> 0.625 ±0.004 <C> 0.788 ±0.004 <C> 0.951 ±0.001 <R> <C> HRDE <C> 0.914 ±0.001 <C> 0.649 ±0.001 <C> 0.813 ±0.001 <C> 0.964 ±0.001 <R> <C> HRDE-LTC <C> [BOLD] 0.915 ±0.002 <C> 0.652 ±0.003 <C> [BOLD] 0.815 ±0.001 <C> [BOLD] 0.966 ±0.001 <CAP> Table 4: Model performance results for the Ubuntu-v2 dataset. Models [1,3-6] are from Lowe et al. (2015); Wang and Jiang (2016); Wang et al. (2017); Baudiš et al. (2016); Tan et al. (2015), respectively. <COT> Looking at the "Ubuntu-v2 1 in 10R@2" column, we can see that the HRDE-LTC model achieves the highest performance with a score of [BOLD] 0.815 ±0.001.
<R> <C> [BOLD] Model <C> [BOLD] Ubuntu-v2 1 in 2R@1 <C> [BOLD] Ubuntu-v2 1 in 10R@1 <C> [BOLD] Ubuntu-v2 1 in 10R@2 <C> [BOLD] Ubuntu-v2 1 in 10R@5 <R> <C> LSTM [1] <C> 0.869 <C> 0.552 <C> 0.721 <C> 0.924 <R> <C> RNN [5] <C> 0.907 ±0.002 <C> 0.664 ±0.004 <C> 0.799 ±0.004 <C> 0.951 ±0.001 <R> <C> CNN [5] <C> 0.863 ±0.003 <C> 0.587 ±0.004 <C> 0.721 ±0.005 <C> 0.907 ±0.003 <R> <C> RNN-CNN [5] <C> 0.911 ±0.001 <C> [BOLD] 0.672 ±0.002 <C> 0.809 ±0.002 <C> 0.956 ±0.001 <R> <C> Attention \scriptsize{[6]}(RNN-CNN) <C> 0.903 ±0.002 <C> 0.653 ±0.005 <C> 0.788 ±0.005 <C> 0.945 ±0.002 <R> <C> CompAgg [3] <C> 0.895 <C> 0.641 <C> 0.776 <C> 0.937 <R> <C> BiMPM [4] <C> 0.877 <C> 0.611 <C> 0.747 <C> 0.921 <R> <C> RDE <C> 0.894 ±0.002 <C> 0.610 ±0.008 <C> 0.776 ±0.006 <C> 0.947 ±0.002 <R> <C> RDE-LTC <C> 0.899 ±0.002 <C> 0.625 ±0.004 <C> 0.788 ±0.004 <C> 0.951 ±0.001 <R> <C> HRDE <C> 0.914 ±0.001 <C> 0.649 ±0.001 <C> 0.813 ±0.001 <C> 0.964 ±0.001 <R> <C> HRDE-LTC <C> [BOLD] 0.915 ±0.002 <C> 0.652 ±0.003 <C> [BOLD] 0.815 ±0.001 <C> [BOLD] 0.966 ±0.001 <CAP> Table 4: Model performance results for the Ubuntu-v2 dataset. Models [1,3-6] are from Lowe et al. (2015); Wang and Jiang (2016); Wang et al. (2017); Baudiš et al. (2016); Tan et al. (2015), respectively. <COT> Looking at the "LSTM" row, we can compare the performance of the LSTM model across different metrics. The LSTM model achieves the highest score of 0.924 in the "Ubuntu-v2 1 in 10R@5" metric.
<R> <C> [BOLD] Model <C> [BOLD] Samsung QA 1 in 2R@1 <C> [BOLD] Samsung QA 1 in 10R@1 <C> [BOLD] Samsung QA 1 in 10R@2 <C> [BOLD] Samsung QA 1 in 10R@5 <R> <C> TF-IDF <C> 0.939 <C> 0.834 <C> 0.897 <C> 0.953 <R> <C> RDE <C> 0.978 ±0.002 <C> 0.869 ±0.009 <C> 0.966 ±0.003 <C> 0.997 ±0.001 <R> <C> RDE-LTC <C> 0.981 ±0.002 <C> 0.880 ±0.009 <C> 0.970 ±0.003 <C> 0.997 ±0.001 <R> <C> HRDE <C> 0.981 ±0.002 <C> 0.885 ±0.011 <C> 0.971 ±0.004 <C> 0.997 ±0.001 <R> <C> HRDE-LTC <C> [BOLD] 0.983 ±0.002 <C> [BOLD] 0.890 ±0.010 <C> [BOLD] 0.972 ±0.003 <C> [BOLD] 0.998 ±0.001 <CAP> Table 5: Model performance results for the Samsung QA dataset. <COT> Looking at the "Model" column, we can see that the highest performance scores are achieved by the HRDE-LTC model.
<R> <C> [BOLD] Model <C> [BOLD] Samsung QA 1 in 2R@1 <C> [BOLD] Samsung QA 1 in 10R@1 <C> [BOLD] Samsung QA 1 in 10R@2 <C> [BOLD] Samsung QA 1 in 10R@5 <R> <C> TF-IDF <C> 0.939 <C> 0.834 <C> 0.897 <C> 0.953 <R> <C> RDE <C> 0.978 ±0.002 <C> 0.869 ±0.009 <C> 0.966 ±0.003 <C> 0.997 ±0.001 <R> <C> RDE-LTC <C> 0.981 ±0.002 <C> 0.880 ±0.009 <C> 0.970 ±0.003 <C> 0.997 ±0.001 <R> <C> HRDE <C> 0.981 ±0.002 <C> 0.885 ±0.011 <C> 0.971 ±0.004 <C> 0.997 ±0.001 <R> <C> HRDE-LTC <C> [BOLD] 0.983 ±0.002 <C> [BOLD] 0.890 ±0.010 <C> [BOLD] 0.972 ±0.003 <C> [BOLD] 0.998 ±0.001 <CAP> Table 5: Model performance results for the Samsung QA dataset. <COT> Looking at the "Samsung QA 1 in 10R@1" column, we can see that the RDE model performs better than the TF-IDF model.
<R> <C> Model <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> METEOR <C> ROUGE-L <R> <C> Zhou et al. ( 2017 ) <C> - <C> - <C> - <C> 13.29 <C> - <C> - <R> <C> Zhao et al. ( 2018 )* <C> 45.69 <C> 29.58 <C> 22.16 <C> 16.85 <C> 20.62 <C> 44.99 <R> <C> Kim et al. ( 2019 ) <C> - <C> - <C> - <C> 16.17 <C> - <C> - <R> <C> Liu et al. ( 2019 ) <C> 46.58 <C> 30.90 <C> 22.82 <C> 17.55 <C> 21.24 <C> 44.53 <R> <C> [BOLD] IWAQG <C> [BOLD] 47.69 <C> [BOLD] 32.24 <C> [BOLD] 24.01 <C> [BOLD] 18.53 <C> [BOLD] 22.33 <C> [BOLD] 46.94 <CAP> Table 2: Comparison of our model with the baselines. “*” is our QG module. <COT> Looking at the "Model" column, the row with the model name "IWAQG" has all its BLEU and METEOR scores marked as [BOLD].
<R> <C> Model <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> METEOR <C> ROUGE-L <R> <C> Zhou et al. ( 2017 ) <C> - <C> - <C> - <C> 13.29 <C> - <C> - <R> <C> Zhao et al. ( 2018 )* <C> 45.69 <C> 29.58 <C> 22.16 <C> 16.85 <C> 20.62 <C> 44.99 <R> <C> Kim et al. ( 2019 ) <C> - <C> - <C> - <C> 16.17 <C> - <C> - <R> <C> Liu et al. ( 2019 ) <C> 46.58 <C> 30.90 <C> 22.82 <C> 17.55 <C> 21.24 <C> 44.53 <R> <C> [BOLD] IWAQG <C> [BOLD] 47.69 <C> [BOLD] 32.24 <C> [BOLD] 24.01 <C> [BOLD] 18.53 <C> [BOLD] 22.33 <C> [BOLD] 46.94 <CAP> Table 2: Comparison of our model with the baselines. “*” is our QG module. <COT> Looking at the "BLEU-4" column, the row with the model name "IWAQG" has a BLEU-4 score of 18.53.
<R> <C> Accuracy <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> METEOR <C> ROUGE-L <R> <C> [BOLD] Only QG* <C> [BOLD] 45.63 <C> [BOLD] 30.43 <C> [BOLD] 22.51 <C> [BOLD] 17.30 <C> [BOLD] 21.06 <C> [BOLD] 45.42 <R> <C> 60% <C> 45.80 <C> 30.61 <C> 22.57 <C> 17.30 <C> 21.47 <C> 44.70 <R> <C> 70% <C> 47.05 <C> 31.62 <C> 23.46 <C> 18.05 <C> 22.00 <C> 45.88 <R> <C> [BOLD] IWAQG (73.8%) <C> [BOLD] 47.69 <C> [BOLD] 32.24 <C> [BOLD] 24.01 <C> [BOLD] 18.53 <C> [BOLD] 22.33 <C> [BOLD] 46.94 <R> <C> 80% <C> 48.11 <C> 32.36 <C> 24.00 <C> 18.42 <C> 22.43 <C> 47.22 <R> <C> 90% <C> 49.33 <C> 33.43 <C> 24.91 <C> 19.20 <C> 22.98 <C> 48.41 <R> <C> [BOLD] Upper Bound (100%) <C> [BOLD] 50.51 <C> [BOLD] 34.28 <C> [BOLD] 25.60 <C> [BOLD] 19.75 <C> [BOLD] 23.45 <C> [BOLD] 49.65 <CAP> Table 3: Performance of the QG model with respect to the accuracy of the interrogative-word classifier. “*” is our implementation of the QG module without our interrogative-word classifier Zhao et al. (2018). <COT> Looking at the "Accuracy" column, we can observe that the accuracy increases as the percentage of Only QG* increases, except for the Upper Bound (100%) case.
<R> <C> Accuracy <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> METEOR <C> ROUGE-L <R> <C> [BOLD] Only QG* <C> [BOLD] 45.63 <C> [BOLD] 30.43 <C> [BOLD] 22.51 <C> [BOLD] 17.30 <C> [BOLD] 21.06 <C> [BOLD] 45.42 <R> <C> 60% <C> 45.80 <C> 30.61 <C> 22.57 <C> 17.30 <C> 21.47 <C> 44.70 <R> <C> 70% <C> 47.05 <C> 31.62 <C> 23.46 <C> 18.05 <C> 22.00 <C> 45.88 <R> <C> [BOLD] IWAQG (73.8%) <C> [BOLD] 47.69 <C> [BOLD] 32.24 <C> [BOLD] 24.01 <C> [BOLD] 18.53 <C> [BOLD] 22.33 <C> [BOLD] 46.94 <R> <C> 80% <C> 48.11 <C> 32.36 <C> 24.00 <C> 18.42 <C> 22.43 <C> 47.22 <R> <C> 90% <C> 49.33 <C> 33.43 <C> 24.91 <C> 19.20 <C> 22.98 <C> 48.41 <R> <C> [BOLD] Upper Bound (100%) <C> [BOLD] 50.51 <C> [BOLD] 34.28 <C> [BOLD] 25.60 <C> [BOLD] 19.75 <C> [BOLD] 23.45 <C> [BOLD] 49.65 <CAP> Table 3: Performance of the QG model with respect to the accuracy of the interrogative-word classifier. “*” is our implementation of the QG module without our interrogative-word classifier Zhao et al. (2018). <COT> Looking at the "BLEU-4" column, we can observe that the BLEU-4 score is higher for the IWAQG (73.8%) case compared to the Upper Bound (100%) case.
<R> <C> Model <C> What <C> Which <C> Where <C> When <C> Who <C> Why <C> How <C> Others <C> Total <R> <C> Only QG* <C> 82.24% <C> 0.29% <C> 51.90% <C> 60.82% <C> 68.34% <C> 12.66% <C> 60.62% <C> 2.13% <C> 68.29% <R> <C> IWAQG <C> 87.66% <C> 1.46% <C> 66.24% <C> 49.41% <C> 76.41% <C> 50.63% <C> 70.26% <C> 14.89% <C> 74.10% <R> <C> Upper Bound <C> 99.87% <C> 99.71% <C> 100.00% <C> 99.71% <C> 99.84% <C> 98.73% <C> 99.67% <C> 89.36% <C> 99.72% <CAP> Table 4: Recall of interrogative words of the QG model. “*” is our implementation of the QG module without our interrogative-word classifier Zhao et al. (2018). <COT> Looking at the "Model" column, "Only QG*" row, and the "What" column, we can see that the QG model without the interrogative-word classifier has a recall rate of 82.24% for the question type "What".
<R> <C> Model <C> What <C> Which <C> Where <C> When <C> Who <C> Why <C> How <C> Others <C> Total <R> <C> Only QG* <C> 82.24% <C> 0.29% <C> 51.90% <C> 60.82% <C> 68.34% <C> 12.66% <C> 60.62% <C> 2.13% <C> 68.29% <R> <C> IWAQG <C> 87.66% <C> 1.46% <C> 66.24% <C> 49.41% <C> 76.41% <C> 50.63% <C> 70.26% <C> 14.89% <C> 74.10% <R> <C> Upper Bound <C> 99.87% <C> 99.71% <C> 100.00% <C> 99.71% <C> 99.84% <C> 98.73% <C> 99.67% <C> 89.36% <C> 99.72% <CAP> Table 4: Recall of interrogative words of the QG model. “*” is our implementation of the QG module without our interrogative-word classifier Zhao et al. (2018). <COT> Looking at the "Model" column, "Upper Bound" row, and the "Total" column, we can see that the upper bound model has a recall rate of 99.72% for all question types combined.
<R> <C> Classifier <C> Accuracy <R> <C> CLS <C> 56.0% <R> <C> CLS + NER <C> 56.6% <R> <C> CLS + AE <C> 70.3% <R> <C> CLS + AT <C> 73.3% <R> <C> [BOLD] CLS + AT + NER <C> [BOLD] 73.8% <CAP> Table 6: Ablation Study of our interrogative-word classifier. <COT> Looking at the "Classifier" column, we can see that adding NER to the CLS classifier increases the accuracy from 56.0% to 56.6%.
<R> <C> Classifier <C> Accuracy <R> <C> CLS <C> 56.0% <R> <C> CLS + NER <C> 56.6% <R> <C> CLS + AE <C> 70.3% <R> <C> CLS + AT <C> 73.3% <R> <C> [BOLD] CLS + AT + NER <C> [BOLD] 73.8% <CAP> Table 6: Ablation Study of our interrogative-word classifier. <COT> Looking at the "Classifier" column, we can see that adding AT (Attention Transfer) to the CLS classifier increases the accuracy from 70.3% to 73.3%.
<R> <C> Class <C> Recall <C> Precision <R> <C> What <C> 87.7% <C> 76.0% <R> <C> Which <C> 1.4% <C> 38.0% <R> <C> Where <C> 65.9% <C> 55.8% <R> <C> When <C> 49.2% <C> 69.8% <R> <C> Who <C> 76.9% <C> 66.7% <R> <C> Why <C> 50.1% <C> 74.1% <R> <C> How <C> 70.5% <C> 79.0% <R> <C> Others <C> 10.5% <C> 57.0% <CAP> Table 7: Recall and precision of interrogative words of our interrogative-word classifier. <COT> Looking at the "Recall" column, we can see that the highest recall percentage is for the class "What" with 87.7%.
<R> <C> Class <C> Recall <C> Precision <R> <C> What <C> 87.7% <C> 76.0% <R> <C> Which <C> 1.4% <C> 38.0% <R> <C> Where <C> 65.9% <C> 55.8% <R> <C> When <C> 49.2% <C> 69.8% <R> <C> Who <C> 76.9% <C> 66.7% <R> <C> Why <C> 50.1% <C> 74.1% <R> <C> How <C> 70.5% <C> 79.0% <R> <C> Others <C> 10.5% <C> 57.0% <CAP> Table 7: Recall and precision of interrogative words of our interrogative-word classifier. <COT> Looking at the "Precision" column, we can see that the highest precision percentage is for the class "How" with 79.0%.
<R> <C> [ITALIC] γ <C> #Edge/#Node <C> LAS <C> Conn. Ratio(%) <R> <C> 0.05 <C> 2.09 <C> 92.5 <C> 100.0 <R> <C> 0.1 <C> 1.57 <C> 91.2 <C> 99.5 <R> <C> 0.2 <C> 1.34 <C> 90.5 <C> 94.2 <R> <C> 0.3 <C> 1.04 <C> 88.0 <C> 77.6 <R> <C> [ITALIC] K <C> #Edge/#Node <C> LAS <C> Conn. Ratio(%) <R> <C> 1 <C> 1.00 <C> 86.4 <C> 100.0 <R> <C> 2 <C> 1.03 <C> 87.3 <C> 100.0 <R> <C> 5 <C> 1.09 <C> 89.1 <C> 100.0 <R> <C> 10 <C> 1.14 <C> 89.8 <C> 100.0 <CAP> Table 1: Statistics on forests generated with various γ (upper half) and K (lower half) on the development set. <COT> Looking at the "Conn. Ratio(%)" column under the "γ" section, we can see that as the value of γ increases, the connectivity ratio decreases.
<R> <C> [ITALIC] γ <C> #Edge/#Node <C> LAS <C> Conn. Ratio(%) <R> <C> 0.05 <C> 2.09 <C> 92.5 <C> 100.0 <R> <C> 0.1 <C> 1.57 <C> 91.2 <C> 99.5 <R> <C> 0.2 <C> 1.34 <C> 90.5 <C> 94.2 <R> <C> 0.3 <C> 1.04 <C> 88.0 <C> 77.6 <R> <C> [ITALIC] K <C> #Edge/#Node <C> LAS <C> Conn. Ratio(%) <R> <C> 1 <C> 1.00 <C> 86.4 <C> 100.0 <R> <C> 2 <C> 1.03 <C> 87.3 <C> 100.0 <R> <C> 5 <C> 1.09 <C> 89.1 <C> 100.0 <R> <C> 10 <C> 1.14 <C> 89.8 <C> 100.0 <CAP> Table 1: Statistics on forests generated with various γ (upper half) and K (lower half) on the development set. <COT> Looking at the "LAS" column under the "K" section, we can see that as the value of K increases, the LAS score also increases.
<R> <C> Model <C> F1 score <R> <C> GRU+Attn (Liu et al.,  2017 )† <C> 49.5 <R> <C> Bran (Verga et al.,  2018 )† <C> 50.8 <R> <C> TextOnly <C> 50.6 <R> <C> DepTree <C> 51.4 <R> <C> KBestEisnerPS <C> **52.4** <R> <C> EdgewisePS <C> ** [BOLD] 53.4** <CAP> Table 2: Test results of Biocreative VI CPR. † indicates previously reported numbers. ** means significant over DepTree at p<0.01 with 1000 bootstrap tests (Efron and Tibshirani, 1994). <COT> Looking at the "Model" column, we can see that the F1 score for "EdgewisePS" is bolded and higher than the F1 score for "DepTree".
<R> <C> Model <C> F1 score <R> <C> GRU+Attn (Liu et al.,  2017 )† <C> 49.5 <R> <C> Bran (Verga et al.,  2018 )† <C> 50.8 <R> <C> TextOnly <C> 50.6 <R> <C> DepTree <C> 51.4 <R> <C> KBestEisnerPS <C> **52.4** <R> <C> EdgewisePS <C> ** [BOLD] 53.4** <CAP> Table 2: Test results of Biocreative VI CPR. † indicates previously reported numbers. ** means significant over DepTree at p<0.01 with 1000 bootstrap tests (Efron and Tibshirani, 1994). <COT> Looking at the "Model" column, we can see that the F1 score for "KBestEisnerPS" is higher than the F1 score for "TextOnly".
<R> <C> Model <C> F1 score <R> <C> BO-LSTM (Lamurias et al.,  2019 )† <C> 52.3 <R> <C> BioBERT (Lee et al.,  2019 )† <C> 67.2 <R> <C> TextOnly <C> 76.0 <R> <C> DepTree <C> 78.9 <R> <C> KBestEisnerPS <C> *83.6* <R> <C> EdgewisePS <C> ** [BOLD] 85.7** <CAP> Table 3: Main results on PGR testest. † denotes previous numbers rounded into 3 significant digits. * and ** indicate significance over DepTree at p<0.05 and p<0.01 with 1000 bootstrap tests. <COT> Looking at the "Model" column, we can see different models and their corresponding F1 scores.
<R> <C> Model <C> F1 score <R> <C> BO-LSTM (Lamurias et al.,  2019 )† <C> 52.3 <R> <C> BioBERT (Lee et al.,  2019 )† <C> 67.2 <R> <C> TextOnly <C> 76.0 <R> <C> DepTree <C> 78.9 <R> <C> KBestEisnerPS <C> *83.6* <R> <C> EdgewisePS <C> ** [BOLD] 85.7** <CAP> Table 3: Main results on PGR testest. † denotes previous numbers rounded into 3 significant digits. * and ** indicate significance over DepTree at p<0.05 and p<0.01 with 1000 bootstrap tests. <COT> Looking at the "Model" column and the F1 scores, we can see that the model "EdgewisePS" has the highest F1 score.
<R> <C> Model <C> F1 score <R> <C> C-GCN (Zhang et al.,  2018b )† <C> 84.8 <R> <C> C-AGGCN (Guo et al.,  2019 )† <C> 85.7 <R> <C> DepTree <C> 84.6 <R> <C> KBestEisnerPS <C> 85.8 <R> <C> EdgewisePS <C> 86.3 <CAP> Table 4: Main results on SemEval-2010 task 8 testest. † denotes previous numbers. <COT> Looking at the "Model" column, we can see that the models listed are different graph-based models for the SemEval-2010 task 8 test set.
<R> <C> Model <C> F1 score <R> <C> C-GCN (Zhang et al.,  2018b )† <C> 84.8 <R> <C> C-AGGCN (Guo et al.,  2019 )† <C> 85.7 <R> <C> DepTree <C> 84.6 <R> <C> KBestEisnerPS <C> 85.8 <R> <C> EdgewisePS <C> 86.3 <CAP> Table 4: Main results on SemEval-2010 task 8 testest. † denotes previous numbers. <COT> Comparing the "F1 score" column, we can see that the model "EdgewisePS" achieved the highest F1 score.
<R> <C> Training Instances Hits@K (Macro) <C> Training Instances Hits@K (Macro) <C> <100 10 <C> <100 15 <C> <100 20 <C> <200 10 <C> <200 15 <C> <200 20 <R> <C> CNN <C> +ATT <C> <5.0 <C> <5.0 <C> 18.5 <C> <5.0 <C> 16.2 <C> 33.3 <R> <C> [EMPTY] <C> +HATT <C> 5.6 <C> 31.5 <C> 57.4 <C> 22.7 <C> 43.9 <C> 65.1 <R> <C> [EMPTY] <C> +KATT <C> [BOLD] 9.1 <C> [BOLD] 41.3 <C> [BOLD] 58.5 <C> [BOLD] 23.3 <C> [BOLD] 44.1 <C> [BOLD] 65.4 <R> <C> PCNN <C> +ATT <C> <5.0 <C> 7.4 <C> 40.7 <C> 17.2 <C> 24.2 <C> 51.5 <R> <C> [EMPTY] <C> +HATT <C> 29.6 <C> 51.9 <C> 61.1 <C> 41.4 <C> 60.6 <C> 68.2 <R> <C> [EMPTY] <C> +KATT <C> [BOLD] 35.3 <C> [BOLD] 62.4 <C> [BOLD] 65.1 <C> [BOLD] 43.2 <C> [BOLD] 61.3 <C> [BOLD] 69.2 <CAP> Table 1: Accuracy (%) of Hits@K on relations with training instances fewer than 100/200. <COT> Looking at the table, we can see that the Hits@K accuracy increases when the number of training instances increases, regardless of the model used.
<R> <C> Training Instances Hits@K (Macro) <C> Training Instances Hits@K (Macro) <C> <100 10 <C> <100 15 <C> <100 20 <C> <200 10 <C> <200 15 <C> <200 20 <R> <C> CNN <C> +ATT <C> <5.0 <C> <5.0 <C> 18.5 <C> <5.0 <C> 16.2 <C> 33.3 <R> <C> [EMPTY] <C> +HATT <C> 5.6 <C> 31.5 <C> 57.4 <C> 22.7 <C> 43.9 <C> 65.1 <R> <C> [EMPTY] <C> +KATT <C> [BOLD] 9.1 <C> [BOLD] 41.3 <C> [BOLD] 58.5 <C> [BOLD] 23.3 <C> [BOLD] 44.1 <C> [BOLD] 65.4 <R> <C> PCNN <C> +ATT <C> <5.0 <C> 7.4 <C> 40.7 <C> 17.2 <C> 24.2 <C> 51.5 <R> <C> [EMPTY] <C> +HATT <C> 29.6 <C> 51.9 <C> 61.1 <C> 41.4 <C> 60.6 <C> 68.2 <R> <C> [EMPTY] <C> +KATT <C> [BOLD] 35.3 <C> [BOLD] 62.4 <C> [BOLD] 65.1 <C> [BOLD] 43.2 <C> [BOLD] 61.3 <C> [BOLD] 69.2 <CAP> Table 1: Accuracy (%) of Hits@K on relations with training instances fewer than 100/200. <COT> Looking at the "+KATT" column, we can see that the Hits@K accuracy is consistently higher compared to the other models.
<R> <C> WordNet <C> LCH  [ITALIC] 100 <C> ShP  [ITALIC] 100 <C> WuP  [ITALIC] 100 <C> LCH 51.3 <C> ShP 51.3 <C> WuP 47.4 <R> <C> path2vec <C> [BOLD] 93.5 <C> [BOLD] 95.2 <C> [BOLD] 93.1 <C> [BOLD] 53.2 <C> [BOLD] 55.5 <C> [BOLD] 55.5 <R> <C> TransR <C> 77.6 <C> 77.6 <C> 72.5 <C> 38.6 <C> 38.6 <C> 38.6 <R> <C> node2vec <C> 75.9 <C> 75.9 <C> 78.7 <C> 46.2 <C> 46.2 <C> 46.2 <R> <C> DeepWalk <C> 86.8 <C> 86.8 <C> 85.0 <C> 53.3 <C> 53.3 <C> 53.3 <R> <C> FSE <C> 90.0 <C> 90.0 <C> 89.0 <C> 55.6 <C> 55.6 <C> 55.6 <CAP> Table 1: Spearman correlations with WordNet similarities (left) and human judgments (right) ×100. <COT> Looking at the "CAP" tag, we can see that Table 1 displays the Spearman correlations with WordNet similarities and human judgments multiplied by 100.
<R> <C> WordNet <C> LCH  [ITALIC] 100 <C> ShP  [ITALIC] 100 <C> WuP  [ITALIC] 100 <C> LCH 51.3 <C> ShP 51.3 <C> WuP 47.4 <R> <C> path2vec <C> [BOLD] 93.5 <C> [BOLD] 95.2 <C> [BOLD] 93.1 <C> [BOLD] 53.2 <C> [BOLD] 55.5 <C> [BOLD] 55.5 <R> <C> TransR <C> 77.6 <C> 77.6 <C> 72.5 <C> 38.6 <C> 38.6 <C> 38.6 <R> <C> node2vec <C> 75.9 <C> 75.9 <C> 78.7 <C> 46.2 <C> 46.2 <C> 46.2 <R> <C> DeepWalk <C> 86.8 <C> 86.8 <C> 85.0 <C> 53.3 <C> 53.3 <C> 53.3 <R> <C> FSE <C> 90.0 <C> 90.0 <C> 89.0 <C> 55.6 <C> 55.6 <C> 55.6 <CAP> Table 1: Spearman correlations with WordNet similarities (left) and human judgments (right) ×100. <COT> Looking at the "TransR" row and the "WuP 47.4" cell, we can see that the WuP similarity score for TransR is 47.4.
<R> <C> [BOLD] Model <C> [BOLD] Senseval2 <C> [BOLD] Senseval3 <C> [BOLD] SemEval-15 <R> <C> Random sense <C> 0.381 <C> 0.312 <C> 0.393 <R> <C> [ITALIC] Graph-based vs vector-based measures <C> [ITALIC] Graph-based vs vector-based measures <C> [ITALIC] Graph-based vs vector-based measures <C> [ITALIC] Graph-based vs vector-based measures <R> <C> LCH (WordNet) <C> 0.547↓0.000 <C> 0.494↓0.000 <C> 0.550↓0.000 <R> <C> LCH (path2vec) <C> 0.527↓0.020 <C> 0.472↓0.022 <C> 0.536↓0.014 <R> <C> ShP (WordNet) <C> 0.548↓0.000 <C> 0.495↓0.000 <C> 0.550↓0.000 <R> <C> ShP (path2vec) <C> 0.534↓0.014 <C> 0.489↓0.006 <C> 0.563↑0.013 <R> <C> WuP (WordNet) <C> 0.547↓0.000 <C> 0.487↓0.000 <C> 0.542↓0.000 <R> <C> WuP (path2vec) <C> 0.543↓0.004 <C> 0.489↑0.002 <C> 0.545↑0.003 <R> <C> [ITALIC] Various baseline graph embeddings trained on WordNet <C> [ITALIC] Various baseline graph embeddings trained on WordNet <C> [ITALIC] Various baseline graph embeddings trained on WordNet <C> [ITALIC] Various baseline graph embeddings trained on WordNet <R> <C> TransR <C> 0.540 <C> 0.466 <C> 0.536 <R> <C> node2vec <C> 0.503 <C> 0.467 <C> 0.489 <R> <C> DeepWalk <C> 0.528 <C> 0.476 <C> 0.552 <R> <C> FSE <C> 0.536 <C> 0.476 <C> 0.523 <CAP> Table 2: F1 scores of a graph-based WSD algorithm on WordNet versus its vectorized counterparts. <COT> Looking at the "Model" column, we can see that there are different graph-based and vector-based models used for Word Sense Disambiguation (WSD) on WordNet.
<R> <C> [BOLD] Model <C> [BOLD] Senseval2 <C> [BOLD] Senseval3 <C> [BOLD] SemEval-15 <R> <C> Random sense <C> 0.381 <C> 0.312 <C> 0.393 <R> <C> [ITALIC] Graph-based vs vector-based measures <C> [ITALIC] Graph-based vs vector-based measures <C> [ITALIC] Graph-based vs vector-based measures <C> [ITALIC] Graph-based vs vector-based measures <R> <C> LCH (WordNet) <C> 0.547↓0.000 <C> 0.494↓0.000 <C> 0.550↓0.000 <R> <C> LCH (path2vec) <C> 0.527↓0.020 <C> 0.472↓0.022 <C> 0.536↓0.014 <R> <C> ShP (WordNet) <C> 0.548↓0.000 <C> 0.495↓0.000 <C> 0.550↓0.000 <R> <C> ShP (path2vec) <C> 0.534↓0.014 <C> 0.489↓0.006 <C> 0.563↑0.013 <R> <C> WuP (WordNet) <C> 0.547↓0.000 <C> 0.487↓0.000 <C> 0.542↓0.000 <R> <C> WuP (path2vec) <C> 0.543↓0.004 <C> 0.489↑0.002 <C> 0.545↑0.003 <R> <C> [ITALIC] Various baseline graph embeddings trained on WordNet <C> [ITALIC] Various baseline graph embeddings trained on WordNet <C> [ITALIC] Various baseline graph embeddings trained on WordNet <C> [ITALIC] Various baseline graph embeddings trained on WordNet <R> <C> TransR <C> 0.540 <C> 0.466 <C> 0.536 <R> <C> node2vec <C> 0.503 <C> 0.467 <C> 0.489 <R> <C> DeepWalk <C> 0.528 <C> 0.476 <C> 0.552 <R> <C> FSE <C> 0.536 <C> 0.476 <C> 0.523 <CAP> Table 2: F1 scores of a graph-based WSD algorithm on WordNet versus its vectorized counterparts. <COT> Looking at the "SemEval-15" column, we can see that the F1 scores for the different models vary for the SemEval-15 dataset.
<R> <C> [EMPTY] <C> Emb. <C> Vocab <C> Polyglot <C> All <R> <C> [EMPTY] <C> dim <C> size <C> in-vocab <C> pairs <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] N=862 <C> [ITALIC] N=2034 <R> <C> VarEmbed <C> 128 <C> 100K <C> 41.9 <C> 25.5 <R> <C> Polyglot <C> 64 <C> 100K <C> 40.8 <C> 8.7 <R> <C> mimick <C> 64 <C> 0 <C> 17.9 <C> 17.5 <R> <C> Polyglot <C> 64 <C> 100K <C> 40.8 <C> 27.0 <R> <C> +mimick <C> 64 <C> 100K <C> 40.8 <C> 27.0 <R> <C> Fasttext <C> 300 <C> 2.51M <C> [EMPTY] <C> 47.3 <CAP> Table 3: Similarity results on the RareWord set, measured as Spearman’s ρ×100. VarEmbed was trained on a 20-million token dataset, Polyglot on a 1.7B-token dataset. <COT> Looking at the "Table 3" caption, we can see that the table shows similarity results on the RareWord set, measured as Spearman's ρ×100.
<R> <C> [EMPTY] <C> Emb. <C> Vocab <C> Polyglot <C> All <R> <C> [EMPTY] <C> dim <C> size <C> in-vocab <C> pairs <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] N=862 <C> [ITALIC] N=2034 <R> <C> VarEmbed <C> 128 <C> 100K <C> 41.9 <C> 25.5 <R> <C> Polyglot <C> 64 <C> 100K <C> 40.8 <C> 8.7 <R> <C> mimick <C> 64 <C> 0 <C> 17.9 <C> 17.5 <R> <C> Polyglot <C> 64 <C> 100K <C> 40.8 <C> 27.0 <R> <C> +mimick <C> 64 <C> 100K <C> 40.8 <C> 27.0 <R> <C> Fasttext <C> 300 <C> 2.51M <C> [EMPTY] <C> 47.3 <CAP> Table 3: Similarity results on the RareWord set, measured as Spearman’s ρ×100. VarEmbed was trained on a 20-million token dataset, Polyglot on a 1.7B-token dataset. <COT> Looking at the "VarEmbed" row, we can see that the embedding dimension is 128 and the vocabulary size is 100K.
<R> <C> [EMPTY] <C> [ITALIC] w/ System Retrieval  [BOLD] BLEU <C> [ITALIC] w/ System Retrieval  [BOLD] MTR <C> [ITALIC] w/ System Retrieval  [BOLD] Len <C> [ITALIC] w/ Oracle Retrieval  [BOLD] BLEU <C> [ITALIC] w/ Oracle Retrieval  [BOLD] MTR <C> [ITALIC] w/ Oracle Retrieval  [BOLD] Len <R> <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <R> <C> Retrieval <C> 15.32 <C> [BOLD] 12.19 <C> 151.2 <C> 10.24 <C> [BOLD] 16.22 <C> 132.7 <R> <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <R> <C> Seq2seq <C> 10.21 <C> 5.74 <C> 34.9 <C> 7.44 <C> 5.25 <C> 31.1 <R> <C> +  [ITALIC] encode evd <C> 18.03 <C> 7.32 <C> 67.0 <C> 13.79 <C> 10.06 <C> 68.1 <R> <C> +  [ITALIC] encode KP <C> 21.94 <C> 8.63 <C> 74.4 <C> 12.96 <C> 10.50 <C> 78.2 <R> <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <R> <C> Dec-shared <C> 21.22 <C> 8.91 <C> 69.1 <C> 15.78 <C> 11.52 <C> 68.2 <R> <C> +  [ITALIC] attend KP <C> [BOLD] 24.71 <C> 10.05 <C> 74.8 <C> 11.48 <C> 10.08 <C> 40.5 <R> <C> Dec-separate <C> 24.24 <C> 10.63 <C> 88.6 <C> 17.48 <C> 13.15 <C> 86.9 <R> <C> +  [ITALIC] attend KP <C> 24.52 <C> 11.27 <C> 88.3 <C> [BOLD] 17.80 <C> 13.67 <C> 86.8 <CAP> Table 3: Results on argument generation by BLEU and METEOR (MTR), with system retrieved evidence and oracle retrieval. The best performing model is highlighted in bold per metric. Our separate decoder models, with and without keyphrase attention, statistically significantly outperform all seq2seq-based models based on approximation randomization testing Noreen (1989), p<0.0001. <COT> Looking at the "Dec-separate" row, the "[BOLD] 17.80" cell, and the "w/ Oracle Retrieval BLEU" cell, we can see that the Dec-separate model with attend KP performs the best on argument generation with system retrieved evidence and oracle retrieval in terms of BLEU score.
<R> <C> [EMPTY] <C> [ITALIC] w/ System Retrieval  [BOLD] BLEU <C> [ITALIC] w/ System Retrieval  [BOLD] MTR <C> [ITALIC] w/ System Retrieval  [BOLD] Len <C> [ITALIC] w/ Oracle Retrieval  [BOLD] BLEU <C> [ITALIC] w/ Oracle Retrieval  [BOLD] MTR <C> [ITALIC] w/ Oracle Retrieval  [BOLD] Len <R> <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <R> <C> Retrieval <C> 15.32 <C> [BOLD] 12.19 <C> 151.2 <C> 10.24 <C> [BOLD] 16.22 <C> 132.7 <R> <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <R> <C> Seq2seq <C> 10.21 <C> 5.74 <C> 34.9 <C> 7.44 <C> 5.25 <C> 31.1 <R> <C> +  [ITALIC] encode evd <C> 18.03 <C> 7.32 <C> 67.0 <C> 13.79 <C> 10.06 <C> 68.1 <R> <C> +  [ITALIC] encode KP <C> 21.94 <C> 8.63 <C> 74.4 <C> 12.96 <C> 10.50 <C> 78.2 <R> <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <R> <C> Dec-shared <C> 21.22 <C> 8.91 <C> 69.1 <C> 15.78 <C> 11.52 <C> 68.2 <R> <C> +  [ITALIC] attend KP <C> [BOLD] 24.71 <C> 10.05 <C> 74.8 <C> 11.48 <C> 10.08 <C> 40.5 <R> <C> Dec-separate <C> 24.24 <C> 10.63 <C> 88.6 <C> 17.48 <C> 13.15 <C> 86.9 <R> <C> +  [ITALIC] attend KP <C> 24.52 <C> 11.27 <C> 88.3 <C> [BOLD] 17.80 <C> 13.67 <C> 86.8 <CAP> Table 3: Results on argument generation by BLEU and METEOR (MTR), with system retrieved evidence and oracle retrieval. The best performing model is highlighted in bold per metric. Our separate decoder models, with and without keyphrase attention, statistically significantly outperform all seq2seq-based models based on approximation randomization testing Noreen (1989), p<0.0001. <COT> Looking at the "Retrieval" row, the "12.19" cell, and the "w/ System Retrieval MTR" cell, we can see that the Retrieval baseline has the second highest MTR score among all models.
<R> <C> [EMPTY] <C> [ITALIC] Ntrain=5000 No-Char <C> [ITALIC] Ntrain=5000 mimick <C> [ITALIC] Ntrain=5000 char <C> [ITALIC] Ntrain=5000 Both <C> Full data  [ITALIC] Ntrain <C> Full data No-Char <C> Full data mimick <C> Full data char <C> Both <C> PSG <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> →tag <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> →tag <C> [EMPTY] <C> 2016* <R> <C> kk <C> — <C> — <C> — <C> — <C> 4,949 <C> 81.94 <C> 83.95 <C> 83.64 <C> 84.88 <C> [EMPTY] <R> <C> ta <C> 82.30 <C> 81.55 <C> 84.97 <C> 85.22 <C> 6,329 <C> 80.44 <C> [BOLD] 82.96 <C> 84.11 <C> 84.46 <C> [EMPTY] <R> <C> lv <C> 80.44 <C> [BOLD] 84.32 <C> 84.49 <C> [BOLD] 85.91 <C> 13,781 <C> 85.77 <C> [BOLD] 87.95 <C> 89.55 <C> 89.99 <C> [EMPTY] <R> <C> vi <C> 85.67 <C> [ITALIC] 84.22 <C> 84.85 <C> 85.43 <C> 31,800 <C> 89.94 <C> 90.34 <C> 90.50 <C> 90.19 <C> [EMPTY] <R> <C> hu <C> 82.88 <C> [BOLD] 88.93 <C> 85.83 <C> [BOLD] 88.34 <C> 33,017 <C> 91.52 <C> [BOLD] 93.88 <C> 94.07 <C> 93.74 <C> [EMPTY] <R> <C> tr <C> 83.69 <C> [BOLD] 85.60 <C> 84.23 <C> [BOLD] 86.25 <C> 41,748 <C> 90.19 <C> [BOLD] 91.82 <C> 93.11 <C> 92.68 <C> [EMPTY] <R> <C> el <C> 93.10 <C> [BOLD] 93.63 <C> 94.05 <C> [BOLD] 94.64 <C> 47,449 <C> 97.27 <C> [BOLD] 98.08 <C> 98.09 <C> 98.22 <C> [EMPTY] <R> <C> bg <C> 90.97 <C> [BOLD] 93.16 <C> 93.03 <C> [BOLD] 93.52 <C> 50,000 <C> 96.63 <C> [BOLD] 97.29 <C> 97.95 <C> 97.78 <C> 98.23 <R> <C> sv <C> 90.87 <C> [BOLD] 92.30 <C> 92.27 <C> [BOLD] 93.02 <C> 66,645 <C> 95.26 <C> [BOLD] 96.27 <C> 96.69 <C> 96.87 <C> 96.60 <R> <C> eu <C> 82.67 <C> [BOLD] 84.44 <C> 86.01 <C> [BOLD] 86.93 <C> 72,974 <C> 91.67 <C> [BOLD] 93.16 <C> 94.46 <C> 94.29 <C> 95.38 <R> <C> ru <C> 87.40 <C> [BOLD] 89.72 <C> 88.65 <C> [BOLD] 90.91 <C> 79,772 <C> 92.59 <C> [BOLD] 95.21 <C> 95.98 <C> 95.84 <C> [EMPTY] <R> <C> da <C> 89.46 <C> 90.13 <C> 89.96 <C> 90.55 <C> 88,980 <C> 94.14 <C> [BOLD] 95.04 <C> 96.13 <C> 96.02 <C> 96.16 <R> <C> id <C> 89.07 <C> 89.34 <C> 89.81 <C> 90.21 <C> 97,531 <C> 92.92 <C> 93.24 <C> 93.41 <C> [BOLD] 93.70 <C> 93.32 <R> <C> zh <C> 80.84 <C> [BOLD] 85.69 <C> 81.84 <C> [BOLD] 85.53 <C> 98,608 <C> 90.91 <C> [BOLD] 93.31 <C> 93.36 <C> 93.72 <C> [EMPTY] <R> <C> fa <C> 93.50 <C> 93.58 <C> 93.53 <C> 93.71 <C> 121,064 <C> 96.77 <C> [BOLD] 97.03 <C> 97.20 <C> 97.16 <C> 97.60 <R> <C> he <C> 90.73 <C> [BOLD] 91.69 <C> 91.93 <C> 91.70 <C> 135,496 <C> 95.65 <C> [BOLD] 96.15 <C> 96.59 <C> 96.37 <C> 96.62 <R> <C> ro <C> 87.73 <C> [BOLD] 89.18 <C> 88.96 <C> [BOLD] 89.38 <C> 163,262 <C> 95.68 <C> [BOLD] 96.72 <C> 97.07 <C> 97.09 <C> [EMPTY] <R> <C> en <C> 87.48 <C> [BOLD] 88.45 <C> 88.89 <C> 88.89 <C> 204,587 <C> 93.39 <C> [BOLD] 94.04 <C> 94.90 <C> 94.70 <C> 95.17 <R> <C> ar <C> 89.01 <C> [BOLD] 90.58 <C> 90.49 <C> 90.62 <C> 225,853 <C> 95.51 <C> [BOLD] 95.72 <C> 96.37 <C> 96.24 <C> 98.87 <R> <C> hi <C> 87.89 <C> 87.77 <C> 87.92 <C> 88.09 <C> 281,057 <C> 96.31 <C> 96.45 <C> 96.64 <C> 96.61 <C> 96.97 <R> <C> it <C> 91.35 <C> [BOLD] 92.50 <C> 92.45 <C> [BOLD] 93.01 <C> 289,440 <C> 97.22 <C> 97.47 <C> 97.76 <C> 97.69 <C> 97.90 <R> <C> es <C> 90.54 <C> [BOLD] 91.41 <C> 91.71 <C> 91.78 <C> 382,436 <C> 94.68 <C> 94.84 <C> 95.08 <C> 95.05 <C> 95.67 <R> <C> cs <C> 87.97 <C> [BOLD] 90.81 <C> 90.17 <C> [BOLD] 91.29 <C> 1,173,282 <C> 96.34 <C> [BOLD] 97.62 <C> 98.18 <C> [ITALIC] 97.93 <C> 98.02 <CAP> Table 5: POS tagging accuracy (UD 1.4 Test). Bold (Italic) indicates significant improvement (degradation) by McNemar’s test, p <COT> Looking at the "Ntrain=5000 No-Char" column, we can see that the accuracy for the language "cs" is [BOLD] 90.81.
<R> <C> [EMPTY] <C> [ITALIC] Ntrain=5000 No-Char <C> [ITALIC] Ntrain=5000 mimick <C> [ITALIC] Ntrain=5000 char <C> [ITALIC] Ntrain=5000 Both <C> Full data  [ITALIC] Ntrain <C> Full data No-Char <C> Full data mimick <C> Full data char <C> Both <C> PSG <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> →tag <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> →tag <C> [EMPTY] <C> 2016* <R> <C> kk <C> — <C> — <C> — <C> — <C> 4,949 <C> 81.94 <C> 83.95 <C> 83.64 <C> 84.88 <C> [EMPTY] <R> <C> ta <C> 82.30 <C> 81.55 <C> 84.97 <C> 85.22 <C> 6,329 <C> 80.44 <C> [BOLD] 82.96 <C> 84.11 <C> 84.46 <C> [EMPTY] <R> <C> lv <C> 80.44 <C> [BOLD] 84.32 <C> 84.49 <C> [BOLD] 85.91 <C> 13,781 <C> 85.77 <C> [BOLD] 87.95 <C> 89.55 <C> 89.99 <C> [EMPTY] <R> <C> vi <C> 85.67 <C> [ITALIC] 84.22 <C> 84.85 <C> 85.43 <C> 31,800 <C> 89.94 <C> 90.34 <C> 90.50 <C> 90.19 <C> [EMPTY] <R> <C> hu <C> 82.88 <C> [BOLD] 88.93 <C> 85.83 <C> [BOLD] 88.34 <C> 33,017 <C> 91.52 <C> [BOLD] 93.88 <C> 94.07 <C> 93.74 <C> [EMPTY] <R> <C> tr <C> 83.69 <C> [BOLD] 85.60 <C> 84.23 <C> [BOLD] 86.25 <C> 41,748 <C> 90.19 <C> [BOLD] 91.82 <C> 93.11 <C> 92.68 <C> [EMPTY] <R> <C> el <C> 93.10 <C> [BOLD] 93.63 <C> 94.05 <C> [BOLD] 94.64 <C> 47,449 <C> 97.27 <C> [BOLD] 98.08 <C> 98.09 <C> 98.22 <C> [EMPTY] <R> <C> bg <C> 90.97 <C> [BOLD] 93.16 <C> 93.03 <C> [BOLD] 93.52 <C> 50,000 <C> 96.63 <C> [BOLD] 97.29 <C> 97.95 <C> 97.78 <C> 98.23 <R> <C> sv <C> 90.87 <C> [BOLD] 92.30 <C> 92.27 <C> [BOLD] 93.02 <C> 66,645 <C> 95.26 <C> [BOLD] 96.27 <C> 96.69 <C> 96.87 <C> 96.60 <R> <C> eu <C> 82.67 <C> [BOLD] 84.44 <C> 86.01 <C> [BOLD] 86.93 <C> 72,974 <C> 91.67 <C> [BOLD] 93.16 <C> 94.46 <C> 94.29 <C> 95.38 <R> <C> ru <C> 87.40 <C> [BOLD] 89.72 <C> 88.65 <C> [BOLD] 90.91 <C> 79,772 <C> 92.59 <C> [BOLD] 95.21 <C> 95.98 <C> 95.84 <C> [EMPTY] <R> <C> da <C> 89.46 <C> 90.13 <C> 89.96 <C> 90.55 <C> 88,980 <C> 94.14 <C> [BOLD] 95.04 <C> 96.13 <C> 96.02 <C> 96.16 <R> <C> id <C> 89.07 <C> 89.34 <C> 89.81 <C> 90.21 <C> 97,531 <C> 92.92 <C> 93.24 <C> 93.41 <C> [BOLD] 93.70 <C> 93.32 <R> <C> zh <C> 80.84 <C> [BOLD] 85.69 <C> 81.84 <C> [BOLD] 85.53 <C> 98,608 <C> 90.91 <C> [BOLD] 93.31 <C> 93.36 <C> 93.72 <C> [EMPTY] <R> <C> fa <C> 93.50 <C> 93.58 <C> 93.53 <C> 93.71 <C> 121,064 <C> 96.77 <C> [BOLD] 97.03 <C> 97.20 <C> 97.16 <C> 97.60 <R> <C> he <C> 90.73 <C> [BOLD] 91.69 <C> 91.93 <C> 91.70 <C> 135,496 <C> 95.65 <C> [BOLD] 96.15 <C> 96.59 <C> 96.37 <C> 96.62 <R> <C> ro <C> 87.73 <C> [BOLD] 89.18 <C> 88.96 <C> [BOLD] 89.38 <C> 163,262 <C> 95.68 <C> [BOLD] 96.72 <C> 97.07 <C> 97.09 <C> [EMPTY] <R> <C> en <C> 87.48 <C> [BOLD] 88.45 <C> 88.89 <C> 88.89 <C> 204,587 <C> 93.39 <C> [BOLD] 94.04 <C> 94.90 <C> 94.70 <C> 95.17 <R> <C> ar <C> 89.01 <C> [BOLD] 90.58 <C> 90.49 <C> 90.62 <C> 225,853 <C> 95.51 <C> [BOLD] 95.72 <C> 96.37 <C> 96.24 <C> 98.87 <R> <C> hi <C> 87.89 <C> 87.77 <C> 87.92 <C> 88.09 <C> 281,057 <C> 96.31 <C> 96.45 <C> 96.64 <C> 96.61 <C> 96.97 <R> <C> it <C> 91.35 <C> [BOLD] 92.50 <C> 92.45 <C> [BOLD] 93.01 <C> 289,440 <C> 97.22 <C> 97.47 <C> 97.76 <C> 97.69 <C> 97.90 <R> <C> es <C> 90.54 <C> [BOLD] 91.41 <C> 91.71 <C> 91.78 <C> 382,436 <C> 94.68 <C> 94.84 <C> 95.08 <C> 95.05 <C> 95.67 <R> <C> cs <C> 87.97 <C> [BOLD] 90.81 <C> 90.17 <C> [BOLD] 91.29 <C> 1,173,282 <C> 96.34 <C> [BOLD] 97.62 <C> 98.18 <C> [ITALIC] 97.93 <C> 98.02 <CAP> Table 5: POS tagging accuracy (UD 1.4 Test). Bold (Italic) indicates significant improvement (degradation) by McNemar’s test, p <COT> Looking at the "Full data char" column, we can see that the accuracy for the language "es" is 91.78.
<R> <C> [EMPTY] <C> [ITALIC] Ntrain=5000 No-Char <C> [ITALIC] Ntrain=5000 mimick <C> [ITALIC] Ntrain=5000 char <C> [ITALIC] Ntrain=5000 Both <C> Full data No-Char <C> Full data mimick <C> Full data char <C> Full data Both <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> →tag <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> →tag <C> [EMPTY] <R> <C> kk <C> — <C> — <C> — <C> — <C> 21.48 <C> 20.07 <C> 28.47 <C> 20.98 <R> <C> ta <C> 80.68 <C> [BOLD] 81.96 <C> 84.26 <C> [BOLD] 85.63 <C> 79.90 <C> [BOLD] 81.93 <C> 84.55 <C> 85.01 <R> <C> lv <C> 56.98 <C> [BOLD] 59.86 <C> 64.81 <C> [BOLD] 65.82 <C> 66.16 <C> 66.61 <C> 76.11 <C> 75.44 <R> <C> hu <C> 73.13 <C> [BOLD] 76.30 <C> 73.62 <C> [BOLD] 76.85 <C> 80.04 <C> 80.64 <C> 86.43 <C> 84.12 <R> <C> tr <C> 69.58 <C> [BOLD] 75.21 <C> 75.81 <C> [BOLD] 78.93 <C> 78.31 <C> [BOLD] 83.32 <C> 91.51 <C> 90.86 <R> <C> el <C> 86.87 <C> [ITALIC] 86.07 <C> 86.40 <C> [BOLD] 87.50 <C> 94.64 <C> [BOLD] 94.96 <C> 96.55 <C> [BOLD] 96.76 <R> <C> bg <C> 78.26 <C> [BOLD] 81.77 <C> 82.74 <C> [BOLD] 84.93 <C> 91.98 <C> [BOLD] 93.48 <C> 96.12 <C> 95.96 <R> <C> sv <C> 82.09 <C> [BOLD] 84.12 <C> 85.26 <C> [BOLD] 88.16 <C> 92.45 <C> [BOLD] 94.20 <C> 96.37 <C> [BOLD] 96.57 <R> <C> eu <C> 65.29 <C> [BOLD] 66.00 <C> 70.67 <C> [ITALIC] 70.27 <C> 82.75 <C> [BOLD] 84.74 <C> 90.58 <C> [BOLD] 91.39 <R> <C> ru <C> 77.31 <C> [BOLD] 81.84 <C> 79.83 <C> [BOLD] 83.53 <C> 88.80 <C> [BOLD] 91.24 <C> 93.54 <C> 93.56 <R> <C> da <C> 80.26 <C> [BOLD] 82.74 <C> 83.59 <C> 82.65 <C> 92.06 <C> [BOLD] 94.14 <C> 96.05 <C> 95.96 <R> <C> zh <C> 63.29 <C> [BOLD] 71.44 <C> 63.50 <C> [BOLD] 74.66 <C> 84.95 <C> 85.70 <C> 84.86 <C> 85.87 <R> <C> fa <C> 84.73 <C> [BOLD] 86.07 <C> 85.94 <C> 81.75 <C> 95.30 <C> [BOLD] 95.55 <C> 96.90 <C> 96.80 <R> <C> he <C> 75.35 <C> 68.57 <C> 81.06 <C> 75.24 <C> 90.25 <C> [BOLD] 90.99 <C> 93.35 <C> 93.63 <R> <C> ro <C> 84.20 <C> [BOLD] 85.64 <C> 85.61 <C> [BOLD] 87.31 <C> 94.97 <C> [BOLD] 96.10 <C> 97.18 <C> 97.14 <R> <C> en <C> 86.71 <C> [BOLD] 87.99 <C> 88.50 <C> [BOLD] 89.61 <C> 95.30 <C> [BOLD] 95.59 <C> 96.40 <C> 96.30 <R> <C> ar <C> 84.14 <C> 84.17 <C> 81.41 <C> [ITALIC] 81.11 <C> 94.43 <C> [BOLD] 94.85 <C> 95.50 <C> 95.37 <R> <C> hi <C> 83.45 <C> [BOLD] 86.89 <C> 85.64 <C> 85.27 <C> 96.15 <C> 96.21 <C> 96.59 <C> [BOLD] 96.67 <R> <C> it <C> 89.96 <C> [BOLD] 92.07 <C> 91.27 <C> [BOLD] 92.62 <C> 97.32 <C> [BOLD] 97.80 <C> 98.18 <C> 98.31 <R> <C> es <C> 88.11 <C> [BOLD] 89.81 <C> 88.58 <C> [BOLD] 89.63 <C> 94.84 <C> [BOLD] 95.44 <C> 96.21 <C> [BOLD] 96.84 <R> <C> cs <C> 68.66 <C> [BOLD] 72.65 <C> 71.02 <C> [BOLD] 73.61 <C> 91.75 <C> [BOLD] 93.71 <C> 95.29 <C> 95.31 <CAP> Table 6: Micro-F1 for morphosyntactic attributes (UD 1.4 Test). Bold (Italic) type indicates significant improvement (degradation) by a bootstrapped Z-test, p <COT> Looking at the table, we can see that the Full data Both column consistently has the highest scores for all languages.
<R> <C> [EMPTY] <C> [ITALIC] Ntrain=5000 No-Char <C> [ITALIC] Ntrain=5000 mimick <C> [ITALIC] Ntrain=5000 char <C> [ITALIC] Ntrain=5000 Both <C> Full data No-Char <C> Full data mimick <C> Full data char <C> Full data Both <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> →tag <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> →tag <C> [EMPTY] <R> <C> kk <C> — <C> — <C> — <C> — <C> 21.48 <C> 20.07 <C> 28.47 <C> 20.98 <R> <C> ta <C> 80.68 <C> [BOLD] 81.96 <C> 84.26 <C> [BOLD] 85.63 <C> 79.90 <C> [BOLD] 81.93 <C> 84.55 <C> 85.01 <R> <C> lv <C> 56.98 <C> [BOLD] 59.86 <C> 64.81 <C> [BOLD] 65.82 <C> 66.16 <C> 66.61 <C> 76.11 <C> 75.44 <R> <C> hu <C> 73.13 <C> [BOLD] 76.30 <C> 73.62 <C> [BOLD] 76.85 <C> 80.04 <C> 80.64 <C> 86.43 <C> 84.12 <R> <C> tr <C> 69.58 <C> [BOLD] 75.21 <C> 75.81 <C> [BOLD] 78.93 <C> 78.31 <C> [BOLD] 83.32 <C> 91.51 <C> 90.86 <R> <C> el <C> 86.87 <C> [ITALIC] 86.07 <C> 86.40 <C> [BOLD] 87.50 <C> 94.64 <C> [BOLD] 94.96 <C> 96.55 <C> [BOLD] 96.76 <R> <C> bg <C> 78.26 <C> [BOLD] 81.77 <C> 82.74 <C> [BOLD] 84.93 <C> 91.98 <C> [BOLD] 93.48 <C> 96.12 <C> 95.96 <R> <C> sv <C> 82.09 <C> [BOLD] 84.12 <C> 85.26 <C> [BOLD] 88.16 <C> 92.45 <C> [BOLD] 94.20 <C> 96.37 <C> [BOLD] 96.57 <R> <C> eu <C> 65.29 <C> [BOLD] 66.00 <C> 70.67 <C> [ITALIC] 70.27 <C> 82.75 <C> [BOLD] 84.74 <C> 90.58 <C> [BOLD] 91.39 <R> <C> ru <C> 77.31 <C> [BOLD] 81.84 <C> 79.83 <C> [BOLD] 83.53 <C> 88.80 <C> [BOLD] 91.24 <C> 93.54 <C> 93.56 <R> <C> da <C> 80.26 <C> [BOLD] 82.74 <C> 83.59 <C> 82.65 <C> 92.06 <C> [BOLD] 94.14 <C> 96.05 <C> 95.96 <R> <C> zh <C> 63.29 <C> [BOLD] 71.44 <C> 63.50 <C> [BOLD] 74.66 <C> 84.95 <C> 85.70 <C> 84.86 <C> 85.87 <R> <C> fa <C> 84.73 <C> [BOLD] 86.07 <C> 85.94 <C> 81.75 <C> 95.30 <C> [BOLD] 95.55 <C> 96.90 <C> 96.80 <R> <C> he <C> 75.35 <C> 68.57 <C> 81.06 <C> 75.24 <C> 90.25 <C> [BOLD] 90.99 <C> 93.35 <C> 93.63 <R> <C> ro <C> 84.20 <C> [BOLD] 85.64 <C> 85.61 <C> [BOLD] 87.31 <C> 94.97 <C> [BOLD] 96.10 <C> 97.18 <C> 97.14 <R> <C> en <C> 86.71 <C> [BOLD] 87.99 <C> 88.50 <C> [BOLD] 89.61 <C> 95.30 <C> [BOLD] 95.59 <C> 96.40 <C> 96.30 <R> <C> ar <C> 84.14 <C> 84.17 <C> 81.41 <C> [ITALIC] 81.11 <C> 94.43 <C> [BOLD] 94.85 <C> 95.50 <C> 95.37 <R> <C> hi <C> 83.45 <C> [BOLD] 86.89 <C> 85.64 <C> 85.27 <C> 96.15 <C> 96.21 <C> 96.59 <C> [BOLD] 96.67 <R> <C> it <C> 89.96 <C> [BOLD] 92.07 <C> 91.27 <C> [BOLD] 92.62 <C> 97.32 <C> [BOLD] 97.80 <C> 98.18 <C> 98.31 <R> <C> es <C> 88.11 <C> [BOLD] 89.81 <C> 88.58 <C> [BOLD] 89.63 <C> 94.84 <C> [BOLD] 95.44 <C> 96.21 <C> [BOLD] 96.84 <R> <C> cs <C> 68.66 <C> [BOLD] 72.65 <C> 71.02 <C> [BOLD] 73.61 <C> 91.75 <C> [BOLD] 93.71 <C> 95.29 <C> 95.31 <CAP> Table 6: Micro-F1 for morphosyntactic attributes (UD 1.4 Test). Bold (Italic) type indicates significant improvement (degradation) by a bootstrapped Z-test, p <COT> Looking at the table, we can see that the No-Char column consistently has the lowest scores for all languages.
<R> <C> Test set embeddings <C> Missing embeddings <C> Full vocabulary <C> Full vocabulary <C> OOV (UD) <C> OOV (UD) <R> <C> char→tag <C> [EMPTY] <C> w/o <C> with <C> w/o <C> with <R> <C> Persian <C> 2.2% <C> 0.03 <C> [BOLD] 0.41 <C> [BOLD] 0.83 <C> [BOLD] 0.81 <R> <C> Hindi <C> 3.8% <C> [BOLD] 0.59 <C> 0.21 <C> [BOLD] 3.61 <C> 0.36 <R> <C> English <C> 4.5% <C> [BOLD] 0.83 <C> 0.25 <C> [BOLD] 3.26 <C> 0.49 <R> <C> Spanish <C> 5.2% <C> 0.33 <C> -0.26 <C> 1.03 <C> -0.66 <R> <C> Italian <C> 6.6% <C> [BOLD] 0.84 <C> 0.28 <C> [BOLD] 1.83 <C> 0.21 <R> <C> Danish <C> 7.8% <C> 0.65 <C> [BOLD] 0.99 <C> [BOLD] 2.41 <C> [BOLD] 1.72 <R> <C> Hebrew <C> 9.2% <C> [BOLD] 1.25 <C> [BOLD] 0.40 <C> [BOLD] 3.03 <C> 0.06 <R> <C> Swedish <C> 9.2% <C> [BOLD] 1.50 <C> [BOLD] 0.55 <C> [BOLD] 4.75 <C> [BOLD] 1.79 <R> <C> Bulgarian <C> 9.4% <C> [BOLD] 0.96 <C> 0.12 <C> [BOLD] 1.83 <C> -0.11 <R> <C> Czech <C> 10.6% <C> [BOLD] 2.24 <C> [BOLD] 1.32 <C> [BOLD] 5.84 <C> [BOLD] 2.20 <R> <C> Latvian <C> 11.1% <C> [BOLD] 2.87 <C> [BOLD] 1.03 <C> [BOLD] 7.29 <C> [BOLD] 2.71 <R> <C> Hungarian <C> 11.6% <C> [BOLD] 2.62 <C> [BOLD] 2.01 <C> [BOLD] 5.76 <C> [BOLD] 4.85 <R> <C> Turkish <C> 14.5% <C> [BOLD] 1.73 <C> [BOLD] 1.69 <C> [BOLD] 3.58 <C> [BOLD] 2.71 <R> <C> Tamil* <C> 16.2% <C> [BOLD] 2.52 <C> 0.35 <C> 2.09 <C> 1.35 <R> <C> Russian <C> 16.5% <C> [BOLD] 2.17 <C> [BOLD] 1.82 <C> [BOLD] 4.55 <C> [BOLD] 3.52 <R> <C> Greek <C> 17.5% <C> [BOLD] 1.07 <C> 0.34 <C> [BOLD] 3.30 <C> 1.17 <R> <C> Indonesian <C> 19.1% <C> [BOLD] 0.46 <C> 0.25 <C> [BOLD] 1.19 <C> 0.75 <R> <C> Kazakh* <C> 21.0% <C> 2.01 <C> 1.24 <C> [BOLD] 5.34 <C> [BOLD] 4.20 <R> <C> Vietnamese <C> 21.9% <C> 0.53 <C> [BOLD] 1.18 <C> 1.07 <C> [BOLD] 5.73 <R> <C> Romanian <C> 27.1% <C> [BOLD] 1.49 <C> [BOLD] 0.47 <C> [BOLD] 4.22 <C> [BOLD] 1.24 <R> <C> Arabic <C> 27.1% <C> [BOLD] 1.23 <C> [BOLD] 0.32 <C> [BOLD] 2.15 <C> 0.22 <R> <C> Basque <C> 35.3% <C> [BOLD] 2.39 <C> [BOLD] 1.06 <C> [BOLD] 5.42 <C> [BOLD] 1.68 <R> <C> Chinese <C> 69.9% <C> [BOLD] 4.19 <C> [BOLD] 2.57 <C> [BOLD] 9.52 <C> [BOLD] 5.24 <CAP> Table 7: Absolute gain in POS tagging accuracy from using mimick for 10,000-token datasets (all tokens for Tamil and Kazakh). Bold denotes statistical significance (McNemar’s test,p<0.01). <COT> Looking at the "Test set embeddings" column, we can see that the percentage of missing embeddings is higher for languages with lower POS tagging accuracy.
<R> <C> Test set embeddings <C> Missing embeddings <C> Full vocabulary <C> Full vocabulary <C> OOV (UD) <C> OOV (UD) <R> <C> char→tag <C> [EMPTY] <C> w/o <C> with <C> w/o <C> with <R> <C> Persian <C> 2.2% <C> 0.03 <C> [BOLD] 0.41 <C> [BOLD] 0.83 <C> [BOLD] 0.81 <R> <C> Hindi <C> 3.8% <C> [BOLD] 0.59 <C> 0.21 <C> [BOLD] 3.61 <C> 0.36 <R> <C> English <C> 4.5% <C> [BOLD] 0.83 <C> 0.25 <C> [BOLD] 3.26 <C> 0.49 <R> <C> Spanish <C> 5.2% <C> 0.33 <C> -0.26 <C> 1.03 <C> -0.66 <R> <C> Italian <C> 6.6% <C> [BOLD] 0.84 <C> 0.28 <C> [BOLD] 1.83 <C> 0.21 <R> <C> Danish <C> 7.8% <C> 0.65 <C> [BOLD] 0.99 <C> [BOLD] 2.41 <C> [BOLD] 1.72 <R> <C> Hebrew <C> 9.2% <C> [BOLD] 1.25 <C> [BOLD] 0.40 <C> [BOLD] 3.03 <C> 0.06 <R> <C> Swedish <C> 9.2% <C> [BOLD] 1.50 <C> [BOLD] 0.55 <C> [BOLD] 4.75 <C> [BOLD] 1.79 <R> <C> Bulgarian <C> 9.4% <C> [BOLD] 0.96 <C> 0.12 <C> [BOLD] 1.83 <C> -0.11 <R> <C> Czech <C> 10.6% <C> [BOLD] 2.24 <C> [BOLD] 1.32 <C> [BOLD] 5.84 <C> [BOLD] 2.20 <R> <C> Latvian <C> 11.1% <C> [BOLD] 2.87 <C> [BOLD] 1.03 <C> [BOLD] 7.29 <C> [BOLD] 2.71 <R> <C> Hungarian <C> 11.6% <C> [BOLD] 2.62 <C> [BOLD] 2.01 <C> [BOLD] 5.76 <C> [BOLD] 4.85 <R> <C> Turkish <C> 14.5% <C> [BOLD] 1.73 <C> [BOLD] 1.69 <C> [BOLD] 3.58 <C> [BOLD] 2.71 <R> <C> Tamil* <C> 16.2% <C> [BOLD] 2.52 <C> 0.35 <C> 2.09 <C> 1.35 <R> <C> Russian <C> 16.5% <C> [BOLD] 2.17 <C> [BOLD] 1.82 <C> [BOLD] 4.55 <C> [BOLD] 3.52 <R> <C> Greek <C> 17.5% <C> [BOLD] 1.07 <C> 0.34 <C> [BOLD] 3.30 <C> 1.17 <R> <C> Indonesian <C> 19.1% <C> [BOLD] 0.46 <C> 0.25 <C> [BOLD] 1.19 <C> 0.75 <R> <C> Kazakh* <C> 21.0% <C> 2.01 <C> 1.24 <C> [BOLD] 5.34 <C> [BOLD] 4.20 <R> <C> Vietnamese <C> 21.9% <C> 0.53 <C> [BOLD] 1.18 <C> 1.07 <C> [BOLD] 5.73 <R> <C> Romanian <C> 27.1% <C> [BOLD] 1.49 <C> [BOLD] 0.47 <C> [BOLD] 4.22 <C> [BOLD] 1.24 <R> <C> Arabic <C> 27.1% <C> [BOLD] 1.23 <C> [BOLD] 0.32 <C> [BOLD] 2.15 <C> 0.22 <R> <C> Basque <C> 35.3% <C> [BOLD] 2.39 <C> [BOLD] 1.06 <C> [BOLD] 5.42 <C> [BOLD] 1.68 <R> <C> Chinese <C> 69.9% <C> [BOLD] 4.19 <C> [BOLD] 2.57 <C> [BOLD] 9.52 <C> [BOLD] 5.24 <CAP> Table 7: Absolute gain in POS tagging accuracy from using mimick for 10,000-token datasets (all tokens for Tamil and Kazakh). Bold denotes statistical significance (McNemar’s test,p<0.01). <COT> Looking at the "Full vocabulary with" column, we can see that using mimick improves the POS tagging accuracy compared to using the vocabulary without mimick.
<R> <C> Scenario <C> Human Model Accuracy <C> Human Model Perplexity <C> Script Model Accuracy <C> Script Model Perplexity <C> Linguistic Model Accuracy <C> Linguistic Model Perplexity <C> Tily Model Accuracy <C> Tily Model Perplexity <R> <C> Grocery Shopping <C> 74.80 <C> 2.13 <C> 68.17 <C> 3.16 <C> 53.85 <C> 6.54 <C> 32.89 <C> 24.48 <R> <C> Repairing a flat bicycle tyre <C> 78.34 <C> 2.72 <C> 62.09 <C> 3.89 <C> 51.26 <C> 6.38 <C> 29.24 <C> 19.08 <R> <C> Riding a public bus <C> 72.19 <C> 2.28 <C> 64.57 <C> 3.67 <C> 52.65 <C> 6.34 <C> 32.78 <C> 23.39 <R> <C> Getting a haircut <C> 71.06 <C> 2.45 <C> 58.82 <C> 3.79 <C> 42.82 <C> 7.11 <C> 28.70 <C> 15.40 <R> <C> Planting a tree <C> 71.86 <C> 2.46 <C> 59.32 <C> 4.25 <C> 47.80 <C> 7.31 <C> 28.14 <C> 24.28 <R> <C> Borrowing book from library <C> 77.49 <C> 1.93 <C> 64.07 <C> 3.55 <C> 43.29 <C> 8.40 <C> 33.33 <C> 20.26 <R> <C> Taking Bath <C> 81.29 <C> 1.84 <C> 67.42 <C> 3.14 <C> 61.29 <C> 4.33 <C> 43.23 <C> 16.33 <R> <C> Going on a train <C> 70.79 <C> 2.39 <C> 58.73 <C> 4.20 <C> 47.62 <C> 7.68 <C> 30.16 <C> 35.11 <R> <C> Baking a cake <C> 76.43 <C> 2.16 <C> 61.79 <C> 5.11 <C> 46.40 <C> 9.16 <C> 24.07 <C> 23.67 <R> <C> Flying in an airplane <C> 62.04 <C> 3.08 <C> 61.31 <C> 4.01 <C> 48.18 <C> 7.27 <C> 30.90 <C> 30.18 <R> <C> Average <C> 73.63 <C> 2.34 <C> 62.63 <C> 3.88 <C> 49.52 <C> 7.05 <C> 31.34 <C> 23.22 <CAP> Table 3: Accuracies (in %) and perplexities for different models and scenarios. The script model substantially outperforms linguistic and base models (with p<0.001, significance tested with McNemar’s test [Everitt1992]). As expected, the human prediction model outperforms the script model (with p<0.001, significance tested by McNemar’s test). <COT> Looking at the "Script Model Perplexity" column, we can see that the perplexity values for all scenarios are higher than the perplexity values for the Human Model.
<R> <C> [EMPTY] <C> [BOLD] Standard Decoder MRR <C> [BOLD] Standard Decoder P@1 <C> [BOLD] Our Decoder MRR <C> [BOLD] Our Decoder P@1 <R> <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <R> <C> Retrieval <C> 81.08 <C> 65.45 <C> - <C> - <R> <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <R> <C> Seq2seq <C> 75.29 <C> 58.85 <C> 74.46 <C> 57.06 <R> <C> +  [ITALIC] encode evd <C> 83.73 <C> 71.59 <C> 88.24 <C> 78.76 <R> <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <R> <C> Dec-shared <C> 79.80 <C> 65.57 <C> [BOLD] 95.18 <C> [BOLD] 90.91 <R> <C> +  [ITALIC] attend KP <C> [BOLD] 94.33 <C> [BOLD] 89.76 <C> 93.48 <C> 87.91 <R> <C> Dec-separate <C> 86.85 <C> 76.74 <C> 91.70 <C> 84.72 <R> <C> +  [ITALIC] attend KP <C> 88.53 <C> 79.05 <C> 92.77 <C> 86.46 <CAP> Table 4: Evaluation on topic relevance—models that generate arguments highly related with OP should be ranked high by a separately trained relevance estimation model, i.e., higher Mean Reciprocal Rank (MRR) and Precision at 1 (P@1) scores. All models trained with evidence significantly outperform seq2seq trained without evidence (approximation randomization testing, p<0.0001). <COT> Looking at the "Our Decoder MRR" column, we can see that the "Our Models" with "Dec-shared" and "+ attend KP" has the highest MRR score.
<R> <C> [EMPTY] <C> [BOLD] Standard Decoder MRR <C> [BOLD] Standard Decoder P@1 <C> [BOLD] Our Decoder MRR <C> [BOLD] Our Decoder P@1 <R> <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <C> [BOLD] Baseline <R> <C> Retrieval <C> 81.08 <C> 65.45 <C> - <C> - <R> <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <R> <C> Seq2seq <C> 75.29 <C> 58.85 <C> 74.46 <C> 57.06 <R> <C> +  [ITALIC] encode evd <C> 83.73 <C> 71.59 <C> 88.24 <C> 78.76 <R> <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <R> <C> Dec-shared <C> 79.80 <C> 65.57 <C> [BOLD] 95.18 <C> [BOLD] 90.91 <R> <C> +  [ITALIC] attend KP <C> [BOLD] 94.33 <C> [BOLD] 89.76 <C> 93.48 <C> 87.91 <R> <C> Dec-separate <C> 86.85 <C> 76.74 <C> 91.70 <C> 84.72 <R> <C> +  [ITALIC] attend KP <C> 88.53 <C> 79.05 <C> 92.77 <C> 86.46 <CAP> Table 4: Evaluation on topic relevance—models that generate arguments highly related with OP should be ranked high by a separately trained relevance estimation model, i.e., higher Mean Reciprocal Rank (MRR) and Precision at 1 (P@1) scores. All models trained with evidence significantly outperform seq2seq trained without evidence (approximation randomization testing, p<0.0001). <COT> Looking at the "Standard Decoder P@1" column, we can see that the "Baseline" has the lowest P@1 score.
<R> <C> [EMPTY] <C> Estimate Human <C> Estimate Script <C> Estimate Linguistic <C> Estimate Base <C> Std. Error Human <C> Std. Error Script <C> Std. Error Linguistic <C> Std. Error Base <C> Pr(>∣ [ITALIC] z∣) Human <C> Pr(>∣ [ITALIC] z∣) Script <C> Pr(>∣ [ITALIC] z∣) Linguistic <C> Pr(>∣ [ITALIC] z∣) Base <R> <C> (Intercept) <C> -3.4 <C> -3.418 <C> -3.245 <C> -3.061 <C> 0.244 <C> 0.279 <C> 0.321 <C> 0.791 <C> <2e-16 *** <C> <2e-16 *** <C> <2e-16 *** <C> 0.00011 *** <R> <C> recency <C> 1.322 <C> 1.322 <C> 1.324 <C> 1.322 <C> 0.095 <C> 0.095 <C> 0.096 <C> 0.097 <C> <2e-16 *** <C> <2e-16 *** <C> <2e-16 *** <C> <2e-16 *** <R> <C> frequency <C> 0.097 <C> 0.103 <C> 0.112 <C> 0.114 <C> 0.098 <C> 0.097 <C> 0.098 <C> 0.102 <C> 0.317 <C> 0.289 <C> 0.251 <C> 0.262 <R> <C> pastObj <C> 0.407 <C> 0.396 <C> 0.423 <C> 0.395 <C> 0.293 <C> 0.294 <C> 0.295 <C> 0.3 <C> 0.165 <C> 0.178 <C> 0.151 <C> 0.189 <R> <C> pastSubj <C> -0.967 <C> -0.973 <C> -0.909 <C> -0.926 <C> 0.559 <C> 0.564 <C> 0.562 <C> 0.565 <C> 0.0838 . <C> 0.0846 . <C> 0.106 <C> 0.101 <R> <C> pastExpPronoun <C> 1.603 <C> 1.619 <C> 1.616 <C> 1.602 <C> 0.21 <C> 0.207 <C> 0.208 <C> 0.245 <C> 2.19e-14 *** <C> 5.48e-15 *** <C> 7.59e-15 *** <C> 6.11e-11 *** <R> <C> depTypeSubj <C> 2.939 <C> 2.942 <C> 2.656 <C> 2.417 <C> 0.299 <C> 0.347 <C> 0.429 <C> 1.113 <C> <2e-16 *** <C> <2e-16 *** <C> 5.68e-10 *** <C> 0.02994 * <R> <C> depTypeObj <C> 1.199 <C> 1.227 <C> 0.977 <C> 0.705 <C> 0.248 <C> 0.306 <C> 0.389 <C> 1.109 <C> 1.35e-06 *** <C> 6.05e-05 *** <C> 0.0119 * <C> 0.525 <R> <C> surprisal <C> -0.04 <C> -0.006 <C> 0.002 <C> -0.131 <C> 0.099 <C> 0.097 <C> 0.117 <C> 0.387 <C> 0.684 <C> 0.951 <C> 0.988 <C> 0.735 <R> <C> residualEntropy <C> -0.009 <C> 0.023 <C> -0.141 <C> -0.128 <C> 0.088 <C> 0.128 <C> 0.168 <C> 0.258 <C> 0.916 <C> 0.859 <C> 0.401 <C> 0.619 <CAP> Table 5: Coefficients obtained from regression analysis for different models. Two NP types considered: full NP and Pronoun/ProperNoun, with base class full NP. Significance: ‘***’ <0.001, ‘**’ <0.01, ‘*’ <0.05, and ‘.’ <0.1. <COT> Looking at the table, we can see that the coefficient for the variable "pastExpPronoun" is positive for all four models (Estimate Human, Estimate Script, Estimate Linguistic, and Estimate Base). 
<R> <C> [EMPTY] <C> Estimate Human <C> Estimate Script <C> Estimate Linguistic <C> Estimate Base <C> Std. Error Human <C> Std. Error Script <C> Std. Error Linguistic <C> Std. Error Base <C> Pr(>∣ [ITALIC] z∣) Human <C> Pr(>∣ [ITALIC] z∣) Script <C> Pr(>∣ [ITALIC] z∣) Linguistic <C> Pr(>∣ [ITALIC] z∣) Base <R> <C> (Intercept) <C> -3.4 <C> -3.418 <C> -3.245 <C> -3.061 <C> 0.244 <C> 0.279 <C> 0.321 <C> 0.791 <C> <2e-16 *** <C> <2e-16 *** <C> <2e-16 *** <C> 0.00011 *** <R> <C> recency <C> 1.322 <C> 1.322 <C> 1.324 <C> 1.322 <C> 0.095 <C> 0.095 <C> 0.096 <C> 0.097 <C> <2e-16 *** <C> <2e-16 *** <C> <2e-16 *** <C> <2e-16 *** <R> <C> frequency <C> 0.097 <C> 0.103 <C> 0.112 <C> 0.114 <C> 0.098 <C> 0.097 <C> 0.098 <C> 0.102 <C> 0.317 <C> 0.289 <C> 0.251 <C> 0.262 <R> <C> pastObj <C> 0.407 <C> 0.396 <C> 0.423 <C> 0.395 <C> 0.293 <C> 0.294 <C> 0.295 <C> 0.3 <C> 0.165 <C> 0.178 <C> 0.151 <C> 0.189 <R> <C> pastSubj <C> -0.967 <C> -0.973 <C> -0.909 <C> -0.926 <C> 0.559 <C> 0.564 <C> 0.562 <C> 0.565 <C> 0.0838 . <C> 0.0846 . <C> 0.106 <C> 0.101 <R> <C> pastExpPronoun <C> 1.603 <C> 1.619 <C> 1.616 <C> 1.602 <C> 0.21 <C> 0.207 <C> 0.208 <C> 0.245 <C> 2.19e-14 *** <C> 5.48e-15 *** <C> 7.59e-15 *** <C> 6.11e-11 *** <R> <C> depTypeSubj <C> 2.939 <C> 2.942 <C> 2.656 <C> 2.417 <C> 0.299 <C> 0.347 <C> 0.429 <C> 1.113 <C> <2e-16 *** <C> <2e-16 *** <C> 5.68e-10 *** <C> 0.02994 * <R> <C> depTypeObj <C> 1.199 <C> 1.227 <C> 0.977 <C> 0.705 <C> 0.248 <C> 0.306 <C> 0.389 <C> 1.109 <C> 1.35e-06 *** <C> 6.05e-05 *** <C> 0.0119 * <C> 0.525 <R> <C> surprisal <C> -0.04 <C> -0.006 <C> 0.002 <C> -0.131 <C> 0.099 <C> 0.097 <C> 0.117 <C> 0.387 <C> 0.684 <C> 0.951 <C> 0.988 <C> 0.735 <R> <C> residualEntropy <C> -0.009 <C> 0.023 <C> -0.141 <C> -0.128 <C> 0.088 <C> 0.128 <C> 0.168 <C> 0.258 <C> 0.916 <C> 0.859 <C> 0.401 <C> 0.619 <CAP> Table 5: Coefficients obtained from regression analysis for different models. Two NP types considered: full NP and Pronoun/ProperNoun, with base class full NP. Significance: ‘***’ <0.001, ‘**’ <0.01, ‘*’ <0.05, and ‘.’ <0.1. <COT> Looking at the table, we can see that the Pr(>|z|) value for the variable "depTypeObj" is less than 0.05 for all four models (Pr(>|z|) Human, Pr(>|z|) Script, Pr(>|z|) Linguistic, and Pr(>|z|) Base).
<R> <C> Model <C> Downstream <C> Keller dobj <C> Keller amod <C> Keller average <C> SP-10K nsubj <C> SP-10K dobj <C> SP-10K amod <C> SP-10K average <R> <C> word2vec <C> Friendly <C> 0.29 <C> 0.28 <C> 0.29 <C> 0.32 <C> 0.53 <C> 0.62 <C> 0.49 <R> <C> GloVe <C> Friendly <C> 0.37 <C> 0.32 <C> 0.35 <C> 0.57 <C> 0.60 <C> 0.68 <C> 0.62 <R> <C> D-embeddings <C> Friendly <C> 0.19 <C> 0.22 <C> 0.21 <C> 0.66 <C> 0.71 <C> 0.77 <C> 0.71 <R> <C> ELMo <C> Friendly <C> 0.23 <C> 0.06 <C> 0.15 <C> 0.09 <C> 0.29 <C> 0.38 <C> 0.25 <R> <C> BERT (static) <C> Friendly <C> 0.11 <C> 0.05 <C> 0.08 <C> 0.25 <C> 0.32 <C> 0.27 <C> 0.28 <R> <C> BERT (dynamic) <C> Friendly <C> 0.19 <C> 0.23 <C> 0.21 <C> 0.35 <C> 0.45 <C> 0.51 <C> 0.41 <R> <C> PP <C> Unfriendly <C> [BOLD] 0.66 <C> 0.26 <C> 0.46 <C> 0.75 <C> 0.74 <C> 0.75 <C> 0.75 <R> <C> DS <C> Unfriendly <C> 0.53 <C> 0.32 <C> 0.43 <C> 0.59 <C> 0.65 <C> 0.67 <C> 0.64 <R> <C> NN <C> Unfriendly <C> 0.16 <C> 0.13 <C> 0.15 <C> 0.70 <C> 0.68 <C> 0.68 <C> 0.69 <R> <C> MWE <C> Friendly <C> 0.63 <C> [BOLD] 0.43† <C> [BOLD] 0.53† <C> [BOLD] 0.76 <C> [BOLD] 0.79† <C> [BOLD] 0.78 <C> [BOLD] 0.78† <CAP> Table 1: Results on different SP acquisition evaluation sets. As Keller is created based on the PP distribution and has relatively small size while SP-10K is created based on random sampling and has a much larger size, we treat the performance on SP-10K as the main evaluation metric. Spearman’s correlation between predicated plausibility and annotations are reported. The best performing models are denoted with bold font. † indicates statistical significant (p <0.005) overall baseline methods. <COT> Looking at the "Downstream" column and the "DS" row, we can see that the Unfriendly category has the highest correlation score with the DS model.
<R> <C> Model <C> WS <C> Dimension <C> Training Time <R> <C> ELMo <C> 0.434 <C> 512 <C> ≈40 <R> <C> BERT <C> 0.486 <C> 768 <C> ≈300 <R> <C> MWE <C> 0.476 <C> 300 <C> 4.17 <CAP> Table 4: Comparison of MWE against language models on the WS task. Overall performance, embedding dimension, and training time (days) on a single GPU are reported. <COT> Looking at the "Model" column, we can see that ELMo has a WS score of 0.434, BERT has a WS score of 0.486, and MWE has a WS score of 0.476. 
<R> <C> Model <C> WS <C> Dimension <C> Training Time <R> <C> ELMo <C> 0.434 <C> 512 <C> ≈40 <R> <C> BERT <C> 0.486 <C> 768 <C> ≈300 <R> <C> MWE <C> 0.476 <C> 300 <C> 4.17 <CAP> Table 4: Comparison of MWE against language models on the WS task. Overall performance, embedding dimension, and training time (days) on a single GPU are reported. <COT> Looking at the "Training Time" column, we can see that ELMo takes approximately 40 days to train, BERT takes approximately 300 days to train, and MWE takes 4.17 days to train.
<R> <C> Training Strategy <C> Averaged SPA <C> Overall WS <R> <C> [ITALIC] λ = 1 <C> 0.762 <C> 0.476 <R> <C> [ITALIC] λ = 0 <C> 0.073 <C> 0.018 <R> <C> [ITALIC] λ = 0.5 <C> 0.493 <C> 0.323 <R> <C> Alternating optimization <C> 0.775 <C> 0.476 <CAP> Table 5: Comparisons of different training strategies. <COT> Looking at the "Training Strategy" column, we can see that there are three different strategies mentioned: λ = 1, λ = 0, and λ = 0.5. 
<R> <C> Architecture <C> Representation <C> Dev BLEU <C> Test BLEU <R> <C> Seq2seq (8-model ensemble) <C> Best WAT17 result Morishita et al. ( 2017 ) <C> - <C> 28.4 <R> <C> Seq2seq <C> Plain BPE <C> 21.6 <C> 21.2 <R> <C> Seq2seq <C> Linearized derivation <C> 21.9 <C> 21.2 <R> <C> Transformer <C> Plain BPE <C> 28.0 <C> 28.9 <R> <C> Transformer <C> Linearized tree <C> 28.2 <C> 28.4 <R> <C> Transformer <C> Linearized derivation <C> 28.5 <C> 28.7 <R> <C> Transformer <C> POS/BPE <C> 28.5 <C> 29.1 <CAP> Table 4: Single models on Ja-En. Previous evaluation result included for comparison. <COT> Looking at the "Representation" column, we can see that the Linearized derivation representation is used by multiple architectures and has relatively high Dev BLEU and Test BLEU scores.
<R> <C> External representation <C> Internal representation <C> Test BLEU <R> <C> Plain BPE <C> Plain BPE <C> 29.2 <R> <C> Linearized derivation <C> Linearized derivation <C> 28.8 <R> <C> Linearized tree <C> Plain BPE <C> 28.9 <R> <C> Plain BPE <C> Linearized derivation <C> 28.8 <R> <C> Linearized derivation <C> Plain BPE <C> 29.4† <R> <C> POS/BPE <C> Plain BPE <C> 29.3† <R> <C> Plain BPE <C> POS/BPE <C> 29.4† <CAP> Table 5: Ja-En Transformer ensembles: † marks significant improvement on plain BPE baseline shown in Table 4 (p<0.05 using bootstrap resampling (Koehn et al., 2007)). <COT> Looking at the "External representation" column, we can see that the "Linearized derivation" external representation is used in combination with both the "Plain BPE" and "POS/BPE" internal representations.
<R> <C> config <C> English→⋆ en-cs <C> English→⋆ en-de <C> English→⋆ en-fi <C> English→⋆ en-lv <C> English→⋆ en-ru <C> English→⋆ en-tr <C> ⋆→English cs-en <C> ⋆→English de-en <C> ⋆→English fi-en <C> ⋆→English lv-en <C> ⋆→English ru-en <C> ⋆→English tr-en <R> <C> basic <C> 20.7 <C> 25.8 <C> 22.2 <C> 16.9 <C> 33.3 <C> 18.5 <C> 26.8 <C> 31.2 <C> 26.6 <C> 21.1 <C> 36.4 <C> 24.4 <R> <C> split <C> 20.7 <C> 26.1 <C> 22.6 <C> 17.0 <C> 33.3 <C> 18.7 <C> 26.9 <C> 31.7 <C> 26.9 <C> 21.3 <C> 36.7 <C> 24.7 <R> <C> unk <C> 20.9 <C> 26.5 <C> 25.4 <C> 18.7 <C> 33.8 <C> 20.6 <C> 26.9 <C> 31.4 <C> 27.6 <C> 22.7 <C> 37.5 <C> 25.2 <R> <C> metric <C> 20.1 <C> 26.6 <C> 22.0 <C> 17.9 <C> 32.0 <C> 19.9 <C> 27.4 <C> 33.0 <C> 27.6 <C> 22.0 <C> 36.9 <C> 25.6 <R> <C> [ITALIC] range <C> 0.6 <C> 0.8 <C> 0.6 <C> 1.0 <C> 1.3 <C> 1.4 <C> 0.6 <C> 1.8 <C> 1.0 <C> 0.9 <C> 0.5 <C> 1.2 <R> <C> basic [ITALIC] lc <C> 21.2 <C> 26.3 <C> 22.5 <C> 17.4 <C> 33.3 <C> 18.9 <C> 27.7 <C> 32.5 <C> 27.5 <C> 22.0 <C> 37.3 <C> 25.2 <R> <C> split [ITALIC] lc <C> 21.3 <C> 26.6 <C> 22.9 <C> 17.5 <C> 33.4 <C> 19.1 <C> 27.8 <C> 32.9 <C> 27.8 <C> 22.2 <C> 37.5 <C> 25.4 <R> <C> unk [ITALIC] lc <C> 21.4 <C> 27.0 <C> 25.6 <C> 19.1 <C> 33.8 <C> 21.0 <C> 27.8 <C> 32.6 <C> 28.3 <C> 23.6 <C> 38.3 <C> 25.9 <R> <C> metric [ITALIC] lc <C> 20.6 <C> 27.2 <C> 22.4 <C> 18.5 <C> 32.8 <C> 20.4 <C> 28.4 <C> 34.2 <C> 28.5 <C> 23.0 <C> 37.8 <C> 26.4 <R> <C> [ITALIC] rangelc <C> 0.6 <C> 0.9 <C> 0.5 <C> 1.1 <C> 0.6 <C> 1.5 <C> 0.7 <C> 1.7 <C> 1.0 <C> 1.0 <C> 0.5 <C> 1.2 <CAP> Table 1: BLEU score variation across WMT’17 language arcs for cased (top) and uncased (bottom) BLEU. Each column varies the processing of the “online-B” system output and its references. basic denotes basic user-supplied tokenization, split adds compound splitting, unk replaces words not appearing at least twice in the training data with UNK, and metric denotes the metric-supplied tokenization used by WMT. The range row lists the difference between the smallest and largest scores, excluding unk. <COT> Looking at the "config" column, we can see that there are different variations of processing for the "English→⋆" and "⋆→English" language arcs.
<R> <C> config <C> English→⋆ en-cs <C> English→⋆ en-de <C> English→⋆ en-fi <C> English→⋆ en-lv <C> English→⋆ en-ru <C> English→⋆ en-tr <C> ⋆→English cs-en <C> ⋆→English de-en <C> ⋆→English fi-en <C> ⋆→English lv-en <C> ⋆→English ru-en <C> ⋆→English tr-en <R> <C> basic <C> 20.7 <C> 25.8 <C> 22.2 <C> 16.9 <C> 33.3 <C> 18.5 <C> 26.8 <C> 31.2 <C> 26.6 <C> 21.1 <C> 36.4 <C> 24.4 <R> <C> split <C> 20.7 <C> 26.1 <C> 22.6 <C> 17.0 <C> 33.3 <C> 18.7 <C> 26.9 <C> 31.7 <C> 26.9 <C> 21.3 <C> 36.7 <C> 24.7 <R> <C> unk <C> 20.9 <C> 26.5 <C> 25.4 <C> 18.7 <C> 33.8 <C> 20.6 <C> 26.9 <C> 31.4 <C> 27.6 <C> 22.7 <C> 37.5 <C> 25.2 <R> <C> metric <C> 20.1 <C> 26.6 <C> 22.0 <C> 17.9 <C> 32.0 <C> 19.9 <C> 27.4 <C> 33.0 <C> 27.6 <C> 22.0 <C> 36.9 <C> 25.6 <R> <C> [ITALIC] range <C> 0.6 <C> 0.8 <C> 0.6 <C> 1.0 <C> 1.3 <C> 1.4 <C> 0.6 <C> 1.8 <C> 1.0 <C> 0.9 <C> 0.5 <C> 1.2 <R> <C> basic [ITALIC] lc <C> 21.2 <C> 26.3 <C> 22.5 <C> 17.4 <C> 33.3 <C> 18.9 <C> 27.7 <C> 32.5 <C> 27.5 <C> 22.0 <C> 37.3 <C> 25.2 <R> <C> split [ITALIC] lc <C> 21.3 <C> 26.6 <C> 22.9 <C> 17.5 <C> 33.4 <C> 19.1 <C> 27.8 <C> 32.9 <C> 27.8 <C> 22.2 <C> 37.5 <C> 25.4 <R> <C> unk [ITALIC] lc <C> 21.4 <C> 27.0 <C> 25.6 <C> 19.1 <C> 33.8 <C> 21.0 <C> 27.8 <C> 32.6 <C> 28.3 <C> 23.6 <C> 38.3 <C> 25.9 <R> <C> metric [ITALIC] lc <C> 20.6 <C> 27.2 <C> 22.4 <C> 18.5 <C> 32.8 <C> 20.4 <C> 28.4 <C> 34.2 <C> 28.5 <C> 23.0 <C> 37.8 <C> 26.4 <R> <C> [ITALIC] rangelc <C> 0.6 <C> 0.9 <C> 0.5 <C> 1.1 <C> 0.6 <C> 1.5 <C> 0.7 <C> 1.7 <C> 1.0 <C> 1.0 <C> 0.5 <C> 1.2 <CAP> Table 1: BLEU score variation across WMT’17 language arcs for cased (top) and uncased (bottom) BLEU. Each column varies the processing of the “online-B” system output and its references. basic denotes basic user-supplied tokenization, split adds compound splitting, unk replaces words not appearing at least twice in the training data with UNK, and metric denotes the metric-supplied tokenization used by WMT. The range row lists the difference between the smallest and largest scores, excluding unk. <COT> Looking at the "range" row, we can see that the values represent the difference between the smallest and largest scores, excluding "unk".
<R> <C> [EMPTY] <C> [BOLD] % perf <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F <R> <C> [BOLD] SPMRL <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] baseline <C> 69.65 <C> – <C> – <C> – <R> <C> [ITALIC] UDPipe <C> 89.65 <C> 93.52 <C> 68.82 <C> 79.29 <R> <C> [ITALIC] yap <C> 94.25 <C> 86.33 <C> 96.33 <C> 91.05 <R> <C> [ITALIC] RF <C> [BOLD] 98.19 <C> [BOLD] 97.59 <C> [BOLD] 96.57 <C> [BOLD] 97.08 <R> <C> [ITALIC] DNN <C> 97.27 <C> 95.90 <C> 95.01 <C> 95.45 <R> <C> [BOLD] Wiki5K <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] baseline <C> 67.61 <C> – <C> – <C> – <R> <C> [ITALIC] UDPipe <C> 87.39 <C> 92.03 <C> 64.88 <C> 76.11 <R> <C> [ITALIC] yap <C> 92.66 <C> 85.55 <C> 92.34 <C> 88.81 <R> <C> [ITALIC] RF <C> [BOLD] 97.63 <C> [BOLD] 97.41 <C> [BOLD] 95.31 <C> [BOLD] 96.35 <R> <C> [ITALIC] DNN <C> 95.72 <C> 94.95 <C> 92.22 <C> 93.56 <CAP> Table 2: System performance on the SPMRL and Wiki5K datasets. <COT> Looking at the "Table 2" caption, we can see that the table provides information about the system performance on the SPMRL and Wiki5K datasets.
<R> <C> [EMPTY] <C> [BOLD] % perf <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F <R> <C> [BOLD] SPMRL <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] baseline <C> 69.65 <C> – <C> – <C> – <R> <C> [ITALIC] UDPipe <C> 89.65 <C> 93.52 <C> 68.82 <C> 79.29 <R> <C> [ITALIC] yap <C> 94.25 <C> 86.33 <C> 96.33 <C> 91.05 <R> <C> [ITALIC] RF <C> [BOLD] 98.19 <C> [BOLD] 97.59 <C> [BOLD] 96.57 <C> [BOLD] 97.08 <R> <C> [ITALIC] DNN <C> 97.27 <C> 95.90 <C> 95.01 <C> 95.45 <R> <C> [BOLD] Wiki5K <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] baseline <C> 67.61 <C> – <C> – <C> – <R> <C> [ITALIC] UDPipe <C> 87.39 <C> 92.03 <C> 64.88 <C> 76.11 <R> <C> [ITALIC] yap <C> 92.66 <C> 85.55 <C> 92.34 <C> 88.81 <R> <C> [ITALIC] RF <C> [BOLD] 97.63 <C> [BOLD] 97.41 <C> [BOLD] 95.31 <C> [BOLD] 96.35 <R> <C> [ITALIC] DNN <C> 95.72 <C> 94.95 <C> 92.22 <C> 93.56 <CAP> Table 2: System performance on the SPMRL and Wiki5K datasets. <COT> Looking at the "RF" row, we can see that the system performance for the RF model is consistently high across all metrics (P, R, F) for both the SPMRL and Wiki5K datasets.
<R> <C> [EMPTY] <C> [BOLD] % perf <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F <R> <C> [BOLD] SPMRL <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> FINAL <C> 98.19 <C> 97.59 <C> 96.57 <C> 97.08 <R> <C> -expansion <C> 98.01 <C> 97.25 <C> 96.35 <C> 96.80 <R> <C> -vowels <C> 97.99 <C> 97.55 <C> 95.97 <C> 96.75 <R> <C> -letters <C> 97.77 <C> 96.98 <C> 95.73 <C> 96.35 <R> <C> -letr-vowl <C> 97.57 <C> 97.56 <C> 94.44 <C> 95.97 <R> <C> -lexicon <C> 94.79 <C> 92.08 <C> 91.46 <C> 91.77 <R> <C> [BOLD] Wiki5K <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> FINAL <C> 97.63 <C> 97.41 <C> 95.31 <C> 96.35 <R> <C> -expansion <C> 97.33 <C> 96.64 <C> 95.31 <C> 95.97 <R> <C> -vowels <C> 97.51 <C> 97.56 <C> 94.87 <C> 96.19 <R> <C> -letters <C> 97.27 <C> 96.89 <C> 94.71 <C> 95.79 <R> <C> -letr-vowl <C> 96.72 <C> 97.17 <C> 92.77 <C> 94.92 <R> <C> -lexicon <C> 94.72 <C> 92.53 <C> 91.51 <C> 92.01 <CAP> Table 3: Effects of removing features on performance, ordered by descending F-score impact on SPMRL. <COT> Looking at the "Table 3" caption, we can see that the table shows the effects of removing features on performance, specifically on the SPMRL dataset.
<R> <C> [EMPTY] <C> [BOLD] % perf <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F <R> <C> [BOLD] SPMRL <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> FINAL <C> 98.19 <C> 97.59 <C> 96.57 <C> 97.08 <R> <C> -expansion <C> 98.01 <C> 97.25 <C> 96.35 <C> 96.80 <R> <C> -vowels <C> 97.99 <C> 97.55 <C> 95.97 <C> 96.75 <R> <C> -letters <C> 97.77 <C> 96.98 <C> 95.73 <C> 96.35 <R> <C> -letr-vowl <C> 97.57 <C> 97.56 <C> 94.44 <C> 95.97 <R> <C> -lexicon <C> 94.79 <C> 92.08 <C> 91.46 <C> 91.77 <R> <C> [BOLD] Wiki5K <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> FINAL <C> 97.63 <C> 97.41 <C> 95.31 <C> 96.35 <R> <C> -expansion <C> 97.33 <C> 96.64 <C> 95.31 <C> 95.97 <R> <C> -vowels <C> 97.51 <C> 97.56 <C> 94.87 <C> 96.19 <R> <C> -letters <C> 97.27 <C> 96.89 <C> 94.71 <C> 95.79 <R> <C> -letr-vowl <C> 96.72 <C> 97.17 <C> 92.77 <C> 94.92 <R> <C> -lexicon <C> 94.72 <C> 92.53 <C> 91.51 <C> 92.01 <CAP> Table 3: Effects of removing features on performance, ordered by descending F-score impact on SPMRL. <COT> Looking at the "FINAL" row and the "F" column, we can see that the F-score for the SPMRL dataset is 97.08%.
<R> <C> Perturbation radius <C> [ITALIC] δ=1 <C> [ITALIC] δ=2 <C> [ITALIC] δ=3 <R> <C> SST-word <C> 49 <C> 674 <C> 5,136 <R> <C> SST-character <C> 206 <C> 21,116 <C> 1,436,026 <R> <C> AG-character <C> 722 <C> 260,282 <C> - <CAP> Table 3: Maximum perturbation space size in the SST and AG News test set using word / character substitutions, which is the maximum number of forward passes per sentence to evaluate in the exhaustive verification. <COT> Looking at the "Perturbation radius" column, we can see that as the perturbation radius increases, the maximum perturbation space size also increases.
<R> <C> Perturbation radius <C> [ITALIC] δ=1 <C> [ITALIC] δ=2 <C> [ITALIC] δ=3 <R> <C> SST-word <C> 49 <C> 674 <C> 5,136 <R> <C> SST-character <C> 206 <C> 21,116 <C> 1,436,026 <R> <C> AG-character <C> 722 <C> 260,282 <C> - <CAP> Table 3: Maximum perturbation space size in the SST and AG News test set using word / character substitutions, which is the maximum number of forward passes per sentence to evaluate in the exhaustive verification. <COT> Looking at the "SST-character" row, we can see that the maximum perturbation space size for SST-character is larger than that for SST-word.
<R> <C> [BOLD] Training <C> [BOLD] Acc. <C> [BOLD] SST-Char-Level  [BOLD] Adv. Acc. <C> [BOLD] Oracle <C> [BOLD] Acc. <C> [BOLD] SST-Word-Level  [BOLD] Adv. Acc. <C> [BOLD] Oracle <C> [BOLD] Acc. <C> [BOLD] AG-Char-Level  [BOLD] Adv. Acc. <C> [BOLD] Oracle <R> <C> Normal <C> 79.8 <C> 36.5 <C> 10.3 <C> 84.8 <C> 71.3 <C> 69.8 <C> 89.5 <C> 75.4 <C> 65.1 <R> <C> Adversarial <C> 79.0 <C> [BOLD] 74.9 <C> 25.8 <C> [BOLD] 85.0 <C> 76.8 <C> 74.6 <C> [BOLD] 90.5 <C> 85.5 <C> 81.6 <R> <C> Data aug. <C> 79.8 <C> 37.8 <C> 13.7 <C> 85.4 <C> 72.7 <C> 71.6 <C> 88.4 <C> 77.5 <C> 72.0 <R> <C> Verifiable (IBP) <C> 74.2 <C> 73.1 <C> [BOLD] 73.1 <C> 81.7 <C> [BOLD] 77.2 <C> [BOLD] 76.5 <C> 87.6 <C> [BOLD] 87.1 <C> [BOLD] 87.1 <CAP> Table 1: Experimental results for changes up to δ=3 and δ=2 symbols on SST and AG dataset, respectively. We compare normal training, adversarial training, data augmentation and IBP-verifiable training, using three metrics on the test set: the nominal accuracy, adversarial accuracy, and exhaustively verified accuracy (Oracle) (%). <COT> Looking at the "Adv. Acc." column, we can see that the adversarial training achieves the highest adversarial accuracy on both SST-Char-Level and SST-Word-Level datasets.
<R> <C> [BOLD] Training <C> [BOLD] Acc. <C> [BOLD] SST-Char-Level  [BOLD] Adv. Acc. <C> [BOLD] Oracle <C> [BOLD] Acc. <C> [BOLD] SST-Word-Level  [BOLD] Adv. Acc. <C> [BOLD] Oracle <C> [BOLD] Acc. <C> [BOLD] AG-Char-Level  [BOLD] Adv. Acc. <C> [BOLD] Oracle <R> <C> Normal <C> 79.8 <C> 36.5 <C> 10.3 <C> 84.8 <C> 71.3 <C> 69.8 <C> 89.5 <C> 75.4 <C> 65.1 <R> <C> Adversarial <C> 79.0 <C> [BOLD] 74.9 <C> 25.8 <C> [BOLD] 85.0 <C> 76.8 <C> 74.6 <C> [BOLD] 90.5 <C> 85.5 <C> 81.6 <R> <C> Data aug. <C> 79.8 <C> 37.8 <C> 13.7 <C> 85.4 <C> 72.7 <C> 71.6 <C> 88.4 <C> 77.5 <C> 72.0 <R> <C> Verifiable (IBP) <C> 74.2 <C> 73.1 <C> [BOLD] 73.1 <C> 81.7 <C> [BOLD] 77.2 <C> [BOLD] 76.5 <C> 87.6 <C> [BOLD] 87.1 <C> [BOLD] 87.1 <CAP> Table 1: Experimental results for changes up to δ=3 and δ=2 symbols on SST and AG dataset, respectively. We compare normal training, adversarial training, data augmentation and IBP-verifiable training, using three metrics on the test set: the nominal accuracy, adversarial accuracy, and exhaustively verified accuracy (Oracle) (%). <COT> Looking at the "Oracle Acc." column, we can see that the IBP-verifiable training achieves the highest exhaustively verified accuracy on all datasets.
<R> <C> [EMPTY] <C> P <C> R <C> F1 <R> <C> majority <C> 32.9 ± 0.0 <C> 50.0 ± 0.0 <C> 39.7 ± 0.0 <R> <C> coin-toss <C> 50.4 ± 0.7 <C> 50.5 ± 0.8 <C> 49.1 ± 0.7 <R> <C> [BOLD] Non-Anonymized <C> [BOLD] Non-Anonymized <C> [BOLD] Non-Anonymized <C> [BOLD] Non-Anonymized <R> <C> bow-svm <C> 71.5 ± 0.0 <C> 72.0 ± 0.0 <C> 71.8 ± 0.0 <R> <C> bigru-att <C> 87.1 ± 1.0 <C> 77.2 ± 3.4 <C> 79.5 ± 2.7 <R> <C> han <C> 88.2 ± 0.4 <C> 78.0 ± 0.2 <C> 80.5 ± 0.2 <R> <C> bert <C> 24.0 ± 0.2 <C> 50.0 ± 0.0 <C> 17.0 ± 0.5 <R> <C> hier-bert <C> [BOLD] 90.4 ± 0.3 <C> [BOLD] 79.3 ± 0.9 <C> [BOLD] 82.0 ± 0.9 <R> <C> [BOLD] Anonymized <C> [BOLD] Anonymized <C> [BOLD] Anonymized <C> [BOLD] Anonymized <R> <C> bow-svm <C> 71.6 ± 0.0 <C> 70.5 ± 0.0 <C> 70.9 ± 0.0 <R> <C> bigru-att <C> [BOLD] 87.0 ± 1.0 <C> 76.6 ± 1.9 <C> 78.9 ± 1.9 <R> <C> han <C> 85.2 ± 4.9 <C> [BOLD] 78.3 ± 2.0 <C> [BOLD] 80.2 ± 2.7 <R> <C> bert <C> 17.0 ± 3.0 <C> 50.0 ± 0.0 <C> 25.4 ± 0.4 <R> <C> hier-bert <C> 85.2 ± 0.3 <C> 78.1 ± 1.3 <C> 80.1 ± 1.1 <CAP> Table 2: Macro precision (P), recall (R), F1 for the binary violation prediction task (± std. dev.). <COT> Looking at the "Anonymized" row, we can see that the "bigru-att" model has a higher F1 score than the "bert" model.
<R> <C> Seq. Tagger <C> en <C> de <C> fr <C> ru <C> fa <C> sw <R> <C> brown <C> 81.37 <C> [BOLD] 81.28 <C> 84.81 <C> [BOLD] 79.78 <C> [BOLD] 86.94 <C> 87.35 <R> <C> marlin <C> [BOLD] 81.53 <C> 81.25 <C> [BOLD] 85.4 <C> 79.14 <C> 86.64 <C> [BOLD] 88.81 <R> <C> a-hmm <C> 77.12 <C> 74.85 <C> 81.48 <C> 73.88 <C> 80.25 <C> 76.69 <R> <C> e-kmeans <C> 63.01 <C> 65.14 <C> 68.68 <C> 70.80 <C> 76.94 <C> 65.08 <CAP> Table 1: Comparison of labeling strategies using many-to-one mapping for target languages with available test data, using 500 clusters or number of states. Accuracy is shown in percentage points. <COT> Looking at the "Seq. Tagger" column, we can see that "marlin" has the highest accuracy for the "fr" language.
<R> <C> Seq. Tagger <C> en <C> de <C> fr <C> ru <C> fa <C> sw <R> <C> brown <C> 81.37 <C> [BOLD] 81.28 <C> 84.81 <C> [BOLD] 79.78 <C> [BOLD] 86.94 <C> 87.35 <R> <C> marlin <C> [BOLD] 81.53 <C> 81.25 <C> [BOLD] 85.4 <C> 79.14 <C> 86.64 <C> [BOLD] 88.81 <R> <C> a-hmm <C> 77.12 <C> 74.85 <C> 81.48 <C> 73.88 <C> 80.25 <C> 76.69 <R> <C> e-kmeans <C> 63.01 <C> 65.14 <C> 68.68 <C> 70.80 <C> 76.94 <C> 65.08 <CAP> Table 1: Comparison of labeling strategies using many-to-one mapping for target languages with available test data, using 500 clusters or number of states. Accuracy is shown in percentage points. <COT> Looking at the "ru" language row, we can see that "fa" has the highest accuracy among all the sequence taggers.
<R> <C> CL <C> Parent Language (PL) en <C> Parent Language (PL) de <C> Parent Language (PL) fr <C> Parent Language (PL) it <C> Parent Language (PL) es <C> Parent Language (PL) ja <C> Parent Language (PL) cs <C> Parent Language (PL) ru <C> Parent Language (PL) ar <C> Parent Language (PL) sw <C> cipher-avg <C> PL=CL <R> <C> en <C> - <C> 57.1 <C> [BOLD] 60.4 <C> 59.9 <C> 59.4 <C> 25.1 <C> 52.8 <C> 49.0 <C> 30.7 <C> 28.4 <C> 56.4 <C> 73.9 <R> <C> fr <C> 58.1 <C> 56.0 <C> - <C> 68.6 <C> [BOLD] 71.9 <C> 23.6 <C> 48.3 <C> 47.8 <C> 35.0 <C> 26.7 <C> 58.6 <C> 76.7 <R> <C> fa <C> 13.8 <C> 32.3 <C> 29.7 <C> 22.7 <C> 33.3 <C> 19.7 <C> 33.3 <C> [BOLD] 43.5 <C> 37.0 <C> 38.2 <C> 37.4 <C> 73.3 <R> <C> sw <C> 24.9 <C> 14.3 <C> 37.3 <C> 21.2 <C> 35.9 <C> 21.3 <C> 25.8 <C> 27.9 <C> [BOLD] 38.96 <C> - <C> 37.8 <C> 69.4 <CAP> Table 2: Performance of cipher grounder using brown (|C|=500) as labeler. The best PL for each CL besides itself, is shown in bold. The artificial case where we have CL POS data (PL=CL) is shown for comparison, as is the ultimately used cipher-avg method. <COT> looking at the "cipher-avg" column, finding the highest value in the "Parent Language (PL) fr" row is 71.9.
<R> <C> CL <C> Parent Language (PL) en <C> Parent Language (PL) de <C> Parent Language (PL) fr <C> Parent Language (PL) it <C> Parent Language (PL) es <C> Parent Language (PL) ja <C> Parent Language (PL) cs <C> Parent Language (PL) ru <C> Parent Language (PL) ar <C> Parent Language (PL) sw <C> cipher-avg <C> PL=CL <R> <C> en <C> - <C> 57.1 <C> [BOLD] 60.4 <C> 59.9 <C> 59.4 <C> 25.1 <C> 52.8 <C> 49.0 <C> 30.7 <C> 28.4 <C> 56.4 <C> 73.9 <R> <C> fr <C> 58.1 <C> 56.0 <C> - <C> 68.6 <C> [BOLD] 71.9 <C> 23.6 <C> 48.3 <C> 47.8 <C> 35.0 <C> 26.7 <C> 58.6 <C> 76.7 <R> <C> fa <C> 13.8 <C> 32.3 <C> 29.7 <C> 22.7 <C> 33.3 <C> 19.7 <C> 33.3 <C> [BOLD] 43.5 <C> 37.0 <C> 38.2 <C> 37.4 <C> 73.3 <R> <C> sw <C> 24.9 <C> 14.3 <C> 37.3 <C> 21.2 <C> 35.9 <C> 21.3 <C> 25.8 <C> 27.9 <C> [BOLD] 38.96 <C> - <C> 37.8 <C> 69.4 <CAP> Table 2: Performance of cipher grounder using brown (|C|=500) as labeler. The best PL for each CL besides itself, is shown in bold. The artificial case where we have CL POS data (PL=CL) is shown for comparison, as is the ultimately used cipher-avg method. <COT> looking at the "PL=CL" column, finding the highest value in the "Parent Language (PL) de" row is 60.4.
<R> <C> CL <C> cipher-avg P <C> cipher-avg R <C> cipher-avg F1 <C> Supervised P <C> Supervised R <C> Supervised F1 <R> <C> en <C> 47.70 <C> 64.4 <C> 54.81 <C> 94.04 <C> 90.44 <C> 92.20 <R> <C> fr <C> 56.26 <C> 78.82 <C> 65.65 <C> 96.15 <C> 93.72 <C> 94.92 <R> <C> fa <C> 64.94 <C> 51.23 <C> 57.27 <C> 96.48 <C> 97.77 <C> 97.12 <R> <C> sw <C> 53.46 <C> 51.82 <C> 52.63 <C> 98.88 <C> 97.50 <C> 98.18 <CAP> Table 3: Comparison of performance over the NOUN tag, as measured by precision (P), recall (R), and F1 scores, between our combined cipher grounder (cipher-avg) and a supervised tagger. <COT> Looking at the "Table 3" caption, we can see that the table compares the performance of the combined cipher grounder (cipher-avg) and a supervised tagger over the NOUN tag.
<R> <C> CL <C> cipher-avg P <C> cipher-avg R <C> cipher-avg F1 <C> Supervised P <C> Supervised R <C> Supervised F1 <R> <C> en <C> 47.70 <C> 64.4 <C> 54.81 <C> 94.04 <C> 90.44 <C> 92.20 <R> <C> fr <C> 56.26 <C> 78.82 <C> 65.65 <C> 96.15 <C> 93.72 <C> 94.92 <R> <C> fa <C> 64.94 <C> 51.23 <C> 57.27 <C> 96.48 <C> 97.77 <C> 97.12 <R> <C> sw <C> 53.46 <C> 51.82 <C> 52.63 <C> 98.88 <C> 97.50 <C> 98.18 <CAP> Table 3: Comparison of performance over the NOUN tag, as measured by precision (P), recall (R), and F1 scores, between our combined cipher grounder (cipher-avg) and a supervised tagger. <COT> Looking at the "Supervised F1" column, we can see that the F1 scores for all languages are higher for the supervised tagger compared to the combined cipher grounder (cipher-avg).
<R> <C> Test Tags <C> de UAS <C> de LAS <C> fr UAS <C> fr LAS <C> es UAS <C> es LAS <C> it UAS <C> it LAS <C> pt UAS <C> pt LAS <C> sv UAS <C> sv LAS <R> <C> gold <C> 65.57 <C> 52.37 <C> 71.27 <C> 59.80 <C> 73.26 <C> 63.13 <C> 71.46 <C> 59.66 <C> 63.28 <C> 54.93 <C> 77.50 <C> 64.90 <R> <C> none <C> 40.90 <C> 18.61 <C> 51.14 <C> 30.91 <C> 43.82 <C> 17.67 <C> 48.22 <C> 33.29 <C> 37.89 <C> 16.72 <C> 38.15 <C> 17.96 <R> <C> cipher (this work) <C> 38.31 <C> [BOLD] 24.72 <C> [BOLD] 54.46 <C> [BOLD] 41.04 <C> [BOLD] 55.56 <C> [BOLD] 41.16 <C> [BOLD] 54.05 <C> [BOLD] 39.78 <C> [BOLD] 46.97 <C> [BOLD] 36.07 <C> [BOLD] 55.06 <C> [BOLD] 36.51 <CAP> Table 4: Impact of grounded unsupervised POS tagging on MaLOPa’s ‘zero-resource’ condition. Bold entries indicate an improvement over the baseline condition of having no POS tag information (beyond punctuation) <COT> Looking at the "CAP" tag, we can see that Table 4 describes the impact of grounded unsupervised POS tagging on MaLOPa's 'zero-resource' condition.
<R> <C> Test Tags <C> de UAS <C> de LAS <C> fr UAS <C> fr LAS <C> es UAS <C> es LAS <C> it UAS <C> it LAS <C> pt UAS <C> pt LAS <C> sv UAS <C> sv LAS <R> <C> gold <C> 65.57 <C> 52.37 <C> 71.27 <C> 59.80 <C> 73.26 <C> 63.13 <C> 71.46 <C> 59.66 <C> 63.28 <C> 54.93 <C> 77.50 <C> 64.90 <R> <C> none <C> 40.90 <C> 18.61 <C> 51.14 <C> 30.91 <C> 43.82 <C> 17.67 <C> 48.22 <C> 33.29 <C> 37.89 <C> 16.72 <C> 38.15 <C> 17.96 <R> <C> cipher (this work) <C> 38.31 <C> [BOLD] 24.72 <C> [BOLD] 54.46 <C> [BOLD] 41.04 <C> [BOLD] 55.56 <C> [BOLD] 41.16 <C> [BOLD] 54.05 <C> [BOLD] 39.78 <C> [BOLD] 46.97 <C> [BOLD] 36.07 <C> [BOLD] 55.06 <C> [BOLD] 36.51 <CAP> Table 4: Impact of grounded unsupervised POS tagging on MaLOPa’s ‘zero-resource’ condition. Bold entries indicate an improvement over the baseline condition of having no POS tag information (beyond punctuation) <COT> Looking at the "cipher (this work)" cell, we can see that the UAS and LAS scores for all languages are lower when using the cipher approach compared to the baseline condition.
<R> <C> Embeddings <C> Test Tags <C> de UAS <C> de LAS <C> fr UAS <C> fr LAS <C> es UAS <C> es LAS <C> it UAS <C> it LAS <C> pt UAS <C> pt LAS <C> sv UAS <C> sv LAS <R> <C> guo <C> gold <C> 65.57 <C> 52.37 <C> 71.27 <C> 59.80 <C> 73.26 <C> 63.13 <C> 71.46 <C> 59.66 <C> 63.28 <C> 54.93 <C> [BOLD] 77.50 <C> [BOLD] 64.90 <R> <C> muse <C> gold <C> [BOLD] 66.19 <C> [BOLD] 56.28 <C> [BOLD] 80.86 <C> [BOLD] 72.65 <C> [BOLD] 81.06 <C> [BOLD] 73.62 <C> [BOLD] 82.08 <C> [BOLD] 72.40 <C> [BOLD] 81.17 <C> [BOLD] 76.17 <C> 72.46 <C> 61.71 <R> <C> muse <C> none <C> 57.26 <C> 45.10 <C> [ITALIC] 73.84 <C> [ITALIC] 63.09 <C> [ITALIC] 77.01 <C> [ITALIC] 67.06 <C> 71.36 <C> [ITALIC] 60.48 <C> [ITALIC] 75.31 <C> [ITALIC] 68.36 <C> 60.82 <C> 45.25 <R> <C> muse <C> cipher <C> 48.56 <C> 37.13 <C> 69.94 <C> 59.22 <C> 73.86 <C> 61.68 <C> 69.30 <C> 56.85 <C> 73.41 <C> 65.23 <C> 57.39 <C> 41.49 <CAP> Table 5: Changing to unsupervised muse embeddings boosts MaLOPa’s zero-resource performance significantly (bold entries), in many cases doing so even without any POS tag information (italic entries), however noisy decipherment-based POS tags are no longer helpful. <COT> Looking at the "muse" row, "gold" column, and the "pt UAS" and "pt LAS" columns, we can see that the UAS and LAS scores for the "muse" embeddings with "gold" tags are higher than the UAS and LAS scores for the "muse" embeddings with "none" tags.
<R> <C> overall (all labels) <C> overall (all labels)  [BOLD] P <C> overall (all labels)  [BOLD] R <C> overall (all labels)  [BOLD] F1 <R> <C> bow-svm <C> 56.3 ± 0.0 <C> 45.5 ± 0.0 <C> 50.4 ± 0.0 <R> <C> bigru-att <C> 62.6 ± 1.2 <C> 50.9 ± 1.5 <C> 56.2 ± 1.3 <R> <C> han <C> 65.0 ± 0.4 <C> [BOLD] 55.5 ± 0.7 <C> 59.9 ± 0.5 <R> <C> lwan <C> 62.5 ± 1.0 <C> 53.5 ± 1.1 <C> 57.6 ± 1.0 <R> <C> hier-bert <C> [BOLD] 65.9 ± 1.4 <C> 55.1 ± 3.2 <C> [BOLD] 60.0 ± 1.3 <R> <C> frequent (≥50) <C> frequent (≥50) <C> frequent (≥50) <C> frequent (≥50) <R> <C> bow-svm <C> 56.3 ± 0.0 <C> 45.6 ± 0.0 <C> 50.4 ± 0.0 <R> <C> bigru-att <C> 62.7 ± 1.2 <C> 52.2 ± 1.6 <C> 57.0 ± 1.4 <R> <C> han <C> 65.1 ± 0.3 <C> [BOLD] 57.0 ± 0.8 <C> [BOLD] 60.8 ± 1.3 <R> <C> lwan <C> 62.8 ± 1.2 <C> 54.7 ± 1.2 <C> 58.5 ± 1.0 <R> <C> hier-bert <C> [BOLD] 66.0 ± 1.4 <C> 56.5 ± 3.3 <C> [BOLD] 60.8 ± 1.3 <R> <C> few ([1,50)) <C> few ([1,50)) <C> few ([1,50)) <C> few ([1,50)) <R> <C> bow-svm <C> - <C> - <C> - <R> <C> bigru-att <C> 36.3 ± 13.8 <C> 03.2 ± 23.1 <C> 05.6 ± 03.8 <R> <C> han <C> 30.2 ± 35.1 <C> 01.6 ± 01.2 <C> 02.8 ± 01.9 <R> <C> lwan <C> 24.9 ± 06.3 <C> [BOLD] 07.0 ± 04.1 <C> [BOLD] 10.6 ± 05.2 <R> <C> hier-bert <C> [BOLD] 43.6 ± 14.5 <C> 05.0 ± 02.8 <C> 08.9 ± 04.9 <CAP> Table 3: Micro precision, recall, F1 in multi-label violation for all, frequent, and few training instances. <COT> Looking at the "overall (all labels)" column, we can see that the F1 score for han is higher than for bigru-att.
<R> <C> overall (all labels) <C> overall (all labels)  [BOLD] P <C> overall (all labels)  [BOLD] R <C> overall (all labels)  [BOLD] F1 <R> <C> bow-svm <C> 56.3 ± 0.0 <C> 45.5 ± 0.0 <C> 50.4 ± 0.0 <R> <C> bigru-att <C> 62.6 ± 1.2 <C> 50.9 ± 1.5 <C> 56.2 ± 1.3 <R> <C> han <C> 65.0 ± 0.4 <C> [BOLD] 55.5 ± 0.7 <C> 59.9 ± 0.5 <R> <C> lwan <C> 62.5 ± 1.0 <C> 53.5 ± 1.1 <C> 57.6 ± 1.0 <R> <C> hier-bert <C> [BOLD] 65.9 ± 1.4 <C> 55.1 ± 3.2 <C> [BOLD] 60.0 ± 1.3 <R> <C> frequent (≥50) <C> frequent (≥50) <C> frequent (≥50) <C> frequent (≥50) <R> <C> bow-svm <C> 56.3 ± 0.0 <C> 45.6 ± 0.0 <C> 50.4 ± 0.0 <R> <C> bigru-att <C> 62.7 ± 1.2 <C> 52.2 ± 1.6 <C> 57.0 ± 1.4 <R> <C> han <C> 65.1 ± 0.3 <C> [BOLD] 57.0 ± 0.8 <C> [BOLD] 60.8 ± 1.3 <R> <C> lwan <C> 62.8 ± 1.2 <C> 54.7 ± 1.2 <C> 58.5 ± 1.0 <R> <C> hier-bert <C> [BOLD] 66.0 ± 1.4 <C> 56.5 ± 3.3 <C> [BOLD] 60.8 ± 1.3 <R> <C> few ([1,50)) <C> few ([1,50)) <C> few ([1,50)) <C> few ([1,50)) <R> <C> bow-svm <C> - <C> - <C> - <R> <C> bigru-att <C> 36.3 ± 13.8 <C> 03.2 ± 23.1 <C> 05.6 ± 03.8 <R> <C> han <C> 30.2 ± 35.1 <C> 01.6 ± 01.2 <C> 02.8 ± 01.9 <R> <C> lwan <C> 24.9 ± 06.3 <C> [BOLD] 07.0 ± 04.1 <C> [BOLD] 10.6 ± 05.2 <R> <C> hier-bert <C> [BOLD] 43.6 ± 14.5 <C> 05.0 ± 02.8 <C> 08.9 ± 04.9 <CAP> Table 3: Micro precision, recall, F1 in multi-label violation for all, frequent, and few training instances. <COT> Looking at the "few ([1,50))" row, we can see that the precision for lwan is higher than for bigru-att.
<R> <C> [BOLD] Dataset/Language <C> [BOLD] Dataset/Language <C> [BOLD] Time Period <C> [BOLD] Genre <C> [BOLD] Tokens Train <C> [BOLD] Tokens Dev <C> [BOLD] Tokens Test <R> <C> DEA <C> German (Anselm) <C> 14th–16th c. <C> Religious <C> 233,947 <C> 45,996 <C> 45,999 <R> <C> DER <C> German (RIDGES) <C> 1482–1652 <C> Science <C> 41,857 <C> 9,712 <C> 9,587 <R> <C> EN <C> English <C> 1386–1698 <C> Letters <C> 147,826 <C> 16,334 <C> 17,644 <R> <C> ES <C> Spanish <C> 15th–19th c. <C> Letters <C> 97,320 <C> 11,650 <C> 12,479 <R> <C> HU <C> Hungarian <C> 1440–1541 <C> Religious <C> 134,028 <C> 16,707 <C> 16,779 <R> <C> IS <C> Icelandic <C> 15th c. <C> Religious <C> 49,633 <C> 6,109 <C> 6,037 <R> <C> PT <C> Portuguese <C> 15th–19th c. <C> Letters <C> 222,525 <C> 26,749 <C> 27,078 <R> <C> SLB <C> Slovene (Bohorič) <C> 1750–1840s <C> Diverse <C> 50,023 <C> 5,841 <C> 5,969 <R> <C> SLG <C> Slovene (Gaj) <C> 1840s–1899 <C> Diverse <C> 161,211 <C> 20,878 <C> 21,493 <R> <C> SV <C> Swedish <C> 1527–1812 <C> Diverse <C> 24,458 <C> 2,245 <C> 29,184 <CAP> Table 1: Historical datasets used in the experiments <COT> Looking at the "Genre" column, we can see that the genre of the "DEA" dataset is "Religious".
<R> <C> [BOLD] Dataset/Language <C> [BOLD] Dataset/Language <C> [BOLD] Time Period <C> [BOLD] Genre <C> [BOLD] Tokens Train <C> [BOLD] Tokens Dev <C> [BOLD] Tokens Test <R> <C> DEA <C> German (Anselm) <C> 14th–16th c. <C> Religious <C> 233,947 <C> 45,996 <C> 45,999 <R> <C> DER <C> German (RIDGES) <C> 1482–1652 <C> Science <C> 41,857 <C> 9,712 <C> 9,587 <R> <C> EN <C> English <C> 1386–1698 <C> Letters <C> 147,826 <C> 16,334 <C> 17,644 <R> <C> ES <C> Spanish <C> 15th–19th c. <C> Letters <C> 97,320 <C> 11,650 <C> 12,479 <R> <C> HU <C> Hungarian <C> 1440–1541 <C> Religious <C> 134,028 <C> 16,707 <C> 16,779 <R> <C> IS <C> Icelandic <C> 15th c. <C> Religious <C> 49,633 <C> 6,109 <C> 6,037 <R> <C> PT <C> Portuguese <C> 15th–19th c. <C> Letters <C> 222,525 <C> 26,749 <C> 27,078 <R> <C> SLB <C> Slovene (Bohorič) <C> 1750–1840s <C> Diverse <C> 50,023 <C> 5,841 <C> 5,969 <R> <C> SLG <C> Slovene (Gaj) <C> 1840s–1899 <C> Diverse <C> 161,211 <C> 20,878 <C> 21,493 <R> <C> SV <C> Swedish <C> 1527–1812 <C> Diverse <C> 24,458 <C> 2,245 <C> 29,184 <CAP> Table 1: Historical datasets used in the experiments <COT> Looking at the "Tokens Train" column, we can see that the "ES" dataset has 97,320 tokens in the training set.
<R> <C> [BOLD] Method <C> [BOLD] Dataset DEA <C> [BOLD] Dataset DER <C> [BOLD] Dataset EN <C> [BOLD] Dataset ES <C> [BOLD] Dataset HU <C> [BOLD] Dataset IS <C> [BOLD] Dataset PT <C> [BOLD] Dataset SLB <C> [BOLD] Dataset SLG <C> [BOLD] Dataset SV <R> <C> [ITALIC] Identity <C> [ITALIC] 30.63 <C> [ITALIC] 44.36 <C> [ITALIC] 75.29 <C> [ITALIC] 73.40 <C> [ITALIC] 17.53 <C> [ITALIC] 47.62 <C> [ITALIC] 65.19 <C> [ITALIC] 40.74 <C> [ITALIC] 85.38 <C> [ITALIC] 58.59 <R> <C> [ITALIC] Maximum <C> [ITALIC] 94.64 <C> [ITALIC] 96.46 <C> [ITALIC] 98.57 <C> [ITALIC] 97.40 <C> [ITALIC] 98.70 <C> [ITALIC] 93.46 <C> [ITALIC] 97.65 <C> [ITALIC] 98.71 <C> [ITALIC] 98.96 <C> [ITALIC] 98.97 <R> <C> Norma, Lookup <C> 83.86 <C> 82.15 <C> 92.45 <C> 92.51 <C> 74.58 <C> 82.84 <C> 91.67 <C> 81.76 <C> 93.90 <C> 83.80 <R> <C> Norma, Rule-based <C> 76.48 <C> 82.52 <C> 90.85 <C> 88.59 <C> 78.73 <C> 83.72 <C> 86.33 <C> 86.09 <C> 91.63 <C> 85.23 <R> <C> Norma, Distance-based <C> 58.92 <C> 73.30 <C> 83.92 <C> 84.41 <C> 62.38 <C> 69.95 <C> 77.28 <C> 71.02 <C> 88.20 <C> 76.03 <R> <C> Norma (Combined) <C> 88.02 <C> 86.55 <C> 94.60 <C> 94.41 <C> 86.83 <C> *86.85 <C> 94.19 <C> 89.45 <C> 91.44 <C> 87.12 <R> <C> cSMTiser <C> 88.82 <C> *88.06 <C> *95.21 <C> *95.01 <C> *91.63 <C> *87.10 <C> *95.09 <C> *93.18 <C> *95.99 <C> [BOLD] 91.13 <R> <C> cSMTiser+LM <C> 86.69 <C> *88.19 <C> [BOLD] 95.24 <C> [BOLD] 95.02 <C> [BOLD] 91.70 <C> *86.83 <C> [BOLD] 95.18 <C> [BOLD] 93.30 <C> [BOLD] 96.01 <C> *91.11 <R> <C> NMT (Bollmann,  2018 ) <C> 89.16 <C> *88.07 <C> 94.80 <C> *94.83 <C> 91.17 <C> 86.45 <C> 94.64 <C> 91.61 <C> 95.19 <C> 90.27 <R> <C> NMT (Tang et al.,  2018 ) <C> [BOLD] 89.64 <C> [BOLD] 88.22 <C> 94.95 <C> *94.84 <C> *91.65 <C> [BOLD] 87.31 <C> 94.51 <C> 92.60 <C> *95.85 <C> 90.39 <R> <C> †SMT (Pettersson et al.,  2014 ) <C> – <C> – <C> 94.3– <C> – <C> 80.1– <C> 71.8– <C> – <C> – <C> – <C> 92.9– <R> <C> †NMT (Tang et al.,  2018 ) <C> – <C> – <C> 94.69 <C> – <C> 91.69 <C> 87.59 <C> – <C> – <C> – <C> 91.56 <CAP> Table 2: Word accuracy of different normalization methods on the test sets of the historical datasets, in percent; best result for each dataset in bold; results marked with an asterisk (*) are not significantly different from the best result using McNemar’s test at p<0.05. † indicates scores that were not (re)produced here, but reported in previous work; they might not be strictly comparable due to differences in data preprocessing (cf. Sec. 3). Additionally, Identity shows the accuracy when leaving all word forms unchanged, while Maximum gives the theoretical maximum accuracy with purely token-level methods. <COT> Looking at the "Method" column, we can see that there are different normalization methods mentioned such as Norma, cSMTiser, NMT, etc. 
<R> <C> [BOLD] Method <C> [BOLD] Dataset DEA <C> [BOLD] Dataset DER <C> [BOLD] Dataset EN <C> [BOLD] Dataset ES <C> [BOLD] Dataset HU <C> [BOLD] Dataset IS <C> [BOLD] Dataset PT <C> [BOLD] Dataset SLB <C> [BOLD] Dataset SLG <C> [BOLD] Dataset SV <R> <C> [ITALIC] Identity <C> [ITALIC] 30.63 <C> [ITALIC] 44.36 <C> [ITALIC] 75.29 <C> [ITALIC] 73.40 <C> [ITALIC] 17.53 <C> [ITALIC] 47.62 <C> [ITALIC] 65.19 <C> [ITALIC] 40.74 <C> [ITALIC] 85.38 <C> [ITALIC] 58.59 <R> <C> [ITALIC] Maximum <C> [ITALIC] 94.64 <C> [ITALIC] 96.46 <C> [ITALIC] 98.57 <C> [ITALIC] 97.40 <C> [ITALIC] 98.70 <C> [ITALIC] 93.46 <C> [ITALIC] 97.65 <C> [ITALIC] 98.71 <C> [ITALIC] 98.96 <C> [ITALIC] 98.97 <R> <C> Norma, Lookup <C> 83.86 <C> 82.15 <C> 92.45 <C> 92.51 <C> 74.58 <C> 82.84 <C> 91.67 <C> 81.76 <C> 93.90 <C> 83.80 <R> <C> Norma, Rule-based <C> 76.48 <C> 82.52 <C> 90.85 <C> 88.59 <C> 78.73 <C> 83.72 <C> 86.33 <C> 86.09 <C> 91.63 <C> 85.23 <R> <C> Norma, Distance-based <C> 58.92 <C> 73.30 <C> 83.92 <C> 84.41 <C> 62.38 <C> 69.95 <C> 77.28 <C> 71.02 <C> 88.20 <C> 76.03 <R> <C> Norma (Combined) <C> 88.02 <C> 86.55 <C> 94.60 <C> 94.41 <C> 86.83 <C> *86.85 <C> 94.19 <C> 89.45 <C> 91.44 <C> 87.12 <R> <C> cSMTiser <C> 88.82 <C> *88.06 <C> *95.21 <C> *95.01 <C> *91.63 <C> *87.10 <C> *95.09 <C> *93.18 <C> *95.99 <C> [BOLD] 91.13 <R> <C> cSMTiser+LM <C> 86.69 <C> *88.19 <C> [BOLD] 95.24 <C> [BOLD] 95.02 <C> [BOLD] 91.70 <C> *86.83 <C> [BOLD] 95.18 <C> [BOLD] 93.30 <C> [BOLD] 96.01 <C> *91.11 <R> <C> NMT (Bollmann,  2018 ) <C> 89.16 <C> *88.07 <C> 94.80 <C> *94.83 <C> 91.17 <C> 86.45 <C> 94.64 <C> 91.61 <C> 95.19 <C> 90.27 <R> <C> NMT (Tang et al.,  2018 ) <C> [BOLD] 89.64 <C> [BOLD] 88.22 <C> 94.95 <C> *94.84 <C> *91.65 <C> [BOLD] 87.31 <C> 94.51 <C> 92.60 <C> *95.85 <C> 90.39 <R> <C> †SMT (Pettersson et al.,  2014 ) <C> – <C> – <C> 94.3– <C> – <C> 80.1– <C> 71.8– <C> – <C> – <C> – <C> 92.9– <R> <C> †NMT (Tang et al.,  2018 ) <C> – <C> – <C> 94.69 <C> – <C> 91.69 <C> 87.59 <C> – <C> – <C> – <C> 91.56 <CAP> Table 2: Word accuracy of different normalization methods on the test sets of the historical datasets, in percent; best result for each dataset in bold; results marked with an asterisk (*) are not significantly different from the best result using McNemar’s test at p<0.05. † indicates scores that were not (re)produced here, but reported in previous work; they might not be strictly comparable due to differences in data preprocessing (cf. Sec. 3). Additionally, Identity shows the accuracy when leaving all word forms unchanged, while Maximum gives the theoretical maximum accuracy with purely token-level methods. <COT> Looking at the "Dataset PT" column, we can see that the highest word accuracy is achieved by the normalization method cSMTiser with a score of 95.09.
<R> <C> [EMPTY] <C> MAE <C> spearman’s  [ITALIC] ρ <R> <C> majority <C> [BOLD] .369 ± .000 <C> [ITALIC] N/ [ITALIC] A* <R> <C> bow-svr <C> .585 ± .000 <C> .370 ± .000 <R> <C> bigru-att <C> .539 ± .073 <C> .459 ± .034 <R> <C> han <C> .524 ± .049 <C> .437 ± .018 <R> <C> hier-bert <C> .437 ± .018 <C> [BOLD] .527 ± .024 <CAP> Table 4: Mean Absolute Error and Spearman’s ρ for case importance. Importance ranges from 1 (most important) to 4 (least). * Not Applicable. <COT> Looking at the "MAE" column, we can see that the "hier-bert" model has the lowest MAE value of .437 ± .018.
<R> <C> [EMPTY] <C> MAE <C> spearman’s  [ITALIC] ρ <R> <C> majority <C> [BOLD] .369 ± .000 <C> [ITALIC] N/ [ITALIC] A* <R> <C> bow-svr <C> .585 ± .000 <C> .370 ± .000 <R> <C> bigru-att <C> .539 ± .073 <C> .459 ± .034 <R> <C> han <C> .524 ± .049 <C> .437 ± .018 <R> <C> hier-bert <C> .437 ± .018 <C> [BOLD] .527 ± .024 <CAP> Table 4: Mean Absolute Error and Spearman’s ρ for case importance. Importance ranges from 1 (most important) to 4 (least). * Not Applicable. <COT> Looking at the "spearman’s ρ" column, we can see that the "hier-bert" model has the highest ρ value of .527 ± .024.
<R> <C> [BOLD] Model <C> [BOLD] 2015 <C> [BOLD] 2017 <R> <C> Ours <C> [EMPTY] <C> [EMPTY] <R> <C> local edge + projective decoder <C> 70.2 \scalebox 0.8$±0.3$ <C> [BOLD] 71.0 ±0.5 <R> <C> local edge + fixed-tree decoder <C> 69.4 \scalebox 0.8$±0.6$ <C> 70.2 \scalebox 0.8$±0.5$ <R> <C> K&G edge + projective decoder <C> 68.6 \scalebox 0.8$±0.7$ <C> 69.4 \scalebox 0.8$±0.4$ <R> <C> K&G edge + fixed-tree decoder <C> 69.6 \scalebox 0.8$±0.4$ <C> 69.9 \scalebox 0.8$±0.2$ <R> <C> Baselines <C> [EMPTY] <C> [EMPTY] <R> <C> fixed-tree (type-unaware) <C> 26.0 \scalebox 0.8$±0.6$ <C> 27.9 \scalebox 0.8$±0.6$ <R> <C> JAMR-style <C> 66.1 <C> 66.2 <R> <C> Previous work <C> [EMPTY] <C> [EMPTY] <R> <C> CAMR Wang et al. ( 2015 ) <C> 66.5 <C> - <R> <C> JAMR Flanigan et al. ( 2016 ) <C> 67 <C> - <R> <C> E17-1051 <C> 64 <C> - <R> <C> van2017neural <C> 68.5 <C> [BOLD] 71.0 <R> <C> foland2017abstract <C> [BOLD] 70.7 <C> - <R> <C> buys2017oxford <C> - <C> 61.9 <CAP> Table 1: 2015 & 2017 test set Smatch scores <COT> Looking at the "Model" column, we can see that the "Ours" model has no scores listed for both 2015 and 2017.
<R> <C> [BOLD] Model <C> [BOLD] 2015 <C> [BOLD] 2017 <R> <C> Ours <C> [EMPTY] <C> [EMPTY] <R> <C> local edge + projective decoder <C> 70.2 \scalebox 0.8$±0.3$ <C> [BOLD] 71.0 ±0.5 <R> <C> local edge + fixed-tree decoder <C> 69.4 \scalebox 0.8$±0.6$ <C> 70.2 \scalebox 0.8$±0.5$ <R> <C> K&G edge + projective decoder <C> 68.6 \scalebox 0.8$±0.7$ <C> 69.4 \scalebox 0.8$±0.4$ <R> <C> K&G edge + fixed-tree decoder <C> 69.6 \scalebox 0.8$±0.4$ <C> 69.9 \scalebox 0.8$±0.2$ <R> <C> Baselines <C> [EMPTY] <C> [EMPTY] <R> <C> fixed-tree (type-unaware) <C> 26.0 \scalebox 0.8$±0.6$ <C> 27.9 \scalebox 0.8$±0.6$ <R> <C> JAMR-style <C> 66.1 <C> 66.2 <R> <C> Previous work <C> [EMPTY] <C> [EMPTY] <R> <C> CAMR Wang et al. ( 2015 ) <C> 66.5 <C> - <R> <C> JAMR Flanigan et al. ( 2016 ) <C> 67 <C> - <R> <C> E17-1051 <C> 64 <C> - <R> <C> van2017neural <C> 68.5 <C> [BOLD] 71.0 <R> <C> foland2017abstract <C> [BOLD] 70.7 <C> - <R> <C> buys2017oxford <C> - <C> 61.9 <CAP> Table 1: 2015 & 2017 test set Smatch scores <COT> Looking at the "Model" column and the "2017" column, we can see that the "foland2017abstract" model has a Smatch score of 70.7 for the 2017 test set.
<R> <C> [BOLD] Metric <C> [BOLD] 2015 W’15 <C> [BOLD] 2015 F’16 <C> [BOLD] 2015 D’17 <C> [BOLD] 2015 PD <C> [BOLD] 2015 FTD <C> [BOLD] 2017 vN’17 <C> [BOLD] 2017 PD <C> [BOLD] 2017 FTD <R> <C> Smatch <C> 67 <C> 67 <C> 64 <C> [BOLD] 70 <C> [BOLD] 70 <C> [BOLD] 71 <C> [BOLD] 71 <C> 70 <R> <C> Unlabeled <C> 69 <C> 69 <C> 69 <C> [BOLD] 73 <C> [BOLD] 73 <C> [BOLD] 74 <C> [BOLD] 74 <C> [BOLD] 74 <R> <C> No WSD <C> 64 <C> 68 <C> 65 <C> [BOLD] 71 <C> 70 <C> [BOLD] 72 <C> [BOLD] 72 <C> 70 <R> <C> Named Ent. <C> 75 <C> 79 <C> [BOLD] 83 <C> 79 <C> 78 <C> [BOLD] 79 <C> 78 <C> 77 <R> <C> Wikification <C> 0 <C> [BOLD] 75 <C> 64 <C> 71 <C> 72 <C> 65 <C> [BOLD] 71 <C> [BOLD] 71 <R> <C> Negations <C> 18 <C> 45 <C> 48 <C> [BOLD] 52 <C> [BOLD] 52 <C> [BOLD] 62 <C> 57 <C> 55 <R> <C> Concepts <C> 80 <C> 83 <C> 83 <C> 83 <C> [BOLD] 84 <C> 82 <C> [BOLD] 84 <C> [BOLD] 84 <R> <C> Reentrancies <C> 41 <C> 42 <C> 41 <C> [BOLD] 46 <C> 44 <C> [BOLD] 52 <C> 49 <C> 46 <R> <C> SRL <C> 60 <C> 60 <C> 56 <C> [BOLD] 63 <C> 61 <C> [BOLD] 66 <C> 64 <C> 62 <CAP> Table 2: Details for the LDC2015E86 and LDC2017T10 test sets <COT> Looking at the "Metric" column, we can see that the highest value for each metric is marked as [BOLD].
<R> <C> [BOLD] Metric <C> [BOLD] 2015 W’15 <C> [BOLD] 2015 F’16 <C> [BOLD] 2015 D’17 <C> [BOLD] 2015 PD <C> [BOLD] 2015 FTD <C> [BOLD] 2017 vN’17 <C> [BOLD] 2017 PD <C> [BOLD] 2017 FTD <R> <C> Smatch <C> 67 <C> 67 <C> 64 <C> [BOLD] 70 <C> [BOLD] 70 <C> [BOLD] 71 <C> [BOLD] 71 <C> 70 <R> <C> Unlabeled <C> 69 <C> 69 <C> 69 <C> [BOLD] 73 <C> [BOLD] 73 <C> [BOLD] 74 <C> [BOLD] 74 <C> [BOLD] 74 <R> <C> No WSD <C> 64 <C> 68 <C> 65 <C> [BOLD] 71 <C> 70 <C> [BOLD] 72 <C> [BOLD] 72 <C> 70 <R> <C> Named Ent. <C> 75 <C> 79 <C> [BOLD] 83 <C> 79 <C> 78 <C> [BOLD] 79 <C> 78 <C> 77 <R> <C> Wikification <C> 0 <C> [BOLD] 75 <C> 64 <C> 71 <C> 72 <C> 65 <C> [BOLD] 71 <C> [BOLD] 71 <R> <C> Negations <C> 18 <C> 45 <C> 48 <C> [BOLD] 52 <C> [BOLD] 52 <C> [BOLD] 62 <C> 57 <C> 55 <R> <C> Concepts <C> 80 <C> 83 <C> 83 <C> 83 <C> [BOLD] 84 <C> 82 <C> [BOLD] 84 <C> [BOLD] 84 <R> <C> Reentrancies <C> 41 <C> 42 <C> 41 <C> [BOLD] 46 <C> 44 <C> [BOLD] 52 <C> 49 <C> 46 <R> <C> SRL <C> 60 <C> 60 <C> 56 <C> [BOLD] 63 <C> 61 <C> [BOLD] 66 <C> 64 <C> 62 <CAP> Table 2: Details for the LDC2015E86 and LDC2017T10 test sets <COT> Looking at the "2015 PD" cell, we can see that the value is higher than the corresponding values in the "2015 W'15", "2015 F'16", and "2015 D'17" cells.
<R> <C> System <C> Dev <C> Test <R> <C> TSP-gen <C> 21.12 <C> 22.44 <R> <C> JAMR-gen <C> 23.00 <C> 23.00 <R> <C> All <C> [BOLD] 25.24 <C> [BOLD] 25.62 <R> <C> NoInducedRule <C> 16.75 <C> 17.43 <R> <C> NoConceptRule <C> 23.99 <C> 24.86 <R> <C> NoMovingDistance <C> 23.48 <C> 24.06 <R> <C> NoReorderModel <C> 25.09 <C> 25.43 <CAP> Table 2: Main results. <COT> Looking at the "System" column, we can see that there are different systems mentioned in the table.
<R> <C> System <C> Dev <C> Test <R> <C> TSP-gen <C> 21.12 <C> 22.44 <R> <C> JAMR-gen <C> 23.00 <C> 23.00 <R> <C> All <C> [BOLD] 25.24 <C> [BOLD] 25.62 <R> <C> NoInducedRule <C> 16.75 <C> 17.43 <R> <C> NoConceptRule <C> 23.99 <C> 24.86 <R> <C> NoMovingDistance <C> 23.48 <C> 24.06 <R> <C> NoReorderModel <C> 25.09 <C> 25.43 <CAP> Table 2: Main results. <COT> Looking at the "Dev" column, we can compare the performance of different systems on the development set.
<R> <C> [EMPTY] <C> Glue <C> Nonterminal <C> Terminal <R> <C> 1-best <C> 30.0% <C> 30.1% <C> 39.9% <CAP> Table 3: Rules used for decoding. <COT> Looking at the "CAP" tag, we can see that Table 3 provides information about the rules used for decoding.
<R> <C> [EMPTY] <C> Glue <C> Nonterminal <C> Terminal <R> <C> 1-best <C> 30.0% <C> 30.1% <C> 39.9% <CAP> Table 3: Rules used for decoding. <COT> Looking at the "Terminal" column, we can see that the percentage for the "1-best" decoding is 39.9%.
<R> <C> [EMPTY] <C> happy <C> angry <C> sad <C> others <C> size <R> <C> Train <C> 14.07% <C> 18.26% <C> 18.11% <C> 49.56% <C> 30160 <R> <C> Dev <C> 5.15% <C> 5.44% <C> 4.54% <C> 84.86% <C> 2755 <R> <C> Test <C> 4.28% <C> 5.57% <C> 4.45% <C> 85.70% <C> 5509 <CAP> Table 2: Label distribution of train, dev, and test set <COT> Looking at the "Train" row, we can see that the majority of the data in the training set is labeled as "others" with a percentage of 49.56%. 
<R> <C> [EMPTY] <C> happy <C> angry <C> sad <C> others <C> size <R> <C> Train <C> 14.07% <C> 18.26% <C> 18.11% <C> 49.56% <C> 30160 <R> <C> Dev <C> 5.15% <C> 5.44% <C> 4.54% <C> 84.86% <C> 2755 <R> <C> Test <C> 4.28% <C> 5.57% <C> 4.45% <C> 85.70% <C> 5509 <CAP> Table 2: Label distribution of train, dev, and test set <COT> Looking at the "Test" row, we can see that the majority of the data in the test set is labeled as "others" with a percentage of 85.70%. 
<R> <C> [EMPTY] <C> F1 <C> Happy <C> Angry <C> Sad <C> Harm. Mean <R> <C> SL <C> Dev <C> 0.6430 <C> 0.7530 <C> 0.7180 <C> 0.7016 <R> <C> SL <C> Test <C> 0.6400 <C> 0.7190 <C> 0.7300 <C> 0.6939 <R> <C> SLD <C> Dev <C> 0.6470 <C> 0.7610 <C> 0.7360 <C> 0.7112 <R> <C> SLD <C> Test <C> 0.6350 <C> 0.7180 <C> 0.7360 <C> 0.6934 <R> <C> HRLCE <C> Dev <C> 0.7460 <C> 0.7590 <C> 0.8100 <C> [BOLD] 0.7706 <R> <C> HRLCE <C> Test <C> 0.7220 <C> 0.766 <C> 0.8180 <C> [BOLD] 0.7666 <R> <C> BERT <C> Dev <C> 0.7138 <C> 0.7736 <C> 0.8106 <C> 0.7638 <R> <C> BERT <C> Test <C> 0.7151 <C> 0.7654 <C> 0.8157 <C> 0.7631 <CAP> Table 1: Macro-F1 scores and its harmonic means of the four models <COT> Looking at the "HRLCE" row, we can see that the harmonic mean of the F1 scores for the HRLCE model is higher on the Dev set compared to the Test set.
<R> <C> [EMPTY] <C> F1 <C> Happy <C> Angry <C> Sad <C> Harm. Mean <R> <C> SL <C> Dev <C> 0.6430 <C> 0.7530 <C> 0.7180 <C> 0.7016 <R> <C> SL <C> Test <C> 0.6400 <C> 0.7190 <C> 0.7300 <C> 0.6939 <R> <C> SLD <C> Dev <C> 0.6470 <C> 0.7610 <C> 0.7360 <C> 0.7112 <R> <C> SLD <C> Test <C> 0.6350 <C> 0.7180 <C> 0.7360 <C> 0.6934 <R> <C> HRLCE <C> Dev <C> 0.7460 <C> 0.7590 <C> 0.8100 <C> [BOLD] 0.7706 <R> <C> HRLCE <C> Test <C> 0.7220 <C> 0.766 <C> 0.8180 <C> [BOLD] 0.7666 <R> <C> BERT <C> Dev <C> 0.7138 <C> 0.7736 <C> 0.8106 <C> 0.7638 <R> <C> BERT <C> Test <C> 0.7151 <C> 0.7654 <C> 0.8157 <C> 0.7631 <CAP> Table 1: Macro-F1 scores and its harmonic means of the four models <COT> Looking at the "Sad" column, we can see that the F1 score for the Sad emotion is higher for the HRLCE model compared to the BERT model on both the Dev and Test sets.
<R> <C> Aligner <C> Alignment F1 <C> Oracle’s Smatch <R> <C> [EMPTY] <C> (on hand-align) <C> (on dev. dataset) <R> <C> JAMR <C> 90.6 <C> 91.7 <R> <C> Our <C> 95.2 <C> 94.7 <CAP> Table 3: The intrinsic evaluation results. <COT> Looking at the "Aligner" column, "JAMR" cell, and "Alignment F1" cell, we can see that JAMR aligner achieves an Alignment F1 score of 90.6.
<R> <C> Aligner <C> Alignment F1 <C> Oracle’s Smatch <R> <C> [EMPTY] <C> (on hand-align) <C> (on dev. dataset) <R> <C> JAMR <C> 90.6 <C> 91.7 <R> <C> Our <C> 95.2 <C> 94.7 <CAP> Table 3: The intrinsic evaluation results. <COT> Looking at the "Aligner" column, "Our" cell, and "Alignment F1" cell, we can see that our aligner achieves an Alignment F1 score of 95.2.
<R> <C> model <C> newswire <C> all <R> <C> JAMR parser: Word, POS, NER, DEP <C> JAMR parser: Word, POS, NER, DEP <C> [EMPTY] <R> <C> + JAMR aligner <C> 71.3 <C> 65.9 <R> <C> + Our aligner <C> 73.1 <C> 67.6 <R> <C> CAMR parser: Word, POS, NER, DEP <C> CAMR parser: Word, POS, NER, DEP <C> [EMPTY] <R> <C> + JAMR aligner <C> 68.4 <C> 64.6 <R> <C> + Our aligner <C> 68.8 <C> 65.1 <CAP> Table 4: The parsing results. <COT> Looking at the table, we can see that there are two different parsers mentioned: JAMR parser and CAMR parser. The table also shows the parsing results for these parsers with and without aligner.
<R> <C> model <C> newswire <C> all <R> <C> JAMR parser: Word, POS, NER, DEP <C> JAMR parser: Word, POS, NER, DEP <C> [EMPTY] <R> <C> + JAMR aligner <C> 71.3 <C> 65.9 <R> <C> + Our aligner <C> 73.1 <C> 67.6 <R> <C> CAMR parser: Word, POS, NER, DEP <C> CAMR parser: Word, POS, NER, DEP <C> [EMPTY] <R> <C> + JAMR aligner <C> 68.4 <C> 64.6 <R> <C> + Our aligner <C> 68.8 <C> 65.1 <CAP> Table 4: The parsing results. <COT> Looking at the table, we can see that the parsing results for JAMR parser with our aligner are higher than the parsing results for JAMR parser with the JAMR aligner.
<R> <C> model <C> newswire <C> all <R> <C> Our single parser: Word only <C> Our single parser: Word only <C> [EMPTY] <R> <C> + JAMR aligner <C> 68.6 <C> 63.9 <R> <C> + Our aligner <C> 69.3 <C> 64.7 <R> <C> Our single parser: Word, POS <C> Our single parser: Word, POS <C> [EMPTY] <R> <C> + JAMR aligner <C> 68.8 <C> 64.6 <R> <C> + Our aligner <C> 69.8 <C> 65.2 <R> <C> Our ensemble: Word only + Our aligner <C> Our ensemble: Word only + Our aligner <C> [EMPTY] <R> <C> x3 <C> 71.9 <C> 67.4 <R> <C> x10 <C> 72.5 <C> 68.1 <R> <C> Our ensemble: Word, POS + Our aligner <C> Our ensemble: Word, POS + Our aligner <C> [EMPTY] <R> <C> x3 <C> 72.5 <C> 67.7 <R> <C> x10 <C> 73.3 <C> [BOLD] 68.4 <R> <C> BA17: Word only† <C> 68 <C> 63 <R> <C> + POS <C> 68 <C> 63 <R> <C> + POS, DEP <C> 69 <C> 64 <R> <C> Damonte et al. ( 2017 )‡ <C> - <C> 66 <R> <C> Artzi et al. ( 2015 ) <C> 66.3 <C> - <R> <C> Wang et al. ( 2015a ) <C> 70 <C> 66 <R> <C> Pust et al. ( 2015 ) <C> - <C> 67.1 <R> <C> Zhou et al. ( 2016 ) <C> 71 <C> 66 <R> <C> Goodman et al. ( 2016 ) <C> 70 <C> - <R> <C> Wang and Xue ( 2017 ) <C> - <C> 68.1 <CAP> Table 6: The parsing results. xn denotes the ensemble of n differently initialized parsers. The difference in rounding is due to previous works report differently rounded results. † BA17 represents the result of Ballesteros and Al-Onaizan (2017), ‡ Damonte et al. (2017)’s result is drawn from Ballesteros and Al-Onaizan (2017). <COT> Looking at the "Table 6" caption, we can see that the table presents the parsing results of different models.
<R> <C> model <C> newswire <C> all <R> <C> Our single parser: Word only <C> Our single parser: Word only <C> [EMPTY] <R> <C> + JAMR aligner <C> 68.6 <C> 63.9 <R> <C> + Our aligner <C> 69.3 <C> 64.7 <R> <C> Our single parser: Word, POS <C> Our single parser: Word, POS <C> [EMPTY] <R> <C> + JAMR aligner <C> 68.8 <C> 64.6 <R> <C> + Our aligner <C> 69.8 <C> 65.2 <R> <C> Our ensemble: Word only + Our aligner <C> Our ensemble: Word only + Our aligner <C> [EMPTY] <R> <C> x3 <C> 71.9 <C> 67.4 <R> <C> x10 <C> 72.5 <C> 68.1 <R> <C> Our ensemble: Word, POS + Our aligner <C> Our ensemble: Word, POS + Our aligner <C> [EMPTY] <R> <C> x3 <C> 72.5 <C> 67.7 <R> <C> x10 <C> 73.3 <C> [BOLD] 68.4 <R> <C> BA17: Word only† <C> 68 <C> 63 <R> <C> + POS <C> 68 <C> 63 <R> <C> + POS, DEP <C> 69 <C> 64 <R> <C> Damonte et al. ( 2017 )‡ <C> - <C> 66 <R> <C> Artzi et al. ( 2015 ) <C> 66.3 <C> - <R> <C> Wang et al. ( 2015a ) <C> 70 <C> 66 <R> <C> Pust et al. ( 2015 ) <C> - <C> 67.1 <R> <C> Zhou et al. ( 2016 ) <C> 71 <C> 66 <R> <C> Goodman et al. ( 2016 ) <C> 70 <C> - <R> <C> Wang and Xue ( 2017 ) <C> - <C> 68.1 <CAP> Table 6: The parsing results. xn denotes the ensemble of n differently initialized parsers. The difference in rounding is due to previous works report differently rounded results. † BA17 represents the result of Ballesteros and Al-Onaizan (2017), ‡ Damonte et al. (2017)’s result is drawn from Ballesteros and Al-Onaizan (2017). <COT> Looking at the "Our ensemble: Word, POS + Our aligner" row, the "[BOLD] 68.4" cell, and the "all" column, we can see that the parsing result for the ensemble of Word, POS, and Our aligner is 68.4.
<R> <C> [BOLD] Model <C> [BOLD] ALL <C> [BOLD] SEQ <C> [BOLD] POS1 <C> [BOLD] POS2 <C> [BOLD] POS3 <R> <C> FP * <C> 34.1 <C> 7.2 <C> 52.6 <C> 25.6 <C> 25.9 <R> <C> NP * <C> 39.4 <C> 10.8 <C> 58.9 <C> 35.9 <C> 24.6 <R> <C> DynSp <C> 42.0 <C> 10.2 <C> [BOLD] 70.9 <C> 35.8 <C> 20.1 <R> <C> FP † * <C> 33.2 <C> 7.7 <C> 51.4 <C> 22.2 <C> 22.3 <R> <C> NP † * <C> 40.2 <C> 11.8 <C> 60.0 <C> 35.9 <C> 25.5 <R> <C> DynSp † <C> 44.7 <C> 12.8 <C> 70.4 <C> 41.1 <C> 23.6 <R> <C> Camp † * <C> 45.6 <C> 13.2 <C> 70.3 <C> 42.6 <C> 24.8 <R> <C> Ours * <C> 45.1 <C> 13.3 <C> 67.2 <C> 42.4 <C> 26.4 <R> <C> Ours † * <C> [BOLD] 55.1 <C> [BOLD] 28.1 <C> 67.2 <C> [BOLD] 52.7 <C> [BOLD] 46.8 <R> <C> Ours † * (RA) <C> 61.7 <C> 28.1 <C> 67.2 <C> 60.1 <C> 57.7 <CAP> Table 1: SQA test results. † marks contextual models using the previous question or the answer to the previous question. * marks the models that use the table content. RA denotes an oracle model that has access to the previous reference answer at test time. ALL is the average question accuracy, SEQ the sequence accuracy, and POS X, the accuracy of the X’th question in a sequence. <COT> Looking at the "DynSp" row, we can see that the accuracy for the "DynSp" model is higher when using the previous question or the answer to the previous question (marked with †) compared to not using them.
<R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Location Eq. 5 <C> [BOLD] Location Eq. 6 <C> [BOLD] Location Eq. 7 <C> [BOLD] Person Eq. 5 <C> [BOLD] Person Eq. 6 <C> [BOLD] Person Eq. 7 <C> [BOLD] Gene Eq. 5 <C> [BOLD] Gene Eq. 6 <C> [BOLD] Gene Eq. 7 <C> [BOLD] Protein Eq. 5 <C> [BOLD] Protein Eq. 6 <C> [BOLD] Protein Eq. 7 <C> [BOLD] Cell Type Eq. 5 <C> [BOLD] Cell Type Eq. 6 <C> [BOLD] Cell Type Eq. 7 <C> [BOLD] Virus Eq. 5 <C> [BOLD] Virus Eq. 6 <C> [BOLD] Virus Eq. 7 <R> <C> [BOLD] Seed 1 <C> Eq. 8 <C> 0.37 <C> 0.40 <C> 0.50 <C> 0.23 <C> 0.23 <C> 0.30 <C> 0.00 <C> 0.03 <C> 0.13 <C> 0.17 <C> 0.23 <C> 0.20 <C> 0.27 <C> 0.50 <C> 0.53 <C> 0.20 <C> 0.13 <C> 0.17 <R> <C> [EMPTY] <C> Eq. 9 <C> 0.63 <C> [BOLD] 0.73 <C> 0.73 <C> 0.03 <C> [BOLD] 0.17 <C> 0.20 <C> 0.03 <C> [BOLD] 0.07 <C> 0.07 <C> 0.43 <C> [BOLD] 0.43 <C> 0.53 <C> 0.17 <C> [BOLD] 0.23 <C> 0.23 <C> 0.07 <C> [BOLD] 0.10 <C> 0.07 <R> <C> [BOLD] Seed 2 <C> Eq. 8 <C> 0.33 <C> 0.33 <C> 0.57 <C> 0.53 <C> 0.40 <C> 0.30 <C> 0.63 <C> 0.63 <C> 0.63 <C> 0.17 <C> 0.60 <C> 0.27 <C> 0.10 <C> 0.20 <C> 0.13 <C> 0.07 <C> 0.03 <C> 0.03 <R> <C> [EMPTY] <C> Eq. 9 <C> 0.57 <C> [BOLD] 0.70 <C> 0.63 <C> 0.47 <C> [BOLD] 0.37 <C> 0.37 <C> 0.60 <C> [BOLD] 0.57 <C> 0.60 <C> 0.07 <C> [BOLD] 0.30 <C> 0.30 <C> 0.07 <C> [BOLD] 0.07 <C> 0.07 <C> 0.03 <C> [BOLD] 0.10 <C> 0.03 <CAP> Table 2: ESE performance (p@k). Best performing combination is bold faced. <COT> Looking at the "Location Eq. 5" column and "Seed 1" row, the value is 0.23.
<R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Location Eq. 5 <C> [BOLD] Location Eq. 6 <C> [BOLD] Location Eq. 7 <C> [BOLD] Person Eq. 5 <C> [BOLD] Person Eq. 6 <C> [BOLD] Person Eq. 7 <C> [BOLD] Gene Eq. 5 <C> [BOLD] Gene Eq. 6 <C> [BOLD] Gene Eq. 7 <C> [BOLD] Protein Eq. 5 <C> [BOLD] Protein Eq. 6 <C> [BOLD] Protein Eq. 7 <C> [BOLD] Cell Type Eq. 5 <C> [BOLD] Cell Type Eq. 6 <C> [BOLD] Cell Type Eq. 7 <C> [BOLD] Virus Eq. 5 <C> [BOLD] Virus Eq. 6 <C> [BOLD] Virus Eq. 7 <R> <C> [BOLD] Seed 1 <C> Eq. 8 <C> 0.37 <C> 0.40 <C> 0.50 <C> 0.23 <C> 0.23 <C> 0.30 <C> 0.00 <C> 0.03 <C> 0.13 <C> 0.17 <C> 0.23 <C> 0.20 <C> 0.27 <C> 0.50 <C> 0.53 <C> 0.20 <C> 0.13 <C> 0.17 <R> <C> [EMPTY] <C> Eq. 9 <C> 0.63 <C> [BOLD] 0.73 <C> 0.73 <C> 0.03 <C> [BOLD] 0.17 <C> 0.20 <C> 0.03 <C> [BOLD] 0.07 <C> 0.07 <C> 0.43 <C> [BOLD] 0.43 <C> 0.53 <C> 0.17 <C> [BOLD] 0.23 <C> 0.23 <C> 0.07 <C> [BOLD] 0.10 <C> 0.07 <R> <C> [BOLD] Seed 2 <C> Eq. 8 <C> 0.33 <C> 0.33 <C> 0.57 <C> 0.53 <C> 0.40 <C> 0.30 <C> 0.63 <C> 0.63 <C> 0.63 <C> 0.17 <C> 0.60 <C> 0.27 <C> 0.10 <C> 0.20 <C> 0.13 <C> 0.07 <C> 0.03 <C> 0.03 <R> <C> [EMPTY] <C> Eq. 9 <C> 0.57 <C> [BOLD] 0.70 <C> 0.63 <C> 0.47 <C> [BOLD] 0.37 <C> 0.37 <C> 0.60 <C> [BOLD] 0.57 <C> 0.60 <C> 0.07 <C> [BOLD] 0.30 <C> 0.30 <C> 0.07 <C> [BOLD] 0.07 <C> 0.07 <C> 0.03 <C> [BOLD] 0.10 <C> 0.03 <CAP> Table 2: ESE performance (p@k). Best performing combination is bold faced. <COT> Looking at the "Person Eq. 7" column and "Seed 2" row, the value is 0.60.
<R> <C> [BOLD] Dataset Name <C> [BOLD] Entity Class <C> [BOLD] EAL @ 1.0 F <C> [BOLD] EAL @ 1.0 F <C> [BOLD] EAA Annotation Mode  [BOLD] FA <C> [BOLD] EAA Annotation Mode  [BOLD] HFA <C> [BOLD] EAA Annotation Mode  [BOLD] UFA <R> <C> [BOLD] Dataset Name <C> [BOLD] Entity Class <C> [ITALIC] σ <C> % cut <C> [BOLD] F-Score (percentage cut) <C> [BOLD] F-Score (percentage cut) <C> [BOLD] F-Score (percentage cut) <R> <C> CoNLL-2003 <C> Location <C> 0.97 <C> 55% <C> 0.99 (46%) <C> 0.93 (83%) <C> 0.82 (91%) <R> <C> CoNLL-2003 <C> Person <C> 0.97 <C> 59% <C> 0.99 (48%) <C> 0.95 (81%) <C> 0.85 (90%) <R> <C> BioCreAtIvE II <C> Gene <C> 0.94 <C> 35% <C> 1.00 (35%) <C> 0.96 (50%) <C> 0.89 (69%) <R> <C> GENIA 3.02 <C> Protein Molecule <C> 0.99 <C> 33% <C> 0.98 (36%) <C> 0.87 (71%) <C> 0.74 (85%) <R> <C> GENIA 3.02 <C> Cell Type <C> 0.99 <C> 62% <C> 0.94 (70%) <C> 0.82 (86%) <C> 0.74 (91%) <R> <C> GENIA 3.02 <C> Virus <C> 0.94 <C> 24% <C> 0.97 (79%) <C> 0.89 (94%) <C> 0.84 (96%) <R> <C> [BOLD] Average <C> [BOLD] Average <C> 0.97 <C> 45% <C> 0.98 (52%) <C> 0.90 (78%) <C> 0.81 (87%) <CAP> Table 3: Pipeline testing results of EAL and EAA annotation modes showing the model confidence (σ), F-Scores, and percentage cut from the pool of sentences. <COT> Looking at the "Dataset Name" column, we can see that the table shows the pipeline testing results of EAL and EAA annotation modes for different datasets.
<R> <C> [BOLD] Dataset Name <C> [BOLD] Entity Class <C> [BOLD] EAL @ 1.0 F <C> [BOLD] EAL @ 1.0 F <C> [BOLD] EAA Annotation Mode  [BOLD] FA <C> [BOLD] EAA Annotation Mode  [BOLD] HFA <C> [BOLD] EAA Annotation Mode  [BOLD] UFA <R> <C> [BOLD] Dataset Name <C> [BOLD] Entity Class <C> [ITALIC] σ <C> % cut <C> [BOLD] F-Score (percentage cut) <C> [BOLD] F-Score (percentage cut) <C> [BOLD] F-Score (percentage cut) <R> <C> CoNLL-2003 <C> Location <C> 0.97 <C> 55% <C> 0.99 (46%) <C> 0.93 (83%) <C> 0.82 (91%) <R> <C> CoNLL-2003 <C> Person <C> 0.97 <C> 59% <C> 0.99 (48%) <C> 0.95 (81%) <C> 0.85 (90%) <R> <C> BioCreAtIvE II <C> Gene <C> 0.94 <C> 35% <C> 1.00 (35%) <C> 0.96 (50%) <C> 0.89 (69%) <R> <C> GENIA 3.02 <C> Protein Molecule <C> 0.99 <C> 33% <C> 0.98 (36%) <C> 0.87 (71%) <C> 0.74 (85%) <R> <C> GENIA 3.02 <C> Cell Type <C> 0.99 <C> 62% <C> 0.94 (70%) <C> 0.82 (86%) <C> 0.74 (91%) <R> <C> GENIA 3.02 <C> Virus <C> 0.94 <C> 24% <C> 0.97 (79%) <C> 0.89 (94%) <C> 0.84 (96%) <R> <C> [BOLD] Average <C> [BOLD] Average <C> 0.97 <C> 45% <C> 0.98 (52%) <C> 0.90 (78%) <C> 0.81 (87%) <CAP> Table 3: Pipeline testing results of EAL and EAA annotation modes showing the model confidence (σ), F-Scores, and percentage cut from the pool of sentences. <COT> Looking at the "Average" row, we can see that the average F-Score for all entity classes is 0.81 (percentage cut).
<R> <C> [BOLD] Spoken bc.conv <C> [BOLD] Spoken 137,223 <C> [BOLD] Written news <C> [BOLD] Written 68,6455 <R> <C> bc.news <C> 244,425 <C> bible <C> 243,040 <R> <C> phone <C> 110,132 <C> trans. <C> 98,143 <R> <C> [EMPTY] <C> [EMPTY] <C> web <C> 71467 <R> <C> [BOLD] total <C> 491,780 <C> [BOLD] total <C> 1,099,105 <R> <C> [BOLD] total 1,590,885 <C> [BOLD] total 1,590,885 <C> [BOLD] total 1,590,885 <C> [BOLD] total 1,590,885 <CAP> Table 1: Coarse text types in OntoNotes <COT> Looking at the "total" cells in the last row and last column, we can see that the total number of spoken texts is 1,590,885 and the total number of written texts is also 1,590,885.
<R> <C> [BOLD] Spoken bc.conv <C> [BOLD] Spoken 137,223 <C> [BOLD] Written news <C> [BOLD] Written 68,6455 <R> <C> bc.news <C> 244,425 <C> bible <C> 243,040 <R> <C> phone <C> 110,132 <C> trans. <C> 98,143 <R> <C> [EMPTY] <C> [EMPTY] <C> web <C> 71467 <R> <C> [BOLD] total <C> 491,780 <C> [BOLD] total <C> 1,099,105 <R> <C> [BOLD] total 1,590,885 <C> [BOLD] total 1,590,885 <C> [BOLD] total 1,590,885 <C> [BOLD] total 1,590,885 <CAP> Table 1: Coarse text types in OntoNotes <COT> Looking at the "Spoken bc.conv" cell and "Spoken 137,223" cell, we can see that the number of spoken texts in the bc.conv category is 137,223.
<R> <C> [EMPTY] <C> [EMPTY] <C> Predicted Sg <C> Predicted Pl <C> Total <R> <C> Actual <C> Sg <C> 222 <C> 39 <C> 261 <R> <C> Actual <C> Pl <C> 7 <C> 81 <C> 88 <R> <C> [EMPTY] <C> Total <C> 229 <C> 120 <C> 349 <CAP> Table 2: Confusion matrix for test data classification <COT> looking at the "Confusion matrix for test data classification" caption, we can see that the table represents the confusion matrix for the classification of test data.
<R> <C> [EMPTY] <C> [EMPTY] <C> Predicted Sg <C> Predicted Pl <C> Total <R> <C> Actual <C> Sg <C> 222 <C> 39 <C> 261 <R> <C> Actual <C> Pl <C> 7 <C> 81 <C> 88 <R> <C> [EMPTY] <C> Total <C> 229 <C> 120 <C> 349 <CAP> Table 2: Confusion matrix for test data classification <COT> looking at the "Predicted Sg" column, we can see that there are 222 instances predicted as singular.
<R> <C> [BOLD] genre  [ITALIC] written <C> [BOLD] agreement  [ITALIC] notional <C> [BOLD] agreement  [ITALIC] strict <C> [BOLD] agreement  [ITALIC] % notional <R> <C> bible <C> 169 <C> 487 <C> 25.76 <R> <C> newswire <C> 344 <C> 843 <C> 28.98 <R> <C> translations <C> 55 <C> 210 <C> 20.75 <R> <C> web <C> 48 <C> 71 <C> 40.33 <R> <C> [BOLD] total written <C> 616 <C> 1611 <C> 27.66 <R> <C> [ITALIC] spoken <C> [ITALIC] notional <C> [ITALIC] strict <C> [ITALIC] % notional <R> <C> bc.conv <C> 237 <C> 201 <C> 54.11 <R> <C> bc.news <C> 296 <C> 378 <C> 43.91 <R> <C> phone <C> 60 <C> 89 <C> 40.26 <R> <C> [BOLD] total spoken <C> 593 <C> 668 <C> 47.02 <CAP> Table 3: Agreement patterns across genres <COT> Looking at the "genre" column, we can see that there are two categories: "written" and "spoken". The table provides agreement patterns for different genres under these categories.
<R> <C> [BOLD] genre  [ITALIC] written <C> [BOLD] agreement  [ITALIC] notional <C> [BOLD] agreement  [ITALIC] strict <C> [BOLD] agreement  [ITALIC] % notional <R> <C> bible <C> 169 <C> 487 <C> 25.76 <R> <C> newswire <C> 344 <C> 843 <C> 28.98 <R> <C> translations <C> 55 <C> 210 <C> 20.75 <R> <C> web <C> 48 <C> 71 <C> 40.33 <R> <C> [BOLD] total written <C> 616 <C> 1611 <C> 27.66 <R> <C> [ITALIC] spoken <C> [ITALIC] notional <C> [ITALIC] strict <C> [ITALIC] % notional <R> <C> bc.conv <C> 237 <C> 201 <C> 54.11 <R> <C> bc.news <C> 296 <C> 378 <C> 43.91 <R> <C> phone <C> 60 <C> 89 <C> 40.26 <R> <C> [BOLD] total spoken <C> 593 <C> 668 <C> 47.02 <CAP> Table 3: Agreement patterns across genres <COT> Looking at the "total written" and "total spoken" rows, we can see that the total number of instances for written genres is 1611, and for spoken genres is 668.
<R> <C> Eval <C> Req <C> Fact <C> Ref <C> Quot <C> Non-A <C> Total <R> <C> 3,982 <C> 1,911 <C> 3,786 <C> 207 <C> 161 <C> 339 <C> 10,386 <CAP> Table 3: Number of propositions per type in AMPERE. <COT> Looking at the "Eval" column, we can see that there are 3,982 propositions evaluated in AMPERE.
<R> <C> [EMPTY] <C> [BOLD] Prec. <C> [BOLD]  Rec. <C> [BOLD]  F1 <R> <C> FullSent <C> 73.68 <C> 56.00 <C> 63.64 <R> <C> PDTB-conn <C> 51.11 <C> 49.71 <C> 50.40 <R> <C> RST-parser <C> 30.28 <C> 43.00 <C> 35.54 <R> <C> CRF <C> 66.53 <C> 52.92 <C> 58.95 <R> <C> BiLSTM-CRF <C> [BOLD] 82.25 <C> [BOLD] 79.96 <C> [BOLD] 81.09∗ <R> <C> CRF-joint <C> 74.99 <C> 63.33 <C> 68.67 <R> <C> BiLSTM-CRF-joint <C> 81.12 <C> 78.42 <C> 79.75 <CAP> Table 4: Proposition segmentation results. Result that is significantly better than all comparisons is marked with ∗ (p<10−6, McNemar test). <COT> Looking at the "BiLSTM-CRF" row, the values in the "Prec.", "Rec.", and "F1" columns are all marked as [BOLD], indicating that the performance of BiLSTM-CRF is significantly better than all comparisons.
<R> <C> [EMPTY] <C> [BOLD] Prec. <C> [BOLD]  Rec. <C> [BOLD]  F1 <R> <C> FullSent <C> 73.68 <C> 56.00 <C> 63.64 <R> <C> PDTB-conn <C> 51.11 <C> 49.71 <C> 50.40 <R> <C> RST-parser <C> 30.28 <C> 43.00 <C> 35.54 <R> <C> CRF <C> 66.53 <C> 52.92 <C> 58.95 <R> <C> BiLSTM-CRF <C> [BOLD] 82.25 <C> [BOLD] 79.96 <C> [BOLD] 81.09∗ <R> <C> CRF-joint <C> 74.99 <C> 63.33 <C> 68.67 <R> <C> BiLSTM-CRF-joint <C> 81.12 <C> 78.42 <C> 79.75 <CAP> Table 4: Proposition segmentation results. Result that is significantly better than all comparisons is marked with ∗ (p<10−6, McNemar test). <COT> Looking at the "RST-parser" row, the values in the "Rec." column is higher than the values in the "Prec." and "F1" columns.
<R> <C> [EMPTY] <C> FactBank MAE <C> FactBank r <C> UW MAE <C> UW r <C> Meantime MAE <C> Meantime r <C> UDS-IH2 MAE <C> UDS-IH2 r <R> <C> All-3.0 <C> 0.8 <C> NAN <C> 0.78 <C> NAN <C> 0.31 <C> NAN <C> 2.255 <C> NAN <R> <C> Lee et al. 2015 <C> - <C> - <C> 0.511 <C> 0.708 <C> - <C> - <C> - <C> - <R> <C> Stanovsky et al. 2017 <C> 0.59 <C> 0.71 <C> [BOLD] 0.42† <C> 0.66 <C> 0.34 <C> 0.47 <C> - <C> - <R> <C> L-biLSTM(2)-S <C> [BOLD] 0.427 <C> [BOLD] 0.826 <C> 0.508 <C> [BOLD] 0.719 <C> 0.427 <C> 0.335 <C> [BOLD] 0.960† <C> [BOLD] 0.768 <R> <C> T-biLSTM(2)-S <C> [BOLD] 0.577 <C> [BOLD] 0.752 <C> 0.600 <C> 0.645 <C> 0.428 <C> 0.094 <C> [BOLD] 1.101 <C> [BOLD] 0.704 <R> <C> L-biLSTM(2)-G <C> [BOLD] 0.412 <C> [BOLD] 0.812 <C> 0.523 <C> 0.703 <C> 0.409 <C> 0.462 <C> - <C> - <R> <C> T-biLSTM(2)-G <C> [BOLD] 0.455 <C> [BOLD] 0.809 <C> 0.567 <C> 0.688 <C> 0.396 <C> 0.368 <C> - <C> - <R> <C> L-biLSTM(2)-S+lexfeats <C> [BOLD] 0.429 <C> [BOLD] 0.796 <C> 0.495 <C> [BOLD] 0.730 <C> 0.427 <C> 0.322 <C> [BOLD] 1.000 <C> [BOLD] 0.755 <R> <C> T-biLSTM(2)-S+lexfeats <C> [BOLD] 0.542 <C> [BOLD] 0.744 <C> 0.567 <C> 0.676 <C> 0.375 <C> 0.242 <C> [BOLD] 1.087 <C> [BOLD] 0.719 <R> <C> L-biLSTM(2)-MultiSimp <C> [BOLD] 0.353 <C> [BOLD] 0.843 <C> 0.503 <C> [BOLD] 0.725 <C> 0.345 <C> [BOLD] 0.540 <C> - <C> - <R> <C> T-biLSTM(2)-MultiSimp <C> [BOLD] 0.482 <C> [BOLD] 0.803 <C> 0.599 <C> 0.645 <C> 0.545 <C> 0.237 <C> - <C> - <R> <C> L-biLSTM(2)-MultiBal <C> [BOLD] 0.391 <C> [BOLD] 0.821 <C> 0.496 <C> [BOLD] 0.724 <C> [BOLD] 0.278 <C> [BOLD] 0.613† <C> - <C> - <R> <C> T-biLSTM(2)-MultiBal <C> [BOLD] 0.517 <C> [BOLD] 0.788 <C> 0.573 <C> 0.659 <C> 0.400 <C> 0.405 <C> - <C> - <R> <C> L-biLSTM(1)-MultiFoc <C> [BOLD] 0.343 <C> [BOLD] 0.823 <C> 0.516 <C> 0.698 <C> [BOLD] 0.229† <C> [BOLD] 0.599 <C> - <C> - <R> <C> L-biLSTM(2)-MultiFoc <C> [BOLD] 0.314 <C> [BOLD] 0.846 <C> 0.502 <C> [BOLD] 0.710 <C> [BOLD] 0.305 <C> 0.377 <C> - <C> - <R> <C> T-biLSTM(2)-MultiFoc <C> 1.100 <C> 0.234 <C> 0.615 <C> 0.616 <C> 0.395 <C> 0.300 <C> - <C> - <R> <C> L-biLSTM(2)-MultiSimp w/UDS-IH2 <C> [BOLD] 0.377 <C> [BOLD] 0.828 <C> 0.508 <C> [BOLD] 0.722 <C> 0.367 <C> 0.469 <C> [BOLD] 0.965 <C> [BOLD] 0.771† <R> <C> T-biLSTM(2)-MultiSimp w/UDS-IH2 <C> 0.595 <C> [BOLD] 0.716 <C> 0.598 <C> 0.609 <C> 0.467 <C> 0.345 <C> [BOLD] 1.072 <C> [BOLD] 0.723 <R> <C> H-biLSTM(2)-S <C> 0.488 <C> [BOLD] 0.775 <C> 0.526 <C> [BOLD] 0.714 <C> 0.442 <C> 0.255 <C> [BOLD] 0.967 <C> [BOLD] 0.768 <R> <C> H-biLSTM(1)-MultiSimp <C> [BOLD] 0.313† <C> [BOLD] 0.857† <C> 0.528 <C> 0.704 <C> 0.314 <C> 0.545 <C> - <C> - <R> <C> H-biLSTM(2)-MultiSimp <C> [BOLD] 0.431 <C> [BOLD] 0.808 <C> 0.514 <C> [BOLD] 0.723 <C> 0.401 <C> 0.461 <C> - <C> - <R> <C> H-biLSTM(2)-MultiBal <C> [BOLD] 0.386 <C> [BOLD] 0.825 <C> 0.502 <C> [BOLD] 0.713 <C> 0.352 <C> [BOLD] 0.564 <C> - <C> - <R> <C> H-biLSTM(2)-MultiSimp w/UDS-IH2 <C> [BOLD] 0.393 <C> [BOLD] 0.820 <C> 0.481 <C> [BOLD] 0.749† <C> 0.374 <C> [BOLD] 0.495 <C> [BOLD] 0.969 <C> [BOLD] 0.760 <CAP> Table 4: All 2-layer systems, and 1-layer systems if best in column. State-of-the-art results in bold; † indicates best in column (corresponding row shaded in purple). Key: L=linear, T=tree, H=hybrid, (1,2)=# layers, S=single-task specific, G=single-task general, +lexfeats=with all lexical features, MultiSimp=multi-task simple, MultiBal=multi-task balanced, MultiFoc=multi-task focused, w/UDS-IH2=trained on all data including UDS-IH2. All-3.0 is a constant baseline, always predicting 3.0. <COT> Looking at the "FactBank r" column, we can see that the L-biLSTM(2)-S model has the highest r value of 0.826.
<R> <C> [EMPTY] <C> FactBank MAE <C> FactBank r <C> UW MAE <C> UW r <C> Meantime MAE <C> Meantime r <C> UDS-IH2 MAE <C> UDS-IH2 r <R> <C> All-3.0 <C> 0.8 <C> NAN <C> 0.78 <C> NAN <C> 0.31 <C> NAN <C> 2.255 <C> NAN <R> <C> Lee et al. 2015 <C> - <C> - <C> 0.511 <C> 0.708 <C> - <C> - <C> - <C> - <R> <C> Stanovsky et al. 2017 <C> 0.59 <C> 0.71 <C> [BOLD] 0.42† <C> 0.66 <C> 0.34 <C> 0.47 <C> - <C> - <R> <C> L-biLSTM(2)-S <C> [BOLD] 0.427 <C> [BOLD] 0.826 <C> 0.508 <C> [BOLD] 0.719 <C> 0.427 <C> 0.335 <C> [BOLD] 0.960† <C> [BOLD] 0.768 <R> <C> T-biLSTM(2)-S <C> [BOLD] 0.577 <C> [BOLD] 0.752 <C> 0.600 <C> 0.645 <C> 0.428 <C> 0.094 <C> [BOLD] 1.101 <C> [BOLD] 0.704 <R> <C> L-biLSTM(2)-G <C> [BOLD] 0.412 <C> [BOLD] 0.812 <C> 0.523 <C> 0.703 <C> 0.409 <C> 0.462 <C> - <C> - <R> <C> T-biLSTM(2)-G <C> [BOLD] 0.455 <C> [BOLD] 0.809 <C> 0.567 <C> 0.688 <C> 0.396 <C> 0.368 <C> - <C> - <R> <C> L-biLSTM(2)-S+lexfeats <C> [BOLD] 0.429 <C> [BOLD] 0.796 <C> 0.495 <C> [BOLD] 0.730 <C> 0.427 <C> 0.322 <C> [BOLD] 1.000 <C> [BOLD] 0.755 <R> <C> T-biLSTM(2)-S+lexfeats <C> [BOLD] 0.542 <C> [BOLD] 0.744 <C> 0.567 <C> 0.676 <C> 0.375 <C> 0.242 <C> [BOLD] 1.087 <C> [BOLD] 0.719 <R> <C> L-biLSTM(2)-MultiSimp <C> [BOLD] 0.353 <C> [BOLD] 0.843 <C> 0.503 <C> [BOLD] 0.725 <C> 0.345 <C> [BOLD] 0.540 <C> - <C> - <R> <C> T-biLSTM(2)-MultiSimp <C> [BOLD] 0.482 <C> [BOLD] 0.803 <C> 0.599 <C> 0.645 <C> 0.545 <C> 0.237 <C> - <C> - <R> <C> L-biLSTM(2)-MultiBal <C> [BOLD] 0.391 <C> [BOLD] 0.821 <C> 0.496 <C> [BOLD] 0.724 <C> [BOLD] 0.278 <C> [BOLD] 0.613† <C> - <C> - <R> <C> T-biLSTM(2)-MultiBal <C> [BOLD] 0.517 <C> [BOLD] 0.788 <C> 0.573 <C> 0.659 <C> 0.400 <C> 0.405 <C> - <C> - <R> <C> L-biLSTM(1)-MultiFoc <C> [BOLD] 0.343 <C> [BOLD] 0.823 <C> 0.516 <C> 0.698 <C> [BOLD] 0.229† <C> [BOLD] 0.599 <C> - <C> - <R> <C> L-biLSTM(2)-MultiFoc <C> [BOLD] 0.314 <C> [BOLD] 0.846 <C> 0.502 <C> [BOLD] 0.710 <C> [BOLD] 0.305 <C> 0.377 <C> - <C> - <R> <C> T-biLSTM(2)-MultiFoc <C> 1.100 <C> 0.234 <C> 0.615 <C> 0.616 <C> 0.395 <C> 0.300 <C> - <C> - <R> <C> L-biLSTM(2)-MultiSimp w/UDS-IH2 <C> [BOLD] 0.377 <C> [BOLD] 0.828 <C> 0.508 <C> [BOLD] 0.722 <C> 0.367 <C> 0.469 <C> [BOLD] 0.965 <C> [BOLD] 0.771† <R> <C> T-biLSTM(2)-MultiSimp w/UDS-IH2 <C> 0.595 <C> [BOLD] 0.716 <C> 0.598 <C> 0.609 <C> 0.467 <C> 0.345 <C> [BOLD] 1.072 <C> [BOLD] 0.723 <R> <C> H-biLSTM(2)-S <C> 0.488 <C> [BOLD] 0.775 <C> 0.526 <C> [BOLD] 0.714 <C> 0.442 <C> 0.255 <C> [BOLD] 0.967 <C> [BOLD] 0.768 <R> <C> H-biLSTM(1)-MultiSimp <C> [BOLD] 0.313† <C> [BOLD] 0.857† <C> 0.528 <C> 0.704 <C> 0.314 <C> 0.545 <C> - <C> - <R> <C> H-biLSTM(2)-MultiSimp <C> [BOLD] 0.431 <C> [BOLD] 0.808 <C> 0.514 <C> [BOLD] 0.723 <C> 0.401 <C> 0.461 <C> - <C> - <R> <C> H-biLSTM(2)-MultiBal <C> [BOLD] 0.386 <C> [BOLD] 0.825 <C> 0.502 <C> [BOLD] 0.713 <C> 0.352 <C> [BOLD] 0.564 <C> - <C> - <R> <C> H-biLSTM(2)-MultiSimp w/UDS-IH2 <C> [BOLD] 0.393 <C> [BOLD] 0.820 <C> 0.481 <C> [BOLD] 0.749† <C> 0.374 <C> [BOLD] 0.495 <C> [BOLD] 0.969 <C> [BOLD] 0.760 <CAP> Table 4: All 2-layer systems, and 1-layer systems if best in column. State-of-the-art results in bold; † indicates best in column (corresponding row shaded in purple). Key: L=linear, T=tree, H=hybrid, (1,2)=# layers, S=single-task specific, G=single-task general, +lexfeats=with all lexical features, MultiSimp=multi-task simple, MultiBal=multi-task balanced, MultiFoc=multi-task focused, w/UDS-IH2=trained on all data including UDS-IH2. All-3.0 is a constant baseline, always predicting 3.0. <COT> Looking at the "Meantime MAE" column, we can see that the H-biLSTM(1)-MultiSimp model has the lowest MAE value of 0.313.
<R> <C> [EMPTY] <C> Overall <C> Eval <C> Req <C> Fact <C> Ref <C> Quot <R> <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <R> <C> Majority <C> 40.75 <C> 57.90 <C> – <C> – <C> – <C> – <R> <C> PropLexicon <C> 36.83 <C> 40.42 <C> 36.07 <C> 32.23 <C> 59.57 <C> 31.28 <R> <C> SVM <C> 60.98 <C> 63.88 <C> [BOLD] 69.02 <C> 54.74 <C> [BOLD] 69.47 <C> 7.69 <R> <C> CNN <C> [BOLD] 66.56∗ <C> [BOLD] 69.02 <C> 63.26 <C> [BOLD] 66.17 <C> 67.44 <C> [BOLD] 52.94 <R> <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <R> <C> Majority <C> 33.30 <C> 47.60 <C> – <C> – <C> – <C> – <R> <C> PropLexicon <C> 23.21 <C> 22.45 <C> 23.97 <C> 23.73 <C> 35.96 <C> 16.67 <R> <C> SVM <C> 51.46 <C> 54.05 <C> 48.16 <C> 52.77 <C> 52.27 <C> 4.71 <R> <C> CNN <C> 55.48 <C> 57.75 <C> 53.71 <C> 55.19 <C> 48.78 <C> 33.33 <R> <C> CRF-joint <C> 50.69 <C> 46.78 <C> 55.74 <C> 52.27 <C> [BOLD] 55.77 <C> 26.47 <R> <C> BiLSTM-CRF-joint <C> [BOLD] 62.64∗ <C> [BOLD] 62.36∗ <C> [BOLD] 67.31∗ <C> [BOLD] 61.86 <C> 54.74 <C> [BOLD] 37.36 <CAP> Table 5: Proposition classification F1 scores. Results that are significant better than other methods are marked with ∗ (p<10−6, McNemar test). <COT> Looking at the "BiLSTM-CRF-joint" row, "[BOLD] 62.64∗" cell, "Overall" column, and "Eval" cell, we can see that the BiLSTM-CRF-joint method achieves the highest F1 score for overall proposition classification.
<R> <C> [EMPTY] <C> Overall <C> Eval <C> Req <C> Fact <C> Ref <C> Quot <R> <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <C> [ITALIC] With Gold-Standard Segments <R> <C> Majority <C> 40.75 <C> 57.90 <C> – <C> – <C> – <C> – <R> <C> PropLexicon <C> 36.83 <C> 40.42 <C> 36.07 <C> 32.23 <C> 59.57 <C> 31.28 <R> <C> SVM <C> 60.98 <C> 63.88 <C> [BOLD] 69.02 <C> 54.74 <C> [BOLD] 69.47 <C> 7.69 <R> <C> CNN <C> [BOLD] 66.56∗ <C> [BOLD] 69.02 <C> 63.26 <C> [BOLD] 66.17 <C> 67.44 <C> [BOLD] 52.94 <R> <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <C> [ITALIC] With Predicted Segments <R> <C> Majority <C> 33.30 <C> 47.60 <C> – <C> – <C> – <C> – <R> <C> PropLexicon <C> 23.21 <C> 22.45 <C> 23.97 <C> 23.73 <C> 35.96 <C> 16.67 <R> <C> SVM <C> 51.46 <C> 54.05 <C> 48.16 <C> 52.77 <C> 52.27 <C> 4.71 <R> <C> CNN <C> 55.48 <C> 57.75 <C> 53.71 <C> 55.19 <C> 48.78 <C> 33.33 <R> <C> CRF-joint <C> 50.69 <C> 46.78 <C> 55.74 <C> 52.27 <C> [BOLD] 55.77 <C> 26.47 <R> <C> BiLSTM-CRF-joint <C> [BOLD] 62.64∗ <C> [BOLD] 62.36∗ <C> [BOLD] 67.31∗ <C> [BOLD] 61.86 <C> 54.74 <C> [BOLD] 37.36 <CAP> Table 5: Proposition classification F1 scores. Results that are significant better than other methods are marked with ∗ (p<10−6, McNemar test). <COT> Looking at the "CNN" row, "[BOLD] 66.56∗" cell, "Overall" column, and "Fact" cell, we can see that the CNN method achieves a higher F1 score for fact proposition classification than for other types of propositions.
<R> <C> # encoders <C> 2 <R> <C> encoder type <C> B-LSTM <R> <C> encoder layers <C> 2 <R> <C> encoder hidden dim <C> 500 <R> <C> # decoders <C> 1 <R> <C> decoder type <C> LSTM <R> <C> decoder layers <C> 2 <R> <C> decoder hidden dim <C> 500 <R> <C> word vector dim <C> 300 <R> <C> attention type <C> [ITALIC] general <R> <C> dropout <C> 0.3 <R> <C> beam size <C> 5 <CAP> Table 1: The model and its hyper-parameters. <COT> Looking at the "decoder type" cell, we can see that the decoder type is LSTM.
<R> <C> # encoders <C> 2 <R> <C> encoder type <C> B-LSTM <R> <C> encoder layers <C> 2 <R> <C> encoder hidden dim <C> 500 <R> <C> # decoders <C> 1 <R> <C> decoder type <C> LSTM <R> <C> decoder layers <C> 2 <R> <C> decoder hidden dim <C> 500 <R> <C> word vector dim <C> 300 <R> <C> attention type <C> [ITALIC] general <R> <C> dropout <C> 0.3 <R> <C> beam size <C> 5 <CAP> Table 1: The model and its hyper-parameters. <COT> Looking at the "attention type" cell, we can see that the attention type is general.
<R> <C> Model <C> TER <C> BLEU <R> <C> MT Bojar et al. ( 2017 ) <C> 24.48 <C> 62.49 <R> <C> SPE Bojar et al. ( 2017 ) <C> 24.69 <C> 62.97 <R> <C> Varis and Bojar ( 2017 ) <C> 24.03 <C> 64.28 <R> <C> Bérard et al. ( 2017 ) <C> 22.81 <C> 65.91 <R> <C> train 11K <C> 41.58 <C> 43.05 <R> <C> train 23K <C> 30.23 <C> 57.14 <R> <C> train 23K + 500K <C> [BOLD] 22.60 <C> [BOLD] 66.21 <CAP> Table 2: Results on the WMT17 IT domain English-German APE test set. <COT> Looking at the "Model" column, we can see that the model "train 23K + 500K" has the lowest TER and the highest BLEU scores, indicating better performance.
<R> <C> Model <C> TER <C> BLEU <R> <C> MT Bojar et al. ( 2017 ) <C> 24.48 <C> 62.49 <R> <C> SPE Bojar et al. ( 2017 ) <C> 24.69 <C> 62.97 <R> <C> Varis and Bojar ( 2017 ) <C> 24.03 <C> 64.28 <R> <C> Bérard et al. ( 2017 ) <C> 22.81 <C> 65.91 <R> <C> train 11K <C> 41.58 <C> 43.05 <R> <C> train 23K <C> 30.23 <C> 57.14 <R> <C> train 23K + 500K <C> [BOLD] 22.60 <C> [BOLD] 66.21 <CAP> Table 2: Results on the WMT17 IT domain English-German APE test set. <COT> Looking at the "Model" column and the "BLEU" column, we can observe that as the training data increases from "train 11K" to "train 23K" to "train 23K + 500K", the BLEU score also increases.
<R> <C> [BOLD] Feature <C> [BOLD] Representation <C> [BOLD] Embedding <C> [BOLD] Window <C> [BOLD] Dimension <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <R> <C> Used for transportation <C> Co-occurrence <C> word2vec SG <C> 10 <C> 300 <C> 74.5 <C> 78.8 <C> 76.6 <R> <C> Is a weapon <C> Backtranslation <C> word2vec CBOW <C> 2 <C> 300 <C> 71.4 <C> 88.2 <C> 78.9 <R> <C> Is round <C> Co-occurrence <C> word2vec CBOW <C> 10 <C> 300 <C> 56.2 <C> 87.1 <C> 68.4 <R> <C> Has various colors <C> Co-occurrence <C> GloVe <C> 2 <C> 200 <C> 70.6 <C> 76.6 <C> 73.5 <R> <C> Made of metal <C> Matrix <C> word2vec SG <C> 5 <C> 300 <C> 78.6 <C> 61.1 <C> 68.8 <CAP> Table 3: The performance of the best setting for each property. <COT> Looking at the "Feature" column, we can see that the feature "Used for transportation" has the highest F1 score of 76.6.
<R> <C> [BOLD] Feature <C> [BOLD] Representation <C> [BOLD] Embedding <C> [BOLD] Window <C> [BOLD] Dimension <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <R> <C> Used for transportation <C> Co-occurrence <C> word2vec SG <C> 10 <C> 300 <C> 74.5 <C> 78.8 <C> 76.6 <R> <C> Is a weapon <C> Backtranslation <C> word2vec CBOW <C> 2 <C> 300 <C> 71.4 <C> 88.2 <C> 78.9 <R> <C> Is round <C> Co-occurrence <C> word2vec CBOW <C> 10 <C> 300 <C> 56.2 <C> 87.1 <C> 68.4 <R> <C> Has various colors <C> Co-occurrence <C> GloVe <C> 2 <C> 200 <C> 70.6 <C> 76.6 <C> 73.5 <R> <C> Made of metal <C> Matrix <C> word2vec SG <C> 5 <C> 300 <C> 78.6 <C> 61.1 <C> 68.8 <CAP> Table 3: The performance of the best setting for each property. <COT> Looking at the "Precision" column, we can see that the precision for the feature "Is a weapon" is 71.4.
<R> <C> Relation <C> Mean Label <C> L-biLSTM <C> T-biLSTM <C> # <R> <C> root <C> 1.07 <C> 1.03 <C> 0.96 <C> 949 <R> <C> conj <C> 0.37 <C> 0.44 <C> 0.46 <C> 316 <R> <C> advcl <C> 0.46 <C> 0.53 <C> 0.45 <C> 303 <R> <C> xcomp <C> -0.42 <C> -0.57 <C> -0.49 <C> 234 <R> <C> acl:relcl <C> 1.28 <C> 1.40 <C> 1.31 <C> 193 <R> <C> ccomp <C> 0.11 <C> 0.31 <C> 0.34 <C> 191 <R> <C> acl <C> 0.77 <C> 0.59 <C> 0.58 <C> 159 <R> <C> parataxis <C> 0.44 <C> 0.63 <C> 0.79 <C> 127 <R> <C> amod <C> 1.92 <C> 1.88 <C> 1.81 <C> 76 <R> <C> csubj <C> 0.36 <C> 0.38 <C> 0.27 <C> 37 <CAP> Table 6: Mean predictions for linear (L-biLSTM-S(2)) and tree models (T-biLSTM-S(2)) on UDS-IH2-dev, grouped by governing dependency relation. Only the 10 most frequent governing dependency relations in UDS-IH2-dev are shown. <COT> Looking at the table, we can see that the "Mean Label" column represents the mean predictions for linear (L-biLSTM-S(2)) and tree models (T-biLSTM-S(2)) on UDS-IH2-dev, grouped by governing dependency relation.
<R> <C> Relation <C> Mean Label <C> L-biLSTM <C> T-biLSTM <C> # <R> <C> root <C> 1.07 <C> 1.03 <C> 0.96 <C> 949 <R> <C> conj <C> 0.37 <C> 0.44 <C> 0.46 <C> 316 <R> <C> advcl <C> 0.46 <C> 0.53 <C> 0.45 <C> 303 <R> <C> xcomp <C> -0.42 <C> -0.57 <C> -0.49 <C> 234 <R> <C> acl:relcl <C> 1.28 <C> 1.40 <C> 1.31 <C> 193 <R> <C> ccomp <C> 0.11 <C> 0.31 <C> 0.34 <C> 191 <R> <C> acl <C> 0.77 <C> 0.59 <C> 0.58 <C> 159 <R> <C> parataxis <C> 0.44 <C> 0.63 <C> 0.79 <C> 127 <R> <C> amod <C> 1.92 <C> 1.88 <C> 1.81 <C> 76 <R> <C> csubj <C> 0.36 <C> 0.38 <C> 0.27 <C> 37 <CAP> Table 6: Mean predictions for linear (L-biLSTM-S(2)) and tree models (T-biLSTM-S(2)) on UDS-IH2-dev, grouped by governing dependency relation. Only the 10 most frequent governing dependency relations in UDS-IH2-dev are shown. <COT> Looking at the table, we can see that the "root" relation has the highest mean label for both L-biLSTM and T-biLSTM models.
<R> <C> Lexicons <C> Annotations <C> # dim. <C> # words <R> <C> LIWC <C> psycho-linguistic <C> 73 <C> 18,504 <R> <C> Bing Liu <C> valence <C> 1 <C> 2,477 <R> <C> AFINN <C> sentiment <C> 1 <C> 6,786 <R> <C> MPQA <C> sentiment <C> 4 <C> 6,886 <R> <C> SemEval15 <C> sentiment <C> 1 <C> 1,515 <R> <C> Emolex <C> emotion <C> 19 <C> 14,182 <CAP> Table 1: The lexicons used as external knowledge. <COT> Looking at the "Annotations" column, we can see that the lexicon "LIWC" has 73 dimensions.
<R> <C> Lexicons <C> Annotations <C> # dim. <C> # words <R> <C> LIWC <C> psycho-linguistic <C> 73 <C> 18,504 <R> <C> Bing Liu <C> valence <C> 1 <C> 2,477 <R> <C> AFINN <C> sentiment <C> 1 <C> 6,786 <R> <C> MPQA <C> sentiment <C> 4 <C> 6,886 <R> <C> SemEval15 <C> sentiment <C> 1 <C> 1,515 <R> <C> Emolex <C> emotion <C> 19 <C> 14,182 <CAP> Table 1: The lexicons used as external knowledge. <COT> Looking at the "Annotations" column, we can see that the lexicon "SemEval15" has 1 dimension.
<R> <C> Model <C> SST-5 <C> Sent17 <C> PhychExp <C> Irony18 <C> SCv1 <C> SCv2 <R> <C> baseline <C> 43.5±0.5 <C> 68.3±0.2 <C> 53.2±0.8 <C> 46.3±1.4 <C> 64.1±0.5 <C> 74.0±0.7 <R> <C> emb. conc. <C> 43.3±0.6 <C> 68.4±0.2 <C> 57.1±1.2 <C> 48.1±1.2 <C> 64.2±0.7 <C> 74.2±0.7 <R> <C> conc. <C> 44.0±0.7 <C> 68.6±0.3 <C> 54.3±0.6 <C> 47.4±0.9 <C> [BOLD] 65.1±0.6 <C> 74.3±1.2 <R> <C> gate <C> 44.2±0.4 <C> 68.7±0.3 <C> 53.4±1.0 <C> [BOLD] 48.5±0.7 <C> 64.7±0.7 <C> 74.3±1.2 <R> <C> affine <C> 43.2±0.7 <C> 68.5±0.3 <C> 53.1±0.9 <C> 45.3±1.5 <C> 60.3±0.8 <C> 74.0±1.0 <R> <C> gate+emb.conc. <C> [BOLD] 46.2±0.5 <C> [BOLD] 68.9±0.3 <C> [BOLD] 57.2±1.1 <C> [BOLD] 48.4±1.0 <C> [BOLD] 64.9±0.6 <C> [BOLD] 74.4±0.9 <R> <C> state-of-the-art <C> 51.7 <C> 68.5 <C> 57.0 <C> 53.6 <C> 69.0 <C> 76.0 <R> <C> state-of-the-art <C> Shen et al. ( 2018 ) <C> Cliche ( 2017 ) <C> Felbo et al. ( 2017 ) <C> Baziotis et al. ( 2018 ) <C> Felbo et al. ( 2017 ) <C> Ilić et al. ( 2018 ) <CAP> Table 3: Comparison across benchmark datasets. Reported values are averaged across ten runs. All reported measures are F1 scores, apart from SST−5 which is evaluated with Accuracy. <COT> Looking at the "gate+emb.conc." row, "[BOLD] 46.2±0.5" cell, "Irony18" cell, we can see that the gate+emb.conc. model achieves the highest F1 score on the Irony18 dataset.
<R> <C> Model <C> SST-5 <C> Sent17 <C> PhychExp <C> Irony18 <C> SCv1 <C> SCv2 <R> <C> baseline <C> 43.5±0.5 <C> 68.3±0.2 <C> 53.2±0.8 <C> 46.3±1.4 <C> 64.1±0.5 <C> 74.0±0.7 <R> <C> emb. conc. <C> 43.3±0.6 <C> 68.4±0.2 <C> 57.1±1.2 <C> 48.1±1.2 <C> 64.2±0.7 <C> 74.2±0.7 <R> <C> conc. <C> 44.0±0.7 <C> 68.6±0.3 <C> 54.3±0.6 <C> 47.4±0.9 <C> [BOLD] 65.1±0.6 <C> 74.3±1.2 <R> <C> gate <C> 44.2±0.4 <C> 68.7±0.3 <C> 53.4±1.0 <C> [BOLD] 48.5±0.7 <C> 64.7±0.7 <C> 74.3±1.2 <R> <C> affine <C> 43.2±0.7 <C> 68.5±0.3 <C> 53.1±0.9 <C> 45.3±1.5 <C> 60.3±0.8 <C> 74.0±1.0 <R> <C> gate+emb.conc. <C> [BOLD] 46.2±0.5 <C> [BOLD] 68.9±0.3 <C> [BOLD] 57.2±1.1 <C> [BOLD] 48.4±1.0 <C> [BOLD] 64.9±0.6 <C> [BOLD] 74.4±0.9 <R> <C> state-of-the-art <C> 51.7 <C> 68.5 <C> 57.0 <C> 53.6 <C> 69.0 <C> 76.0 <R> <C> state-of-the-art <C> Shen et al. ( 2018 ) <C> Cliche ( 2017 ) <C> Felbo et al. ( 2017 ) <C> Baziotis et al. ( 2018 ) <C> Felbo et al. ( 2017 ) <C> Ilić et al. ( 2018 ) <CAP> Table 3: Comparison across benchmark datasets. Reported values are averaged across ten runs. All reported measures are F1 scores, apart from SST−5 which is evaluated with Accuracy. <COT> Looking at the "SCv2" column, "[BOLD] 74.3±1.2" cell, "gate" cell, we can observe that the gate model achieves the highest F1 score on the SCv2 dataset.
<R> <C> Modal <C> Negated <C> Mean Label <C> Linear MAE <C> Tree MAE <C> # <R> <C> none <C> no <C> 1.00 <C> 0.93 <C> 1.03 <C> 2244 <R> <C> none <C> yes <C> -0.19 <C> 1.40 <C> 1.69 <C> 98 <R> <C> may <C> no <C> -0.38 <C> 1.00 <C> 0.99 <C> 14 <R> <C> would <C> no <C> -0.61 <C> 0.85 <C> 0.99 <C> 39 <R> <C> ca(n’t) <C> yes <C> -0.72 <C> 1.28 <C> 1.55 <C> 11 <R> <C> can <C> yes <C> -0.75 <C> 0.99 <C> 0.86 <C> 6 <R> <C> (wi)’ll <C> no <C> -0.94 <C> 1.47 <C> 1.14 <C> 8 <R> <C> could <C> no <C> -1.03 <C> 0.97 <C> 1.32 <C> 20 <R> <C> can <C> no <C> -1.25 <C> 1.02 <C> 1.21 <C> 73 <R> <C> might <C> no <C> -1.25 <C> 0.66 <C> 1.06 <C> 6 <R> <C> would <C> yes <C> -1.27 <C> 0.40 <C> 0.86 <C> 5 <R> <C> should <C> no <C> -1.31 <C> 1.20 <C> 1.01 <C> 22 <R> <C> will <C> no <C> -1.88 <C> 0.75 <C> 0.86 <C> 75 <CAP> Table 5: Mean gold labels, counts, and MAE for L-biLSTM(2)-S and T-biLSTM(2)-S model predictions on UDS-IH2-dev, grouped by modals and negation. <COT> Looking at the "CAP" tag, we can see that Table 5 provides information about the mean gold labels, counts, and MAE for L-biLSTM(2)-S and T-biLSTM(2)-S model predictions on UDS-IH2-dev, grouped by modals and negation.
<R> <C> Modal <C> Negated <C> Mean Label <C> Linear MAE <C> Tree MAE <C> # <R> <C> none <C> no <C> 1.00 <C> 0.93 <C> 1.03 <C> 2244 <R> <C> none <C> yes <C> -0.19 <C> 1.40 <C> 1.69 <C> 98 <R> <C> may <C> no <C> -0.38 <C> 1.00 <C> 0.99 <C> 14 <R> <C> would <C> no <C> -0.61 <C> 0.85 <C> 0.99 <C> 39 <R> <C> ca(n’t) <C> yes <C> -0.72 <C> 1.28 <C> 1.55 <C> 11 <R> <C> can <C> yes <C> -0.75 <C> 0.99 <C> 0.86 <C> 6 <R> <C> (wi)’ll <C> no <C> -0.94 <C> 1.47 <C> 1.14 <C> 8 <R> <C> could <C> no <C> -1.03 <C> 0.97 <C> 1.32 <C> 20 <R> <C> can <C> no <C> -1.25 <C> 1.02 <C> 1.21 <C> 73 <R> <C> might <C> no <C> -1.25 <C> 0.66 <C> 1.06 <C> 6 <R> <C> would <C> yes <C> -1.27 <C> 0.40 <C> 0.86 <C> 5 <R> <C> should <C> no <C> -1.31 <C> 1.20 <C> 1.01 <C> 22 <R> <C> will <C> no <C> -1.88 <C> 0.75 <C> 0.86 <C> 75 <CAP> Table 5: Mean gold labels, counts, and MAE for L-biLSTM(2)-S and T-biLSTM(2)-S model predictions on UDS-IH2-dev, grouped by modals and negation. <COT> Looking at the "Negated" column, we can observe that there are both negated and non-negated instances for each modal.
<R> <C> [EMPTY] <C> Metric <C> Amazon <C> Newsgroups <C> New York Times <R> <C> Global <C> SigVac <C> 0.6960 <C> 0.6081 <C> 0.6063 <R> <C> Global <C> SigUni <C> 0.6310 <C> 0.4839 <C> 0.4935 <R> <C> Global <C> Coherence <C> 0.4907 <C> 0.4463 <C> 0.3799 <CAP> Table 2: Coefficient of determination (r2) between global metrics and crowdsourced topic-word matching annotations. <COT> Looking at the "CAP" tag, we can see that Table 2 displays the coefficient of determination (r2) between global metrics and crowdsourced topic-word matching annotations.
<R> <C> [EMPTY] <C> Metric <C> Amazon <C> Newsgroups <C> New York Times <R> <C> Global <C> SigVac <C> 0.6960 <C> 0.6081 <C> 0.6063 <R> <C> Global <C> SigUni <C> 0.6310 <C> 0.4839 <C> 0.4935 <R> <C> Global <C> Coherence <C> 0.4907 <C> 0.4463 <C> 0.3799 <CAP> Table 2: Coefficient of determination (r2) between global metrics and crowdsourced topic-word matching annotations. <COT> Looking at the "SigVac" cell, we can see that the SigVac metric has a higher coefficient of determination (r2) for the Amazon dataset compared to the Newsgroups and New York Times datasets.
<R> <C> [EMPTY] <C> Metric <C> Amazon <C> Newsgroups <C> New York Times <R> <C> Local <C> SwitchP <C> 0.9077 <C> 0.8737 <C> 0.7022 <R> <C> Local <C> SwitchVI <C> 0.8485 <C> 0.8181 <C> 0.6977 <R> <C> Local <C> AvgRank <C> 0.5103 <C> 0.5089 <C> 0.4473 <R> <C> Local <C> Window <C> 0.4884 <C> 0.3024 <C> 0.1127 <R> <C> Local <C> WordDiv <C> 0.3112 <C> 0.2197 <C> 0.0836 <R> <C> Global <C> SigVac <C> 0.6960 <C> 0.6081 <C> 0.6063 <R> <C> Global <C> SigUni <C> 0.6310 <C> 0.4839 <C> 0.4935 <R> <C> Global <C> Coherence <C> 0.4907 <C> 0.4463 <C> 0.3799 <CAP> Table 3: Coefficient of determination (r2) between automated metrics and crowdsourced topic-word matching annotations. We include metrics measuring both local topic quality and global topic quality. <COT> Looking at the "SwitchP" column, we can see that the local metric "SwitchP" has higher values for the Amazon dataset compared to the Newsgroups and New York Times datasets.
<R> <C> [BOLD] Method Location <C> [BOLD] P@01 0.3555 <C> [BOLD] P@01 – <C> [BOLD] P@05 0.3077 <C> [BOLD] P@05 – <C> [BOLD] P@10 0.2505 <C> [BOLD] P@10 – <C> [BOLD] AUC 0.5226 <C> [BOLD] AUC – <R> <C> PageRank <C> 0.3628 <C> – <C> 0.3438 <C> – <C> 0.3007 <C> – <C> 0.5866 <C> – <R> <C> Frequency <C> 0.4542 <C> – <C> 0.4024 <C> – <C> 0.3445 <C> – <C> 0.5732 <C> – <R> <C> LeToR <C> 0.4753† <C> +4.64% <C> 0.4099† <C> +1.87% <C> 0.3517† <C> +2.10% <C> 0.6373† <C> +11.19% <R> <C> KCE (-EF) <C> 0.4420 <C> −2.69% <C> 0.4038 <C> +0.34% <C> 0.3464† <C> +0.54% <C> 0.6089† <C> +6.23% <R> <C> KCE (-E) <C> 0.4861†‡ <C> +7.01% <C> 0.4227†‡ <C> +5.04% <C> 0.3603†‡ <C> +4.58% <C> 0.6541†‡ <C> +14.12% <R> <C> KCE <C> 0.5049†‡ <C> +11.14% <C> 0.4277†‡ <C> +6.29% <C> 0.3638†‡ <C> +5.61% <C> 0.6557†‡ <C> +14.41% <R> <C> [BOLD] Method <C> [BOLD] R@01 <C> [BOLD] R@01 <C> [BOLD] R@05 <C> [BOLD] R@05 <C> [BOLD] R@10 <C> [BOLD] R@10 <C> [BOLD] W/T/L <C> [BOLD] W/T/L <R> <C> Location <C> 0.0807 <C> – <C> 0.2671 <C> – <C> 0.3792 <C> – <C> –/–/– <C> –/–/– <R> <C> PageRank <C> 0.0758 <C> – <C> 0.2760 <C> – <C> 0.4163 <C> – <C> –/–/– <C> –/–/– <R> <C> Frequency <C> 0.0792 <C> – <C> 0.2846 <C> – <C> 0.4270 <C> – <C> –/–/– <C> –/–/– <R> <C> LeToR <C> 0.0836† <C> +5.61% <C> 0.2980† <C> +4.70% <C> 0.4454† <C> +4.31% <C> 8037 / 48493 / 6770 <C> 8037 / 48493 / 6770 <R> <C> KCE (-EF) <C> 0.0714 <C> −9.77% <C> 0.2812 <C> −1.18% <C> 0.4321† <C> +1.20% <C> 6936 / 48811 / 7553 <C> 6936 / 48811 / 7553 <R> <C> KCE (-E) <C> 0.0925†‡ <C> +16.78% <C> 0.3172†‡ <C> +11.46% <C> 0.4672†‡ <C> +9.41% <C> 11676 / 43294 / 8330 <C> 11676 / 43294 / 8330 <R> <C> KCE <C> 0.0946†‡ <C> +19.44% <C> 0.3215†‡ <C> +12.96% <C> 0.4719†‡ <C> +10.51% <C> 12554 / 41461 / 9285 <C> 12554 / 41461 / 9285 <CAP> Table 3: Event salience performance. (-E) and (-F) marks removing Entity information and Features from the full KCM model. The relative performance differences are computed against Frequency. W/T/L are the number of documents a method wins, ties, and loses compared to Frequency. † and ‡ mark the statistically significant improvements over Frequency†, LeToR‡ respectively. <COT> Looking at the "Method" column, we can see that the "KCE" method has the highest values for all metrics (P@01, P@05, P@10, AUC) compared to other methods (PageRank, Frequency, LeToR). 
<R> <C> [BOLD] Method Location <C> [BOLD] P@01 0.3555 <C> [BOLD] P@01 – <C> [BOLD] P@05 0.3077 <C> [BOLD] P@05 – <C> [BOLD] P@10 0.2505 <C> [BOLD] P@10 – <C> [BOLD] AUC 0.5226 <C> [BOLD] AUC – <R> <C> PageRank <C> 0.3628 <C> – <C> 0.3438 <C> – <C> 0.3007 <C> – <C> 0.5866 <C> – <R> <C> Frequency <C> 0.4542 <C> – <C> 0.4024 <C> – <C> 0.3445 <C> – <C> 0.5732 <C> – <R> <C> LeToR <C> 0.4753† <C> +4.64% <C> 0.4099† <C> +1.87% <C> 0.3517† <C> +2.10% <C> 0.6373† <C> +11.19% <R> <C> KCE (-EF) <C> 0.4420 <C> −2.69% <C> 0.4038 <C> +0.34% <C> 0.3464† <C> +0.54% <C> 0.6089† <C> +6.23% <R> <C> KCE (-E) <C> 0.4861†‡ <C> +7.01% <C> 0.4227†‡ <C> +5.04% <C> 0.3603†‡ <C> +4.58% <C> 0.6541†‡ <C> +14.12% <R> <C> KCE <C> 0.5049†‡ <C> +11.14% <C> 0.4277†‡ <C> +6.29% <C> 0.3638†‡ <C> +5.61% <C> 0.6557†‡ <C> +14.41% <R> <C> [BOLD] Method <C> [BOLD] R@01 <C> [BOLD] R@01 <C> [BOLD] R@05 <C> [BOLD] R@05 <C> [BOLD] R@10 <C> [BOLD] R@10 <C> [BOLD] W/T/L <C> [BOLD] W/T/L <R> <C> Location <C> 0.0807 <C> – <C> 0.2671 <C> – <C> 0.3792 <C> – <C> –/–/– <C> –/–/– <R> <C> PageRank <C> 0.0758 <C> – <C> 0.2760 <C> – <C> 0.4163 <C> – <C> –/–/– <C> –/–/– <R> <C> Frequency <C> 0.0792 <C> – <C> 0.2846 <C> – <C> 0.4270 <C> – <C> –/–/– <C> –/–/– <R> <C> LeToR <C> 0.0836† <C> +5.61% <C> 0.2980† <C> +4.70% <C> 0.4454† <C> +4.31% <C> 8037 / 48493 / 6770 <C> 8037 / 48493 / 6770 <R> <C> KCE (-EF) <C> 0.0714 <C> −9.77% <C> 0.2812 <C> −1.18% <C> 0.4321† <C> +1.20% <C> 6936 / 48811 / 7553 <C> 6936 / 48811 / 7553 <R> <C> KCE (-E) <C> 0.0925†‡ <C> +16.78% <C> 0.3172†‡ <C> +11.46% <C> 0.4672†‡ <C> +9.41% <C> 11676 / 43294 / 8330 <C> 11676 / 43294 / 8330 <R> <C> KCE <C> 0.0946†‡ <C> +19.44% <C> 0.3215†‡ <C> +12.96% <C> 0.4719†‡ <C> +10.51% <C> 12554 / 41461 / 9285 <C> 12554 / 41461 / 9285 <CAP> Table 3: Event salience performance. (-E) and (-F) marks removing Entity information and Features from the full KCM model. The relative performance differences are computed against Frequency. W/T/L are the number of documents a method wins, ties, and loses compared to Frequency. † and ‡ mark the statistically significant improvements over Frequency†, LeToR‡ respectively. <COT> Looking at the "Method" column, we can see that the "KCE (-E)" method has the highest values for all metrics (R@01, R@05, R@10, W/T/L) compared to other methods (Location, PageRank, Frequency, LeToR). 
<R> <C> [BOLD] Feature Groups <C> P@1 <C> P@5 <C> P@10 <C> R@1 <C> R@5 <C> R@10 <C> AUC <R> <C> Loc <C> 0.3548 <C> 0.3069 <C> 0.2497 <C> 0.0807 <C> 0.2671 <C> 0.3792 <C> 0.5226 <R> <C> Frequency <C> 0.4536 <C> 0.4018 <C> 0.3440 <C> 0.0792 <C> 0.2846 <C> 0.4270 <C> 0.5732 <R> <C> + Loc <C> 0.4734 <C> 0.4097 <C> 0.3513 <C> 0.0835 <C> 0.2976 <C> 0.4436 <C> 0.6354 <R> <C> + Loc + Event <C> 0.4726 <C> 0.4101† <C> 0.3516 <C> 0.0831 <C> 0.2969 <C> 0.4431 <C> 0.6365† <R> <C> + Loc + Entity <C> 0.4739 <C> 0.4100 <C> 0.3518 <C> 0.0812 <C> 0.2955 <C> 0.4418 <C> 0.6374 <R> <C> + Loc + Entity + Event <C> 0.4739 <C> 0.4100 <C> 0.3518† <C> 0.0832 <C> 0.2974 <C> 0.4452† <C> 0.6374† <R> <C> + Loc + Entity + Event + Local <C> 0.4754† <C> 0.4100 <C> 0.3517† <C> 0.0837 <C> 0.2981 <C> 0.4454† <C> 0.6373† <CAP> Table 4: Feature Ablation Results. + sign indicates the additional features to Frequency. Loc is the sentence location feature. Event is the event voting feature. Entity is the entity voting feature. Local is the local entity voting feature. † marks the statistically significant improvements over + Loc. <COT> Looking at the "P@1" column, we can see that the performance of the model improves as more features are added.
<R> <C> [BOLD] Feature Groups <C> P@1 <C> P@5 <C> P@10 <C> R@1 <C> R@5 <C> R@10 <C> AUC <R> <C> Loc <C> 0.3548 <C> 0.3069 <C> 0.2497 <C> 0.0807 <C> 0.2671 <C> 0.3792 <C> 0.5226 <R> <C> Frequency <C> 0.4536 <C> 0.4018 <C> 0.3440 <C> 0.0792 <C> 0.2846 <C> 0.4270 <C> 0.5732 <R> <C> + Loc <C> 0.4734 <C> 0.4097 <C> 0.3513 <C> 0.0835 <C> 0.2976 <C> 0.4436 <C> 0.6354 <R> <C> + Loc + Event <C> 0.4726 <C> 0.4101† <C> 0.3516 <C> 0.0831 <C> 0.2969 <C> 0.4431 <C> 0.6365† <R> <C> + Loc + Entity <C> 0.4739 <C> 0.4100 <C> 0.3518 <C> 0.0812 <C> 0.2955 <C> 0.4418 <C> 0.6374 <R> <C> + Loc + Entity + Event <C> 0.4739 <C> 0.4100 <C> 0.3518† <C> 0.0832 <C> 0.2974 <C> 0.4452† <C> 0.6374† <R> <C> + Loc + Entity + Event + Local <C> 0.4754† <C> 0.4100 <C> 0.3517† <C> 0.0837 <C> 0.2981 <C> 0.4454† <C> 0.6373† <CAP> Table 4: Feature Ablation Results. + sign indicates the additional features to Frequency. Loc is the sentence location feature. Event is the event voting feature. Entity is the entity voting feature. Local is the local entity voting feature. † marks the statistically significant improvements over + Loc. <COT> Looking at the "AUC" column, we can see that the AUC increases as more features are added.
<R> <C> Attribute <C> # <R> <C> Grammatical error present, incl. run-ons <C> 16 <R> <C> Is an auxiliary or light verb <C> 14 <R> <C> Annotation is incorrect <C> 13 <R> <C> Future event <C> 12 <R> <C> Is a question <C> 5 <R> <C> Is an imperative <C> 3 <R> <C> Is not an event or state <C> 2 <R> <C> One or more of the above <C> 43 <CAP> Table 7: Notable attributes of 50 instances from UDS-IH2-dev with highest absolute prediction error (using H-biLSTM(2)-MultiSim w/UDS-IH2). <COT> Looking at the "Attribute" column, we can see that there are various attributes listed for the instances from UDS-IH2-dev dataset.
<R> <C> Attribute <C> # <R> <C> Grammatical error present, incl. run-ons <C> 16 <R> <C> Is an auxiliary or light verb <C> 14 <R> <C> Annotation is incorrect <C> 13 <R> <C> Future event <C> 12 <R> <C> Is a question <C> 5 <R> <C> Is an imperative <C> 3 <R> <C> Is not an event or state <C> 2 <R> <C> One or more of the above <C> 43 <CAP> Table 7: Notable attributes of 50 instances from UDS-IH2-dev with highest absolute prediction error (using H-biLSTM(2)-MultiSim w/UDS-IH2). <COT> Looking at the "One or more of the above" cell, we can see that there are 43 instances that have one or more of the listed attributes.
<R> <C> attack <C> kill <C> [BOLD] Word2Vec 0.69 <C> [BOLD] KCE 0.3 <R> <C> arrest <C> charge <C> 0.53 <C> 0.3 <R> <C> USA (E) <C> war <C> 0.46 <C> 0.3 <R> <C> 911 attack (E) <C> attack <C> 0.72 <C> 0.3 <R> <C> attack <C> trade <C> 0.42 <C> 0.9 <R> <C> hotel (E) <C> travel <C> 0.49 <C> 0.9 <R> <C> charge <C> murder <C> 0.49 <C> 0.7 <R> <C> business (E) <C> increase <C> 0.43 <C> 0.7 <R> <C> attack <C> walk <C> 0.44 <C> -0.3 <R> <C> people (E) <C> work <C> 0.40 <C> -0.3 <CAP> Table 5: Similarities between event entity pairs. Word2vec shows the cosine similarity in pre-trained embeddings. KCE lists their closest kernel mean after training. (E) marks entities. <COT> Looking at the "Word2Vec" column, we can see that the similarity score for the entity pair "911 attack (E)" and "attack" is 0.72.
<R> <C> attack <C> kill <C> [BOLD] Word2Vec 0.69 <C> [BOLD] KCE 0.3 <R> <C> arrest <C> charge <C> 0.53 <C> 0.3 <R> <C> USA (E) <C> war <C> 0.46 <C> 0.3 <R> <C> 911 attack (E) <C> attack <C> 0.72 <C> 0.3 <R> <C> attack <C> trade <C> 0.42 <C> 0.9 <R> <C> hotel (E) <C> travel <C> 0.49 <C> 0.9 <R> <C> charge <C> murder <C> 0.49 <C> 0.7 <R> <C> business (E) <C> increase <C> 0.43 <C> 0.7 <R> <C> attack <C> walk <C> 0.44 <C> -0.3 <R> <C> people (E) <C> work <C> 0.40 <C> -0.3 <CAP> Table 5: Similarities between event entity pairs. Word2vec shows the cosine similarity in pre-trained embeddings. KCE lists their closest kernel mean after training. (E) marks entities. <COT> Looking at the "KCE" column, we can see that the closest kernel mean after training for the entity pair "hotel (E)" and "travel" is 0.9.
<R> <C> Model <C> Self-BLEU (↓) <C> % Unique  [ITALIC] n-grams (↑) Self <C> % Unique  [ITALIC] n-grams (↑) Self <C> % Unique  [ITALIC] n-grams (↑) Self <C> % Unique  [ITALIC] n-grams (↑) WT103 <C> % Unique  [ITALIC] n-grams (↑) WT103 <C> % Unique  [ITALIC] n-grams (↑) WT103 <C> % Unique  [ITALIC] n-grams (↑) TBC <C> % Unique  [ITALIC] n-grams (↑) TBC <C> % Unique  [ITALIC] n-grams (↑) TBC <R> <C> Model <C> Self-BLEU (↓) <C> n=2 <C> n=3 <C> n=4 <C> n=2 <C> n=3 <C> n=4 <C> n=2 <C> n=3 <C> n=4 <R> <C> BERT (large) <C> 9.43 <C> 63.15 <C> 92.38 <C> 98.01 <C> 59.91 <C> 91.86 <C> 98.43 <C> 64.59 <C> 93.27 <C> 98.59 <R> <C> BERT (base) <C> 10.06 <C> 60.76 <C> 91.76 <C> 98.14 <C> 57.90 <C> 91.72 <C> 98.55 <C> 60.94 <C> 92.04 <C> 98.56 <R> <C> GPT <C> 40.02 <C> 31.13 <C> 67.01 <C> 87.28 <C> 33.71 <C> 72.86 <C> 91.12 <C> 25.74 <C> 65.04 <C> 88.42 <R> <C> WT103 <C> 9.80 <C> 70.29 <C> 94.36 <C> 99.05 <C> 56.19 <C> 88.05 <C> 97.44 <C> 68.35 <C> 94.20 <C> 99.23 <R> <C> TBC <C> 12.51 <C> 62.19 <C> 92.70 <C> 98.73 <C> 55.30 <C> 91.08 <C> 98.81 <C> 44.75 <C> 82.06 <C> 96.31 <CAP> Table 2: Self-BLEU and percent of generated n-grams that are unique relative to own generations (left) WikiText-103 test set (middle) a sample of 5000 sentences from Toronto Book Corpus (right). For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets. <COT> Looking at the "BERT (large)" row, the "Self-BLEU (↓)" value is 9.43 and the "% Unique n-grams (↑) Self" values for n=2, n=3, and n=4 are 63.15, 92.38, and 98.01 respectively. 
<R> <C> Model <C> Self-BLEU (↓) <C> % Unique  [ITALIC] n-grams (↑) Self <C> % Unique  [ITALIC] n-grams (↑) Self <C> % Unique  [ITALIC] n-grams (↑) Self <C> % Unique  [ITALIC] n-grams (↑) WT103 <C> % Unique  [ITALIC] n-grams (↑) WT103 <C> % Unique  [ITALIC] n-grams (↑) WT103 <C> % Unique  [ITALIC] n-grams (↑) TBC <C> % Unique  [ITALIC] n-grams (↑) TBC <C> % Unique  [ITALIC] n-grams (↑) TBC <R> <C> Model <C> Self-BLEU (↓) <C> n=2 <C> n=3 <C> n=4 <C> n=2 <C> n=3 <C> n=4 <C> n=2 <C> n=3 <C> n=4 <R> <C> BERT (large) <C> 9.43 <C> 63.15 <C> 92.38 <C> 98.01 <C> 59.91 <C> 91.86 <C> 98.43 <C> 64.59 <C> 93.27 <C> 98.59 <R> <C> BERT (base) <C> 10.06 <C> 60.76 <C> 91.76 <C> 98.14 <C> 57.90 <C> 91.72 <C> 98.55 <C> 60.94 <C> 92.04 <C> 98.56 <R> <C> GPT <C> 40.02 <C> 31.13 <C> 67.01 <C> 87.28 <C> 33.71 <C> 72.86 <C> 91.12 <C> 25.74 <C> 65.04 <C> 88.42 <R> <C> WT103 <C> 9.80 <C> 70.29 <C> 94.36 <C> 99.05 <C> 56.19 <C> 88.05 <C> 97.44 <C> 68.35 <C> 94.20 <C> 99.23 <R> <C> TBC <C> 12.51 <C> 62.19 <C> 92.70 <C> 98.73 <C> 55.30 <C> 91.08 <C> 98.81 <C> 44.75 <C> 82.06 <C> 96.31 <CAP> Table 2: Self-BLEU and percent of generated n-grams that are unique relative to own generations (left) WikiText-103 test set (middle) a sample of 5000 sentences from Toronto Book Corpus (right). For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets. <COT> Looking at the "WT103" row, the "Self-BLEU (↓)" value is 9.80 and the "% Unique n-grams (↑) WT103" values for n=2, n=3, and n=4 are 70.29, 94.36, and 99.05 respectively. 
<R> <C> Model <C> Corpus-BLEU (↑) WT103 <C> Corpus-BLEU (↑) TBC <C> PPL (↓) <R> <C> BERT (large) <C> 5.05 <C> 7.60 <C> 331.47 <R> <C> BERT (base) <C> 7.80 <C> 7.06 <C> 279.10 <R> <C> GPT <C> 10.81 <C> 30.75 <C> 154.29 <R> <C> WT103 <C> 17.48 <C> 6.57 <C> 54.00 <R> <C> TBC <C> 10.05 <C> 23.05 <C> 314.28 <CAP> Table 3: Quality metrics of model generations. Perplexity (PPL) is measured using an additional language model (Dauphin et al., 2016). For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets. <COT> Looking at the "Model" column, "BERT (large)" row, and "Corpus-BLEU (↑) WT103" cell, we can see that the Corpus-BLEU score for BERT (large) with the WT103 dataset is 5.05.
<R> <C> Model <C> Corpus-BLEU (↑) WT103 <C> Corpus-BLEU (↑) TBC <C> PPL (↓) <R> <C> BERT (large) <C> 5.05 <C> 7.60 <C> 331.47 <R> <C> BERT (base) <C> 7.80 <C> 7.06 <C> 279.10 <R> <C> GPT <C> 10.81 <C> 30.75 <C> 154.29 <R> <C> WT103 <C> 17.48 <C> 6.57 <C> 54.00 <R> <C> TBC <C> 10.05 <C> 23.05 <C> 314.28 <CAP> Table 3: Quality metrics of model generations. Perplexity (PPL) is measured using an additional language model (Dauphin et al., 2016). For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets. <COT> Looking at the "Model" column, "GPT" row, and "PPL (↓)" cell, we can see that the perplexity score for GPT is 154.29.
<R> <C> Methods <C> AIDA-B <C> MSNBC <C> AQUAINT <C> ACE2004 <C> CWEB <C> WIKI <C> Avg <R> <C> [ITALIC] Wikipedia <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Milne and Witten ( 2008 ) <C> - <C> 78 <C> 85 <C> 81 <C> 64.1 <C> [BOLD] 81.7 <C> 77.96 <R> <C> Ratinov et al. ( 2011a ) <C> - <C> 75 <C> 83 <C> 82 <C> 56.2 <C> 67.2 <C> 72.68 <R> <C> Hoffart et al. ( 2011 ) <C> - <C> 79 <C> 56 <C> 80 <C> 58.6 <C> 63 <C> 67.32 <R> <C> Cheng and Roth ( 2013 ) <C> - <C> 90 <C> 90 <C> 86 <C> 67.5 <C> 73.4 <C> 81.38 <R> <C> Chisholm and Hachey ( 2015 ) <C> 84.9 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [ITALIC] Wiki + unlab <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Lazic et al. ( 2015 ) <C> 86.4 <C> - <C> - <C> - <C> - <C> - <C> [EMPTY] <R> <C> Our model <C> [BOLD] 89.66 ±0.16 <C> [BOLD] 92.2 ±0.2 <C> [BOLD] 90.7 ±0.2 <C> [BOLD] 88.1 ±0.0 <C> [BOLD] 78.2 ±0.2 <C> [BOLD] 81.7 ±0.1 <C> [BOLD] 86.18 <R> <C> [ITALIC] Wiki + Extra supervision <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Chisholm and Hachey ( 2015 ) <C> 88.7 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [ITALIC] Fully-supervised (Wiki + <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] AIDA CoNLL train) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Guo and Barbosa ( 2016 ) <C> 89.0 <C> 92 <C> 87 <C> 88 <C> 77 <C> 84.5 <C> 85.7 <R> <C> Globerson et al. ( 2016 ) <C> 91.0 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> Yamada et al. ( 2016 ) <C> 91.5 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> Ganea and Hofmann ( 2017 ) <C> 92.22 ±0.14 <C> 93.7 ±0.1 <C> 88.5 ±0.4 <C> 88.5 ±0.3 <C> 77.9 ±0.1 <C> 77.5 ±0.1 <C> 85.22 <R> <C> Le and Titov ( 2018 ) <C> 93.07 ±0.27 <C> 93.9 ±0.2 <C> 88.3 ±0.6 <C> 89.9 ±0.8 <C> 77.5 ±0.1 <C> 78.0 ±0.1 <C> 85.5 <CAP> Table 1: F1 scores on six test sets. The last column, Avg, shows the average of F1 scores on MSNBC, AQUAINT, ACE2004, CWEB, and WIKI. <COT> Looking at the "Avg" column, we can see that "Our model" has an average F1 score of 86.18, which is higher than the average F1 scores of other methods.
<R> <C> Our model <C> AIDA-A <C> AIDA-B <C> Avg <R> <C> weakly-supervised <C> 88.05 <C> 89.66 <C> 86.18 <R> <C> fully-supervised <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> on Wikipedia <C> 87.23 <C> 87.83 <C> 85.84 <R> <C> on AIDA CoNLL <C> 91.34 <C> 91.87 <C> 84.55 <CAP> Table 2: F1 scores of our model when it is weakly-supervised and when it is fully-supervised on Wikipedia and on AIDA CoNLL. AIDA-A is our development set. Avg is the average of F1 scores on MSNBC, AQUAINT, ACE2004, CWEB, and WIKI. Each F1 is the mean of five runs. <COT> Looking at the "Table 2" caption, we can see that the table shows the F1 scores of the model when it is weakly-supervised and when it is fully-supervised on Wikipedia and on AIDA CoNLL. AIDA-A is the development set and Avg is the average of F1 scores on MSNBC, AQUAINT, ACE2004, CWEB, and WIKI.
<R> <C> Our model <C> AIDA-A <C> AIDA-B <C> Avg <R> <C> weakly-supervised <C> 88.05 <C> 89.66 <C> 86.18 <R> <C> fully-supervised <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> on Wikipedia <C> 87.23 <C> 87.83 <C> 85.84 <R> <C> on AIDA CoNLL <C> 91.34 <C> 91.87 <C> 84.55 <CAP> Table 2: F1 scores of our model when it is weakly-supervised and when it is fully-supervised on Wikipedia and on AIDA CoNLL. AIDA-A is our development set. Avg is the average of F1 scores on MSNBC, AQUAINT, ACE2004, CWEB, and WIKI. Each F1 is the mean of five runs. <COT> Looking at the "on AIDA CoNLL" row, we can see that the F1 score for the weakly-supervised model is 91.34 and the F1 score for the fully-supervised model is 91.87.
<R> <C> Model <C> AIDA-A <R> <C> Our model <C> 88.05 <R> <C> without local <C> 82.41 <R> <C> without attention <C> 86.82 <R> <C> No disambiguation model ( [ITALIC] sc) <C> 86.42 <CAP> Table 3: Ablation study on AIDA CoNLL development set. Each F1 score is the mean of five runs. <COT> Looking at the "Model" column, the table shows the performance of different models on the AIDA CoNLL development set.
<R> <C> Model <C> AIDA-A <R> <C> Our model <C> 88.05 <R> <C> without local <C> 82.41 <R> <C> without attention <C> 86.82 <R> <C> No disambiguation model ( [ITALIC] sc) <C> 86.42 <CAP> Table 3: Ablation study on AIDA CoNLL development set. Each F1 score is the mean of five runs. <COT> Looking at the "Our model" cell, the F1 score is 88.05.
<R> <C> Type <C> Our model <C> Fully-supervised learning <R> <C> [EMPTY] <C> [EMPTY] <C> on AIDA CoNLL <R> <C> LOC <C> 85.53 <C> 89.41 <R> <C> MISC <C> 75.71 <C> 83.27 <R> <C> ORG <C> 89.51 <C> 92.70 <R> <C> PER <C> 97.20 <C> 97.73 <CAP> Table 4: Accuracy (%) by NER type on AIDA-A. <COT> Looking at the "Type" column, we can see that the table shows the accuracy (%) of our model and fully-supervised learning on different Named Entity Recognition (NER) types on AIDA-A dataset.
<R> <C> Type <C> Our model <C> Fully-supervised learning <R> <C> [EMPTY] <C> [EMPTY] <C> on AIDA CoNLL <R> <C> LOC <C> 85.53 <C> 89.41 <R> <C> MISC <C> 75.71 <C> 83.27 <R> <C> ORG <C> 89.51 <C> 92.70 <R> <C> PER <C> 97.20 <C> 97.73 <CAP> Table 4: Accuracy (%) by NER type on AIDA-A. <COT> Looking at the "Our model" column, we can see that our model performs better than fully-supervised learning on all NER types.
<R> <C> [EMPTY] <C> [EMPTY] <C> All Token- <C> All MWE- <C> Discontinuous <C> Discontinuous <C> Discontinuous <C> Discontinuous <R> <C> [EMPTY] <C> [EMPTY] <C> based <C> based <C> [EMPTY] <C> MWE-based <C> MWE-based <C> MWE-based <R> <C> L <C> model <C> F <C> F <C> % <C> P <C> R <C> F <R> <C> EN <C> baseline <C> 41.37 <C> 35.38 <C> 32 <C> 24.44 <C> 10.48 <C> 14.67 <R> <C> [EMPTY] <C> GCN-based <C> 39.78 <C> 39.11 <C> 32 <C> 39.53 <C> 16.19 <C> 22.97 <R> <C> [EMPTY] <C> Att-based <C> 33.33 <C> 31.79 <C> 32 <C> 46.88 <C> 14.29 <C> 21.90 <R> <C> [EMPTY] <C> H-combined <C> 41.63 <C> [BOLD] 40.76 <C> 32 <C> 63.33 <C> 18.10 <C> [BOLD] 28.15 <R> <C> DE <C> baseline <C> 62.27 <C> 57.17 <C> 43 <C> 69.50 <C> 45.37 <C> 54.90 <R> <C> [EMPTY] <C> GCN-based <C> 65.48 <C> [BOLD] 61.17 <C> 43 <C> 65.19 <C> 47.69 <C> 55.08 <R> <C> [EMPTY] <C> Att-based <C> 61.20 <C> 58.19 <C> 43 <C> 67.86 <C> 43.98 <C> 53.37 <R> <C> [EMPTY] <C> H-combined <C> 63.80 <C> 60.71 <C> 43 <C> 68.59 <C> 49.54 <C> [BOLD] 57.53 <R> <C> FR <C> baseline <C> 76.62 <C> 72.16 <C> 43 <C> 75.27 <C> 52.04 <C> 61.54 <R> <C> [EMPTY] <C> GCN-based <C> 79.59 <C> 75.15 <C> 43 <C> 79.58 <C> 56.51 <C> 66.09 <R> <C> [EMPTY] <C> Att-based <C> 78.21 <C> 74.23 <C> 43 <C> 71.49 <C> 60.59 <C> 65.59 <R> <C> [EMPTY] <C> H-combined <C> 80.25 <C> [BOLD] 76.56 <C> 43 <C> 77.94 <C> 59.11 <C> [BOLD] 67.23 <R> <C> FA <C> baseline <C> 88.45 <C> 86.50 <C> 14 <C> 67.76 <C> 55.88 <C> 61.29 <R> <C> [EMPTY] <C> GCN-based <C> 87.78 <C> 86.42 <C> 14 <C> 78.72 <C> 54.41 <C> 64.35 <R> <C> [EMPTY] <C> Att-based <C> 87.55 <C> 84.20 <C> 14 <C> 62.32 <C> 63.24 <C> 62.77 <R> <C> [EMPTY] <C> H-combined <C> 88.76 <C> [BOLD] 87.15 <C> 14 <C> 75.44 <C> 63.24 <C> [BOLD] 68.80 <CAP> Table 1: Model performance (P, R and F) for development sets for all MWE and only discontinuous ones (%: proportion of discontinuous MWES) <COT> Looking at the "H-combined" row and the "F" column, we can see that the F-score for H-combined is consistently higher than the baselines and other models for all languages (EN, DE, FR, FA).
<R> <C> [EMPTY] <C> [EMPTY] <C> All Token- <C> All MWE- <C> Discontinuous <C> Discontinuous <C> Discontinuous <C> Discontinuous <R> <C> [EMPTY] <C> [EMPTY] <C> based <C> based <C> [EMPTY] <C> MWE-based <C> MWE-based <C> MWE-based <R> <C> L <C> model <C> F <C> F <C> % <C> P <C> R <C> F <R> <C> EN <C> baseline <C> 41.37 <C> 35.38 <C> 32 <C> 24.44 <C> 10.48 <C> 14.67 <R> <C> [EMPTY] <C> GCN-based <C> 39.78 <C> 39.11 <C> 32 <C> 39.53 <C> 16.19 <C> 22.97 <R> <C> [EMPTY] <C> Att-based <C> 33.33 <C> 31.79 <C> 32 <C> 46.88 <C> 14.29 <C> 21.90 <R> <C> [EMPTY] <C> H-combined <C> 41.63 <C> [BOLD] 40.76 <C> 32 <C> 63.33 <C> 18.10 <C> [BOLD] 28.15 <R> <C> DE <C> baseline <C> 62.27 <C> 57.17 <C> 43 <C> 69.50 <C> 45.37 <C> 54.90 <R> <C> [EMPTY] <C> GCN-based <C> 65.48 <C> [BOLD] 61.17 <C> 43 <C> 65.19 <C> 47.69 <C> 55.08 <R> <C> [EMPTY] <C> Att-based <C> 61.20 <C> 58.19 <C> 43 <C> 67.86 <C> 43.98 <C> 53.37 <R> <C> [EMPTY] <C> H-combined <C> 63.80 <C> 60.71 <C> 43 <C> 68.59 <C> 49.54 <C> [BOLD] 57.53 <R> <C> FR <C> baseline <C> 76.62 <C> 72.16 <C> 43 <C> 75.27 <C> 52.04 <C> 61.54 <R> <C> [EMPTY] <C> GCN-based <C> 79.59 <C> 75.15 <C> 43 <C> 79.58 <C> 56.51 <C> 66.09 <R> <C> [EMPTY] <C> Att-based <C> 78.21 <C> 74.23 <C> 43 <C> 71.49 <C> 60.59 <C> 65.59 <R> <C> [EMPTY] <C> H-combined <C> 80.25 <C> [BOLD] 76.56 <C> 43 <C> 77.94 <C> 59.11 <C> [BOLD] 67.23 <R> <C> FA <C> baseline <C> 88.45 <C> 86.50 <C> 14 <C> 67.76 <C> 55.88 <C> 61.29 <R> <C> [EMPTY] <C> GCN-based <C> 87.78 <C> 86.42 <C> 14 <C> 78.72 <C> 54.41 <C> 64.35 <R> <C> [EMPTY] <C> Att-based <C> 87.55 <C> 84.20 <C> 14 <C> 62.32 <C> 63.24 <C> 62.77 <R> <C> [EMPTY] <C> H-combined <C> 88.76 <C> [BOLD] 87.15 <C> 14 <C> 75.44 <C> 63.24 <C> [BOLD] 68.80 <CAP> Table 1: Model performance (P, R and F) for development sets for all MWE and only discontinuous ones (%: proportion of discontinuous MWES) <COT> Looking at the "Discontinuous" column, we can see that the proportion of discontinuous MWEs is consistently 32% for all models and languages.
<R> <C> Verb <C> L-biLSTM(2)-S <C> +lexfeats <C> # <R> <C> decide to <C> 3.28 <C> 2.66 <C> 2 <R> <C> forget to <C> 0.67 <C> 0.48 <C> 2 <R> <C> get to <C> 1.55 <C> 1.43 <C> 9 <R> <C> hope to <C> 1.35 <C> 1.23 <C> 5 <R> <C> intend to <C> 1.18 <C> 0.61 <C> 1 <R> <C> promise to <C> 0.40 <C> 0.49 <C> 1 <R> <C> try to <C> 1.14 <C> 1.42 <C> 12 <R> <C> want to <C> 1.22 <C> 1.17 <C> 24 <CAP> Table 9: MAE of L-biLSTM(2)-S and L-biLSTM(2)-S+lexfeats, for predictions on events in UDS-IH2-dev that are xcomp-governed by an infinitival-taking verb. <COT> Looking at the table, we can see that the number of occurrences for each verb is different.
<R> <C> [EMPTY] <C> All | Discontinuous EN <C> All | Discontinuous DE <C> All | Discontinuous FR <C> All | Discontinuous FA <R> <C> baseline <C> 33.01 | 16.53 <C> 54.12 | 53.94 <C> 67.66 | 58.70 <C> [BOLD] 81.62 | 61.73 <R> <C> GCN-based <C> 36.27 |  [BOLD] 24.15 <C> 56.96 | 54.87 <C> 70.79 | 59.95 <C> 81.00 |  [BOLD] 62.35 <R> <C> H-combined <C> [BOLD] 41.91 | 22.73 <C> [BOLD] 59.29 |  [BOLD] 55.00 <C> [BOLD] 70.97 |  [BOLD] 63.90 <C> 80.04 | 61.90 <R> <C> ATILF-LLF <C> 31.58 | 09.91 <C> 54.43 | 40.34 <C> 58.60 | 51.96 <C> 77.48 | 53.85 <R> <C> SHOMA <C> 26.42 | 01.90 <C> 48.71 | 40.12 <C> 62.00 | 51.43 <C> 78.35 | 56.10 <CAP> Table 2: Comparing the performance of the systems on test data in terms of MWE-based F-score <COT> Looking at the "Table 2" caption, we can see that the table compares the performance of different systems on test data in terms of MWE-based F-score.
<R> <C> [EMPTY] <C> All | Discontinuous EN <C> All | Discontinuous DE <C> All | Discontinuous FR <C> All | Discontinuous FA <R> <C> baseline <C> 33.01 | 16.53 <C> 54.12 | 53.94 <C> 67.66 | 58.70 <C> [BOLD] 81.62 | 61.73 <R> <C> GCN-based <C> 36.27 |  [BOLD] 24.15 <C> 56.96 | 54.87 <C> 70.79 | 59.95 <C> 81.00 |  [BOLD] 62.35 <R> <C> H-combined <C> [BOLD] 41.91 | 22.73 <C> [BOLD] 59.29 |  [BOLD] 55.00 <C> [BOLD] 70.97 |  [BOLD] 63.90 <C> 80.04 | 61.90 <R> <C> ATILF-LLF <C> 31.58 | 09.91 <C> 54.43 | 40.34 <C> 58.60 | 51.96 <C> 77.48 | 53.85 <R> <C> SHOMA <C> 26.42 | 01.90 <C> 48.71 | 40.12 <C> 62.00 | 51.43 <C> 78.35 | 56.10 <CAP> Table 2: Comparing the performance of the systems on test data in terms of MWE-based F-score <COT> Looking at the "H-combined" row, we can see that it has the highest F-score for all languages.
<R> <C> [BOLD] Task <C> [BOLD] Avg <C> [BOLD] CoLA <C> [BOLD] SST <C> [BOLD] STS <C> [BOLD] QQP <C> [BOLD] MNLI <C> [BOLD] QNLI <R> <C> [BOLD] CoLA <C> 0.86 <C> 1.00 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] SST <C> 0.60 <C> 0.25 <C> 1.00 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] MRPC <C> 0.39 <C> 0.21 <C> 0.34 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] STS <C> -0.36 <C> -0.60 <C> 0.01 <C> 1.00 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] QQP <C> 0.61 <C> 0.61 <C> 0.27 <C> -0.58 <C> 1.00 <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] MNLI <C> 0.54 <C> 0.16 <C> 0.66 <C> 0.40 <C> 0.08 <C> 1.00 <C> [EMPTY] <R> <C> [BOLD] QNLI <C> 0.43 <C> 0.13 <C> 0.26 <C> 0.04 <C> 0.27 <C> 0.56 <C> 1.00 <R> <C> [BOLD] RTE <C> 0.34 <C> 0.08 <C> 0.16 <C> -0.10 <C> 0.04 <C> 0.14 <C> 0.32 <R> <C> [BOLD] WNLI <C> -0.21 <C> -0.21 <C> -0.37 <C> 0.31 <C> -0.37 <C> -0.07 <C> -0.26 <CAP> Table 4: Pearson correlations between performances on a subset of all pairs of target tasks, measured over all runs reported in Table 2. The Avg column shows the correlation between performance on a target task and the overall GLUE score. For QQP and STS, the correlations are computed respectively using F1 and Pearson correlation. Negative correlations are underlined. <COT> Looking at the "Task" column, we can see that the tasks are CoLA, SST, MRPC, STS, QQP, MNLI, QNLI, RTE, and WNLI.
<R> <C> [BOLD] Task <C> [BOLD] Avg <C> [BOLD] CoLA <C> [BOLD] SST <C> [BOLD] STS <C> [BOLD] QQP <C> [BOLD] MNLI <C> [BOLD] QNLI <R> <C> [BOLD] CoLA <C> 0.86 <C> 1.00 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] SST <C> 0.60 <C> 0.25 <C> 1.00 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] MRPC <C> 0.39 <C> 0.21 <C> 0.34 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] STS <C> -0.36 <C> -0.60 <C> 0.01 <C> 1.00 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] QQP <C> 0.61 <C> 0.61 <C> 0.27 <C> -0.58 <C> 1.00 <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] MNLI <C> 0.54 <C> 0.16 <C> 0.66 <C> 0.40 <C> 0.08 <C> 1.00 <C> [EMPTY] <R> <C> [BOLD] QNLI <C> 0.43 <C> 0.13 <C> 0.26 <C> 0.04 <C> 0.27 <C> 0.56 <C> 1.00 <R> <C> [BOLD] RTE <C> 0.34 <C> 0.08 <C> 0.16 <C> -0.10 <C> 0.04 <C> 0.14 <C> 0.32 <R> <C> [BOLD] WNLI <C> -0.21 <C> -0.21 <C> -0.37 <C> 0.31 <C> -0.37 <C> -0.07 <C> -0.26 <CAP> Table 4: Pearson correlations between performances on a subset of all pairs of target tasks, measured over all runs reported in Table 2. The Avg column shows the correlation between performance on a target task and the overall GLUE score. For QQP and STS, the correlations are computed respectively using F1 and Pearson correlation. Negative correlations are underlined. <COT> Looking at the "Avg" column, we can see that it represents the correlation between performance on a target task and the overall GLUE score.
<R> <C> NE Tag <C> Frequency <R> <C> date, time, duration, set (temporal) <C> 54.28% <R> <C> number <C> 40.13% <R> <C> [ITALIC]  Relation cardinality <C> [ITALIC] 18.86% <R> <C> percent <C> 2.92% <R> <C> money <C> 2.25% <R> <C> person, location, organization <C> 0.26% <R> <C> ordinal <C> 0.16% <CAP> Table 1: NE-tags of numbers in Wikipedia. <COT> Looking at the "Frequency" column, we can see that the NE tag "date, time, duration, set (temporal)" has the highest frequency.
<R> <C> NE Tag <C> Frequency <R> <C> date, time, duration, set (temporal) <C> 54.28% <R> <C> number <C> 40.13% <R> <C> [ITALIC]  Relation cardinality <C> [ITALIC] 18.86% <R> <C> percent <C> 2.92% <R> <C> money <C> 2.25% <R> <C> person, location, organization <C> 0.26% <R> <C> ordinal <C> 0.16% <CAP> Table 1: NE-tags of numbers in Wikipedia. <COT> Looking at the "Frequency" column, we can see that the NE tag "ordinal" has the lowest frequency.
<R> <C> [ITALIC] p <C> # [ITALIC] s <C> [ITALIC] baseline P <C> [ITALIC] vanilla P <C> [ITALIC] vanilla R <C> [ITALIC] vanilla F1 <C> [ITALIC] only-nummod P <C> [ITALIC] only-nummod R <C> [ITALIC] only-nummod F1 <R> <C> has part (creative work series) <C> 261 <C> .050 <C> .333 <C> .316 <C> .324 <C> .353 <C> .316 <C> .333 <R> <C> contains admin. terr. entity <C> 18,000 <C> .034 <C> .390 <C> .188 <C> .254 <C> .548 <C> .200 <C> .293 <R> <C> spouse <C> 45,917 <C> 0 <C> .014 <C> .011 <C> .013 <C> .028 <C> .017 <C> .021 <R> <C> child <C> 35,057 <C> .112 <C> .151 <C> .129 <C> .139 <C> .320 <C> .219 <C> .260 <R> <C> child (manual ground truth) <C> 6,408 <C> [EMPTY] <C> 0.374 <C> 0.309 <C> 0.338 <C> 0.452 <C> 0.315 <C> 0.371 <CAP> Table 2: Number of Wikidata entities as subjects (#s) of each predicate (p), and evaluation results on manually annotated randomly selected subjects that have at least an object. <COT> Looking at the "child (manual ground truth)" row, we can see that the precision, recall, and F1 score for the "only-nummod" predicate are higher compared to the "vanilla" predicate.
<R> <C> [ITALIC] p <C> # [ITALIC] s <C> [ITALIC] baseline P <C> [ITALIC] vanilla P <C> [ITALIC] vanilla R <C> [ITALIC] vanilla F1 <C> [ITALIC] only-nummod P <C> [ITALIC] only-nummod R <C> [ITALIC] only-nummod F1 <R> <C> has part (creative work series) <C> 261 <C> .050 <C> .333 <C> .316 <C> .324 <C> .353 <C> .316 <C> .333 <R> <C> contains admin. terr. entity <C> 18,000 <C> .034 <C> .390 <C> .188 <C> .254 <C> .548 <C> .200 <C> .293 <R> <C> spouse <C> 45,917 <C> 0 <C> .014 <C> .011 <C> .013 <C> .028 <C> .017 <C> .021 <R> <C> child <C> 35,057 <C> .112 <C> .151 <C> .129 <C> .139 <C> .320 <C> .219 <C> .260 <R> <C> child (manual ground truth) <C> 6,408 <C> [EMPTY] <C> 0.374 <C> 0.309 <C> 0.338 <C> 0.452 <C> 0.315 <C> 0.371 <CAP> Table 2: Number of Wikidata entities as subjects (#s) of each predicate (p), and evaluation results on manually annotated randomly selected subjects that have at least an object. <COT> Looking at the "child" row, we can see that the number of Wikidata entities as subjects (#s) is higher compared to the "spouse" and "contains admin. terr. entity" predicates.
<R> <C> Model <C> Model <C> Ara <C> Baq <C> Fre <C> Ger <C> Heb <C> Hun <C> Kor <C> Pol <C> Swe <C> Avg <R> <C> Int <C> WORD <C> 84.50 <C> 77.87 <C> 82.20 <C> 85.35 <C> [BOLD] 74.68 <C> 76.17 <C> 84.62 <C> 80.71 <C> 79.14 <C> 80.58 <R> <C> Int <C> W2V <C> [BOLD] 85.11 <C> 79.07 <C> [BOLD] 82.73 <C> [BOLD] 86.60 <C> 74.55 <C> 78.21 <C> 85.30 <C> 82.37 <C> 79.67 <C> 81.51 <R> <C> Int <C> LSTM <C> 83.42 <C> 82.97 <C> 81.35 <C> 85.34 <C> 74.03 <C> 83.06 <C> 86.56 <C> 80.13 <C> 77.44 <C> 81.48 <R> <C> Int <C> CNN <C> 84.65 <C> [BOLD] 83.91 <C> 82.41 <C> 85.61 <C> 74.23 <C> [BOLD] 83.68 <C> [BOLD] 86.99 <C> [BOLD] 83.28 <C> [BOLD] 80.00 <C> [BOLD] 82.75 <R> <C> Int <C> LSTM+WORD <C> [BOLD] 84.75 <C> 83.43 <C> [BOLD] 82.25 <C> 85.56 <C> 74.62 <C> 83.43 <C> 86.85 <C> 82.30 <C> 79.85 <C> 82.56 <R> <C> Int <C> CNN+WORD <C> 84.58 <C> [BOLD] 84.22 <C> 81.79 <C> [BOLD] 85.85 <C> [BOLD] 74.79 <C> [BOLD] 83.51 <C> [BOLD] 87.21 <C> [BOLD] 83.66 <C> [BOLD] 80.52 <C> [BOLD] 82.90 <R> <C> Int <C> LSTM+W2V <C> 85.35 <C> 83.94 <C> 83.04 <C> 86.38 <C> [BOLD] 75.15 <C> 83.30 <C> 87.35 <C> 83.00 <C> 79.38 <C> 82.99 <R> <C> Int <C> CNN+W2V <C> [BOLD] 85.67 <C> [BOLD] 84.37 <C> [BOLD] 83.09 <C> [BOLD] 86.81 <C> 74.95 <C> [BOLD] 84.08 <C> [BOLD] 87.72 <C> [BOLD] 84.44 <C> [BOLD] 80.35 <C> [BOLD] 83.50 <R> <C> Ext <C> B15-WORD <C> [BOLD] 83.46 <C> 73.56 <C> [BOLD] 82.03 <C> [BOLD] 84.62 <C> [BOLD] 72.70 <C> 69.31 <C> 83.37 <C> [BOLD] 79.83 <C> [BOLD] 76.40 <C> 78.36 <R> <C> Ext <C> B15-LSTM <C> 83.40 <C> [BOLD] 78.61 <C> 81.08 <C> 84.49 <C> 72.26 <C> [BOLD] 76.34 <C> [BOLD] 86.21 <C> 78.24 <C> 74.47 <C> [BOLD] 79.46 <R> <C> Ext <C> BestPub <C> 86.21 <C> 85.70 <C> 85.66 <C> 89.65 <C> 81.65 <C> 86.13 <C> 87.27 <C> 87.07 <C> 82.75 <C> 85.79 <CAP> Table 1: LAS on the test sets, the best LAS in each group is marked in bold face. <COT> Looking at the "Avg" column, we can see that the average LAS score for the "Int" models is higher than the average LAS score for the "Ext" models.
<R> <C> Model <C> Model <C> Ara <C> Baq <C> Fre <C> Ger <C> Heb <C> Hun <C> Kor <C> Pol <C> Swe <C> Avg <R> <C> Int <C> WORD <C> 84.50 <C> 77.87 <C> 82.20 <C> 85.35 <C> [BOLD] 74.68 <C> 76.17 <C> 84.62 <C> 80.71 <C> 79.14 <C> 80.58 <R> <C> Int <C> W2V <C> [BOLD] 85.11 <C> 79.07 <C> [BOLD] 82.73 <C> [BOLD] 86.60 <C> 74.55 <C> 78.21 <C> 85.30 <C> 82.37 <C> 79.67 <C> 81.51 <R> <C> Int <C> LSTM <C> 83.42 <C> 82.97 <C> 81.35 <C> 85.34 <C> 74.03 <C> 83.06 <C> 86.56 <C> 80.13 <C> 77.44 <C> 81.48 <R> <C> Int <C> CNN <C> 84.65 <C> [BOLD] 83.91 <C> 82.41 <C> 85.61 <C> 74.23 <C> [BOLD] 83.68 <C> [BOLD] 86.99 <C> [BOLD] 83.28 <C> [BOLD] 80.00 <C> [BOLD] 82.75 <R> <C> Int <C> LSTM+WORD <C> [BOLD] 84.75 <C> 83.43 <C> [BOLD] 82.25 <C> 85.56 <C> 74.62 <C> 83.43 <C> 86.85 <C> 82.30 <C> 79.85 <C> 82.56 <R> <C> Int <C> CNN+WORD <C> 84.58 <C> [BOLD] 84.22 <C> 81.79 <C> [BOLD] 85.85 <C> [BOLD] 74.79 <C> [BOLD] 83.51 <C> [BOLD] 87.21 <C> [BOLD] 83.66 <C> [BOLD] 80.52 <C> [BOLD] 82.90 <R> <C> Int <C> LSTM+W2V <C> 85.35 <C> 83.94 <C> 83.04 <C> 86.38 <C> [BOLD] 75.15 <C> 83.30 <C> 87.35 <C> 83.00 <C> 79.38 <C> 82.99 <R> <C> Int <C> CNN+W2V <C> [BOLD] 85.67 <C> [BOLD] 84.37 <C> [BOLD] 83.09 <C> [BOLD] 86.81 <C> 74.95 <C> [BOLD] 84.08 <C> [BOLD] 87.72 <C> [BOLD] 84.44 <C> [BOLD] 80.35 <C> [BOLD] 83.50 <R> <C> Ext <C> B15-WORD <C> [BOLD] 83.46 <C> 73.56 <C> [BOLD] 82.03 <C> [BOLD] 84.62 <C> [BOLD] 72.70 <C> 69.31 <C> 83.37 <C> [BOLD] 79.83 <C> [BOLD] 76.40 <C> 78.36 <R> <C> Ext <C> B15-LSTM <C> 83.40 <C> [BOLD] 78.61 <C> 81.08 <C> 84.49 <C> 72.26 <C> [BOLD] 76.34 <C> [BOLD] 86.21 <C> 78.24 <C> 74.47 <C> [BOLD] 79.46 <R> <C> Ext <C> BestPub <C> 86.21 <C> 85.70 <C> 85.66 <C> 89.65 <C> 81.65 <C> 86.13 <C> 87.27 <C> 87.07 <C> 82.75 <C> 85.79 <CAP> Table 1: LAS on the test sets, the best LAS in each group is marked in bold face. <COT> Looking at the "Ger" column, we can see that the LAS score for the "Ext" model with "BestPub" is higher than the LAS score for the "Int" models.
<R> <C> Model <C> Case <C> Ara <C> Baq <C> Fre <C> Ger <C> Heb <C> Hun <C> Kor <C> Pol <C> Swe <C> Avg <R> <C> CNN <C> ΔIV <C> 0.12 <C> 2.72 <C> -0.44 <C> 0.13 <C> -0.35 <C> 1.48 <C> 1.30 <C> 0.98 <C> 1.39 <C> 0.81 <R> <C> CNN <C> ΔOOV <C> 0.03 <C> 5.78 <C> 0.33 <C> 0.10 <C> -1.04 <C> 5.04 <C> 2.17 <C> 2.34 <C> 0.95 <C> 1.74 <R> <C> LSTM <C> ΔIV <C> -0.58 <C> 1.98 <C> -0.55 <C> -0.08 <C> -1.23 <C> 1.62 <C> 1.12 <C> -0.49 <C> 0.21 <C> 0.22 <R> <C> LSTM <C> ΔOOV <C> -0.32 <C> 5.09 <C> 0.12 <C> -0.21 <C> -1.99 <C> 4.74 <C> 1.51 <C> 0.10 <C> 0.38 <C> 1.05 <CAP> Table 2: LAS improvements by CNN and LSTM in the IV and OOV cases on the development sets. <COT> Looking at the "Model" column, we can see that there are two models mentioned: CNN and LSTM. 
<R> <C> train dataset <C> test dataset Belarusian <C> test dataset Russian <C> test dataset Ukrai-nian <R> <C> Belarusian <C> 647 <C> 326 <C> 373 <R> <C> Russian <C> 495 <C> 738 <C> 516 <R> <C> Ukrainian <C> 556 <C> 553 <C> 683 <R> <C> Ukrainian, Belarusian <C> 769 <C> 597 <C> 701 <R> <C> Russian, Belarusian <C> 740 <C> 740 <C> 563 <R> <C> Russian, Ukrainian <C> 627 <C> 756 <C> [BOLD] 700 <R> <C> Russian, Ukrainian, Belarusian <C> [BOLD] 772 <C> [BOLD] 760 <C> 698 <CAP> Table 3: Accuracy scores × 1000 for different train and test dataset combinations <COT> Looking at the "test dataset Ukrainian" column, we can see that the accuracy score for the combination of "Russian, Ukrainian" is [BOLD] 700.
<R> <C> train dataset <C> test dataset Belarusian <C> test dataset Russian <C> test dataset Ukrai-nian <R> <C> Belarusian <C> 647 <C> 326 <C> 373 <R> <C> Russian <C> 495 <C> 738 <C> 516 <R> <C> Ukrainian <C> 556 <C> 553 <C> 683 <R> <C> Ukrainian, Belarusian <C> 769 <C> 597 <C> 701 <R> <C> Russian, Belarusian <C> 740 <C> 740 <C> 563 <R> <C> Russian, Ukrainian <C> 627 <C> 756 <C> [BOLD] 700 <R> <C> Russian, Ukrainian, Belarusian <C> [BOLD] 772 <C> [BOLD] 760 <C> 698 <CAP> Table 3: Accuracy scores × 1000 for different train and test dataset combinations <COT> Looking at the "train dataset" row, we can see that the number of data points for the combination of "Russian, Ukrainian, Belarusian" is [BOLD] 772.
<R> <C> [BOLD] Model <C> [BOLD] BLEU  [BOLD] (All) <C> [BOLD] BLEU  [BOLD] (WSJ) <C> [BOLD] BLEU  [BOLD] (All overlap) <C> [BOLD] Exact  [BOLD] Match% <C> [BOLD] Coverage% <R> <C> Neural MRS (gold) <C> 66.11 <C> 73.12 <C> 69.27 <C> 24.09 <C> 100 <R> <C> Neural MRS (silver) <C> 75.43 <C> 81.76 <C> 77.13 <C> 25.82 <C> 100 <R> <C> Neural MRS (gold + silver) <C> 77.17 <C> 83.37 <C> 79.15 <C> 32.07 <C> 100 <R> <C> ACE (ERG) <C> – <C> – <C> 62.05 <C> 15.08 <C> 78 <R> <C> DAG transducer Ye et al. ( 2018 ) <C> – <C> 68.07 <C> – <C> – <C> 100 <CAP> Table 1: BLEU and exact-match scores over held-out test set <COT> Looking at the "Model" column, we can see that the "DAG transducer Ye et al. (2018)" model only has a value in the "BLEU (WSJ)" column, indicating that it only has a BLEU score for the WSJ subset of the test set.
<R> <C> [BOLD] Model <C> [BOLD] BLEU  [BOLD] (All) <C> [BOLD] BLEU  [BOLD] (WSJ) <C> [BOLD] BLEU  [BOLD] (All overlap) <C> [BOLD] Exact  [BOLD] Match% <C> [BOLD] Coverage% <R> <C> Neural MRS (gold) <C> 66.11 <C> 73.12 <C> 69.27 <C> 24.09 <C> 100 <R> <C> Neural MRS (silver) <C> 75.43 <C> 81.76 <C> 77.13 <C> 25.82 <C> 100 <R> <C> Neural MRS (gold + silver) <C> 77.17 <C> 83.37 <C> 79.15 <C> 32.07 <C> 100 <R> <C> ACE (ERG) <C> – <C> – <C> 62.05 <C> 15.08 <C> 78 <R> <C> DAG transducer Ye et al. ( 2018 ) <C> – <C> 68.07 <C> – <C> – <C> 100 <CAP> Table 1: BLEU and exact-match scores over held-out test set <COT> Looking at the "Exact Match%" column, we can see that all models have a value of 100, indicating that they have a perfect exact match with the reference data.
<R> <C> [BOLD] Parameter char emb size <C> [BOLD] Value 50 <C> [BOLD] Parameter bigram emb size <C> [BOLD] Value 50 <R> <C> lattice emb size <C> 50 <C> LSTM hidden <C> 200 <R> <C> char dropout <C> 0.5 <C> lattice dropout <C> 0.5 <R> <C> LSTM layer <C> 1 <C> regularization  [ITALIC] λ <C> 1e-8 <R> <C> learning rate  [ITALIC] lr <C> 0.015 <C> [ITALIC] lr decay <C> 0.05 <CAP> Table 3: Hyper-parameter values. <COT> Looking at the "Parameter char emb size" cell, the value is 50. Looking at the "Parameter bigram emb size" cell, the value is also 50.
<R> <C> [BOLD] Parameter char emb size <C> [BOLD] Value 50 <C> [BOLD] Parameter bigram emb size <C> [BOLD] Value 50 <R> <C> lattice emb size <C> 50 <C> LSTM hidden <C> 200 <R> <C> char dropout <C> 0.5 <C> lattice dropout <C> 0.5 <R> <C> LSTM layer <C> 1 <C> regularization  [ITALIC] λ <C> 1e-8 <R> <C> learning rate  [ITALIC] lr <C> 0.015 <C> [ITALIC] lr decay <C> 0.05 <CAP> Table 3: Hyper-parameter values. <COT> Looking at the "LSTM hidden" cell, the value is 200. Looking at the "LSTM layer" cell, the value is 1.
<R> <C> [BOLD] Input <C> [BOLD] Models <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Auto seg <C> Word baseline <C> 73.20 <C> 57.05 <C> 64.12 <R> <C> Auto seg <C> +char LSTM <C> 71.98 <C> 65.41 <C> 68.54 <R> <C> Auto seg <C> +char LSTM′ <C> 71.08 <C> 65.83 <C> 68.35 <R> <C> Auto seg <C> +char+bichar LSTM <C> 72.63 <C> 67.60 <C> 70.03 <R> <C> Auto seg <C> +char CNN <C> 73.06 <C> 66.29 <C> 69.51 <R> <C> Auto seg <C> +char+bichar CNN <C> 72.01 <C> 65.50 <C> 68.60 <R> <C> No seg <C> Char baseline <C> 67.12 <C> 58.42 <C> 62.47 <R> <C> No seg <C> +softword <C> 69.30 <C> 62.47 <C> 65.71 <R> <C> No seg <C> +bichar <C> 71.67 <C> 64.02 <C> 67.63 <R> <C> No seg <C> +bichar+softword <C> 72.64 <C> 66.89 <C> 69.64 <R> <C> No seg <C> Lattice <C> [BOLD] 74.64 <C> [BOLD] 68.83 <C> [BOLD] 71.62 <CAP> Table 4: Development results. <COT> Looking at the "Input" column, we can see that there are two types of input: "Auto seg" and "No seg". 
<R> <C> [BOLD] Input <C> [BOLD] Models <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Auto seg <C> Word baseline <C> 73.20 <C> 57.05 <C> 64.12 <R> <C> Auto seg <C> +char LSTM <C> 71.98 <C> 65.41 <C> 68.54 <R> <C> Auto seg <C> +char LSTM′ <C> 71.08 <C> 65.83 <C> 68.35 <R> <C> Auto seg <C> +char+bichar LSTM <C> 72.63 <C> 67.60 <C> 70.03 <R> <C> Auto seg <C> +char CNN <C> 73.06 <C> 66.29 <C> 69.51 <R> <C> Auto seg <C> +char+bichar CNN <C> 72.01 <C> 65.50 <C> 68.60 <R> <C> No seg <C> Char baseline <C> 67.12 <C> 58.42 <C> 62.47 <R> <C> No seg <C> +softword <C> 69.30 <C> 62.47 <C> 65.71 <R> <C> No seg <C> +bichar <C> 71.67 <C> 64.02 <C> 67.63 <R> <C> No seg <C> +bichar+softword <C> 72.64 <C> 66.89 <C> 69.64 <R> <C> No seg <C> Lattice <C> [BOLD] 74.64 <C> [BOLD] 68.83 <C> [BOLD] 71.62 <CAP> Table 4: Development results. <COT> Looking at the "Models" column, we can see that there are different models used for each input type.
<R> <C> [BOLD] Input <C> [BOLD] Models <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Gold seg <C> Yang et al. ( 2016 ) <C> 65.59 <C> 71.84 <C> 68.57 <R> <C> Gold seg <C> Yang et al. ( 2016 )*† <C> 72.98 <C> [BOLD] 80.15 <C> [BOLD] 76.40 <R> <C> Gold seg <C> Che et al. ( 2013 )* <C> 77.71 <C> 72.51 <C> 75.02 <R> <C> Gold seg <C> Wang et al. ( 2013 )* <C> 76.43 <C> 72.32 <C> 74.32 <R> <C> Gold seg <C> Word baseline <C> 76.66 <C> 63.60 <C> 69.52 <R> <C> Gold seg <C> +char+bichar LSTM <C> [BOLD] 78.62 <C> 73.13 <C> 75.77 <R> <C> Auto seg <C> Word baseline <C> 72.84 <C> 59.72 <C> 65.63 <R> <C> Auto seg <C> +char+bichar LSTM <C> 73.36 <C> 70.12 <C> 71.70 <R> <C> No seg <C> Char baseline <C> 68.79 <C> 60.35 <C> 64.30 <R> <C> No seg <C> +bichar+softword <C> 74.36 <C> 69.43 <C> 71.81 <R> <C> No seg <C> Lattice <C> [BOLD] 76.35 <C> [BOLD] 71.56 <C> [BOLD] 73.88 <CAP> Table 5: Main results on OntoNotes. <COT> Looking at the "Input" column, we can see that there are three different segmentation methods: Gold seg, Auto seg, and No seg.
<R> <C> [BOLD] Input <C> [BOLD] Models <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Gold seg <C> Yang et al. ( 2016 ) <C> 65.59 <C> 71.84 <C> 68.57 <R> <C> Gold seg <C> Yang et al. ( 2016 )*† <C> 72.98 <C> [BOLD] 80.15 <C> [BOLD] 76.40 <R> <C> Gold seg <C> Che et al. ( 2013 )* <C> 77.71 <C> 72.51 <C> 75.02 <R> <C> Gold seg <C> Wang et al. ( 2013 )* <C> 76.43 <C> 72.32 <C> 74.32 <R> <C> Gold seg <C> Word baseline <C> 76.66 <C> 63.60 <C> 69.52 <R> <C> Gold seg <C> +char+bichar LSTM <C> [BOLD] 78.62 <C> 73.13 <C> 75.77 <R> <C> Auto seg <C> Word baseline <C> 72.84 <C> 59.72 <C> 65.63 <R> <C> Auto seg <C> +char+bichar LSTM <C> 73.36 <C> 70.12 <C> 71.70 <R> <C> No seg <C> Char baseline <C> 68.79 <C> 60.35 <C> 64.30 <R> <C> No seg <C> +bichar+softword <C> 74.36 <C> 69.43 <C> 71.81 <R> <C> No seg <C> Lattice <C> [BOLD] 76.35 <C> [BOLD] 71.56 <C> [BOLD] 73.88 <CAP> Table 5: Main results on OntoNotes. <COT> Looking at the "Models" column, we can see that there are different models used for each segmentation method.
<R> <C> [BOLD] Models <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Chen et al. ( 2006a ) <C> 91.22 <C> 81.71 <C> 86.20 <R> <C> Zhang et al. ( 2006 )* <C> 92.20 <C> 90.18 <C> 91.18 <R> <C> Zhou et al. ( 2013 ) <C> 91.86 <C> 88.75 <C> 90.28 <R> <C> Lu et al. ( 2016 ) <C> – <C> – <C> 87.94 <R> <C> Dong et al. ( 2016 ) <C> 91.28 <C> 90.62 <C> 90.95 <R> <C> Word baseline <C> 90.57 <C> 83.06 <C> 86.65 <R> <C> +char+bichar LSTM <C> 91.05 <C> 89.53 <C> 90.28 <R> <C> Char baseline <C> 90.74 <C> 86.96 <C> 88.81 <R> <C> +bichar+softword <C> 92.97 <C> 90.80 <C> 91.87 <R> <C> Lattice <C> [BOLD] 93.57 <C> [BOLD] 92.79 <C> [BOLD] 93.18 <CAP> Table 6: Main results on MSRA. <COT> Looking at the "Models" column, we can see that the "Lattice" model has the highest F1 score compared to other models.
<R> <C> [BOLD] Models <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Chen et al. ( 2006a ) <C> 91.22 <C> 81.71 <C> 86.20 <R> <C> Zhang et al. ( 2006 )* <C> 92.20 <C> 90.18 <C> 91.18 <R> <C> Zhou et al. ( 2013 ) <C> 91.86 <C> 88.75 <C> 90.28 <R> <C> Lu et al. ( 2016 ) <C> – <C> – <C> 87.94 <R> <C> Dong et al. ( 2016 ) <C> 91.28 <C> 90.62 <C> 90.95 <R> <C> Word baseline <C> 90.57 <C> 83.06 <C> 86.65 <R> <C> +char+bichar LSTM <C> 91.05 <C> 89.53 <C> 90.28 <R> <C> Char baseline <C> 90.74 <C> 86.96 <C> 88.81 <R> <C> +bichar+softword <C> 92.97 <C> 90.80 <C> 91.87 <R> <C> Lattice <C> [BOLD] 93.57 <C> [BOLD] 92.79 <C> [BOLD] 93.18 <CAP> Table 6: Main results on MSRA. <COT> Looking at the "Chen et al. (2006a)" row, we can see that the precision, recall, and F1 score are all lower than the corresponding scores of the "Lattice" model.
<R> <C> [BOLD] Models <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Word baseline <C> 93.72 <C> 93.44 <C> 93.58 <R> <C> +char+bichar LSTM <C> 94.07 <C> 94.42 <C> 94.24 <R> <C> Char baseline <C> 93.66 <C> 93.31 <C> 93.48 <R> <C> +bichar+softword <C> 94.53 <C> [BOLD] 94.29 <C> 94.41 <R> <C> Lattice <C> [BOLD] 94.81 <C> 94.11 <C> [BOLD] 94.46 <CAP> Table 8: Main results on resume NER. <COT> Looking at the "Models" column, we can see that the "Lattice" model has the highest F1 score of 94.46.
<R> <C> [BOLD] Models <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Word baseline <C> 93.72 <C> 93.44 <C> 93.58 <R> <C> +char+bichar LSTM <C> 94.07 <C> 94.42 <C> 94.24 <R> <C> Char baseline <C> 93.66 <C> 93.31 <C> 93.48 <R> <C> +bichar+softword <C> 94.53 <C> [BOLD] 94.29 <C> 94.41 <R> <C> Lattice <C> [BOLD] 94.81 <C> 94.11 <C> [BOLD] 94.46 <CAP> Table 8: Main results on resume NER. <COT> Looking at the "P" column, we can see that the "Lattice" model has the highest precision score of 94.81.
<R> <C> [EMPTY] <C> chrF-1.0 <C> BLEU% <R> <C> en-et <C> dev <C> dev <R> <C> BPE <C> 56.52 <C> 17.93 <R> <C> monolingual <C> 53.44 <C> 15.82 <R> <C> Cognate Morfessor <C> 57.05 <C> 18.40 <R> <C> +finetuned <C> 57.23 <C> 18.45 <R> <C> +ensemble-of-5 <C> [BOLD] 57.75 <C> [BOLD] 19.09 <R> <C> +ensemble-of-3 <C> 57.64 <C> 18.96 <R> <C> +linked embeddings <C> 56.20 <C> 17.48 <R> <C> −LM filtering <C> 52.94 <C> 14.65 <R> <C> 6+6 layers <C> 57.35 <C> 18.84 <CAP> Table 3: Development set results for English–Estonian. character-F and BLEU scores in percentages. +/− stands for adding/removing a component. Multiple modifications are indicated by increasing the indentation. <COT> Looking at the "CAP" tag, we can see that the table provides development set results for English-Estonian in terms of character-F and BLEU scores.
<R> <C> [EMPTY] <C> chrF-1.0 <C> BLEU% <R> <C> en-et <C> dev <C> dev <R> <C> BPE <C> 56.52 <C> 17.93 <R> <C> monolingual <C> 53.44 <C> 15.82 <R> <C> Cognate Morfessor <C> 57.05 <C> 18.40 <R> <C> +finetuned <C> 57.23 <C> 18.45 <R> <C> +ensemble-of-5 <C> [BOLD] 57.75 <C> [BOLD] 19.09 <R> <C> +ensemble-of-3 <C> 57.64 <C> 18.96 <R> <C> +linked embeddings <C> 56.20 <C> 17.48 <R> <C> −LM filtering <C> 52.94 <C> 14.65 <R> <C> 6+6 layers <C> 57.35 <C> 18.84 <CAP> Table 3: Development set results for English–Estonian. character-F and BLEU scores in percentages. +/− stands for adding/removing a component. Multiple modifications are indicated by increasing the indentation. <COT> Looking at the "+ensemble-of-5" row, we can see that it has the highest character-F and BLEU scores.
<R> <C> [EMPTY] <C> Sestorain et al. ( 2018 )† PBSMT <C> Sestorain et al. ( 2018 )† NMT-0 <C> Sestorain et al. ( 2018 )† Dual-0 <C> Our baselines Basic <C> Our baselines Pivot <C> Agree <R> <C> \En → \Es <C> 61.26 <C> 51.93 <C> — <C> 56.58 <C> 56.58 <C> 56.36 <R> <C> \En → \Fr <C> 50.09 <C> 40.56 <C> — <C> 44.27 <C> 44.27 <C> 44.80 <R> <C> \Es → \En <C> 59.89 <C> 51.58 <C> — <C> 55.70 <C> 55.70 <C> 55.24 <R> <C> \Fr → \En <C> 52.22 <C> 43.33 <C> — <C> 46.46 <C> 46.46 <C> 46.17 <R> <C> Supervised (avg.) <C> 55.87 <C> 46.85 <C> — <C> 50.75 <C> 50.75 <C> 50.64 <R> <C> \Es → \Fr <C> 52.44 <C> 20.29 <C> 36.68 <C> 34.75 <C> [BOLD] 38.10 <C> 37.54 <R> <C> \Fr → \Es <C> 49.79 <C> 19.01 <C> 39.19 <C> 37.67 <C> [BOLD] 40.84 <C> 40.02 <R> <C> Zero-shot (avg.) <C> 51.11 <C> 19.69 <C> 37.93 <C> 36.21 <C> [BOLD] 39.47 <C> 38.78 <CAP> Table 1: Results on UNCorpus-1. <COT> Looking at the table, we can see that the highest score for the \Es → \Fr translation task is 38.10, which is marked as [BOLD].
<R> <C> [EMPTY] <C> Sestorain et al. ( 2018 )† PBSMT <C> Sestorain et al. ( 2018 )† NMT-0 <C> Sestorain et al. ( 2018 )† Dual-0 <C> Our baselines Basic <C> Our baselines Pivot <C> Agree <R> <C> \En → \Es <C> 61.26 <C> 51.93 <C> — <C> 56.58 <C> 56.58 <C> 56.36 <R> <C> \En → \Fr <C> 50.09 <C> 40.56 <C> — <C> 44.27 <C> 44.27 <C> 44.80 <R> <C> \Es → \En <C> 59.89 <C> 51.58 <C> — <C> 55.70 <C> 55.70 <C> 55.24 <R> <C> \Fr → \En <C> 52.22 <C> 43.33 <C> — <C> 46.46 <C> 46.46 <C> 46.17 <R> <C> Supervised (avg.) <C> 55.87 <C> 46.85 <C> — <C> 50.75 <C> 50.75 <C> 50.64 <R> <C> \Es → \Fr <C> 52.44 <C> 20.29 <C> 36.68 <C> 34.75 <C> [BOLD] 38.10 <C> 37.54 <R> <C> \Fr → \Es <C> 49.79 <C> 19.01 <C> 39.19 <C> 37.67 <C> [BOLD] 40.84 <C> 40.02 <R> <C> Zero-shot (avg.) <C> 51.11 <C> 19.69 <C> 37.93 <C> 36.21 <C> [BOLD] 39.47 <C> 38.78 <CAP> Table 1: Results on UNCorpus-1. <COT> Looking at the table, we can see that the average score for the supervised translation task is 50.64.
<R> <C> [BOLD] Ablation <C> [BOLD] BLEU <R> <C> All attributes <C> 72.06 <R> <C> No node attributes <C> 59.37 <R> <C> No node attr except num, tense <C> 67.34 <R> <C> No edge features <C> 71.27 <CAP> Table 3: Results of semantic feature ablation, model trained with gold data only <COT> Looking at the "Ablation" column, we can see that the "No node attributes" row has a lower BLEU score compared to the "All attributes" row.
<R> <C> [BOLD] Ablation <C> [BOLD] BLEU <R> <C> All attributes <C> 72.06 <R> <C> No node attributes <C> 59.37 <R> <C> No node attr except num, tense <C> 67.34 <R> <C> No edge features <C> 71.27 <CAP> Table 3: Results of semantic feature ablation, model trained with gold data only <COT> Looking at the "No edge features" row, we can see that the BLEU score is slightly lower compared to the "All attributes" row.
<R> <C> [EMPTY] <C> Sestorain et al. ( 2018 ) PBSMT <C> Sestorain et al. ( 2018 ) NMT-0 <C> Sestorain et al. ( 2018 ) Dual-0 <C> Our baselines Basic <C> Our baselines Pivot <C> Agree <R> <C> \En → \Es <C> 61.26 <C> 47.51 <C> 44.30 <C> 55.15 <C> 55.15 <C> 54.30 <R> <C> \En → \Fr <C> 50.09 <C> 36.70 <C> 34.34 <C> 43.42 <C> 43.42 <C> 42.57 <R> <C> \En → \Ru <C> 43.25 <C> 30.45 <C> 29.47 <C> 36.26 <C> 36.26 <C> 35.89 <R> <C> \Es → \En <C> 59.89 <C> 48.56 <C> 45.55 <C> 54.35 <C> 54.35 <C> 54.33 <R> <C> \Fr → \En <C> 52.22 <C> 40.75 <C> 37.75 <C> 45.55 <C> 45.55 <C> 45.87 <R> <C> \Ru → \En <C> 52.59 <C> 39.35 <C> 37.96 <C> 45.52 <C> 45.52 <C> 44.67 <R> <C> Supervised (avg.) <C> 53.22 <C> 40.55 <C> 36.74 <C> 46.71 <C> 46.71 <C> 46.27 <R> <C> \Es → \Fr <C> 52.44 <C> 25.85 <C> 34.51 <C> 34.73 <C> 35.93 <C> [BOLD] 36.02 <R> <C> \Fr → \Es <C> 49.79 <C> 22.68 <C> 37.71 <C> 38.20 <C> 39.51 <C> [BOLD] 39.94 <R> <C> \Es → \Ru <C> 39.69 <C> 9.36 <C> 24.55 <C> 26.29 <C> 27.15 <C> [BOLD] 28.08 <R> <C> \Ru → \Es <C> 49.61 <C> 26.26 <C> 33.23 <C> 33.43 <C> [BOLD] 37.17 <C> 35.01 <R> <C> \Fr → \Ru <C> 36.48 <C> 9.35 <C> 22.76 <C> 23.88 <C> 24.99 <C> [BOLD] 25.13 <R> <C> \Ru → \Fr <C> 43.37 <C> 22.43 <C> 26.49 <C> 28.52 <C> [BOLD] 30.06 <C> 29.53 <R> <C> Zero-shot (avg.) <C> 45.23 <C> 26.26 <C> 29.88 <C> 30.84 <C> [BOLD] 32.47 <C> 32.29 <CAP> Table 2: Results on UNCorpus-2. <COT> Looking at the "Supervised (avg.)" row, the "Pivot" column has the same value as the "Our baselines Pivot" column, which is 46.71.
<R> <C> [EMPTY] <C> Previous work Soft‡ <C> Previous work Distill† <C> Our baselines Basic <C> Our baselines Pivot <C> Agree <R> <C> \En → \Es <C> — <C> — <C> 34.69 <C> 34.69 <C> 33.80 <R> <C> \En → \De <C> — <C> — <C> 23.06 <C> 23.06 <C> 22.44 <R> <C> \En → \Fr <C> 31.40 <C> — <C> 33.87 <C> 33.87 <C> 32.55 <R> <C> \Es → \En <C> 31.96 <C> — <C> 34.77 <C> 34.77 <C> 34.53 <R> <C> \De → \En <C> 26.55 <C> — <C> 29.06 <C> 29.06 <C> 29.07 <R> <C> \Fr → \En <C> — <C> — <C> 33.67 <C> 33.67 <C> 33.30 <R> <C> Supervised (avg.) <C> — <C> — <C> 31.52 <C> 31.52 <C> 30.95 <R> <C> \Es → \De <C> — <C> — <C> 18.23 <C> 20.14 <C> [BOLD] 20.70 <R> <C> \De → \Es <C> — <C> — <C> 20.28 <C> [BOLD] 26.50 <C> 22.45 <R> <C> \Es → \Fr <C> 30.57 <C> [BOLD] 33.86 <C> 27.99 <C> 32.56 <C> 30.94 <R> <C> \Fr → \Es <C> — <C> — <C> 27.12 <C> [BOLD] 32.96 <C> 29.91 <R> <C> \De → \Fr <C> 23.79 <C> [BOLD] 27.03 <C> 21.36 <C> 25.67 <C> 24.45 <R> <C> \Fr → \De <C> — <C> — <C> 18.57 <C> [BOLD] 19.86 <C> 19.15 <R> <C> Zero-shot (avg.) <C> — <C> — <C> 22.25 <C> 26.28 <C> 24.60 <CAP> Table 3: Zero-shot results on Europarl. Note that Soft and Distill are not multilingual systems. <COT> Looking at the "Supervised (avg.)" row and the "Our baselines Basic" and "Our baselines Pivot" columns, we can see that the average performance of the supervised models is 31.52, while the average performance of our baselines (Basic and Pivot) is 30.95. This suggests that the supervised models perform slightly better than our baselines on average.
<R> <C> [EMPTY] <C> Previous work Soft‡ <C> Previous work Distill† <C> Our baselines Basic <C> Our baselines Pivot <C> Agree <R> <C> \En → \Es <C> — <C> — <C> 34.69 <C> 34.69 <C> 33.80 <R> <C> \En → \De <C> — <C> — <C> 23.06 <C> 23.06 <C> 22.44 <R> <C> \En → \Fr <C> 31.40 <C> — <C> 33.87 <C> 33.87 <C> 32.55 <R> <C> \Es → \En <C> 31.96 <C> — <C> 34.77 <C> 34.77 <C> 34.53 <R> <C> \De → \En <C> 26.55 <C> — <C> 29.06 <C> 29.06 <C> 29.07 <R> <C> \Fr → \En <C> — <C> — <C> 33.67 <C> 33.67 <C> 33.30 <R> <C> Supervised (avg.) <C> — <C> — <C> 31.52 <C> 31.52 <C> 30.95 <R> <C> \Es → \De <C> — <C> — <C> 18.23 <C> 20.14 <C> [BOLD] 20.70 <R> <C> \De → \Es <C> — <C> — <C> 20.28 <C> [BOLD] 26.50 <C> 22.45 <R> <C> \Es → \Fr <C> 30.57 <C> [BOLD] 33.86 <C> 27.99 <C> 32.56 <C> 30.94 <R> <C> \Fr → \Es <C> — <C> — <C> 27.12 <C> [BOLD] 32.96 <C> 29.91 <R> <C> \De → \Fr <C> 23.79 <C> [BOLD] 27.03 <C> 21.36 <C> 25.67 <C> 24.45 <R> <C> \Fr → \De <C> — <C> — <C> 18.57 <C> [BOLD] 19.86 <C> 19.15 <R> <C> Zero-shot (avg.) <C> — <C> — <C> 22.25 <C> 26.28 <C> 24.60 <CAP> Table 3: Zero-shot results on Europarl. Note that Soft and Distill are not multilingual systems. <COT> Looking at the "\Es → \De" row and the "Our baselines Pivot" column, we can see that the performance of our baselines (Pivot) is 20.14, while the performance of the supervised models is 18.23. This suggests that our baselines perform better than the supervised models in the \Es → \De translation task.
<R> <C> [EMPTY] <C> Previous work SOTA† <C> Previous work CPG‡ <C> Our baselines Basic <C> Our baselines Pivot <C> Agree <R> <C> Supervised (avg.) <C> 24.10 <C> 19.75 <C> 24.63 <C> 24.63 <C> 23.97 <R> <C> Zero-shot (avg.) <C> 20.55 <C> 11.69 <C> 19.86 <C> 19.26 <C> [BOLD] 20.58 <CAP> Table 4: Results on the official IWSLT17 multilingual task. <COT> Looking at the "Supervised (avg.)" cell, "24.10" cell, and "Our baselines Basic" cell, we can see that our baselines using the Basic approach achieve a similar performance to the supervised setting.
<R> <C> [EMPTY] <C> Basic <C> Pivot <C> Agree <R> <C> Supervised (avg.) <C> 28.72 <C> 28.72 <C> [BOLD] 29.17 <R> <C> Zero-shot (avg.) <C> 12.61 <C> [BOLD] 17.68 <C> 15.23 <CAP> Table 5: Results on our proposed IWSLT17? <COT> Looking at the "CAP" tag, we can see that Table 5 shows the results on the proposed IWSLT17.
<R> <C> [EMPTY] <C> Basic <C> Pivot <C> Agree <R> <C> Supervised (avg.) <C> 28.72 <C> 28.72 <C> [BOLD] 29.17 <R> <C> Zero-shot (avg.) <C> 12.61 <C> [BOLD] 17.68 <C> 15.23 <CAP> Table 5: Results on our proposed IWSLT17? <COT> Looking at the "Supervised (avg.)" column, we can see that the average score for the "Agree" system is higher than the average scores for the "Basic" and "Pivot" systems.
<R> <C> [EMPTY] <C> [BOLD] Europarl  [BOLD] En-Fr <C> [BOLD] Europarl  [BOLD] En-Fr <C> [BOLD] Europarl  [BOLD] En-Fr <C> [BOLD] Europarl  [BOLD] En-Et <C> [BOLD] Europarl  [BOLD] En-Et <C> [BOLD] Europarl  [BOLD] En-Et <C> [BOLD] Europarl  [BOLD] En-De <C> [BOLD] Europarl  [BOLD] En-De <C> [BOLD] Europarl  [BOLD] En-De <C> [BOLD] Subtitles  [BOLD] En-Ru <C> [BOLD] Subtitles  [BOLD] En-Ru <C> [BOLD] Subtitles  [BOLD] En-Ru <R> <C> [EMPTY] <C> Overall <C> En→Fr <C> Fr→En <C> Overall <C> En→Et <C> Et→En <C> Overall <C> En→De <C> De→En <C> Overall <C> En→Ru <C> Ru→En <R> <C> [ITALIC] Base Model <C> 37.36 <C> 38.13 <C> 36.03 <C> 20.68 <C> 18.64 <C> 26.65 <C> 24.74 <C> 21.80 <C> 27.74 <C> 19.05 <C> 14.90 <C> 23.04 <R> <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> InitDec <C> 38.40† <C> 39.19† <C> 36.86† <C> [BOLD] 21.79† <C> 19.54† <C> [BOLD] 28.33† <C> [BOLD] 26.34† <C> [BOLD] 23.31† <C> 29.39† <C> 18.88 <C> 14.89 <C> 22.56 <R> <C> AddDec <C> 38.50† <C> [BOLD] 39.35† <C> 36.98† <C> 21.65† <C> [BOLD] 19.66† <C> 27.48† <C> 26.30† <C> 23.09† <C> [BOLD] 29.52† <C> 19.34 <C> 15.16 <C> 23.12 <R> <C> InitDec+AddDec <C> [BOLD] 38.55† <C> 39.34† <C> [BOLD] 37.14† <C> 21.49† <C> 19.43† <C> 27.55† <C> 26.25† <C> 23.18† <C> 29.30† <C> [BOLD] 19.35 <C> [BOLD] 15.16 <C> [BOLD] 23.14 <R> <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Direct Tranformation <C> 38.35† <C> 39.13† <C> 36.96† <C> 21.75† <C> [BOLD] 19.59† <C> 28.07† <C> 26.29† <C> 23.34† <C> 29.22† <C> 19.09 <C> 14.89 <C> 22.76 <R> <C> Hierarchical Gating <C> 38.33† <C> 39.14† <C> 36.89† <C> 21.62† <C> 19.55† <C> 27.64† <C> 26.31† <C> 23.17† <C> 29.45† <C> 19.20 <C> 15.10 <C> 22.73 <R> <C> Lang-Specific Attention <C> 38.40† <C> 39.19† <C> 36.86† <C> 21.79† <C> 19.54† <C> 28.33† <C> 26.34† <C> 23.31† <C> 29.39† <C> [BOLD] 19.35 <C> [BOLD] 15.16 <C> [BOLD] 23.14 <R> <C> Combined Attention <C> [BOLD] 38.50† <C> [BOLD] 39.36† <C> 36.94† <C> 21.66† <C> 19.52† <C> 27.90† <C> 26.38† <C> 23.31† <C> 29.44† <C> 18.96 <C> 14.82 <C> 22.92 <R> <C> Lang-Specific S-Attention <C> 38.46† <C> 39.24† <C> [BOLD] 37.06† <C> [BOLD] 21.84† <C> 19.58† <C> [BOLD] 28.43† <C> [BOLD] 26.49† <C> [BOLD] 23.49† <C> [BOLD] 29.49† <C> 19.09 <C> 14.59 <C> 22.98 <R> <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Source Context <C> 38.46† <C> 39.24† <C> 37.06† <C> [BOLD] 21.84† <C> 19.58† <C> [BOLD] 28.43† <C> [BOLD] 26.49† <C> [BOLD] 23.49† <C> 29.49† <C> 19.09 <C> 14.59 <C> 22.98 <R> <C> Target Context <C> 38.76† <C> [BOLD] 39.57† <C> 37.35† <C> 21.77† <C> [BOLD] 19.68† <C> 27.86† <C> 26.21† <C> 23.16† <C> 29.26† <C> 19.23 <C> 14.77 <C> [BOLD] 23.23 <R> <C> Dual Context Src-Tgt <C> [BOLD] 38.80† <C> 39.51† <C> [BOLD] 37.50† <C> 21.74† <C> 19.60† <C> 27.98† <C> 26.39† <C> 23.28† <C> [BOLD] 29.50† <C> 18.89 <C> 14.52 <C> 23.06 <R> <C> Dual Context Src-Tgt-Mix <C> 38.76† <C> 39.52† <C> 37.43† <C> 21.68† <C> 19.63† <C> 27.71† <C> 26.37† <C> 23.26† <C> 29.48† <C> [BOLD] 19.26 <C> [BOLD] 14.86 <C> 23.01 <CAP> Table 2: BLEU scores for the bilingual test sets. Here all contexts are incorporated as InitDec for Europarl and InitDec+AddDec for Subtitles unless otherwise specified. bold: Best performance, †: Statistically significantly better than the base model, based on bootstrap resampling Clark et al. (2011) with p < 0.05. <COT> Looking at the "Overall" column, we can see that the "Lang-Specific S-Attention" model performs better than the "Source Context" model on all language pairs.
<R> <C> [EMPTY] <C> [BOLD] Europarl  [BOLD] En-Fr <C> [BOLD] Europarl  [BOLD] En-Fr <C> [BOLD] Europarl  [BOLD] En-Fr <C> [BOLD] Europarl  [BOLD] En-Et <C> [BOLD] Europarl  [BOLD] En-Et <C> [BOLD] Europarl  [BOLD] En-Et <C> [BOLD] Europarl  [BOLD] En-De <C> [BOLD] Europarl  [BOLD] En-De <C> [BOLD] Europarl  [BOLD] En-De <C> [BOLD] Subtitles  [BOLD] En-Ru <C> [BOLD] Subtitles  [BOLD] En-Ru <C> [BOLD] Subtitles  [BOLD] En-Ru <R> <C> [EMPTY] <C> Overall <C> En→Fr <C> Fr→En <C> Overall <C> En→Et <C> Et→En <C> Overall <C> En→De <C> De→En <C> Overall <C> En→Ru <C> Ru→En <R> <C> [ITALIC] Base Model <C> 37.36 <C> 38.13 <C> 36.03 <C> 20.68 <C> 18.64 <C> 26.65 <C> 24.74 <C> 21.80 <C> 27.74 <C> 19.05 <C> 14.90 <C> 23.04 <R> <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> + [ITALIC] Source Context as Lang-Specific Attention via <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> InitDec <C> 38.40† <C> 39.19† <C> 36.86† <C> [BOLD] 21.79† <C> 19.54† <C> [BOLD] 28.33† <C> [BOLD] 26.34† <C> [BOLD] 23.31† <C> 29.39† <C> 18.88 <C> 14.89 <C> 22.56 <R> <C> AddDec <C> 38.50† <C> [BOLD] 39.35† <C> 36.98† <C> 21.65† <C> [BOLD] 19.66† <C> 27.48† <C> 26.30† <C> 23.09† <C> [BOLD] 29.52† <C> 19.34 <C> 15.16 <C> 23.12 <R> <C> InitDec+AddDec <C> [BOLD] 38.55† <C> 39.34† <C> [BOLD] 37.14† <C> 21.49† <C> 19.43† <C> 27.55† <C> 26.25† <C> 23.18† <C> 29.30† <C> [BOLD] 19.35 <C> [BOLD] 15.16 <C> [BOLD] 23.14 <R> <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> + [ITALIC] Source Context via <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Direct Tranformation <C> 38.35† <C> 39.13† <C> 36.96† <C> 21.75† <C> [BOLD] 19.59† <C> 28.07† <C> 26.29† <C> 23.34† <C> 29.22† <C> 19.09 <C> 14.89 <C> 22.76 <R> <C> Hierarchical Gating <C> 38.33† <C> 39.14† <C> 36.89† <C> 21.62† <C> 19.55† <C> 27.64† <C> 26.31† <C> 23.17† <C> 29.45† <C> 19.20 <C> 15.10 <C> 22.73 <R> <C> Lang-Specific Attention <C> 38.40† <C> 39.19† <C> 36.86† <C> 21.79† <C> 19.54† <C> 28.33† <C> 26.34† <C> 23.31† <C> 29.39† <C> [BOLD] 19.35 <C> [BOLD] 15.16 <C> [BOLD] 23.14 <R> <C> Combined Attention <C> [BOLD] 38.50† <C> [BOLD] 39.36† <C> 36.94† <C> 21.66† <C> 19.52† <C> 27.90† <C> 26.38† <C> 23.31† <C> 29.44† <C> 18.96 <C> 14.82 <C> 22.92 <R> <C> Lang-Specific S-Attention <C> 38.46† <C> 39.24† <C> [BOLD] 37.06† <C> [BOLD] 21.84† <C> 19.58† <C> [BOLD] 28.43† <C> [BOLD] 26.49† <C> [BOLD] 23.49† <C> [BOLD] 29.49† <C> 19.09 <C> 14.59 <C> 22.98 <R> <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> + [ITALIC] Lang-Specific S-Attention using <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Source Context <C> 38.46† <C> 39.24† <C> 37.06† <C> [BOLD] 21.84† <C> 19.58† <C> [BOLD] 28.43† <C> [BOLD] 26.49† <C> [BOLD] 23.49† <C> 29.49† <C> 19.09 <C> 14.59 <C> 22.98 <R> <C> Target Context <C> 38.76† <C> [BOLD] 39.57† <C> 37.35† <C> 21.77† <C> [BOLD] 19.68† <C> 27.86† <C> 26.21† <C> 23.16† <C> 29.26† <C> 19.23 <C> 14.77 <C> [BOLD] 23.23 <R> <C> Dual Context Src-Tgt <C> [BOLD] 38.80† <C> 39.51† <C> [BOLD] 37.50† <C> 21.74† <C> 19.60† <C> 27.98† <C> 26.39† <C> 23.28† <C> [BOLD] 29.50† <C> 18.89 <C> 14.52 <C> 23.06 <R> <C> Dual Context Src-Tgt-Mix <C> 38.76† <C> 39.52† <C> 37.43† <C> 21.68† <C> 19.63† <C> 27.71† <C> 26.37† <C> 23.26† <C> 29.48† <C> [BOLD] 19.26 <C> [BOLD] 14.86 <C> 23.01 <CAP> Table 2: BLEU scores for the bilingual test sets. Here all contexts are incorporated as InitDec for Europarl and InitDec+AddDec for Subtitles unless otherwise specified. bold: Best performance, †: Statistically significantly better than the base model, based on bootstrap resampling Clark et al. (2011) with p < 0.05. <COT> Looking at the "En→Fr" row, we can see that the "Lang-Specific Attention" model performs better than the "Base Model" on the "Europarl En-Fr" dataset.
<R> <C> [EMPTY] <C> [BOLD] Europarl  [BOLD] En-Fr <C> [BOLD] Europarl  [BOLD] En-Et <C> [BOLD] Europarl  [BOLD] En-De <C> [BOLD] Subtitles  [BOLD] En-Ru <R> <C> [ITALIC] Prev Sent <C> 38.15 <C> 21.70 <C> 26.09 <C> [BOLD] 19.13 <R> <C> Our Model <C> [BOLD] 38.46† <C> [BOLD] 21.84 <C> [BOLD] 26.49† <C> 19.09 <CAP> Table 3: BLEU scores for the bilingual test sets. bold: Best performance, †: Statistically significantly better than the contextual baseline. <COT> Looking at the "CAP" tag, we can see that Table 3 provides BLEU scores for the bilingual test sets.
<R> <C> [EMPTY] <C> [BOLD] Europarl  [BOLD] En-Fr <C> [BOLD] Europarl  [BOLD] En-Et <C> [BOLD] Europarl  [BOLD] En-De <C> [BOLD] Subtitles  [BOLD] En-Ru <R> <C> [ITALIC] Prev Sent <C> 38.15 <C> 21.70 <C> 26.09 <C> [BOLD] 19.13 <R> <C> Our Model <C> [BOLD] 38.46† <C> [BOLD] 21.84 <C> [BOLD] 26.49† <C> 19.09 <CAP> Table 3: BLEU scores for the bilingual test sets. bold: Best performance, †: Statistically significantly better than the contextual baseline. <COT> Looking at the "Our Model" row and the corresponding cells, we can see that the BLEU scores for our model are higher than the previous sentence scores for all language pairs except for the "Subtitles En-Ru" pair.
<R> <C> [BOLD] Type of Context <C> [BOLD] BLEU <R> <C> No context (Base Model) <C> 24.74 <R> <C> Current Turn <C> 26.39 <R> <C> Current Language from Previous Turns <C> 26.21 <R> <C> Other Language from Previous Turns <C> 26.32 <R> <C> Complete Context <C> [BOLD] 26.49 <CAP> Table 4: BLEU scores for En-De bilingual test set. <COT> Looking at the "Type of Context" column, we can see that the BLEU score is highest for the "Complete Context" type of context.
<R> <C> [BOLD] Type of Context <C> [BOLD] BLEU <R> <C> No context (Base Model) <C> 24.74 <R> <C> Current Turn <C> 26.39 <R> <C> Current Language from Previous Turns <C> 26.21 <R> <C> Other Language from Previous Turns <C> 26.32 <R> <C> Complete Context <C> [BOLD] 26.49 <CAP> Table 4: BLEU scores for En-De bilingual test set. <COT> Comparing the "Current Turn" and "Current Language from Previous Turns" cells, we can see that the BLEU score is slightly higher for the "Current Turn" type of context.
<R> <C> Method <C> AvgSimC <C> MaxSimC <R> <C> Liu et al. ( 2015a ) <C> 67.3 <C> 68.1 <R> <C> Liu et al. ( 2015b ) <C> 69.5 <C> 67.9 <R> <C> Amiri et al. ( 2016 ) <C> 70.9 <C> - <R> <C> Lee and Chen ( 2017 ) <C> 68.7 <C> 67.9 <R> <C> Guo et al. ( 2018 ) <C> 69.3 <C> 68.2 <R> <C> [ITALIC] 300-dimensions <C> [ITALIC] 300-dimensions <C> [ITALIC] 300-dimensions <R> <C> Global-DSM <C> 67.1 <C> 67.1 <R> <C> UTDSM Random <C> 69.1±0.1 <C> 66.4±0.2 <R> <C> UTDSM <C> [BOLD] 69.6 <C> 67.1 <R> <C> UTDSM + GMM (1) <C> 67.4 <C> 67.4 <R> <C> UTDSM + GMM (2) <C> 68.4 <C> [BOLD] 68.3 <R> <C> UTDSM + GMM (3) <C> 68.9 <C> [BOLD] 68.3 <R> <C> UTDSM + GMM (8) <C> 69.1 <C> 68.0 <R> <C> UTDSM + GMM (10) <C> 69.0 <C> 67.8 <R> <C> [ITALIC] 500-dimensions <C> [ITALIC] 500-dimensions <C> [ITALIC] 500-dimensions <R> <C> Global-DSM <C> 67.6 <C> 67.6 <R> <C> UTDSM Random <C> 69.4±0.1 <C> 66.5±0.3 <R> <C> UTDSM <C> [BOLD] 70.2 <C> 68.0 <R> <C> UTDSM + GMM (1) <C> 67.6 <C> 67.6 <R> <C> UTDSM + GMM (2) <C> 68.8 <C> [BOLD] 68.6 <R> <C> UTDSM + GMM (3) <C> 69.0 <C> 68.5 <R> <C> UTDSM + GMM (8) <C> 69.5 <C> 68.5 <R> <C> UTDSM + GMM (10) <C> 69.2 <C> 68.0 <CAP> Table 1: Performance comparison between different state-of-the-art approaches on SCWS, in terms of Spearman’s correlation. UTDSM refers to the projected cross-topic representation, UTDSM Random refers to the case when random words served as anchors and GMM (c) corresponds to GMM smoothing with c components. <COT> Looking at the "Method" column, we can see that there are different approaches mentioned such as Liu et al. (2015a), Liu et al. (2015b), Amiri et al. (2016), Lee and Chen (2017), and Guo et al. (2018).
<R> <C> Method <C> AvgSimC <C> MaxSimC <R> <C> Liu et al. ( 2015a ) <C> 67.3 <C> 68.1 <R> <C> Liu et al. ( 2015b ) <C> 69.5 <C> 67.9 <R> <C> Amiri et al. ( 2016 ) <C> 70.9 <C> - <R> <C> Lee and Chen ( 2017 ) <C> 68.7 <C> 67.9 <R> <C> Guo et al. ( 2018 ) <C> 69.3 <C> 68.2 <R> <C> [ITALIC] 300-dimensions <C> [ITALIC] 300-dimensions <C> [ITALIC] 300-dimensions <R> <C> Global-DSM <C> 67.1 <C> 67.1 <R> <C> UTDSM Random <C> 69.1±0.1 <C> 66.4±0.2 <R> <C> UTDSM <C> [BOLD] 69.6 <C> 67.1 <R> <C> UTDSM + GMM (1) <C> 67.4 <C> 67.4 <R> <C> UTDSM + GMM (2) <C> 68.4 <C> [BOLD] 68.3 <R> <C> UTDSM + GMM (3) <C> 68.9 <C> [BOLD] 68.3 <R> <C> UTDSM + GMM (8) <C> 69.1 <C> 68.0 <R> <C> UTDSM + GMM (10) <C> 69.0 <C> 67.8 <R> <C> [ITALIC] 500-dimensions <C> [ITALIC] 500-dimensions <C> [ITALIC] 500-dimensions <R> <C> Global-DSM <C> 67.6 <C> 67.6 <R> <C> UTDSM Random <C> 69.4±0.1 <C> 66.5±0.3 <R> <C> UTDSM <C> [BOLD] 70.2 <C> 68.0 <R> <C> UTDSM + GMM (1) <C> 67.6 <C> 67.6 <R> <C> UTDSM + GMM (2) <C> 68.8 <C> [BOLD] 68.6 <R> <C> UTDSM + GMM (3) <C> 69.0 <C> 68.5 <R> <C> UTDSM + GMM (8) <C> 69.5 <C> 68.5 <R> <C> UTDSM + GMM (10) <C> 69.2 <C> 68.0 <CAP> Table 1: Performance comparison between different state-of-the-art approaches on SCWS, in terms of Spearman’s correlation. UTDSM refers to the projected cross-topic representation, UTDSM Random refers to the case when random words served as anchors and GMM (c) corresponds to GMM smoothing with c components. <COT> Looking at the "UTDSM" row and the "AvgSimC" column, we can see that the average similarity score for UTDSM is 69.6.
<R> <C> [BOLD] Test domain <C> [BOLD] Training Data  [BOLD] WSJ <C> [BOLD] Training Data  [BOLD] WSJ + Giga <R> <C> WSJ <C> 65.78 <C> 83.42 <R> <C> Brown <C> 45.00 <C> 76.99 <R> <C> Wikipedia <C> 35.90 <C> 62.26 <CAP> Table 2: BLEU scores for domain match experiments <COT> Looking at the "Test domain" column, we can see that the BLEU scores for the WSJ test domain are higher when trained on WSJ + Giga compared to when trained only on WSJ.
<R> <C> Method <C> Precision <C> Recall <C> F1-score <C> Accuracy <R> <C> LDA <C> 39.7 <C> 41.8 <C> 38.8 <C> 41.8 <R> <C> Global-DSM <C> 62.9 <C> 63.3 <C> 62.9 <C> 63.3 <R> <C> MaxCD <C> 61.9 <C> 63.0 <C> 62.0 <C> 63.0 <R> <C> AvgD <C> 63.5 <C> 64.6 <C> 63.3 <C> 64.3 <R> <C> AvgCD <C> [BOLD] 64.6 <C> [BOLD] 65.5 <C> [BOLD] 64.5 <C> [BOLD] 65.5 <CAP> Table 2: Evaluation results of multi-class text classification. <COT> Looking at the "Method" column, finding the method "AvgCD" has the highest Precision, Recall, F1-score, and Accuracy.
<R> <C> Method <C> Precision <C> Recall <C> F1-score <C> Accuracy <R> <C> LDA <C> 39.7 <C> 41.8 <C> 38.8 <C> 41.8 <R> <C> Global-DSM <C> 62.9 <C> 63.3 <C> 62.9 <C> 63.3 <R> <C> MaxCD <C> 61.9 <C> 63.0 <C> 62.0 <C> 63.0 <R> <C> AvgD <C> 63.5 <C> 64.6 <C> 63.3 <C> 64.3 <R> <C> AvgCD <C> [BOLD] 64.6 <C> [BOLD] 65.5 <C> [BOLD] 64.5 <C> [BOLD] 65.5 <CAP> Table 2: Evaluation results of multi-class text classification. <COT> Looking at the "Precision" column, finding the Precision values for "AvgD" and "AvgCD" are higher than the Precision values for "LDA", "Global-DSM", and "MaxCD".
<R> <C> Method <C> Precision <C> Recall <C> F1-score <C> Accuracy <R> <C> Global-DSM <C> 68.6 <C> 69.2 <C> 62.0 <C> 69.2 <R> <C> MaxCD <C> [BOLD] 69.0 <C> 69.3 <C> 62.1 <C> 69.3 <R> <C> AvgD <C> 67.7 <C> [BOLD] 69.4 <C> [BOLD] 64.0 <C> [BOLD] 69.4 <R> <C> AvgCD <C> 68.8 <C> 69.4 <C> 62.6 <C> 69.4 <CAP> Table 3: Evaluation results on paraphrase detection task. <COT> Looking at the "Method" column, we can see that the MaxCD method has the highest Precision, Recall, F1-score, and Accuracy values among all the methods.
<R> <C> Method <C> Precision <C> Recall <C> F1-score <C> Accuracy <R> <C> Global-DSM <C> 68.6 <C> 69.2 <C> 62.0 <C> 69.2 <R> <C> MaxCD <C> [BOLD] 69.0 <C> 69.3 <C> 62.1 <C> 69.3 <R> <C> AvgD <C> 67.7 <C> [BOLD] 69.4 <C> [BOLD] 64.0 <C> [BOLD] 69.4 <R> <C> AvgCD <C> 68.8 <C> 69.4 <C> 62.6 <C> 69.4 <CAP> Table 3: Evaluation results on paraphrase detection task. <COT> Looking at the "F1-score" column, we can see that the AvgD and AvgCD methods have the highest F1-score values.
<R> <C> % of known intents <C> SNIPS 25% <C> 50% <C> 75% <C> ATIS 25% <C> 50% <C> 75% <R> <C> MSP <C> 0.0 <C> 6.2 <C> 8.3 <C> 8.1 <C> 15.3 <C> 17.2 <R> <C> DOC <C> 72.5 <C> 67.9 <C> 63.9 <C> 61.6 <C> 62.8 <C> 37.7 <R> <C> DOC (Softmax) <C> 72.8 <C> 65.7 <C> 61.8 <C> 63.6 <C> 63.3 <C> 38.7 <R> <C> LOF (Softmax) <C> 76.0 <C> 69.4 <C> 65.8 <C> 67.3 <C> 61.8 <C> 38.9 <R> <C> LOF (LMCL) <C> [BOLD] 79.2 <C> [BOLD] 84.1 <C> [BOLD] 78.8 <C> [BOLD] 69.6 <C> [BOLD] 63.4 <C> [BOLD] 39.6 <CAP> Table 2: Macro f1-score of unknown intent detection with different proportion (25%, 50% and 75%) of classes are treated as known intents on SNIPS and ATIS dataset. <COT> Looking at the "MSP" row and the "SNIPS 25%" cell, we can see that the percentage of known intents is 0.0% for the SNIPS dataset when 25% of classes are treated as known intents.
<R> <C> Statistics  [ITALIC] i <C> Statistics 1 <C> Statistics 2 <C> Statistics 3 <C> Statistics 4 <C> Scores  [ITALIC] τ=2 <C> Scores  [ITALIC] τ=| [BOLD] y| <R> <C> [ITALIC] gi <C> 3 <C> 4 <C> 4 <C> 4 <C> [EMPTY] <C> [EMPTY] <R> <C> AL [ITALIC] i <C> 3 <C> 3 <C> 2 <C> 1 <C> AL = 3 <C> AL = 2.25 <CAP> Table 1: Comparing AL with and without its truncated average, tracking time-indexed lag ALi=gi−i−1γ when |x|=|y|=4 for a wait-3 system. <COT> Looking at the "CAP" tag, we can see that Table 1 compares AL with and without its truncated average, tracking time-indexed lag ALi=gi−i−1γ when |x|=|y|=4 for a wait-3 system.
<R> <C> Statistics  [ITALIC] i <C> Statistics 1 <C> Statistics 2 <C> Statistics 3 <C> Statistics 4 <C> Scores  [ITALIC] τ=2 <C> Scores  [ITALIC] τ=| [BOLD] y| <R> <C> [ITALIC] gi <C> 3 <C> 4 <C> 4 <C> 4 <C> [EMPTY] <C> [EMPTY] <R> <C> AL [ITALIC] i <C> 3 <C> 3 <C> 2 <C> 1 <C> AL = 3 <C> AL = 2.25 <CAP> Table 1: Comparing AL with and without its truncated average, tracking time-indexed lag ALi=gi−i−1γ when |x|=|y|=4 for a wait-3 system. <COT> Looking at the "AL = 2.25" cell and the "Statistics 2" cell, we can see that when ALi=gi−i−1γ, AL is equal to 2.25.
<R> <C> [BOLD] Representation <C> [BOLD] Train on  [BOLD] Gold <C> [BOLD] Train on  [BOLD] Gold+Silver <R> <C> AMR <C> 22.0 <C> 33.8 <R> <C> DMRS - no attributes <C> 40.1 <C> 63.6 <R> <C> DMRS - all attributes <C> 56.9 <C> 75.8 <CAP> Table 4: BLEU scores for evaluating AMR and DMRS generators on an AMR test set <COT> Looking at the "Train on Gold" column, we can see that the BLEU scores for DMRS - all attributes are higher than the BLEU scores for DMRS - no attributes.
<R> <C> [BOLD] Representation <C> [BOLD] Train on  [BOLD] Gold <C> [BOLD] Train on  [BOLD] Gold+Silver <R> <C> AMR <C> 22.0 <C> 33.8 <R> <C> DMRS - no attributes <C> 40.1 <C> 63.6 <R> <C> DMRS - all attributes <C> 56.9 <C> 75.8 <CAP> Table 4: BLEU scores for evaluating AMR and DMRS generators on an AMR test set <COT> Looking at the "Representation" column, we can see that the BLEU scores for DMRS - all attributes are higher than the BLEU scores for AMR.
<R> <C> [BOLD] Complexity <C> [BOLD] Ent. <C> [BOLD] Neutral <C> [BOLD] Cont. <C> [BOLD] All <R> <C> MultiNLI Matched Dev Set <C> MultiNLI Matched Dev Set <C> MultiNLI Matched Dev Set <C> MultiNLI Matched Dev Set <C> MultiNLI Matched Dev Set <R> <C> [BOLD] All <C> 83.56 <C> 84.12 <C> 86.37 <C> [BOLD] 84.66 <R> <C> Dative Alternation <C> Dative Alternation <C> Dative Alternation <C> Dative Alternation <C> Dative Alternation <R> <C> [BOLD] Simple <C> 100 <C> - <C> 4.22 <C> [BOLD] 52.63 <R> <C> [BOLD] Medium <C> 100 <C> - <C> 2.16 <C> [BOLD] 49.27 <R> <C> [BOLD] Complex <C> 99.77 <C> - <C> 0.36 <C> [BOLD] 50.45 <R> <C> [BOLD] All <C> 99.92 <C> - <C> 2.25 <C> [BOLD] 50.78 <R> <C> Numerical Reasoning <C> Numerical Reasoning <C> Numerical Reasoning <C> Numerical Reasoning <C> Numerical Reasoning <R> <C> [BOLD] Simple <C> 38.14 <C> 0.66 <C> 69.53 <C> [BOLD] 45.04 <R> <C> [BOLD] Medium <C> 57.14 <C> 1.36 <C> 50.14 <C> [BOLD] 38.11 <R> <C> [BOLD] Complex <C> 55.48 <C> 3.04 <C> 46.26 <C> [BOLD] 36.15 <R> <C> [BOLD] All <C> 50.25 <C> 1.69 <C> 55.31 <C> [BOLD] 39.77 <CAP> Table 3: Accuracy of the model trained only on MultiNLI on our datasets, which are used as probing datasets. The Complexity column refers to the syntactic complexity of the sentences. <COT> Looking at the "All" row, "Ent." column, and "MultiNLI Matched Dev Set" column, we can see that the accuracy for the Entailment class is 84.12%.
<R> <C> [BOLD] Complexity <C> [BOLD] Ent. <C> [BOLD] Neutral <C> [BOLD] Cont. <C> [BOLD] All <R> <C> MultiNLI Matched Dev Set <C> MultiNLI Matched Dev Set <C> MultiNLI Matched Dev Set <C> MultiNLI Matched Dev Set <C> MultiNLI Matched Dev Set <R> <C> [BOLD] All <C> 83.56 <C> 84.12 <C> 86.37 <C> [BOLD] 84.66 <R> <C> Dative Alternation <C> Dative Alternation <C> Dative Alternation <C> Dative Alternation <C> Dative Alternation <R> <C> [BOLD] Simple <C> 100 <C> - <C> 4.22 <C> [BOLD] 52.63 <R> <C> [BOLD] Medium <C> 100 <C> - <C> 2.16 <C> [BOLD] 49.27 <R> <C> [BOLD] Complex <C> 99.77 <C> - <C> 0.36 <C> [BOLD] 50.45 <R> <C> [BOLD] All <C> 99.92 <C> - <C> 2.25 <C> [BOLD] 50.78 <R> <C> Numerical Reasoning <C> Numerical Reasoning <C> Numerical Reasoning <C> Numerical Reasoning <C> Numerical Reasoning <R> <C> [BOLD] Simple <C> 38.14 <C> 0.66 <C> 69.53 <C> [BOLD] 45.04 <R> <C> [BOLD] Medium <C> 57.14 <C> 1.36 <C> 50.14 <C> [BOLD] 38.11 <R> <C> [BOLD] Complex <C> 55.48 <C> 3.04 <C> 46.26 <C> [BOLD] 36.15 <R> <C> [BOLD] All <C> 50.25 <C> 1.69 <C> 55.31 <C> [BOLD] 39.77 <CAP> Table 3: Accuracy of the model trained only on MultiNLI on our datasets, which are used as probing datasets. The Complexity column refers to the syntactic complexity of the sentences. <COT> Looking at the "Complexity" row, "All" column, and "Cont." column, we can see that the accuracy for the Cont. class in sentences with complex syntactic complexity is 50.45%.
<R> <C> [EMPTY] <C> [BOLD] Memory-to-Context BLEU <C> [BOLD] Memory-to-Context BLEU <C> [BOLD] Memory-to-Context BLEU <C> [BOLD] Memory-to-Context BLEU <C> [BOLD] Memory-to-Context METEOR <C> [BOLD] Memory-to-Context METEOR <C> [BOLD] Memory-to-Context METEOR <C> [BOLD] Memory-to-Context METEOR <C> [BOLD] Memory-to-Output BLEU <C> [BOLD] Memory-to-Output BLEU <C> [BOLD] Memory-to-Output BLEU <C> [BOLD] Memory-to-Output BLEU <C> [BOLD] Memory-to-Output METEOR <C> [BOLD] Memory-to-Output METEOR <C> [BOLD] Memory-to-Output METEOR <C> [BOLD] Memory-to-Output METEOR <R> <C> [EMPTY] <C> Fr→En <C> De→En <C> De→En <C> Et→En <C> Fr→En <C> De→En <C> De→En <C> Et→En <C> Fr→En <C> De→En <C> De→En <C> Et→En <C> Fr→En <C> De→En <C> De→En <C> Et→En <R> <C> [EMPTY] <C> [EMPTY] <C> NC-11 <C> NC-16 <C> [EMPTY] <C> [EMPTY] <C> NC-11 <C> NC-16 <C> [EMPTY] <C> [EMPTY] <C> NC-11 <C> NC-16 <C> [EMPTY] <C> [EMPTY] <C> NC-11 <C> NC-16 <C> [EMPTY] <R> <C> [ITALIC] S-NMT <C> 20.85 <C> 5.24 <C> 9.18 <C> 20.42 <C> 23.27 <C> 10.90 <C> 14.35 <C> 24.65 <C> 20.85 <C> 5.24 <C> 9.18 <C> 20.42 <C> 23.27 <C> 10.90 <C> 14.35 <C> 24.65 <R> <C> [ITALIC]  +src <C> 21.91† <C> 6.26† <C> 10.20† <C> 22.10† <C> 24.04† <C> 11.52† <C> 15.45† <C> 25.92† <C> [BOLD] 21.80† <C> 6.10† <C> 9.98† <C> 21.50† <C> 23.99† <C> 11.53† <C> 15.29† <C> 25.44† <R> <C> [ITALIC]  +trg <C> 21.74† <C> 6.24† <C> 9.97† <C> 21.94† <C> 23.98† <C> 11.58† <C> 15.32† <C> 25.89† <C> 21.76† <C> [BOLD] 6.31† <C> 10.04† <C> 21.82† <C> 24.06† <C> [BOLD] 12.10† <C> 15.75† <C> 25.93† <R> <C> [ITALIC]  +both <C> [BOLD] 22.00† <C> [BOLD] 6.57† <C> [BOLD] 10.54† <C> [BOLD] 22.32† <C> [BOLD] 24.40† <C> [BOLD] 12.24† <C> [BOLD] 16.18† <C> [BOLD] 26.34† <C> 21.77† <C> 6.20† <C> [BOLD] 10.23† <C> [BOLD] 22.20† <C> [BOLD] 24.27† <C> 11.84† <C> [BOLD] 15.82† <C> [BOLD] 26.10† <CAP> Table 2: BLEU and METEOR scores for the sentence-level baseline (S-NMT) vs. variants of our Document NMT model. bold: Best performance, †: Statistically significantly better than the baseline. <COT> Looking at the "Memory-to-Context BLEU" column, we can see that the scores for "De→En" are consistently higher than the scores for "Fr→En" and "Et→En".
<R> <C> [EMPTY] <C> [BOLD] Memory-to-Context BLEU <C> [BOLD] Memory-to-Context BLEU <C> [BOLD] Memory-to-Context BLEU <C> [BOLD] Memory-to-Context BLEU <C> [BOLD] Memory-to-Context METEOR <C> [BOLD] Memory-to-Context METEOR <C> [BOLD] Memory-to-Context METEOR <C> [BOLD] Memory-to-Context METEOR <C> [BOLD] Memory-to-Output BLEU <C> [BOLD] Memory-to-Output BLEU <C> [BOLD] Memory-to-Output BLEU <C> [BOLD] Memory-to-Output BLEU <C> [BOLD] Memory-to-Output METEOR <C> [BOLD] Memory-to-Output METEOR <C> [BOLD] Memory-to-Output METEOR <C> [BOLD] Memory-to-Output METEOR <R> <C> [EMPTY] <C> Fr→En <C> De→En <C> De→En <C> Et→En <C> Fr→En <C> De→En <C> De→En <C> Et→En <C> Fr→En <C> De→En <C> De→En <C> Et→En <C> Fr→En <C> De→En <C> De→En <C> Et→En <R> <C> [EMPTY] <C> [EMPTY] <C> NC-11 <C> NC-16 <C> [EMPTY] <C> [EMPTY] <C> NC-11 <C> NC-16 <C> [EMPTY] <C> [EMPTY] <C> NC-11 <C> NC-16 <C> [EMPTY] <C> [EMPTY] <C> NC-11 <C> NC-16 <C> [EMPTY] <R> <C> [ITALIC] S-NMT <C> 20.85 <C> 5.24 <C> 9.18 <C> 20.42 <C> 23.27 <C> 10.90 <C> 14.35 <C> 24.65 <C> 20.85 <C> 5.24 <C> 9.18 <C> 20.42 <C> 23.27 <C> 10.90 <C> 14.35 <C> 24.65 <R> <C> [ITALIC]  +src <C> 21.91† <C> 6.26† <C> 10.20† <C> 22.10† <C> 24.04† <C> 11.52† <C> 15.45† <C> 25.92† <C> [BOLD] 21.80† <C> 6.10† <C> 9.98† <C> 21.50† <C> 23.99† <C> 11.53† <C> 15.29† <C> 25.44† <R> <C> [ITALIC]  +trg <C> 21.74† <C> 6.24† <C> 9.97† <C> 21.94† <C> 23.98† <C> 11.58† <C> 15.32† <C> 25.89† <C> 21.76† <C> [BOLD] 6.31† <C> 10.04† <C> 21.82† <C> 24.06† <C> [BOLD] 12.10† <C> 15.75† <C> 25.93† <R> <C> [ITALIC]  +both <C> [BOLD] 22.00† <C> [BOLD] 6.57† <C> [BOLD] 10.54† <C> [BOLD] 22.32† <C> [BOLD] 24.40† <C> [BOLD] 12.24† <C> [BOLD] 16.18† <C> [BOLD] 26.34† <C> 21.77† <C> 6.20† <C> [BOLD] 10.23† <C> [BOLD] 22.20† <C> [BOLD] 24.27† <C> 11.84† <C> [BOLD] 15.82† <C> [BOLD] 26.10† <CAP> Table 2: BLEU and METEOR scores for the sentence-level baseline (S-NMT) vs. variants of our Document NMT model. bold: Best performance, †: Statistically significantly better than the baseline. <COT> Looking at the "Memory-to-Output METEOR" column, we can see that the scores for "+both" are consistently higher than the scores for "+src" and "+trg".
<R> <C> Model <C> [BOLD] Surface SentLen <C> [BOLD] Surface WC <C> [BOLD] Syntactic TreeDepth <C> [BOLD] Syntactic TopConst <C> [BOLD] Syntactic BShift <C> [BOLD] Semantic Tense <C> [BOLD] Semantic SubjNum <C> [BOLD] Semantic ObjNum <C> [BOLD] Semantic SOMO <C> [BOLD] Semantic CoordInv <R> <C> Majority <C> 20.0 <C> 0.5 <C> 17.9 <C> 5.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <R> <C> Human <C> 100 <C> 100 <C> 84.0 <C> 84.0 <C> 98.0 <C> 85.0 <C> 88.0 <C> 86.5 <C> 81.2 <C> 85.0 <R> <C> Length <C> 100 <C> 0.2 <C> 18.1 <C> 9.3 <C> 50.6 <C> 56.5 <C> 50.3 <C> 50.1 <C> 50.2 <C> 50.0 <R> <C> AVG <C> 64.12 <C> 82.1 <C> 36.38 <C> 68.04 <C> 50.16 <C> 87.9 <C> 80.89 <C> 80.24 <C> 50.39 <C> 51.95 <R> <C> MAX <C> 62.67 <C> 88.97 <C> 33.02 <C> 62.63 <C> 50.31 <C> 85.66 <C> 77.11 <C> 76.04 <C> 51.86 <C> 52.33 <R> <C> [ITALIC] c[0] <C> [BOLD] 98.67 <C> [BOLD] 91.11 <C> 38.6 <C> 70.54 <C> 50.42 <C> 88.25 <C> 80.88 <C> 80.56 <C> [BOLD] 55.6 <C> 55 <R> <C> [ITALIC] c[0:1] <C> 97.18 <C> 89.16 <C> 40.41 <C> 78.34 <C> 52.25 <C> 88.58 <C> 86.59 <C> 84.36 <C> 54.62 <C> 70.42 <R> <C> [ITALIC] c[0:2] <C> 95.84 <C> 86.77 <C> 43.01 <C> 80.41 <C> 54.84 <C> 88.87 <C> 88.06 <C> 86.26 <C> 53.07 <C> 71.87 <R> <C> [ITALIC] c[0:3] <C> 94.63 <C> 84.96 <C> [BOLD] 43.35 <C> 81.01 <C> 57.29 <C> 88.88 <C> 88.36 <C> 86.51 <C> 53.79 <C> [BOLD] 72.01 <R> <C> [ITALIC] c[0:4] <C> 93.25 <C> 83.24 <C> 43.26 <C> 81.49 <C> 60.31 <C> [BOLD] 88.91 <C> [BOLD] 88.65 <C> 87.15 <C> 52.77 <C> 71.91 <R> <C> [ITALIC] c[0:5] <C> 92.29 <C> 81.84 <C> 42.75 <C> [BOLD] 81.60 <C> 62.01 <C> 88.82 <C> 88.44 <C> 87.98 <C> 52.38 <C> 70.96 <R> <C> [ITALIC] c[0:6] <C> 91.56 <C> 79.83 <C> 43.05 <C> 81.41 <C> [BOLD] 62.59 <C> 88.87 <C> [BOLD] 88.65 <C> [BOLD] 88.28 <C> 52.07 <C> 70.63 <CAP> Table 2: Probing tasks performance of vector averaging (AVG) and max pooling (MAX) vs. DCT embeddings with various K. Majority (baseline), Human (human-bound), and a linear classifier with sentence length as sole feature (Length) as reported in Conneau et al. (2018), respectively. <COT> Looking at the "Model" column, we can see that there are different models used in the table, including Majority, Human, Length, AVG, MAX, and c[0:6]. 
<R> <C> Model <C> [BOLD] Surface SentLen <C> [BOLD] Surface WC <C> [BOLD] Syntactic TreeDepth <C> [BOLD] Syntactic TopConst <C> [BOLD] Syntactic BShift <C> [BOLD] Semantic Tense <C> [BOLD] Semantic SubjNum <C> [BOLD] Semantic ObjNum <C> [BOLD] Semantic SOMO <C> [BOLD] Semantic CoordInv <R> <C> Majority <C> 20.0 <C> 0.5 <C> 17.9 <C> 5.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <R> <C> Human <C> 100 <C> 100 <C> 84.0 <C> 84.0 <C> 98.0 <C> 85.0 <C> 88.0 <C> 86.5 <C> 81.2 <C> 85.0 <R> <C> Length <C> 100 <C> 0.2 <C> 18.1 <C> 9.3 <C> 50.6 <C> 56.5 <C> 50.3 <C> 50.1 <C> 50.2 <C> 50.0 <R> <C> AVG <C> 64.12 <C> 82.1 <C> 36.38 <C> 68.04 <C> 50.16 <C> 87.9 <C> 80.89 <C> 80.24 <C> 50.39 <C> 51.95 <R> <C> MAX <C> 62.67 <C> 88.97 <C> 33.02 <C> 62.63 <C> 50.31 <C> 85.66 <C> 77.11 <C> 76.04 <C> 51.86 <C> 52.33 <R> <C> [ITALIC] c[0] <C> [BOLD] 98.67 <C> [BOLD] 91.11 <C> 38.6 <C> 70.54 <C> 50.42 <C> 88.25 <C> 80.88 <C> 80.56 <C> [BOLD] 55.6 <C> 55 <R> <C> [ITALIC] c[0:1] <C> 97.18 <C> 89.16 <C> 40.41 <C> 78.34 <C> 52.25 <C> 88.58 <C> 86.59 <C> 84.36 <C> 54.62 <C> 70.42 <R> <C> [ITALIC] c[0:2] <C> 95.84 <C> 86.77 <C> 43.01 <C> 80.41 <C> 54.84 <C> 88.87 <C> 88.06 <C> 86.26 <C> 53.07 <C> 71.87 <R> <C> [ITALIC] c[0:3] <C> 94.63 <C> 84.96 <C> [BOLD] 43.35 <C> 81.01 <C> 57.29 <C> 88.88 <C> 88.36 <C> 86.51 <C> 53.79 <C> [BOLD] 72.01 <R> <C> [ITALIC] c[0:4] <C> 93.25 <C> 83.24 <C> 43.26 <C> 81.49 <C> 60.31 <C> [BOLD] 88.91 <C> [BOLD] 88.65 <C> 87.15 <C> 52.77 <C> 71.91 <R> <C> [ITALIC] c[0:5] <C> 92.29 <C> 81.84 <C> 42.75 <C> [BOLD] 81.60 <C> 62.01 <C> 88.82 <C> 88.44 <C> 87.98 <C> 52.38 <C> 70.96 <R> <C> [ITALIC] c[0:6] <C> 91.56 <C> 79.83 <C> 43.05 <C> 81.41 <C> [BOLD] 62.59 <C> 88.87 <C> [BOLD] 88.65 <C> [BOLD] 88.28 <C> 52.07 <C> 70.63 <CAP> Table 2: Probing tasks performance of vector averaging (AVG) and max pooling (MAX) vs. DCT embeddings with various K. Majority (baseline), Human (human-bound), and a linear classifier with sentence length as sole feature (Length) as reported in Conneau et al. (2018), respectively. <COT> Looking at the "Semantic Tense" column, we can see that the values for c[0:6] gradually decrease from 88.88 to 88.28.
<R> <C> Model <C> [BOLD] Sentiment Analysis MR <C> [BOLD] Sentiment Analysis SST2 <C> [BOLD] Sentiment Analysis SST5 <C> [BOLD] Sentiment Analysis CR <C> [BOLD] Sentiment Analysis MPQA <C> [BOLD] SUBJ <C> [BOLD] Relatedness/Paraphrase SICK-R <C> [BOLD] Relatedness/Paraphrase STSB <C> [BOLD] Relatedness/Paraphrase MRPC <C> [BOLD] Inference SICK-E <C> [BOLD] TREC <R> <C> AVG <C> 78.3 <C> [BOLD] 84.13 <C> 44.16 <C> 79.6 <C> 87.94 <C> 92.33 <C> 81.95 <C> 69.26 <C> 74.43 <C> 79.5 <C> 83.2 <R> <C> MAX <C> 73.31 <C> 79.24 <C> 41.86 <C> 73.35 <C> 86.54 <C> 88.02 <C> 81.93 <C> [BOLD] 71.57 <C> 72.5 <C> 77.98 <C> 76.2 <R> <C> [ITALIC] c[0] <C> [BOLD] 78.45 <C> 83.53 <C> 44.57 <C> 79.81 <C> [BOLD] 88.36 <C> [BOLD] 92.79 <C> 82.61 <C> 71.11 <C> 72.93 <C> 78.91 <C> 84.8 <R> <C> [ITALIC] c[0:1] <C> 78.15 <C> 83.47 <C> [BOLD] 46.06 <C> 79.84 <C> 87.76 <C> 92.61 <C> 82.73 <C> 70.82 <C> 72.81 <C> 79.64 <C> 88.2 <R> <C> [ITALIC] c[0:2] <C> 78.02 <C> 82.98 <C> 45.16 <C> 79.68 <C> 87.62 <C> 92.5 <C> [BOLD] 82.95 <C> 70.36 <C> 72.87 <C> 79.76 <C> [BOLD] 89.8 <R> <C> [ITALIC] c[0:3] <C> 77.81 <C> 83.8 <C> 45.79 <C> 79.66 <C> 87.54 <C> 92.4 <C> 82.93 <C> 69.79 <C> 73.57 <C> [BOLD] 80.56 <C> 88.2 <R> <C> [ITALIC] c[0:4] <C> 77.72 <C> 83.75 <C> 44.03 <C> [BOLD] 80.08 <C> 87.4 <C> 92.61 <C> 82.53 <C> 69.31 <C> 72.35 <C> 79.72 <C> [BOLD] 89.8 <R> <C> [ITALIC] c[0:5] <C> 77.42 <C> 82.43 <C> 43.3 <C> 78.6 <C> 87.21 <C> 92.19 <C> 82.36 <C> 68.9 <C> 73.91 <C> 79.89 <C> 88.8 <R> <C> [ITALIC] c[0:6] <C> 77.47 <C> 82.81 <C> 42.99 <C> 78.78 <C> 87.06 <C> 92.15 <C> 81.86 <C> 68.17 <C> [BOLD] 75.07 <C> 79.76 <C> 86.4 <CAP> Table 3: DCT embedding Performance in SentEval downstream tasks compared to vector averaging (AVG) and max pooling (MAX). <COT> Looking at the "Model" column, we can see that the model "c[0:2]" consistently outperforms the other models in the "Relatedness/Paraphrase STSB" task.
<R> <C> Model <C> [BOLD] Sentiment Analysis MR <C> [BOLD] Sentiment Analysis SST2 <C> [BOLD] Sentiment Analysis SST5 <C> [BOLD] Sentiment Analysis CR <C> [BOLD] Sentiment Analysis MPQA <C> [BOLD] SUBJ <C> [BOLD] Relatedness/Paraphrase SICK-R <C> [BOLD] Relatedness/Paraphrase STSB <C> [BOLD] Relatedness/Paraphrase MRPC <C> [BOLD] Inference SICK-E <C> [BOLD] TREC <R> <C> AVG <C> 78.3 <C> [BOLD] 84.13 <C> 44.16 <C> 79.6 <C> 87.94 <C> 92.33 <C> 81.95 <C> 69.26 <C> 74.43 <C> 79.5 <C> 83.2 <R> <C> MAX <C> 73.31 <C> 79.24 <C> 41.86 <C> 73.35 <C> 86.54 <C> 88.02 <C> 81.93 <C> [BOLD] 71.57 <C> 72.5 <C> 77.98 <C> 76.2 <R> <C> [ITALIC] c[0] <C> [BOLD] 78.45 <C> 83.53 <C> 44.57 <C> 79.81 <C> [BOLD] 88.36 <C> [BOLD] 92.79 <C> 82.61 <C> 71.11 <C> 72.93 <C> 78.91 <C> 84.8 <R> <C> [ITALIC] c[0:1] <C> 78.15 <C> 83.47 <C> [BOLD] 46.06 <C> 79.84 <C> 87.76 <C> 92.61 <C> 82.73 <C> 70.82 <C> 72.81 <C> 79.64 <C> 88.2 <R> <C> [ITALIC] c[0:2] <C> 78.02 <C> 82.98 <C> 45.16 <C> 79.68 <C> 87.62 <C> 92.5 <C> [BOLD] 82.95 <C> 70.36 <C> 72.87 <C> 79.76 <C> [BOLD] 89.8 <R> <C> [ITALIC] c[0:3] <C> 77.81 <C> 83.8 <C> 45.79 <C> 79.66 <C> 87.54 <C> 92.4 <C> 82.93 <C> 69.79 <C> 73.57 <C> [BOLD] 80.56 <C> 88.2 <R> <C> [ITALIC] c[0:4] <C> 77.72 <C> 83.75 <C> 44.03 <C> [BOLD] 80.08 <C> 87.4 <C> 92.61 <C> 82.53 <C> 69.31 <C> 72.35 <C> 79.72 <C> [BOLD] 89.8 <R> <C> [ITALIC] c[0:5] <C> 77.42 <C> 82.43 <C> 43.3 <C> 78.6 <C> 87.21 <C> 92.19 <C> 82.36 <C> 68.9 <C> 73.91 <C> 79.89 <C> 88.8 <R> <C> [ITALIC] c[0:6] <C> 77.47 <C> 82.81 <C> 42.99 <C> 78.78 <C> 87.06 <C> 92.15 <C> 81.86 <C> 68.17 <C> [BOLD] 75.07 <C> 79.76 <C> 86.4 <CAP> Table 3: DCT embedding Performance in SentEval downstream tasks compared to vector averaging (AVG) and max pooling (MAX). <COT> Looking at the "AVG" row, we can see that the average performance across all tasks is 78.3.
<R> <C> Model <C> [BOLD] 20-NG P <C> [BOLD] 20-NG R <C> [BOLD] 20-NG F1 <C> [BOLD] R-8 P <C> [BOLD] R-8 R <C> [BOLD] R-8 F1 <C> [BOLD] SST-5 P <C> [BOLD] SST-5 R <C> [BOLD] SST-5 F1 <R> <C> PCA <C> 55.43 <C> 54.67 <C> 54.77 <C> 83.83 <C> 83.42 <C> 83.41 <C> 26.47 <C> 25.08 <C> 25.23 <R> <C> DCT* <C> 61.07 <C> 59.16 <C> 59.78 <C> 90.41 <C> 90.78 <C> 90.38 <C> 30.11 <C> 30.09 <C> 29.53 <R> <C> Avg. vec. <C> 68.72 <C> 68.19 <C> 68.25 <C> 96.34 <C> 96.30 <C> 96.27 <C> 27.88 <C> 26.44 <C> 24.81 <R> <C> p-means <C> [ITALIC] 72.20 <C> [ITALIC] 71.65 <C> [BOLD] 71.79 <C> 96.69 <C> 96.67 <C> 96.65 <C> 33.77 <C> 33.41 <C> 33.26 <R> <C> ELMo <C> 71.20 <C> [BOLD] 71.79 <C> 71.36 <C> 94.54 <C> 91.32 <C> 91.32 <C> [ITALIC] 42.35 <C> [ITALIC] 41.51 <C> [ITALIC] 41.54 <R> <C> BERT <C> 70.89 <C> 70.79 <C> 70.88 <C> 95.52 <C> 95.39 <C> 95.39 <C> 39.92 <C> 39.38 <C> 39.35 <R> <C> EigenSent <C> 66.98 <C> 66.40 <C> 66.54 <C> 95.91 <C> 95.80 <C> 95.76 <C> 35.32 <C> 33.69 <C> 33.91 <R> <C> EigenSent⊕Avg <C> [BOLD] 72.24 <C> 71.62 <C> [ITALIC] 71.78 <C> [BOLD] 97.18 <C> [BOLD] 97.13 <C> [BOLD] 97.14 <C> [BOLD] 42.77 <C> [BOLD] 41.67 <C> [BOLD] 41.81 <R> <C> c[k] <C> [ITALIC] 72.20 <C> 71.58 <C> 71.73 <C> [ITALIC] 96.98 <C> [ITALIC] 96.98 <C> [ITALIC] 96.94 <C> 37.67 <C> 34.47 <C> 34.54 <CAP> Table 4: Performance in text classification (20-NG, R-8) and sentiment (SST-5) tasks of various models as reported in Kayal and Tsatsaronis (2019), where DCT* refers to the implementation in Kayal and Tsatsaronis (2019). Our DCT embeddings are denoted as c[k] in the bottom row. Bold indicates the best result, and italic indicates second-best. <COT> Looking at the "Model" column, we can see that "EigenSent⊕Avg" has the highest F1 score in the 20-NG P task.
<R> <C> Model <C> [BOLD] 20-NG P <C> [BOLD] 20-NG R <C> [BOLD] 20-NG F1 <C> [BOLD] R-8 P <C> [BOLD] R-8 R <C> [BOLD] R-8 F1 <C> [BOLD] SST-5 P <C> [BOLD] SST-5 R <C> [BOLD] SST-5 F1 <R> <C> PCA <C> 55.43 <C> 54.67 <C> 54.77 <C> 83.83 <C> 83.42 <C> 83.41 <C> 26.47 <C> 25.08 <C> 25.23 <R> <C> DCT* <C> 61.07 <C> 59.16 <C> 59.78 <C> 90.41 <C> 90.78 <C> 90.38 <C> 30.11 <C> 30.09 <C> 29.53 <R> <C> Avg. vec. <C> 68.72 <C> 68.19 <C> 68.25 <C> 96.34 <C> 96.30 <C> 96.27 <C> 27.88 <C> 26.44 <C> 24.81 <R> <C> p-means <C> [ITALIC] 72.20 <C> [ITALIC] 71.65 <C> [BOLD] 71.79 <C> 96.69 <C> 96.67 <C> 96.65 <C> 33.77 <C> 33.41 <C> 33.26 <R> <C> ELMo <C> 71.20 <C> [BOLD] 71.79 <C> 71.36 <C> 94.54 <C> 91.32 <C> 91.32 <C> [ITALIC] 42.35 <C> [ITALIC] 41.51 <C> [ITALIC] 41.54 <R> <C> BERT <C> 70.89 <C> 70.79 <C> 70.88 <C> 95.52 <C> 95.39 <C> 95.39 <C> 39.92 <C> 39.38 <C> 39.35 <R> <C> EigenSent <C> 66.98 <C> 66.40 <C> 66.54 <C> 95.91 <C> 95.80 <C> 95.76 <C> 35.32 <C> 33.69 <C> 33.91 <R> <C> EigenSent⊕Avg <C> [BOLD] 72.24 <C> 71.62 <C> [ITALIC] 71.78 <C> [BOLD] 97.18 <C> [BOLD] 97.13 <C> [BOLD] 97.14 <C> [BOLD] 42.77 <C> [BOLD] 41.67 <C> [BOLD] 41.81 <R> <C> c[k] <C> [ITALIC] 72.20 <C> 71.58 <C> 71.73 <C> [ITALIC] 96.98 <C> [ITALIC] 96.98 <C> [ITALIC] 96.94 <C> 37.67 <C> 34.47 <C> 34.54 <CAP> Table 4: Performance in text classification (20-NG, R-8) and sentiment (SST-5) tasks of various models as reported in Kayal and Tsatsaronis (2019), where DCT* refers to the implementation in Kayal and Tsatsaronis (2019). Our DCT embeddings are denoted as c[k] in the bottom row. Bold indicates the best result, and italic indicates second-best. <COT> Looking at the "20-NG F1" column, we can see that the F1 score of "p-means" is higher than that of "Avg. vec." and "ELMo" in the 20-NG task.
<R> <C> [EMPTY] <C> 1-best <C> 10-best <C> cnet <C> pruned cnet <R> <C> all words <C> 69.3 <C> 78.6 <C> 85.7 <C> 83.1 <R> <C> slots/values <C> 69.8 <C> 75.6 <C> 82.4 <C> 80.6 <CAP> Table 1: Coverage of words from the manual transcripts in the DSTC2 development set of different batch ASR output types (%). In the pruned cnet interjections and hypotheses with scores below 0.001 were removed. <COT> Looking at the "CAP" tag, we can see that Table 1 provides the coverage of words from the manual transcripts in the DSTC2 development set for different batch ASR output types.
<R> <C> [EMPTY] <C> 1-best <C> 10-best <C> cnet <C> pruned cnet <R> <C> all words <C> 69.3 <C> 78.6 <C> 85.7 <C> 83.1 <R> <C> slots/values <C> 69.8 <C> 75.6 <C> 82.4 <C> 80.6 <CAP> Table 1: Coverage of words from the manual transcripts in the DSTC2 development set of different batch ASR output types (%). In the pruned cnet interjections and hypotheses with scores below 0.001 were removed. <COT> Looking at the "pruned cnet" cell, we can see that the coverage of words is lower compared to the "all words" cell.
<R> <C> [BOLD] method <C> [BOLD] goals <C> [BOLD] requests <R> <C> 1-best baseline <C> 63.6 66.658.7 <C> 96.8 97.196.5 <R> <C> [ITALIC] cnet - no pruning <C> [ITALIC] cnet - no pruning <C> [ITALIC] cnet - no pruning <R> <C> weighted pooling <C> 63.7 65.661.6 <C> 96.7 97.096.3 <R> <C> [ITALIC] cnet - score threshold 0.001 <C> [ITALIC] cnet - score threshold 0.001 <C> [ITALIC] cnet - score threshold 0.001 <R> <C> average pooling <C> 63.7 66.460.0 <C> 96.6 96.896.0 <R> <C> weighted pooling <C> [BOLD] 65.2 68.559.1 <C> 97.0 97.496.6 <R> <C> [ITALIC] cnet - score threshold 0.01 <C> [ITALIC] cnet - score threshold 0.01 <C> [ITALIC] cnet - score threshold 0.01 <R> <C> average pooling <C> 64.6 67.959.7 <C> 96.9⋆ 97.296.5 <R> <C> weighted pooling <C> 64.7 68.462.2 <C> [BOLD] 97.1⋆ 97.396.9 <R> <C> [ITALIC] ensemble models <C> [ITALIC] ensemble models <C> [ITALIC] ensemble models <R> <C> baseline <C> 69.7 <C> 96.7 <R> <C> cnet <C> [BOLD] 71.4 <C> [BOLD] 97.2 <R> <C> [ITALIC] results from related work <C> [ITALIC] results from related work <C> [ITALIC] results from related work <R> <C> Vodolán et al. ( 2017 ) <C> 80.0 <C> - <R> <C> Williams ( 2014 ) <C> 78.4 <C> 98.0 <R> <C> Mrksic et al. ( 2017 ) <C> 73.4 <C> 96.5 <CAP> Table 3: DSTC2 test set accuracy of ten runs with different random seeds in the format average maximumminimum. ⋆ denotes a statistically significant higher result than the baseline (p<0.05, Wilcoxon signed-rank test with Bonferroni correction for ten repeated comparisons). The cnet ensemble corresponds to the best cnet model with pruning threshold 0.001 and weighted pooling. <COT> Looking at the "method" column, we can see different methods used for the experiments, such as "1-best baseline", "weighted pooling", "[ITALIC] cnet - score threshold 0.01", and "[ITALIC] ensemble models".
<R> <C> [BOLD] method <C> [BOLD] goals <C> [BOLD] requests <R> <C> 1-best baseline <C> 63.6 66.658.7 <C> 96.8 97.196.5 <R> <C> [ITALIC] cnet - no pruning <C> [ITALIC] cnet - no pruning <C> [ITALIC] cnet - no pruning <R> <C> weighted pooling <C> 63.7 65.661.6 <C> 96.7 97.096.3 <R> <C> [ITALIC] cnet - score threshold 0.001 <C> [ITALIC] cnet - score threshold 0.001 <C> [ITALIC] cnet - score threshold 0.001 <R> <C> average pooling <C> 63.7 66.460.0 <C> 96.6 96.896.0 <R> <C> weighted pooling <C> [BOLD] 65.2 68.559.1 <C> 97.0 97.496.6 <R> <C> [ITALIC] cnet - score threshold 0.01 <C> [ITALIC] cnet - score threshold 0.01 <C> [ITALIC] cnet - score threshold 0.01 <R> <C> average pooling <C> 64.6 67.959.7 <C> 96.9⋆ 97.296.5 <R> <C> weighted pooling <C> 64.7 68.462.2 <C> [BOLD] 97.1⋆ 97.396.9 <R> <C> [ITALIC] ensemble models <C> [ITALIC] ensemble models <C> [ITALIC] ensemble models <R> <C> baseline <C> 69.7 <C> 96.7 <R> <C> cnet <C> [BOLD] 71.4 <C> [BOLD] 97.2 <R> <C> [ITALIC] results from related work <C> [ITALIC] results from related work <C> [ITALIC] results from related work <R> <C> Vodolán et al. ( 2017 ) <C> 80.0 <C> - <R> <C> Williams ( 2014 ) <C> 78.4 <C> 98.0 <R> <C> Mrksic et al. ( 2017 ) <C> 73.4 <C> 96.5 <CAP> Table 3: DSTC2 test set accuracy of ten runs with different random seeds in the format average maximumminimum. ⋆ denotes a statistically significant higher result than the baseline (p<0.05, Wilcoxon signed-rank test with Bonferroni correction for ten repeated comparisons). The cnet ensemble corresponds to the best cnet model with pruning threshold 0.001 and weighted pooling. <COT> Looking at the "cnet" row, we can see that the "cnet" method has higher accuracy than the "baseline" method for both "goals" and "requests" columns.
<R> <C> [BOLD] test data <C> [BOLD] goals <C> [BOLD] requests <R> <C> [ITALIC] train on transcripts + batch ASR (baseline) <C> [ITALIC] train on transcripts + batch ASR (baseline) <C> [ITALIC] train on transcripts + batch ASR (baseline) <R> <C> [ITALIC] batch ASR <C> 63.6 66.658.7 <C> 96.8 97.196.5 <R> <C> [ITALIC] train on transcripts + live ASR <C> [ITALIC] train on transcripts + live ASR <C> [ITALIC] train on transcripts + live ASR <R> <C> [ITALIC] live ASR <C> 63.8 67.060.2 <C> 97.5 97.797.2 <R> <C> transcripts <C> 78.3 82.474.3 <C> 98.7 99.098.0 <CAP> Table 2: DSTC2 test set accuracy for 1-best ASR outputs of ten runs with different random seeds in the format average maximumminimum. <COT> Looking at the "test data" column, we can see that the "batch ASR" method has higher accuracy than the "live ASR" method.
<R> <C> [BOLD] test data <C> [BOLD] goals <C> [BOLD] requests <R> <C> [ITALIC] train on transcripts + batch ASR (baseline) <C> [ITALIC] train on transcripts + batch ASR (baseline) <C> [ITALIC] train on transcripts + batch ASR (baseline) <R> <C> [ITALIC] batch ASR <C> 63.6 66.658.7 <C> 96.8 97.196.5 <R> <C> [ITALIC] train on transcripts + live ASR <C> [ITALIC] train on transcripts + live ASR <C> [ITALIC] train on transcripts + live ASR <R> <C> [ITALIC] live ASR <C> 63.8 67.060.2 <C> 97.5 97.797.2 <R> <C> transcripts <C> 78.3 82.474.3 <C> 98.7 99.098.0 <CAP> Table 2: DSTC2 test set accuracy for 1-best ASR outputs of ten runs with different random seeds in the format average maximumminimum. <COT> Looking at the "goals" column, we can see that the accuracy of the "transcripts" method is higher than the accuracy of both the "batch ASR" and "live ASR" methods.
<R> <C> [BOLD] Gold Class <C> [BOLD] Train <C> [BOLD] Dev <C> [BOLD] Test <R> <C> None <C> 15,401 <C> 3,905 <C> 4,141 <R> <C> Obligation <C> 11,005 <C> 2,860 <C> 970 <R> <C> Prohibition <C> 1,172 <C> 314 <C> 108 <R> <C> Obligation List Intro <C> 828 <C> 203 <C> 70 <R> <C> Obligation List Item <C> 2888 <C> 726 <C> 255 <R> <C> Prohibition List Item <C> 251 <C> 28 <C> 19 <R> <C> [BOLD] Total <C> [BOLD] 31,545 <C> [BOLD] 8,036 <C> [BOLD] 5,563 <CAP> Table 2: Sentences/clauses after sentence splitting. <COT> Looking at the "Total" row, "Train" cell, and "Test" cell, we can see that the total number of sentences/clauses in the training set is 31,545 and in the test set is 5,563.
<R> <C> [BOLD] Gold Class <C> [BOLD] Train <C> [BOLD] Dev <C> [BOLD] Test <R> <C> None <C> 15,401 <C> 3,905 <C> 4,141 <R> <C> Obligation <C> 11,005 <C> 2,860 <C> 970 <R> <C> Prohibition <C> 1,172 <C> 314 <C> 108 <R> <C> Obligation List Intro <C> 828 <C> 203 <C> 70 <R> <C> Obligation List Item <C> 2888 <C> 726 <C> 255 <R> <C> Prohibition List Item <C> 251 <C> 28 <C> 19 <R> <C> [BOLD] Total <C> [BOLD] 31,545 <C> [BOLD] 8,036 <C> [BOLD] 5,563 <CAP> Table 2: Sentences/clauses after sentence splitting. <COT> Looking at the "Obligation" row, "Dev" cell, and "Test" cell, we can see that the number of sentences/clauses labeled as "Obligation" in the development set is 2,860 and in the test set is 970.
<R> <C> Ru <C> Ja <C> En <C> #sent. <C> Usage test <C> Usage development <R> <C> ✓ <C> ✓ <C> ✓ <C> 913 <C> 600 <C> 313 <R> <C> ✓ <C> ✓ <C> [EMPTY] <C> 173 <C> - <C> 173 <R> <C> [EMPTY] <C> ✓ <C> ✓ <C> 276 <C> - <C> 276 <R> <C> ✓ <C> [EMPTY] <C> ✓ <C> 0 <C> - <C> - <R> <C> ✓ <C> [EMPTY] <C> [EMPTY] <C> 4 <C> - <C> - <R> <C> [EMPTY] <C> ✓ <C> [EMPTY] <C> 287 <C> - <C> - <R> <C> [EMPTY] <C> [EMPTY] <C> ✓ <C> 1 <C> - <C> - <R> <C> Total <C> Total <C> Total <C> 1,654 <C> - <C> - <CAP> Table 1: Manually aligned News Commentary data. <COT> Looking at the "Total" row, we can see that the total number of sentences in the Ru, Ja, and En columns is 1,654.
<R> <C> Ru <C> Ja <C> En <C> #sent. <C> Usage test <C> Usage development <R> <C> ✓ <C> ✓ <C> ✓ <C> 913 <C> 600 <C> 313 <R> <C> ✓ <C> ✓ <C> [EMPTY] <C> 173 <C> - <C> 173 <R> <C> [EMPTY] <C> ✓ <C> ✓ <C> 276 <C> - <C> 276 <R> <C> ✓ <C> [EMPTY] <C> ✓ <C> 0 <C> - <C> - <R> <C> ✓ <C> [EMPTY] <C> [EMPTY] <C> 4 <C> - <C> - <R> <C> [EMPTY] <C> ✓ <C> [EMPTY] <C> 287 <C> - <C> - <R> <C> [EMPTY] <C> [EMPTY] <C> ✓ <C> 1 <C> - <C> - <R> <C> Total <C> Total <C> Total <C> 1,654 <C> - <C> - <CAP> Table 1: Manually aligned News Commentary data. <COT> Looking at the "Usage test" column, we can see that there are 600 sentences in the Ru column and 313 sentences in the En column.
<R> <C> [BOLD] Gold Class <C> [BOLD] bilstm P <C> [BOLD] bilstm R <C> [BOLD] bilstm F1 <C> [BOLD] bilstm AUC <C> [BOLD] bilstm-att P <C> [BOLD] bilstm-att R <C> [BOLD] bilstm-att F1 <C> [BOLD] bilstm-att AUC <C> [BOLD] x-bilstm-att P <C> [BOLD] x-bilstm-att R <C> [BOLD] x-bilstm-att F1 <C> [BOLD] x-bilstm-att AUC <C> [BOLD] h-bilstm-att P <C> [BOLD] h-bilstm-att R <C> [BOLD] h-bilstm-att F1 <C> [BOLD] h-bilstm-att AUC <R> <C> None <C> 0.95 <C> 0.91 <C> 0.93 <C> 0.98 <C> 0.97 <C> 0.90 <C> 0.93 <C> [BOLD] 0.99 <C> 0.96 <C> 0.90 <C> 0.93 <C> 0.98 <C> [BOLD] 0.98 <C> [BOLD] 0.96 <C> [BOLD] 0.97 <C> [BOLD] 0.99 <R> <C> Obligation <C> 0.75 <C> 0.85 <C> 0.79 <C> 0.86 <C> 0.75 <C> 0.88 <C> 0.81 <C> 0.86 <C> 0.75 <C> 0.87 <C> 0.81 <C> 0.88 <C> [BOLD] 0.87 <C> [BOLD] 0.92 <C> [BOLD] 0.90 <C> [BOLD] 0.96 <R> <C> Prohibition <C> 0.67 <C> 0.62 <C> 0.64 <C> 0.75 <C> 0.74 <C> 0.75 <C> 0.74 <C> 0.80 <C> 0.65 <C> 0.75 <C> 0.70 <C> 0.74 <C> [BOLD] 0.84 <C> [BOLD] 0.83 <C> [BOLD] 0.84 <C> [BOLD] 0.90 <R> <C> Obl. List Begin <C> 0.70 <C> 0.86 <C> 0.77 <C> 0.81 <C> 0.71 <C> 0.85 <C> 0.77 <C> 0.83 <C> 0.72 <C> 0.75 <C> 0.74 <C> 0.80 <C> [BOLD] 0.90 <C> [BOLD] 0.89 <C> [BOLD] 0.89 <C> [BOLD] 0.93 <R> <C> Obl. List Item <C> 0.53 <C> 0.66 <C> 0.59 <C> 0.64 <C> 0.48 <C> 0.70 <C> 0.57 <C> 0.60 <C> 0.49 <C> 0.78 <C> 0.60 <C> 0.66 <C> [BOLD] 0.85 <C> [BOLD] 0.94 <C> [BOLD] 0.89 <C> [BOLD] 0.94 <R> <C> Proh. List Item <C> 0.59 <C> 0.35 <C> 0.43 <C> 0.50 <C> 0.61 <C> 0.55 <C> 0.59 <C> 0.62 <C> [BOLD] 0.83 <C> 0.50 <C> 0.62 <C> 0.67 <C> 0.80 <C> [BOLD] 0.84 <C> [BOLD] 0.82 <C> [BOLD] 0.92 <R> <C> [BOLD] Macro-average <C> 0.70 <C> 0.70 <C> 0.70 <C> 0.74 <C> 0.73 <C> 0.78 <C> 0.74 <C> 0.78 <C> 0.73 <C> 0.76 <C> 0.73 <C> 0.79 <C> [BOLD] 0.87 <C> [BOLD] 0.90 <C> [BOLD] 0.89 <C> [BOLD] 0.94 <R> <C> [BOLD] Micro-average <C> 0.90 <C> 0.88 <C> 0.88 <C> 0.94 <C> 0.90 <C> 0.88 <C> 0.89 <C> 0.96 <C> 0.90 <C> 0.88 <C> 0.89 <C> 0.94 <C> [BOLD] 0.95 <C> [BOLD] 0.95 <C> [BOLD] 0.95 <C> [BOLD] 0.98 <CAP> Table 3: Precision, recall, f1, and auc scores, with the best results in bold and gray background. <COT> Looking at the "Micro-average" row, we can see that the bilstm-att model has a higher F1 score than the bilstm model.
<R> <C> [BOLD] Gold Class <C> [BOLD] bilstm P <C> [BOLD] bilstm R <C> [BOLD] bilstm F1 <C> [BOLD] bilstm AUC <C> [BOLD] bilstm-att P <C> [BOLD] bilstm-att R <C> [BOLD] bilstm-att F1 <C> [BOLD] bilstm-att AUC <C> [BOLD] x-bilstm-att P <C> [BOLD] x-bilstm-att R <C> [BOLD] x-bilstm-att F1 <C> [BOLD] x-bilstm-att AUC <C> [BOLD] h-bilstm-att P <C> [BOLD] h-bilstm-att R <C> [BOLD] h-bilstm-att F1 <C> [BOLD] h-bilstm-att AUC <R> <C> None <C> 0.95 <C> 0.91 <C> 0.93 <C> 0.98 <C> 0.97 <C> 0.90 <C> 0.93 <C> [BOLD] 0.99 <C> 0.96 <C> 0.90 <C> 0.93 <C> 0.98 <C> [BOLD] 0.98 <C> [BOLD] 0.96 <C> [BOLD] 0.97 <C> [BOLD] 0.99 <R> <C> Obligation <C> 0.75 <C> 0.85 <C> 0.79 <C> 0.86 <C> 0.75 <C> 0.88 <C> 0.81 <C> 0.86 <C> 0.75 <C> 0.87 <C> 0.81 <C> 0.88 <C> [BOLD] 0.87 <C> [BOLD] 0.92 <C> [BOLD] 0.90 <C> [BOLD] 0.96 <R> <C> Prohibition <C> 0.67 <C> 0.62 <C> 0.64 <C> 0.75 <C> 0.74 <C> 0.75 <C> 0.74 <C> 0.80 <C> 0.65 <C> 0.75 <C> 0.70 <C> 0.74 <C> [BOLD] 0.84 <C> [BOLD] 0.83 <C> [BOLD] 0.84 <C> [BOLD] 0.90 <R> <C> Obl. List Begin <C> 0.70 <C> 0.86 <C> 0.77 <C> 0.81 <C> 0.71 <C> 0.85 <C> 0.77 <C> 0.83 <C> 0.72 <C> 0.75 <C> 0.74 <C> 0.80 <C> [BOLD] 0.90 <C> [BOLD] 0.89 <C> [BOLD] 0.89 <C> [BOLD] 0.93 <R> <C> Obl. List Item <C> 0.53 <C> 0.66 <C> 0.59 <C> 0.64 <C> 0.48 <C> 0.70 <C> 0.57 <C> 0.60 <C> 0.49 <C> 0.78 <C> 0.60 <C> 0.66 <C> [BOLD] 0.85 <C> [BOLD] 0.94 <C> [BOLD] 0.89 <C> [BOLD] 0.94 <R> <C> Proh. List Item <C> 0.59 <C> 0.35 <C> 0.43 <C> 0.50 <C> 0.61 <C> 0.55 <C> 0.59 <C> 0.62 <C> [BOLD] 0.83 <C> 0.50 <C> 0.62 <C> 0.67 <C> 0.80 <C> [BOLD] 0.84 <C> [BOLD] 0.82 <C> [BOLD] 0.92 <R> <C> [BOLD] Macro-average <C> 0.70 <C> 0.70 <C> 0.70 <C> 0.74 <C> 0.73 <C> 0.78 <C> 0.74 <C> 0.78 <C> 0.73 <C> 0.76 <C> 0.73 <C> 0.79 <C> [BOLD] 0.87 <C> [BOLD] 0.90 <C> [BOLD] 0.89 <C> [BOLD] 0.94 <R> <C> [BOLD] Micro-average <C> 0.90 <C> 0.88 <C> 0.88 <C> 0.94 <C> 0.90 <C> 0.88 <C> 0.89 <C> 0.96 <C> 0.90 <C> 0.88 <C> 0.89 <C> 0.94 <C> [BOLD] 0.95 <C> [BOLD] 0.95 <C> [BOLD] 0.95 <C> [BOLD] 0.98 <CAP> Table 3: Precision, recall, f1, and auc scores, with the best results in bold and gray background. <COT> Looking at the "Prohibition" row, we can see that the bilstm-att model has a higher AUC score than the bilstm model.
<R> <C> ID <C> Pseudo-parallel data involved Ja∗→Ru <C> Pseudo-parallel data involved Ru∗→Ja <C> Pseudo-parallel data involved Ja∗→En <C> Pseudo-parallel data involved En∗→Ja <C> Pseudo-parallel data involved Ru∗→En <C> Pseudo-parallel data involved En∗→Ru <C> BLEU score Ja→Ru <C> BLEU score Ru→Ja <C> BLEU score Ja→En <C> BLEU score En→Ja <C> BLEU score Ru→En <C> BLEU score En→Ru <R> <C> (b3) <C> - <C> - <C> - <C> - <C> - <C> - <C> 3.72 <C> 8.35 <C> 10.24 <C> 12.43 <C> 22.10 <C> 16.92 <R> <C> #1 <C> ✓ <C> - <C> - <C> - <C> - <C> - <C> ∙ [BOLD] 4.59 <C> [BOLD] 8.63 <C> [BOLD] 10.64 <C> [BOLD] 12.94 <C> [BOLD] 22.21 <C> [BOLD] 17.30 <R> <C> #2 <C> - <C> ✓ <C> - <C> - <C> - <C> - <C> [BOLD] 3.74 <C> ∙ [BOLD] 8.85 <C> 10.13 <C> [BOLD] 13.05 <C> [BOLD] 22.48 <C> [BOLD] 17.20 <R> <C> #3 <C> ✓ <C> ✓ <C> - <C> - <C> - <C> - <C> ∙ [BOLD] 4.56 <C> ∙ [BOLD] 9.09 <C> [BOLD] 10.57 <C> ∙ [BOLD] 13.23 <C> [BOLD] 22.48 <C> ∙ [BOLD] 17.89 <R> <C> 2-13 #4 <C> - <C> - <C> ✓ <C> - <C> - <C> - <C> 3.71 <C> 8.05 <C> ∙ [BOLD] 11.00 <C> [BOLD] 12.66 <C> [BOLD] 22.17 <C> 16.76 <R> <C> #5 <C> - <C> - <C> - <C> ✓ <C> - <C> - <C> 3.62 <C> 8.10 <C> 9.92 <C> ∙ [BOLD] 14.06 <C> 21.66 <C> 16.68 <R> <C> #6 <C> - <C> - <C> ✓ <C> ✓ <C> - <C> - <C> 3.61 <C> 7.94 <C> ∙ [BOLD] 11.51 <C> ∙ [BOLD] 14.38 <C> [BOLD] 22.22 <C> 16.80 <R> <C> 2-13 #7 <C> - <C> - <C> - <C> - <C> ✓ <C> - <C> [BOLD] 3.80 <C> [BOLD] 8.37 <C> [BOLD] 10.67 <C> [BOLD] 13.00 <C> [BOLD] 22.51 <C> ∙ [BOLD] 17.73 <R> <C> #8 <C> - <C> - <C> - <C> - <C> - <C> ✓ <C> [BOLD] 3.77 <C> 8.04 <C> [BOLD] 10.52 <C> 12.43 <C> ∙ [BOLD] 22.85 <C> [BOLD] 17.13 <R> <C> #9 <C> - <C> - <C> - <C> - <C> ✓ <C> ✓ <C> 3.37 <C> 8.03 <C> 10.19 <C> [BOLD] 12.79 <C> [BOLD] 22.77 <C> [BOLD] 17.26 <R> <C> 2-13 #10 <C> ✓ <C> ✓ <C> ✓ <C> ✓ <C> ✓ <C> ✓ <C> ∙ [BOLD] 4.43 <C> ∙ [BOLD] 9.38 <C> ∙ [BOLD] 12.06 <C> ∙ [BOLD] 14.43 <C> ∙ [BOLD] 23.09 <C> [BOLD] 17.30 <CAP> Table 8: BLEU scores of M2M Transformer NMT systems trained on the mixture of given parallel corpus and pseudo-parallel data generated by back-translation using (b3). Six “X∗→Y” columns show whether the pseudo-parallel data for each translation direction is involved. Bold indicates the scores higher than (b3) and “∙” indicates statistical significance of the improvement. <COT> Looking at the "BLEU score Ja→En" column, we can see that the scores for translations involving pseudo-parallel data generated by back-translation from Ja to En are higher than the score for translation involving pseudo-parallel data generated by back-translation from En to Ja.
<R> <C> ID <C> Pseudo-parallel data involved Ja∗→Ru <C> Pseudo-parallel data involved Ru∗→Ja <C> Pseudo-parallel data involved Ja∗→En <C> Pseudo-parallel data involved En∗→Ja <C> Pseudo-parallel data involved Ru∗→En <C> Pseudo-parallel data involved En∗→Ru <C> BLEU score Ja→Ru <C> BLEU score Ru→Ja <C> BLEU score Ja→En <C> BLEU score En→Ja <C> BLEU score Ru→En <C> BLEU score En→Ru <R> <C> (b3) <C> - <C> - <C> - <C> - <C> - <C> - <C> 3.72 <C> 8.35 <C> 10.24 <C> 12.43 <C> 22.10 <C> 16.92 <R> <C> #1 <C> ✓ <C> - <C> - <C> - <C> - <C> - <C> ∙ [BOLD] 4.59 <C> [BOLD] 8.63 <C> [BOLD] 10.64 <C> [BOLD] 12.94 <C> [BOLD] 22.21 <C> [BOLD] 17.30 <R> <C> #2 <C> - <C> ✓ <C> - <C> - <C> - <C> - <C> [BOLD] 3.74 <C> ∙ [BOLD] 8.85 <C> 10.13 <C> [BOLD] 13.05 <C> [BOLD] 22.48 <C> [BOLD] 17.20 <R> <C> #3 <C> ✓ <C> ✓ <C> - <C> - <C> - <C> - <C> ∙ [BOLD] 4.56 <C> ∙ [BOLD] 9.09 <C> [BOLD] 10.57 <C> ∙ [BOLD] 13.23 <C> [BOLD] 22.48 <C> ∙ [BOLD] 17.89 <R> <C> 2-13 #4 <C> - <C> - <C> ✓ <C> - <C> - <C> - <C> 3.71 <C> 8.05 <C> ∙ [BOLD] 11.00 <C> [BOLD] 12.66 <C> [BOLD] 22.17 <C> 16.76 <R> <C> #5 <C> - <C> - <C> - <C> ✓ <C> - <C> - <C> 3.62 <C> 8.10 <C> 9.92 <C> ∙ [BOLD] 14.06 <C> 21.66 <C> 16.68 <R> <C> #6 <C> - <C> - <C> ✓ <C> ✓ <C> - <C> - <C> 3.61 <C> 7.94 <C> ∙ [BOLD] 11.51 <C> ∙ [BOLD] 14.38 <C> [BOLD] 22.22 <C> 16.80 <R> <C> 2-13 #7 <C> - <C> - <C> - <C> - <C> ✓ <C> - <C> [BOLD] 3.80 <C> [BOLD] 8.37 <C> [BOLD] 10.67 <C> [BOLD] 13.00 <C> [BOLD] 22.51 <C> ∙ [BOLD] 17.73 <R> <C> #8 <C> - <C> - <C> - <C> - <C> - <C> ✓ <C> [BOLD] 3.77 <C> 8.04 <C> [BOLD] 10.52 <C> 12.43 <C> ∙ [BOLD] 22.85 <C> [BOLD] 17.13 <R> <C> #9 <C> - <C> - <C> - <C> - <C> ✓ <C> ✓ <C> 3.37 <C> 8.03 <C> 10.19 <C> [BOLD] 12.79 <C> [BOLD] 22.77 <C> [BOLD] 17.26 <R> <C> 2-13 #10 <C> ✓ <C> ✓ <C> ✓ <C> ✓ <C> ✓ <C> ✓ <C> ∙ [BOLD] 4.43 <C> ∙ [BOLD] 9.38 <C> ∙ [BOLD] 12.06 <C> ∙ [BOLD] 14.43 <C> ∙ [BOLD] 23.09 <C> [BOLD] 17.30 <CAP> Table 8: BLEU scores of M2M Transformer NMT systems trained on the mixture of given parallel corpus and pseudo-parallel data generated by back-translation using (b3). Six “X∗→Y” columns show whether the pseudo-parallel data for each translation direction is involved. Bold indicates the scores higher than (b3) and “∙” indicates statistical significance of the improvement. <COT> Looking at the "Pseudo-parallel data involved Ja∗→Ru" column, we can see that there is no pseudo-parallel data involved in the translation from Ja to Ru.
<R> <C> [BOLD] Model <C> [BOLD] 5-Way 1-Shot  [BOLD] On 1.0 <C> [BOLD] 5-Way 1-Shot  [BOLD] On 2.0 <C> [BOLD] 5-Way 5-Shot  [BOLD] On 1.0 <C> [BOLD] 5-Way 5-Shot  [BOLD] On 2.0 <R> <C> GNN (CNN) <C> 66.23±0.75 <C> 27.94±0.03 <C> 81.28±0.62 <C> 29.33±0.11 <R> <C> Proto (CNN) <C> 74.52±0.07 <C> 35.09±0.10 <C> 88.40±0.06 <C> 49.37±0.10 <R> <C> Proto-ADV (CNN) <C> 70.28±0.15 <C> 42.21±0.09 <C> 84.63±0.07 <C> 58.71±0.06 <R> <C> Proto (BERT) <C> 80.68±0.28 <C> 40.12±0.19 <C> 89.60±0.09 <C> 51.50±0.29 <R> <C> Proto-ADV (BERT) <C> 73.35±0.95 <C> 41.90±0.44 <C> 82.30±0.53 <C> 54.74±0.22 <R> <C> BERT-PAIR <C> 88.32±0.64 <C> 56.25±0.40 <C> 93.22±0.13 <C> 67.44±0.54 <R> <C> [BOLD] Model <C> [BOLD] 10-Way 1-Shot <C> [BOLD] 10-Way 1-Shot <C> [BOLD] 10-Way 5-Shot <C> [BOLD] 10-Way 5-Shot <R> <C> [BOLD] Model <C> [BOLD] On 1.0 <C> [BOLD] On 2.0 <C> [BOLD] On 1.0 <C> [BOLD] On 2.0 <R> <C> GNN (CNN) <C> 46.27±0.80 <C> 16.44±0.04 <C> 64.02±0.77 <C> 18.26±0.03 <R> <C> Proto (CNN) <C> 62.38±0.06 <C> 22.98±0.05 <C> 80.45±0.08 <C> 35.22±0.06 <R> <C> Proto-ADV (CNN) <C> 56.34±0.08 <C> 28.91±0.10 <C> 74.67±0.12 <C> 44.35±0.09 <R> <C> Proto (BERT) <C> 71.48±0.15 <C> 26.45±0.10 <C> 82.89±0.11 <C> 36.93±0.01 <R> <C> Proto-ADV (BERT) <C> 61.49±0.69 <C> 27.36±0.50 <C> 72.60±0.38 <C> 37.40±0.36 <R> <C> BERT-PAIR <C> 80.63±0.17 <C> 43.64±0.46 <C> 87.02±0.12 <C> 53.17±0.09 <CAP> Table 2: Accuracies (%) on few-shot DA. “On 1.0” represents the results on the original FewRel dataset and “On 2.0” represents the results on the new test set. The models with “-ADV” use adversarial training described in Section 3. <COT> Looking at the "Model" column, we can see that there are two models mentioned - GNN (CNN) and Proto (CNN). 
<R> <C> [BOLD] Model <C> [BOLD] 5-Way-1-Shot  [BOLD] 0% NOTA <C> [BOLD] 5-Way-1-Shot  [BOLD] 15% NOTA <C> [BOLD] 5-Way-1-Shot  [BOLD] 30% NOTA <C> [BOLD] 5-Way-1-Shot  [BOLD] 50% NOTA <R> <C> Proto (CNN)* <C> 74.52±0.07 <C> 62.18±0.22 <C> 53.38±0.14 <C> 37.26±0.04 <R> <C> Proto (CNN) <C> 69.17±0.07 <C> 60.59±0.05 <C> 53.18±0.12 <C> 40.00±0.10 <R> <C> Proto (BERT)* <C> 80.68±0.28 <C> 67.92±0.31 <C> 58.22±0.20 <C> 40.64±0.14 <R> <C> Proto (BERT) <C> 81.65±0.97 <C> 70.02±0.23 <C> 61.08±0.28 <C> 45.94±0.50 <R> <C> BERT-PAIR* <C> 88.32±0.64 <C> 73.60±0.51 <C> 63.00±0.47 <C> 43.99±0.09 <R> <C> BERT-PAIR <C> 76.73±0.55 <C> 77.67±0.14 <C> 78.49±0.21 <C> 80.31±0.12 <CAP> Table 3: Accuracies (%) on few-shot NOTA. Models with * simply ignore the NOTA setting and assume all queries can be classified as one of the given relations. <COT> Looking at the "Model" column, we can see that there are two versions of each model, one with an asterisk (*) and one without. The models with an asterisk ignore the NOTA setting and assume all queries can be classified as one of the given relations.
<R> <C> [BOLD] Model <C> [BOLD] 5-Way-1-Shot  [BOLD] 0% NOTA <C> [BOLD] 5-Way-1-Shot  [BOLD] 15% NOTA <C> [BOLD] 5-Way-1-Shot  [BOLD] 30% NOTA <C> [BOLD] 5-Way-1-Shot  [BOLD] 50% NOTA <R> <C> Proto (CNN)* <C> 74.52±0.07 <C> 62.18±0.22 <C> 53.38±0.14 <C> 37.26±0.04 <R> <C> Proto (CNN) <C> 69.17±0.07 <C> 60.59±0.05 <C> 53.18±0.12 <C> 40.00±0.10 <R> <C> Proto (BERT)* <C> 80.68±0.28 <C> 67.92±0.31 <C> 58.22±0.20 <C> 40.64±0.14 <R> <C> Proto (BERT) <C> 81.65±0.97 <C> 70.02±0.23 <C> 61.08±0.28 <C> 45.94±0.50 <R> <C> BERT-PAIR* <C> 88.32±0.64 <C> 73.60±0.51 <C> 63.00±0.47 <C> 43.99±0.09 <R> <C> BERT-PAIR <C> 76.73±0.55 <C> 77.67±0.14 <C> 78.49±0.21 <C> 80.31±0.12 <CAP> Table 3: Accuracies (%) on few-shot NOTA. Models with * simply ignore the NOTA setting and assume all queries can be classified as one of the given relations. <COT> Looking at the "5-Way-1-Shot 30% NOTA" column, we can see that the accuracy decreases as the percentage of NOTA increases.
<R> <C> Typing methods <C> Wiki/Figer(gold) Acc. <C> Wiki/Figer(gold) Ma-F1 <C> Wiki/Figer(gold) Mi-F1 <C> OntoNotes Acc. <C> OntoNotes Ma-F1 <C> OntoNotes Mi-F1 <C> BBN Acc. <C> BBN Ma-F1 <C> BBN Mi-F1 <R> <C> [BOLD] FIGER*  <C> 0.474 <C> 0.692 <C> 0.655 <C> 0.369 <C> 0.578 <C> 0.516 <C> 0.467 <C> 0.672 <C> 0.612 <R> <C> [BOLD] HYENA*  <C> 0.288 <C> 0.528 <C> 0.506 <C> 0.249 <C> 0.497 <C> 0.446 <C> 0.523 <C> 0.576 <C> 0.587 <R> <C> [BOLD] AFET-NoCo*  <C> 0.526 <C> 0.693 <C> 0.654 <C> 0.486 <C> 0.652 <C> 0.594 <C> 0.655 <C> 0.711 <C> 0.716 <R> <C> [BOLD] AFET-CoH*  <C> 0.433 <C> 0.583 <C> 0.551 <C> 0.521 <C> 0.680 <C> 0.609 <C> 0.657 <C> 0.703 <C> 0.712 <R> <C> [BOLD] AFET*  <C> 0.533 <C> 0.693 <C> 0.664 <C> 0.551 <C> 0.711 <C> 0.647 <C> 0.670 <C> 0.727 <C> 0.735 <R> <C> [BOLD] AFET†‡  <C> 0.509 <C> 0.689 <C> 0.653 <C> [BOLD] 0.553 <C> [BOLD] 0.712 <C> [BOLD] 0.646 <C> 0.683 <C> 0.744 <C> 0.747 <R> <C> [BOLD] Attentive†  <C> 0.581 <C> 0.780 <C> 0.744 <C> 0.473 <C> 0.655 <C> 0.586 <C> 0.484 <C> 0.732 <C> 0.724 <R> <C> [BOLD] our-AllC† <C> [BOLD] 0.662 <C> 0.805 <C> 0.770 <C> 0.514 <C> 0.672 <C> 0.626 <C> 0.655 <C> 0.736 <C> 0.752 <R> <C> [BOLD] our-NoM† <C> 0.646 <C> 0.808 <C> 0.768 <C> 0.521 <C> 0.683 <C> 0.626 <C> 0.615 <C> 0.742 <C> 0.755 <R> <C> [BOLD] our† <C> 0.658 <C> [BOLD] 0.812 <C> [BOLD] 0.774 <C> 0.522 <C> 0.685 <C> 0.633 <C> 0.604 <C> 0.741 <C> 0.757 <R> <C> [BOLD] model level transfer-learning† <C> - <C> - <C> - <C> 0.531 <C> 0.684 <C> 0.637 <C> 0.645 <C> 0.784 <C> [BOLD] 0.795 <R> <C> [BOLD] feature level transfer-learning† <C> - <C> - <C> - <C> 0.471 <C> 0.689 <C> 0.635 <C> [BOLD] 0.733 <C> [BOLD] 0.791 <C> 0.792 <CAP> Table 2: Performance analysis of entity classification methods on the three datasets. <COT> Looking at the table, we can see that the AFET†‡ method has the highest performance in terms of OntoNotes Acc., OntoNotes Ma-F1, and OntoNotes Mi-F1.
<R> <C> Typing methods <C> Wiki/Figer(gold) Acc. <C> Wiki/Figer(gold) Ma-F1 <C> Wiki/Figer(gold) Mi-F1 <C> OntoNotes Acc. <C> OntoNotes Ma-F1 <C> OntoNotes Mi-F1 <C> BBN Acc. <C> BBN Ma-F1 <C> BBN Mi-F1 <R> <C> [BOLD] FIGER*  <C> 0.474 <C> 0.692 <C> 0.655 <C> 0.369 <C> 0.578 <C> 0.516 <C> 0.467 <C> 0.672 <C> 0.612 <R> <C> [BOLD] HYENA*  <C> 0.288 <C> 0.528 <C> 0.506 <C> 0.249 <C> 0.497 <C> 0.446 <C> 0.523 <C> 0.576 <C> 0.587 <R> <C> [BOLD] AFET-NoCo*  <C> 0.526 <C> 0.693 <C> 0.654 <C> 0.486 <C> 0.652 <C> 0.594 <C> 0.655 <C> 0.711 <C> 0.716 <R> <C> [BOLD] AFET-CoH*  <C> 0.433 <C> 0.583 <C> 0.551 <C> 0.521 <C> 0.680 <C> 0.609 <C> 0.657 <C> 0.703 <C> 0.712 <R> <C> [BOLD] AFET*  <C> 0.533 <C> 0.693 <C> 0.664 <C> 0.551 <C> 0.711 <C> 0.647 <C> 0.670 <C> 0.727 <C> 0.735 <R> <C> [BOLD] AFET†‡  <C> 0.509 <C> 0.689 <C> 0.653 <C> [BOLD] 0.553 <C> [BOLD] 0.712 <C> [BOLD] 0.646 <C> 0.683 <C> 0.744 <C> 0.747 <R> <C> [BOLD] Attentive†  <C> 0.581 <C> 0.780 <C> 0.744 <C> 0.473 <C> 0.655 <C> 0.586 <C> 0.484 <C> 0.732 <C> 0.724 <R> <C> [BOLD] our-AllC† <C> [BOLD] 0.662 <C> 0.805 <C> 0.770 <C> 0.514 <C> 0.672 <C> 0.626 <C> 0.655 <C> 0.736 <C> 0.752 <R> <C> [BOLD] our-NoM† <C> 0.646 <C> 0.808 <C> 0.768 <C> 0.521 <C> 0.683 <C> 0.626 <C> 0.615 <C> 0.742 <C> 0.755 <R> <C> [BOLD] our† <C> 0.658 <C> [BOLD] 0.812 <C> [BOLD] 0.774 <C> 0.522 <C> 0.685 <C> 0.633 <C> 0.604 <C> 0.741 <C> 0.757 <R> <C> [BOLD] model level transfer-learning† <C> - <C> - <C> - <C> 0.531 <C> 0.684 <C> 0.637 <C> 0.645 <C> 0.784 <C> [BOLD] 0.795 <R> <C> [BOLD] feature level transfer-learning† <C> - <C> - <C> - <C> 0.471 <C> 0.689 <C> 0.635 <C> [BOLD] 0.733 <C> [BOLD] 0.791 <C> 0.792 <CAP> Table 2: Performance analysis of entity classification methods on the three datasets. <COT> Looking at the table, we can see that the Attentive† method has the highest performance in terms of Wiki/Figer(gold) Acc., Wiki/Figer(gold) Ma-F1, and Wiki/Figer(gold) Mi-F1.
<R> <C> Label type <C> Support <C> our Prec. <C> our Rec. <C> our F-1 <C> AFET Prec. <C> AFET Rec. <C> AFET F-1 <R> <C> [ITALIC] /other <C> 42.6% <C> 0.838 <C> 0.809 <C> 0.823 <C> 0.774 <C> 0.962 <C> [BOLD] 0.858 <R> <C> [ITALIC] /organization <C> 11.0% <C> 0.588 <C> 0.490 <C> [BOLD] 0.534 <C> 0.903 <C> 0.273 <C> 0.419 <R> <C> [ITALIC] /person <C> 9.9% <C> 0.559 <C> 0.467 <C> [BOLD] 0.508 <C> 0.669 <C> 0.352 <C> 0.461 <R> <C> [ITALIC] /organization/company <C> 7.8% <C> 0.932 <C> 0.166 <C> [BOLD] 0.282 <C> 1.0 <C> 0.127 <C> 0.225 <R> <C> [ITALIC] /location <C> 7.5% <C> 0.687 <C> 0.796 <C> [BOLD] 0.737 <C> 0.787 <C> 0.609 <C> 0.687 <R> <C> [ITALIC] /organization/government <C> 2.1% <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> [ITALIC] /location/country <C> 2.0% <C> 0.783 <C> 0.614 <C> [BOLD] 0.688 <C> 0.838 <C> 0.498 <C> 0.625 <R> <C> [ITALIC] /other/legal <C> 1.8% <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> [ITALIC] /location/city <C> 1.8% <C> 0.919 <C> 0.610 <C> [BOLD] 0.733 <C> 0.816 <C> 0.637 <C> 0.715 <R> <C> [ITALIC] /person/political_figure <C> 1.6% <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <CAP> Table 4: Performance analysis of the proposed model and AFET on top 10 (in terms of type frequency) types present in OntoNotes dataset. <COT> Looking at the table, we can see that the AFET F-1 score is higher than our F-1 score for all label types except for "/other".
<R> <C> Label type <C> Support <C> our Prec. <C> our Rec. <C> our F-1 <C> AFET Prec. <C> AFET Rec. <C> AFET F-1 <R> <C> [ITALIC] /other <C> 42.6% <C> 0.838 <C> 0.809 <C> 0.823 <C> 0.774 <C> 0.962 <C> [BOLD] 0.858 <R> <C> [ITALIC] /organization <C> 11.0% <C> 0.588 <C> 0.490 <C> [BOLD] 0.534 <C> 0.903 <C> 0.273 <C> 0.419 <R> <C> [ITALIC] /person <C> 9.9% <C> 0.559 <C> 0.467 <C> [BOLD] 0.508 <C> 0.669 <C> 0.352 <C> 0.461 <R> <C> [ITALIC] /organization/company <C> 7.8% <C> 0.932 <C> 0.166 <C> [BOLD] 0.282 <C> 1.0 <C> 0.127 <C> 0.225 <R> <C> [ITALIC] /location <C> 7.5% <C> 0.687 <C> 0.796 <C> [BOLD] 0.737 <C> 0.787 <C> 0.609 <C> 0.687 <R> <C> [ITALIC] /organization/government <C> 2.1% <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> [ITALIC] /location/country <C> 2.0% <C> 0.783 <C> 0.614 <C> [BOLD] 0.688 <C> 0.838 <C> 0.498 <C> 0.625 <R> <C> [ITALIC] /other/legal <C> 1.8% <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> [ITALIC] /location/city <C> 1.8% <C> 0.919 <C> 0.610 <C> [BOLD] 0.733 <C> 0.816 <C> 0.637 <C> 0.715 <R> <C> [ITALIC] /person/political_figure <C> 1.6% <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <CAP> Table 4: Performance analysis of the proposed model and AFET on top 10 (in terms of type frequency) types present in OntoNotes dataset. <COT> Looking at the table, we can see that the highest F-1 score achieved by our model is for the label type "/location".
<R> <C> feature <C> cos <C> f1-neigh <C> f1-lr <C> f1-net <C> type <R> <C> is_heavy <C> 0.15 <C> 0.15 <C> 0.17 <C> 0.21 <C> op <R> <C> is_strong <C> 0.15 <C> 0.13 <C> 0.13 <C> 0.34 <C> e <R> <C> is_thin <C> 0.16 <C> 0 <C> 0.05 <C> 0.1 <C> vp <R> <C> is_hard <C> 0.16 <C> 0.15 <C> 0.08 <C> 0.26 <C> op <R> <C> is_expensive <C> 0.16 <C> 0 <C> 0.28 <C> 0.37 <C> e <R> <C> … <C> … <C> … <C> … <C> … <C> [EMPTY] <R> <C> is_black <C> 0.2 <C> 0.29 <C> 0.23 <C> 0.24 <C> vp <R> <C> is_electric <C> 0.21 <C> 0.48 <C> 0.5 <C> 0.69 <C> vp <R> <C> is_dangerous <C> 0.21 <C> 0.53 <C> 0.57 <C> 0.59 <C> e <R> <C> is_colourful <C> 0.21 <C> 0.14 <C> 0.25 <C> 0.32 <C> vp <R> <C> is_brown <C> 0.21 <C> 0.13 <C> 0.22 <C> 0.33 <C> vp <R> <C> has_a_handle _handles <C> 0.22 <C> 0.44 <C> 0.57 <C> 0.58 <C> p <R> <C> has_a_seat _seats <C> 0.22 <C> 0.43 <C> 0.3 <C> 0.48 <C> p <R> <C> does_smell _is_smelly <C> 0.22 <C> 0.08 <C> 0.15 <C> 0.37 <C> op <R> <C> made_of_glass <C> 0.22 <C> 0.29 <C> 0 <C> 0.28 <C> vp <R> <C> has_a_point <C> 0.23 <C> 0.38 <C> 0.23 <C> 0.47 <C> p <R> <C> does_protect <C> 0.24 <C> 0.38 <C> 0.26 <C> 0.37 <C> f <R> <C> is_yellow <C> 0.24 <C> 0.22 <C> 0 <C> 0.23 <C> vp <R> <C> is_soft <C> 0.24 <C> 0.12 <C> 0 <C> 0.16 <C> op <R> <C> is_red <C> 0.25 <C> 0.34 <C> 0.13 <C> 0.27 <C> vp <R> <C> is_fast <C> 0.25 <C> 0.3 <C> 0.31 <C> 0.48 <C> vp <R> <C> is_tall <C> 0.25 <C> 0.43 <C> 0.57 <C> 0.65 <C> vp <R> <C> is_a_tool <C> 0.26 <C> 0.5 <C> 0.51 <C> 0.47 <C> t <R> <C> … <C> … <C> … <C> … <C> … <C> [EMPTY] <R> <C> is_a_weapon <C> 0.3 <C> 0.74 <C> 0.56 <C> 0.63 <C> t <R> <C> is_green <C> 0.31 <C> 0.45 <C> 0.45 <C> 0.45 <C> vp <R> <C> has_a_ blade_blades <C> 0.32 <C> 0.68 <C> 0.65 <C> 0.74 <C> p <R> <C> is_worn <C> 0.32 <C> 0.47 <C> 0.86 <C> 0.9 <C> f <R> <C> has_wheels <C> 0.32 <C> 0.82 <C> 0.83 <C> 0.87 <C> p <R> <C> is_found _in_kitchens <C> 0.33 <C> 0.56 <C> 0.73 <C> 0.76 <C> e <R> <C> does_fly <C> 0.33 <C> 0.57 <C> 0.76 <C> 0.76 <C> f <R> <C> has_a_tail <C> 0.33 <C> 0.53 <C> 0.68 <C> 0.69 <C> p <R> <C> is_an_animal <C> 0.33 <C> 0.64 <C> 0.76 <C> 0.78 <C> t <R> <C> is_eaten_edible <C> 0.33 <C> 0.37 <C> 0.88 <C> 0.85 <C> f <R> <C> has_four_legs <C> 0.34 <C> 0.67 <C> 0.66 <C> 0.66 <C> p <R> <C> is_a_vehicle <C> 0.34 <C> 0.76 <C> 0.69 <C> 0.79 <C> t <R> <C> does_eat <C> 0.34 <C> 0.68 <C> 0.71 <C> 0.68 <C> f <R> <C> … <C> … <C> … <C> … <C> … <C> [EMPTY] <R> <C> has_a_beak <C> 0.37 <C> 0.63 <C> 0.83 <C> 0.87 <C> p <R> <C> made_of_cotton <C> 0.37 <C> 0.68 <C> 0.56 <C> 0.64 <C> vp <R> <C> has_roots <C> 0.37 <C> 0.3 <C> 0.65 <C> 0.72 <C> p <R> <C> is_a_mammal <C> 0.37 <C> 0.69 <C> 0.85 <C> 0.86 <C> t <R> <C> does_grow <C> 0.37 <C> 0.52 <C> 0.81 <C> 0.81 <C> e <R> <C> is_a_plant <C> 0.37 <C> 0.43 <C> 0.63 <C> 0.64 <C> t <R> <C> has_leaves <C> 0.37 <C> 0.41 <C> 0.71 <C> 0.78 <C> p <R> <C> … <C> … <C> … <C> … <C> … <C> [EMPTY] <R> <C> has_pips_seeds <C> 0.47 <C> 0.5 <C> 0.08 <C> 0.46 <C> p <R> <C> is_juicy <C> 0.5 <C> 0.71 <C> 0.48 <C> 0.56 <C> op <R> <C> is_a_vegetable <C> 0.52 <C> 0.78 <C> 0.75 <C> 0.81 <C> t <R> <C> is_played _does_play <C> 0.53 <C> 0.9 <C> 0.98 <C> 0.98 <C> f <R> <C> does_make_music <C> 0.55 <C> 0.89 <C> 0.95 <C> 0.92 <C> f <R> <C> spearman-r <C> [EMPTY] <C> 0.72 <C> 0.52 <C> 0.59 <C> [EMPTY] <CAP> Table 2: Performance of different approaches in relation to the average cosine similarity of words associated with a property (cos). The last row shows the Spearman Rank correlation between f1-scores and average cosine similarity. Property types are listed under type (p = part, vp = visual-perceptual, op = other-perceptual, e = encyclopaedic, f = functional, t = taxonomic). <COT> Looking at the "type" column, we can see that there are different property types listed such as part, visual-perceptual, other-perceptual, encyclopaedic, functional, and taxonomic.
<R> <C> feature <C> cos <C> f1-neigh <C> f1-lr <C> f1-net <C> type <R> <C> is_heavy <C> 0.15 <C> 0.15 <C> 0.17 <C> 0.21 <C> op <R> <C> is_strong <C> 0.15 <C> 0.13 <C> 0.13 <C> 0.34 <C> e <R> <C> is_thin <C> 0.16 <C> 0 <C> 0.05 <C> 0.1 <C> vp <R> <C> is_hard <C> 0.16 <C> 0.15 <C> 0.08 <C> 0.26 <C> op <R> <C> is_expensive <C> 0.16 <C> 0 <C> 0.28 <C> 0.37 <C> e <R> <C> … <C> … <C> … <C> … <C> … <C> [EMPTY] <R> <C> is_black <C> 0.2 <C> 0.29 <C> 0.23 <C> 0.24 <C> vp <R> <C> is_electric <C> 0.21 <C> 0.48 <C> 0.5 <C> 0.69 <C> vp <R> <C> is_dangerous <C> 0.21 <C> 0.53 <C> 0.57 <C> 0.59 <C> e <R> <C> is_colourful <C> 0.21 <C> 0.14 <C> 0.25 <C> 0.32 <C> vp <R> <C> is_brown <C> 0.21 <C> 0.13 <C> 0.22 <C> 0.33 <C> vp <R> <C> has_a_handle _handles <C> 0.22 <C> 0.44 <C> 0.57 <C> 0.58 <C> p <R> <C> has_a_seat _seats <C> 0.22 <C> 0.43 <C> 0.3 <C> 0.48 <C> p <R> <C> does_smell _is_smelly <C> 0.22 <C> 0.08 <C> 0.15 <C> 0.37 <C> op <R> <C> made_of_glass <C> 0.22 <C> 0.29 <C> 0 <C> 0.28 <C> vp <R> <C> has_a_point <C> 0.23 <C> 0.38 <C> 0.23 <C> 0.47 <C> p <R> <C> does_protect <C> 0.24 <C> 0.38 <C> 0.26 <C> 0.37 <C> f <R> <C> is_yellow <C> 0.24 <C> 0.22 <C> 0 <C> 0.23 <C> vp <R> <C> is_soft <C> 0.24 <C> 0.12 <C> 0 <C> 0.16 <C> op <R> <C> is_red <C> 0.25 <C> 0.34 <C> 0.13 <C> 0.27 <C> vp <R> <C> is_fast <C> 0.25 <C> 0.3 <C> 0.31 <C> 0.48 <C> vp <R> <C> is_tall <C> 0.25 <C> 0.43 <C> 0.57 <C> 0.65 <C> vp <R> <C> is_a_tool <C> 0.26 <C> 0.5 <C> 0.51 <C> 0.47 <C> t <R> <C> … <C> … <C> … <C> … <C> … <C> [EMPTY] <R> <C> is_a_weapon <C> 0.3 <C> 0.74 <C> 0.56 <C> 0.63 <C> t <R> <C> is_green <C> 0.31 <C> 0.45 <C> 0.45 <C> 0.45 <C> vp <R> <C> has_a_ blade_blades <C> 0.32 <C> 0.68 <C> 0.65 <C> 0.74 <C> p <R> <C> is_worn <C> 0.32 <C> 0.47 <C> 0.86 <C> 0.9 <C> f <R> <C> has_wheels <C> 0.32 <C> 0.82 <C> 0.83 <C> 0.87 <C> p <R> <C> is_found _in_kitchens <C> 0.33 <C> 0.56 <C> 0.73 <C> 0.76 <C> e <R> <C> does_fly <C> 0.33 <C> 0.57 <C> 0.76 <C> 0.76 <C> f <R> <C> has_a_tail <C> 0.33 <C> 0.53 <C> 0.68 <C> 0.69 <C> p <R> <C> is_an_animal <C> 0.33 <C> 0.64 <C> 0.76 <C> 0.78 <C> t <R> <C> is_eaten_edible <C> 0.33 <C> 0.37 <C> 0.88 <C> 0.85 <C> f <R> <C> has_four_legs <C> 0.34 <C> 0.67 <C> 0.66 <C> 0.66 <C> p <R> <C> is_a_vehicle <C> 0.34 <C> 0.76 <C> 0.69 <C> 0.79 <C> t <R> <C> does_eat <C> 0.34 <C> 0.68 <C> 0.71 <C> 0.68 <C> f <R> <C> … <C> … <C> … <C> … <C> … <C> [EMPTY] <R> <C> has_a_beak <C> 0.37 <C> 0.63 <C> 0.83 <C> 0.87 <C> p <R> <C> made_of_cotton <C> 0.37 <C> 0.68 <C> 0.56 <C> 0.64 <C> vp <R> <C> has_roots <C> 0.37 <C> 0.3 <C> 0.65 <C> 0.72 <C> p <R> <C> is_a_mammal <C> 0.37 <C> 0.69 <C> 0.85 <C> 0.86 <C> t <R> <C> does_grow <C> 0.37 <C> 0.52 <C> 0.81 <C> 0.81 <C> e <R> <C> is_a_plant <C> 0.37 <C> 0.43 <C> 0.63 <C> 0.64 <C> t <R> <C> has_leaves <C> 0.37 <C> 0.41 <C> 0.71 <C> 0.78 <C> p <R> <C> … <C> … <C> … <C> … <C> … <C> [EMPTY] <R> <C> has_pips_seeds <C> 0.47 <C> 0.5 <C> 0.08 <C> 0.46 <C> p <R> <C> is_juicy <C> 0.5 <C> 0.71 <C> 0.48 <C> 0.56 <C> op <R> <C> is_a_vegetable <C> 0.52 <C> 0.78 <C> 0.75 <C> 0.81 <C> t <R> <C> is_played _does_play <C> 0.53 <C> 0.9 <C> 0.98 <C> 0.98 <C> f <R> <C> does_make_music <C> 0.55 <C> 0.89 <C> 0.95 <C> 0.92 <C> f <R> <C> spearman-r <C> [EMPTY] <C> 0.72 <C> 0.52 <C> 0.59 <C> [EMPTY] <CAP> Table 2: Performance of different approaches in relation to the average cosine similarity of words associated with a property (cos). The last row shows the Spearman Rank correlation between f1-scores and average cosine similarity. Property types are listed under type (p = part, vp = visual-perceptual, op = other-perceptual, e = encyclopaedic, f = functional, t = taxonomic). <COT> Looking at the "is_eaten_edible" row, we can see that the f1-neigh score is 0.37, the f1-lr score is 0.88, and the f1-net score is 0.85.
<R> <C> [BOLD] Network <C> [BOLD] Training Time <C> [BOLD] Parameters <R> <C> [BOLD] bilstm <C> 5h 30m <C> 1,278M <R> <C> [BOLD] bilstm-att <C> 8h 30m <C> 1,279M <R> <C> [BOLD] x-bilstm-att <C> 25h 40m <C> 1,279M <R> <C> [BOLD] h-bilstm-att <C> 2h 30m <C> 1,837M <CAP> Table 4: Training times and parameters to learn. <COT> Looking at the "Network" column, we can see that the "x-bilstm-att" model has the longest training time.
<R> <C> [BOLD] Network <C> [BOLD] Training Time <C> [BOLD] Parameters <R> <C> [BOLD] bilstm <C> 5h 30m <C> 1,278M <R> <C> [BOLD] bilstm-att <C> 8h 30m <C> 1,279M <R> <C> [BOLD] x-bilstm-att <C> 25h 40m <C> 1,279M <R> <C> [BOLD] h-bilstm-att <C> 2h 30m <C> 1,837M <CAP> Table 4: Training times and parameters to learn. <COT> Looking at the "Parameters" column, we can see that the "h-bilstm-att" model has the highest number of parameters.
<R> <C> property <C> av-cos <C> neigh <C> lr <C> net1 <C> net2 <R> <C> full_is_yellow <C> 0.23 <C> 0.19 <C> 0.47 <C> 0.64 <C> 0.64 <R> <C> full_is_used_in _cooking <C> 0.37 <C> 0.29 <C> 0.98 <C> 0.98 <C> 0.98 <R> <C> full_is_black <C> 0.19 <C> 0.35 <C> 0.75 <C> 0.77 <C> 0.77 <R> <C> full_is_red <C> 0.23 <C> 0.36 <C> 0.51 <C> 0.54 <C> 0.52 <R> <C> full_is_dangerous <C> 0.24 <C> 0.58 <C> 0.88 <C> 0.88 <C> 0.87 <R> <C> crowd_is_dangerous <C> 0.26 <C> 0.61 <C> 0.86 <C> 0.86 <C> 0.86 <R> <C> full_has_wheels <C> 0.38 <C> 0.90 <C> 0.96 <C> 0.96 <C> 0.95 <R> <C> full_is_found_in_seas <C> 0.44 <C> 0.87 <C> 0.97 <C> 0.98 <C> 0.98 <R> <C> crowd_is_found _in_seas <C> 0.50 <C> 0.87 <C> 0.94 <C> 0.96 <C> 0.96 <R> <C> full_does_kill <C> 0.27 <C> 0.67 <C> 0.83 <C> 0.86 <C> 0.82 <R> <C> crowd_does_kill <C> 0.30 <C> 0.70 <C> 0.82 <C> 0.84 <C> 0.80 <R> <C> full_made_of_wood <C> 0.17 <C> 0.14 <C> 0.84 <C> 0.85 <C> 0.85 <R> <C> full_is_food_test <C> 0.37 <C> 0.00 <C> 0.36 <C> 0.36 <C> 0.36 <R> <C> full_is_an _animal_test <C> 0.37 <C> 0.52 <C> 0.88 <C> 0.88 <C> 0.88 <CAP> Table 4: F1 scores achieved by logistic regression (lr) two runs of a neural net classifier (net1 and net2 and the n-best nearest neighbors evaluated with leave-one-out on the full datasets (marked as full_ and the crow-only sets (marked as crowd_). <COT> Looking at the "property" column, we can see that there are different properties listed, such as "full_is_yellow" and "full_is_used_in_cooking".
<R> <C> SNLI <C> w <C> [BOLD] MEN 71.78 <C> [BOLD] MTurk287 35.40 <C> [BOLD] MTurk771 49.05 <C> [BOLD] RG65 61.80 <C> [BOLD] RW 18.43 <C> [BOLD] SimLex999 19.17 <C> [BOLD] SimVerb3500 10.32 <C> [BOLD] WS353 39.27 <C> [BOLD] WS353R 28.01 <C> [BOLD] WS353S 53.42 <R> <C> [EMPTY] <C> c <C> 9.85 <C> -5.65 <C> 0.82 <C> -5.28 <C> 17.81 <C> 0.86 <C> 2.76 <C> -2.20 <C> 0.20 <C> -3.87 <R> <C> [EMPTY] <C> cat <C> 71.91 <C> [BOLD] 35.52 <C> 48.84 <C> 62.12 <C> 18.46 <C> 19.10 <C> 10.21 <C> 39.35 <C> 28.16 <C> 53.40 <R> <C> [EMPTY] <C> sg <C> 70.49 <C> 34.49 <C> 46.15 <C> 59.75 <C> 18.24 <C> 17.20 <C> 8.73 <C> 35.86 <C> 23.48 <C> 50.83 <R> <C> [EMPTY] <C> vg <C> [BOLD] 80.00 <C> 32.54 <C> [BOLD] 62.09 <C> [BOLD] 68.90 <C> [BOLD] 20.76 <C> [BOLD] 37.70 <C> [BOLD] 20.45 <C> [BOLD] 54.72 <C> [BOLD] 47.24 <C> [BOLD] 65.60 <R> <C> MNLI <C> w <C> 68.76 <C> 50.15 <C> 68.81 <C> 65.83 <C> 18.43 <C> 42.21 <C> 25.18 <C> 61.10 <C> 58.21 <C> 70.17 <R> <C> [EMPTY] <C> c <C> 4.84 <C> 0.06 <C> 1.95 <C> -0.06 <C> 12.18 <C> 3.01 <C> 1.52 <C> -4.68 <C> -3.63 <C> -3.65 <R> <C> [EMPTY] <C> cat <C> 68.77 <C> 50.40 <C> 68.77 <C> 65.92 <C> 18.35 <C> 42.22 <C> 25.12 <C> 61.15 <C> 58.26 <C> 70.21 <R> <C> [EMPTY] <C> sg <C> 67.66 <C> 49.58 <C> 68.29 <C> 64.84 <C> 18.36 <C> 41.81 <C> 24.57 <C> 60.13 <C> 57.09 <C> 69.41 <R> <C> [EMPTY] <C> vg <C> [BOLD] 76.69 <C> [BOLD] 56.06 <C> [BOLD] 70.13 <C> [BOLD] 69.00 <C> [BOLD] 25.35 <C> [BOLD] 48.40 <C> [BOLD] 35.12 <C> [BOLD] 68.91 <C> [BOLD] 64.70 <C> [BOLD] 77.23 <CAP> Table 1: Word-level evaluation results. Each value corresponds to average Pearson correlation of 7 identical models initialized with different random seeds. Correlations were scaled to the [−100;100] range for easier reading. Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset. For each task and dataset, every best-performing method was significantly different to other methods (p<0.05), except for w trained in SNLI at the MTurk287 task. Statistical significance was obtained with a two-sided Welch’s t-test for two independent samples without assuming equal variance (Welch, 1947). <COT> Looking at the "SNLI" row and the "vg" cell, we can see that the best-performing method for the Word-level evaluation results in the VG task is the model trained with the SNLI dataset, with a correlation of [BOLD] 76.69.
<R> <C> SNLI <C> w <C> [BOLD] MEN 71.78 <C> [BOLD] MTurk287 35.40 <C> [BOLD] MTurk771 49.05 <C> [BOLD] RG65 61.80 <C> [BOLD] RW 18.43 <C> [BOLD] SimLex999 19.17 <C> [BOLD] SimVerb3500 10.32 <C> [BOLD] WS353 39.27 <C> [BOLD] WS353R 28.01 <C> [BOLD] WS353S 53.42 <R> <C> [EMPTY] <C> c <C> 9.85 <C> -5.65 <C> 0.82 <C> -5.28 <C> 17.81 <C> 0.86 <C> 2.76 <C> -2.20 <C> 0.20 <C> -3.87 <R> <C> [EMPTY] <C> cat <C> 71.91 <C> [BOLD] 35.52 <C> 48.84 <C> 62.12 <C> 18.46 <C> 19.10 <C> 10.21 <C> 39.35 <C> 28.16 <C> 53.40 <R> <C> [EMPTY] <C> sg <C> 70.49 <C> 34.49 <C> 46.15 <C> 59.75 <C> 18.24 <C> 17.20 <C> 8.73 <C> 35.86 <C> 23.48 <C> 50.83 <R> <C> [EMPTY] <C> vg <C> [BOLD] 80.00 <C> 32.54 <C> [BOLD] 62.09 <C> [BOLD] 68.90 <C> [BOLD] 20.76 <C> [BOLD] 37.70 <C> [BOLD] 20.45 <C> [BOLD] 54.72 <C> [BOLD] 47.24 <C> [BOLD] 65.60 <R> <C> MNLI <C> w <C> 68.76 <C> 50.15 <C> 68.81 <C> 65.83 <C> 18.43 <C> 42.21 <C> 25.18 <C> 61.10 <C> 58.21 <C> 70.17 <R> <C> [EMPTY] <C> c <C> 4.84 <C> 0.06 <C> 1.95 <C> -0.06 <C> 12.18 <C> 3.01 <C> 1.52 <C> -4.68 <C> -3.63 <C> -3.65 <R> <C> [EMPTY] <C> cat <C> 68.77 <C> 50.40 <C> 68.77 <C> 65.92 <C> 18.35 <C> 42.22 <C> 25.12 <C> 61.15 <C> 58.26 <C> 70.21 <R> <C> [EMPTY] <C> sg <C> 67.66 <C> 49.58 <C> 68.29 <C> 64.84 <C> 18.36 <C> 41.81 <C> 24.57 <C> 60.13 <C> 57.09 <C> 69.41 <R> <C> [EMPTY] <C> vg <C> [BOLD] 76.69 <C> [BOLD] 56.06 <C> [BOLD] 70.13 <C> [BOLD] 69.00 <C> [BOLD] 25.35 <C> [BOLD] 48.40 <C> [BOLD] 35.12 <C> [BOLD] 68.91 <C> [BOLD] 64.70 <C> [BOLD] 77.23 <CAP> Table 1: Word-level evaluation results. Each value corresponds to average Pearson correlation of 7 identical models initialized with different random seeds. Correlations were scaled to the [−100;100] range for easier reading. Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset. For each task and dataset, every best-performing method was significantly different to other methods (p<0.05), except for w trained in SNLI at the MTurk287 task. Statistical significance was obtained with a two-sided Welch’s t-test for two independent samples without assuming equal variance (Welch, 1947). <COT> Looking at the "MNLI" row and the "ws" cell, we can see that the correlation for the MNLI dataset in the WS353 task is 61.10.
<R> <C> [EMPTY] <C> [EMPTY] <C> Classification  [BOLD] CR <C> Classification  [BOLD] MPQA <C> Classification  [BOLD] MR <C> Classification  [BOLD] SST2 <C> Classification  [BOLD] SST5 <C> Classification  [BOLD] SUBJ <C> Classification  [BOLD] TREC <C> Entailment  [BOLD] SICKE <C> Relatedness  [BOLD] SICKR† <C> Semantic Textual Similarity  [BOLD] STS16† <C> Semantic Textual Similarity  [BOLD] STSB† <R> <C> SNLI <C> w <C> 80.50 <C> 84.59 <C> 74.18 <C> 78.86 <C> 42.33 <C> [BOLD] 90.38 <C> [BOLD] 86.83 <C> 86.37 <C> 88.52 <C> 59.90∗ <C> 71.29∗ <R> <C> [EMPTY] <C> c <C> 74.90∗ <C> 78.86∗ <C> 65.93∗ <C> 69.42∗ <C> 35.56∗ <C> 82.97∗ <C> 83.31∗ <C> 84.13∗ <C> 83.89∗ <C> 59.33∗ <C> 67.20∗ <R> <C> [EMPTY] <C> cat <C> 80.44 <C> 84.66 <C> 74.31 <C> 78.37 <C> 41.34∗ <C> 90.28 <C> 85.80∗ <C> [BOLD] 86.40 <C> 88.44 <C> 59.90∗ <C> 71.24∗ <R> <C> [EMPTY] <C> sg <C> [BOLD] 80.59 <C> 84.60 <C> [BOLD] 74.49 <C> [BOLD] 79.04 <C> 41.63∗ <C> 90.16 <C> 86.00 <C> 86.10∗ <C> [BOLD] 88.57 <C> 60.05∗ <C> 71.34∗ <R> <C> [EMPTY] <C> vg <C> 80.42 <C> [BOLD] 84.66 <C> 74.26 <C> 78.87 <C> [BOLD] 42.38 <C> 90.07 <C> 85.97 <C> 85.67 <C> 88.31∗ <C> [BOLD] 60.92 <C> [BOLD] 71.99 <R> <C> MNLI <C> w <C> 83.80 <C> [BOLD] 89.13 <C> 79.05 <C> 83.38 <C> 45.21 <C> 91.79 <C> 89.23 <C> 84.92 <C> 86.33 <C> 66.08 <C> 71.96∗ <R> <C> [EMPTY] <C> c <C> 70.23∗ <C> 72.19∗ <C> 62.83∗ <C> 64.55∗ <C> 32.47∗ <C> 79.49∗ <C> 74.74∗ <C> 81.53∗ <C> 75.92∗ <C> 51.47∗ <C> 61.74∗ <R> <C> [EMPTY] <C> cat <C> [BOLD] 83.96 <C> 89.12 <C> [BOLD] 79.23 <C> 83.70 <C> 45.08∗ <C> [BOLD] 91.92 <C> [BOLD] 90.03 <C> [BOLD] 85.06 <C> 86.45 <C> [BOLD] 66.17 <C> 71.82∗ <R> <C> [EMPTY] <C> sg <C> 83.88 <C> 89.06 <C> 79.22 <C> 83.71 <C> 45.26 <C> 91.66∗ <C> 88.83∗ <C> 84.96 <C> 86.40 <C> 65.49∗ <C> 71.87∗ <R> <C> [EMPTY] <C> vg <C> 83.45∗ <C> 89.05 <C> 79.13 <C> [BOLD] 83.87 <C> [BOLD] 45.88 <C> 91.55∗ <C> 89.49 <C> 84.82 <C> [BOLD] 86.50 <C> 65.75 <C> [BOLD] 72.82 <CAP> Table 2: Experimental results. Each value shown in the table is the average result of 7 identical models initialized with different random seeds. Values represent accuracy (%) unless indicated by †, in which case they represent Pearson correlation scaled to the range [−100,100] for easier reading. Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset. Values marked with an asterisk (∗) are significantly different to the average performance of the best model trained on the same dataset (p<0.05). Results for every best-performing method trained on one dataset are significantly different to the best-performing method trained on the other. Statistical significance was obtained in the same way as described in table 1. <COT> Looking at the "Classification CR" column, we can see that the best-performing method for the CR task is achieved by training on the SNLI dataset with the "w" setting, with an accuracy of 90.38% (see Table 2).
<R> <C> [EMPTY] <C> [EMPTY] <C> Classification  [BOLD] CR <C> Classification  [BOLD] MPQA <C> Classification  [BOLD] MR <C> Classification  [BOLD] SST2 <C> Classification  [BOLD] SST5 <C> Classification  [BOLD] SUBJ <C> Classification  [BOLD] TREC <C> Entailment  [BOLD] SICKE <C> Relatedness  [BOLD] SICKR† <C> Semantic Textual Similarity  [BOLD] STS16† <C> Semantic Textual Similarity  [BOLD] STSB† <R> <C> SNLI <C> w <C> 80.50 <C> 84.59 <C> 74.18 <C> 78.86 <C> 42.33 <C> [BOLD] 90.38 <C> [BOLD] 86.83 <C> 86.37 <C> 88.52 <C> 59.90∗ <C> 71.29∗ <R> <C> [EMPTY] <C> c <C> 74.90∗ <C> 78.86∗ <C> 65.93∗ <C> 69.42∗ <C> 35.56∗ <C> 82.97∗ <C> 83.31∗ <C> 84.13∗ <C> 83.89∗ <C> 59.33∗ <C> 67.20∗ <R> <C> [EMPTY] <C> cat <C> 80.44 <C> 84.66 <C> 74.31 <C> 78.37 <C> 41.34∗ <C> 90.28 <C> 85.80∗ <C> [BOLD] 86.40 <C> 88.44 <C> 59.90∗ <C> 71.24∗ <R> <C> [EMPTY] <C> sg <C> [BOLD] 80.59 <C> 84.60 <C> [BOLD] 74.49 <C> [BOLD] 79.04 <C> 41.63∗ <C> 90.16 <C> 86.00 <C> 86.10∗ <C> [BOLD] 88.57 <C> 60.05∗ <C> 71.34∗ <R> <C> [EMPTY] <C> vg <C> 80.42 <C> [BOLD] 84.66 <C> 74.26 <C> 78.87 <C> [BOLD] 42.38 <C> 90.07 <C> 85.97 <C> 85.67 <C> 88.31∗ <C> [BOLD] 60.92 <C> [BOLD] 71.99 <R> <C> MNLI <C> w <C> 83.80 <C> [BOLD] 89.13 <C> 79.05 <C> 83.38 <C> 45.21 <C> 91.79 <C> 89.23 <C> 84.92 <C> 86.33 <C> 66.08 <C> 71.96∗ <R> <C> [EMPTY] <C> c <C> 70.23∗ <C> 72.19∗ <C> 62.83∗ <C> 64.55∗ <C> 32.47∗ <C> 79.49∗ <C> 74.74∗ <C> 81.53∗ <C> 75.92∗ <C> 51.47∗ <C> 61.74∗ <R> <C> [EMPTY] <C> cat <C> [BOLD] 83.96 <C> 89.12 <C> [BOLD] 79.23 <C> 83.70 <C> 45.08∗ <C> [BOLD] 91.92 <C> [BOLD] 90.03 <C> [BOLD] 85.06 <C> 86.45 <C> [BOLD] 66.17 <C> 71.82∗ <R> <C> [EMPTY] <C> sg <C> 83.88 <C> 89.06 <C> 79.22 <C> 83.71 <C> 45.26 <C> 91.66∗ <C> 88.83∗ <C> 84.96 <C> 86.40 <C> 65.49∗ <C> 71.87∗ <R> <C> [EMPTY] <C> vg <C> 83.45∗ <C> 89.05 <C> 79.13 <C> [BOLD] 83.87 <C> [BOLD] 45.88 <C> 91.55∗ <C> 89.49 <C> 84.82 <C> [BOLD] 86.50 <C> 65.75 <C> [BOLD] 72.82 <CAP> Table 2: Experimental results. Each value shown in the table is the average result of 7 identical models initialized with different random seeds. Values represent accuracy (%) unless indicated by †, in which case they represent Pearson correlation scaled to the range [−100,100] for easier reading. Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset. Values marked with an asterisk (∗) are significantly different to the average performance of the best model trained on the same dataset (p<0.05). Results for every best-performing method trained on one dataset are significantly different to the best-performing method trained on the other. Statistical significance was obtained in the same way as described in table 1. <COT> Looking at the "Semantic Textual Similarity STSB" column, we can see that the best-performing method for the STSB task is achieved by training on the SNLI dataset with the "vg" setting, with a score of 72.82 (see Table 2).
<R> <C> System <C> “Gotcha”? <C> Female <C> Male <R> <C> Rule <C> no <C> 38.3 <C> 51.7 <R> <C> Rule <C> [HTML]CBCEFByes <C> [HTML]CBCEFB10.0 <C> [HTML]CBCEFB37.5 <R> <C> Stat <C> no <C> 50.8 <C> 61.7 <R> <C> Stat <C> [HTML]CBCEFByes <C> [HTML]CBCEFB45.8 <C> [HTML]CBCEFB40.0 <R> <C> Neural <C> no <C> 50.8 <C> 49.2 <R> <C> Neural <C> [HTML]CBCEFByes <C> [HTML]CBCEFB36.7 <C> [HTML]CBCEFB46.7 <CAP> Table 2: System accuracy (%) bucketed by gender and difficulty (so-called “gotchas,” shaded in purple). For female pronouns, a “gotcha” sentence is one where either (1) the correct answer is occupation but the occupation is <50% female (according to BLS); or (2) the occupation is ≥50% female but the correct answer is participant; this is reversed for male pronouns. Systems do uniformly worse on “gotchas.” <COT> Looking at the "System" column, we can see that there are three different systems: Rule, Stat, and Neural. 
<R> <C> System <C> “Gotcha”? <C> Female <C> Male <R> <C> Rule <C> no <C> 38.3 <C> 51.7 <R> <C> Rule <C> [HTML]CBCEFByes <C> [HTML]CBCEFB10.0 <C> [HTML]CBCEFB37.5 <R> <C> Stat <C> no <C> 50.8 <C> 61.7 <R> <C> Stat <C> [HTML]CBCEFByes <C> [HTML]CBCEFB45.8 <C> [HTML]CBCEFB40.0 <R> <C> Neural <C> no <C> 50.8 <C> 49.2 <R> <C> Neural <C> [HTML]CBCEFByes <C> [HTML]CBCEFB36.7 <C> [HTML]CBCEFB46.7 <CAP> Table 2: System accuracy (%) bucketed by gender and difficulty (so-called “gotchas,” shaded in purple). For female pronouns, a “gotcha” sentence is one where either (1) the correct answer is occupation but the occupation is <50% female (according to BLS); or (2) the occupation is ≥50% female but the correct answer is participant; this is reversed for male pronouns. Systems do uniformly worse on “gotchas.” <COT> Looking at the "“Gotcha”?" column, we can see that there are two categories: "no" and "[HTML]CBCEFByes". 
<R> <C> [BOLD] Model <C> [BOLD] B-1 <C> [BOLD] B-2 <C> [BOLD] B-3 <C> [BOLD] B-4 <C> [BOLD] ROUGE-L <C> [BOLD] METEOR <C> [BOLD] CIDEr <R> <C> Facts-to-seq <C> 0.404 <C> 0.324 <C> 0.274 <C> 0.242 <C> 0.433 <C> 0.214 <C> 1.627 <R> <C> Facts-to-seq w. Attention <C> 0.491 <C> 0.414 <C> 0.366 <C> 0.335 <C> 0.512 <C> 0.257 <C> 2.207 <R> <C> Static Memory <C> 0.374 <C> 0.298 <C> 0.255 <C> 0.223 <C> 0.383 <C> 0.185 <C> 1.328 <R> <C> DMN+ <C> 0.281 <C> 0.234 <C> 0.236 <C> 0.234 <C> 0.275 <C> 0.139 <C> 0.912 <R> <C> Our Model <C> [BOLD] 0.611 <C> [BOLD] 0.535 <C> [BOLD] 0.485 <C> [BOLD] 0.461 <C> [BOLD] 0.641 <C> [BOLD] 0.353 <C> [BOLD] 3.295 <CAP> Table 1: Automatic evaluation results of different models. For a detailed explanation of the baseline models, please refer to Section 3.2. The best performing model for each column is highlighted in boldface. <COT> Looking at the "Model" column, we can see that the model "Our Model" performs the best in all evaluation metrics.
<R> <C> [BOLD] Model <C> [BOLD] B-1 <C> [BOLD] B-2 <C> [BOLD] B-3 <C> [BOLD] B-4 <C> [BOLD] ROUGE-L <C> [BOLD] METEOR <C> [BOLD] CIDEr <R> <C> Facts-to-seq <C> 0.404 <C> 0.324 <C> 0.274 <C> 0.242 <C> 0.433 <C> 0.214 <C> 1.627 <R> <C> Facts-to-seq w. Attention <C> 0.491 <C> 0.414 <C> 0.366 <C> 0.335 <C> 0.512 <C> 0.257 <C> 2.207 <R> <C> Static Memory <C> 0.374 <C> 0.298 <C> 0.255 <C> 0.223 <C> 0.383 <C> 0.185 <C> 1.328 <R> <C> DMN+ <C> 0.281 <C> 0.234 <C> 0.236 <C> 0.234 <C> 0.275 <C> 0.139 <C> 0.912 <R> <C> Our Model <C> [BOLD] 0.611 <C> [BOLD] 0.535 <C> [BOLD] 0.485 <C> [BOLD] 0.461 <C> [BOLD] 0.641 <C> [BOLD] 0.353 <C> [BOLD] 3.295 <CAP> Table 1: Automatic evaluation results of different models. For a detailed explanation of the baseline models, please refer to Section 3.2. The best performing model for each column is highlighted in boldface. <COT> Looking at the "ROUGE-L" column, we can see that the "Our Model" has the highest ROUGE-L score compared to other models.
<R> <C> Model <C> Val <C> Test <C> Hard <R> <C> Baseline <C> 84.25 <C> 84.22 <C> 68.02 <R> <C> AdvCls <C> 84.58 <C> 83.56 <C> 66.27 <R> <C> AdvDat <C> 78.45 <C> 78.30 <C> 55.60 <CAP> Table 2: Accuracies for the approaches. Baseline refers to the unmodified, non-adversarial InferSent. <COT> Looking at the "Model" column, we can see that there are three different models: Baseline, AdvCls, and AdvDat.
<R> <C> Model <C> Val <C> Test <C> Hard <R> <C> Baseline <C> 84.25 <C> 84.22 <C> 68.02 <R> <C> AdvCls <C> 84.58 <C> 83.56 <C> 66.27 <R> <C> AdvDat <C> 78.45 <C> 78.30 <C> 55.60 <CAP> Table 2: Accuracies for the approaches. Baseline refers to the unmodified, non-adversarial InferSent. <COT> Looking at the "Hard" column, we can see that the accuracy scores for the Hard dataset are lower than the scores for the Val and Test datasets for all three models.
<R> <C> Word <C> Count <C> Score ^ [ITALIC] p( [ITALIC] l| [ITALIC] w) <C> Score Baseline <C> Percentage decrease from baseline AdvCls (1,1) <C> Percentage decrease from baseline AdvDat (0.4,1) <C> Percentage decrease from baseline AdvDat (1,1) <R> <C> sleeping <C> 108 <C> 0.88 <C> 0.24 <C> 15.63 <C> 53.13 <C> -81.25 <R> <C> driving <C> 53 <C> 0.81 <C> 0.32 <C> -8.33 <C> 50 <C> -66.67 <R> <C> Nobody <C> 52 <C> 1 <C> 0.42 <C> 14.29 <C> 42.86 <C> 14.29 <R> <C> alone <C> 50 <C> 0.9 <C> 0.32 <C> 0 <C> 83.33 <C> 0 <R> <C> cat <C> 49 <C> 0.84 <C> 0.31 <C> 7.14 <C> 57.14 <C> -85.71 <R> <C> asleep <C> 43 <C> 0.91 <C> 0.39 <C> -18.75 <C> 50 <C> 12.5 <R> <C> no <C> 31 <C> 0.84 <C> 0.36 <C> 0 <C> 52.94 <C> -52.94 <R> <C> empty <C> 28 <C> 0.93 <C> 0.3 <C> -16.67 <C> 83.33 <C> -16.67 <R> <C> eats <C> 24 <C> 0.83 <C> 0.3 <C> 37.5 <C> 87.5 <C> -25 <R> <C> naked <C> 20 <C> 0.95 <C> 0.46 <C> 0 <C> 83.33 <C> -33.33 <CAP> Table 3: Indicator words and how correlated they are with CONTRADICTION predictions. The parentheses indicate hyper-parameter values: (λLoss,λEnc) for AdvCls and (λRand,λEnc) for AdvDat. Baseline refers to the unmodified InferSent. <COT> Looking at the "Percentage decrease from baseline AdvCls (1,1)" column, we can see that for the word "alone", the percentage decrease from the baseline score is 83.33.
<R> <C> Word <C> Count <C> Score ^ [ITALIC] p( [ITALIC] l| [ITALIC] w) <C> Score Baseline <C> Percentage decrease from baseline AdvCls (1,1) <C> Percentage decrease from baseline AdvDat (0.4,1) <C> Percentage decrease from baseline AdvDat (1,1) <R> <C> sleeping <C> 108 <C> 0.88 <C> 0.24 <C> 15.63 <C> 53.13 <C> -81.25 <R> <C> driving <C> 53 <C> 0.81 <C> 0.32 <C> -8.33 <C> 50 <C> -66.67 <R> <C> Nobody <C> 52 <C> 1 <C> 0.42 <C> 14.29 <C> 42.86 <C> 14.29 <R> <C> alone <C> 50 <C> 0.9 <C> 0.32 <C> 0 <C> 83.33 <C> 0 <R> <C> cat <C> 49 <C> 0.84 <C> 0.31 <C> 7.14 <C> 57.14 <C> -85.71 <R> <C> asleep <C> 43 <C> 0.91 <C> 0.39 <C> -18.75 <C> 50 <C> 12.5 <R> <C> no <C> 31 <C> 0.84 <C> 0.36 <C> 0 <C> 52.94 <C> -52.94 <R> <C> empty <C> 28 <C> 0.93 <C> 0.3 <C> -16.67 <C> 83.33 <C> -16.67 <R> <C> eats <C> 24 <C> 0.83 <C> 0.3 <C> 37.5 <C> 87.5 <C> -25 <R> <C> naked <C> 20 <C> 0.95 <C> 0.46 <C> 0 <C> 83.33 <C> -33.33 <CAP> Table 3: Indicator words and how correlated they are with CONTRADICTION predictions. The parentheses indicate hyper-parameter values: (λLoss,λEnc) for AdvCls and (λRand,λEnc) for AdvDat. Baseline refers to the unmodified InferSent. <COT> Looking at the "Score ^ [ITALIC] p( [ITALIC] l| [ITALIC] w)" column, we can see that for the word "cat", the score is 0.84.
<R> <C> Model <C> amh <C> ara <C> ben <C> fas <C> hin <C> hun <C> orm <C> rus <C> som <C> tgl <C> tir <C> uig <C> yor <C> avg <R> <C> Cap. <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> SRILM <C> 43.9 <C> 30.9 <C> 59.8 <C> 38.2 <C> 50.1 <C> 64.3 <C> 50.1 <C> 53.1 <C> 68.3 <C> 68.0 <C> 62.5 <C> 35.5 <C> 66.5 <C> 53.1 <R> <C> Skip-gram <C> 25.9 <C> 10.5 <C> 20.4 <C> 15.4 <C> 16.1 <C> 46.8 <C> 31.5 <C> 36.4 <C> 36.2 <C> 44.3 <C> 35.3 <C> 13.4 <C> 43.0 <C> 28.9 <R> <C> CBOW <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BiLSTM <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: NEI F1 scores. There are two language models: one for entity, and one for non-entity. All LMs are trained on the standard Train split. At test time, the tag associated with the language model which gives the lowest perplexity is used as prediction. Consecutive tokens with the same tag are joined into a phrase. Scores are Phrase-level F1. <COT> Looking at the "Model" column, we can see that the BiLSTM model has no scores listed for any language.
<R> <C> Model <C> amh <C> ara <C> ben <C> fas <C> hin <C> hun <C> orm <C> rus <C> som <C> tgl <C> tir <C> uig <C> yor <C> avg <R> <C> Cap. <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> SRILM <C> 43.9 <C> 30.9 <C> 59.8 <C> 38.2 <C> 50.1 <C> 64.3 <C> 50.1 <C> 53.1 <C> 68.3 <C> 68.0 <C> 62.5 <C> 35.5 <C> 66.5 <C> 53.1 <R> <C> Skip-gram <C> 25.9 <C> 10.5 <C> 20.4 <C> 15.4 <C> 16.1 <C> 46.8 <C> 31.5 <C> 36.4 <C> 36.2 <C> 44.3 <C> 35.3 <C> 13.4 <C> 43.0 <C> 28.9 <R> <C> CBOW <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BiLSTM <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: NEI F1 scores. There are two language models: one for entity, and one for non-entity. All LMs are trained on the standard Train split. At test time, the tag associated with the language model which gives the lowest perplexity is used as prediction. Consecutive tokens with the same tag are joined into a phrase. Scores are Phrase-level F1. <COT> Looking at the "Model" column, we can see that the Skip-gram model has the lowest F1 score for the "hin" language.
<R> <C> [BOLD] Dataset <C> [BOLD] #D <C> [BOLD] #L <C> [BOLD] #T <C> [BOLD] #S <R> <C> MUC 4 <C> 1700 <C> 400 <C> 4 <C> 4 <R> <C> ACE 2005 <C> 599 <C> 599 <C> 33 <C> 36 <R> <C> ERE <C> 562 <C> 562 <C> 38 <C> 27 <R> <C> ASTRE <C> 1038 <C> 100 <C> 12 <C> 18 <R> <C> [BOLD] GNBusiness <C> 12,985 <C> 680 <C> – <C> 8 <CAP> Table 2: Comparison with existing datasets. (D documents or news clusters; L labeled documents or news clusters; T event types; S slots.) <COT> Looking at the "Dataset" column, we can see that the GNBusiness dataset has the highest number of documents or news clusters compared to other datasets.
<R> <C> [BOLD] Dataset <C> [BOLD] #D <C> [BOLD] #L <C> [BOLD] #T <C> [BOLD] #S <R> <C> MUC 4 <C> 1700 <C> 400 <C> 4 <C> 4 <R> <C> ACE 2005 <C> 599 <C> 599 <C> 33 <C> 36 <R> <C> ERE <C> 562 <C> 562 <C> 38 <C> 27 <R> <C> ASTRE <C> 1038 <C> 100 <C> 12 <C> 18 <R> <C> [BOLD] GNBusiness <C> 12,985 <C> 680 <C> – <C> 8 <CAP> Table 2: Comparison with existing datasets. (D documents or news clusters; L labeled documents or news clusters; T event types; S slots.) <COT> Looking at the "Dataset" column and the "#T" column, we can see that the ACE 2005 dataset has the highest number of event types compared to other datasets.
<R> <C> [ITALIC] λ <C> unpreserved BLEU <C> unpreserved DAL <C> preserved BLEU <C> preserved DAL <R> <C> 0.0 <C> 27.7 <C> 21.0 <C> 27.7 <C> 27.9 <R> <C> 0.1 <C> 27.0 <C> 13.6 <C> 27.6 <C> 10.5 <R> <C> 0.2 <C> 25.7 <C> 11.6 <C> 27.5 <C> 8.7 <CAP> Table 3: Varying MILk’s λ with and without mass preservation on the DeEn development set. <COT> Looking at the table, we can see that as the value of λ increases, both the unpreserved BLEU and preserved BLEU decrease.
<R> <C> [ITALIC] λ <C> unpreserved BLEU <C> unpreserved DAL <C> preserved BLEU <C> preserved DAL <R> <C> 0.0 <C> 27.7 <C> 21.0 <C> 27.7 <C> 27.9 <R> <C> 0.1 <C> 27.0 <C> 13.6 <C> 27.6 <C> 10.5 <R> <C> 0.2 <C> 25.7 <C> 11.6 <C> 27.5 <C> 8.7 <CAP> Table 3: Varying MILk’s λ with and without mass preservation on the DeEn development set. <COT> Looking at the table, we can see that the preserved DAL is consistently lower than the unpreserved DAL for all values of λ.
<R> <C> [BOLD] Method <C> [BOLD] Schema Matching (%)  [ITALIC] P <C> [BOLD] Schema Matching (%)  [ITALIC] R <C> [BOLD] Schema Matching (%)  [ITALIC] F1 <R> <C> DBLP:conf/acl/NguyenTFB15 <C> 41.5 <C> 53.4 <C> 46.7 <R> <C> Clustering <C> 41.2 <C> 50.6 <C> 45.4 <R> <C> ODEE-F <C> 41.7 <C> 53.2 <C> 46.8 <R> <C> ODEE-FE <C> 42.4 <C> 56.1 <C> 48.3 <R> <C> ODEE-FER <C> [BOLD] 43.4 <C> [BOLD] 58.3 <C> [BOLD] 49.8 <CAP> Table 4: Overall performance of schema matching. <COT> Looking at the "Method" column, we can see that the method "ODEE-FER" has the highest values in all three metrics: Schema Matching (%P), Schema Matching (%R), and Schema Matching (%F1).
<R> <C> [BOLD] Method <C> [BOLD] Schema Matching (%)  [ITALIC] P <C> [BOLD] Schema Matching (%)  [ITALIC] R <C> [BOLD] Schema Matching (%)  [ITALIC] F1 <R> <C> DBLP:conf/acl/NguyenTFB15 <C> 41.5 <C> 53.4 <C> 46.7 <R> <C> Clustering <C> 41.2 <C> 50.6 <C> 45.4 <R> <C> ODEE-F <C> 41.7 <C> 53.2 <C> 46.8 <R> <C> ODEE-FE <C> 42.4 <C> 56.1 <C> 48.3 <R> <C> ODEE-FER <C> [BOLD] 43.4 <C> [BOLD] 58.3 <C> [BOLD] 49.8 <CAP> Table 4: Overall performance of schema matching. <COT> Looking at the "Schema Matching (%F1)" column, we can see that the method "ODEE-FER" has the highest F1 score among all methods.
<R> <C> [BOLD] Method <C> [BOLD] Ave Slot Coherence <R> <C> DBLP:conf/acl/NguyenTFB15 <C> 0.10 <R> <C> ODEE-F <C> 0.10 <R> <C> ODEE-FE <C> 0.16 <R> <C> ODEE-FER <C> [BOLD] 0.18 <CAP> Table 5: Averaged slot coherence results. <COT> Looking at the "Method" column, we can see that the method "ODEE-FER" has the highest average slot coherence score.
<R> <C> [BOLD] Method <C> [BOLD] Ave Slot Coherence <R> <C> DBLP:conf/acl/NguyenTFB15 <C> 0.10 <R> <C> ODEE-F <C> 0.10 <R> <C> ODEE-FE <C> 0.16 <R> <C> ODEE-FER <C> [BOLD] 0.18 <CAP> Table 5: Averaged slot coherence results. <COT> Looking at the "DBLP:conf/acl/NguyenTFB15" row, we can see that the average slot coherence score is 0.10.
<R> <C> [BOLD] Method <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <C> [BOLD] HUMAN  [BOLD] PREFER- <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [BOLD] ENCE <R> <C> LSTM-LM <C> 8.7 <C> 15.1 <C> 0 <R> <C> Seq2seq <C> 13.5 <C> 19.2 <C> 22 <R> <C> ED(1) <C> 13.3 <C> [BOLD] 20.3 <C> 30 <R> <C> ED(2) <C> [BOLD] 14.0 <C> 19.8 <C> [BOLD] 48 <CAP> Table 2: Method Comparison (%). <COT> Looking at the "HUMAN PREFERENCE" column, we can see that the highest score is for ED(2) with a score of [BOLD] 48.
<R> <C> [BOLD] Method <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <C> [BOLD] HUMAN  [BOLD] PREFER- <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [BOLD] ENCE <R> <C> LSTM-LM <C> 8.7 <C> 15.1 <C> 0 <R> <C> Seq2seq <C> 13.5 <C> 19.2 <C> 22 <R> <C> ED(1) <C> 13.3 <C> [BOLD] 20.3 <C> 30 <R> <C> ED(2) <C> [BOLD] 14.0 <C> 19.8 <C> [BOLD] 48 <CAP> Table 2: Method Comparison (%). <COT> Comparing the "ROUGE-L" scores for LSTM-LM and Seq2seq, we can see that Seq2seq has a higher score of 19.2 compared to LSTM-LM's score of 15.1.
<R> <C> [ITALIC] n <C> [BOLD] 1 <C> [BOLD] 2 <C> [BOLD] 3 <C> [BOLD] 4 <C> [BOLD] 5 <C> [BOLD] 6 <R> <C> [BOLD] System <C> 100 <C> 94.4 <C> 67.3 <C> 35.0 <C> 15.9 <C> 6.6 <R> <C> [BOLD] Human <C> 98.2 <C> 78.5 <C> 42.2 <C> 17.9 <C> 7.7 <C> 4.1 <CAP> Table 3: Plagiarism Check: Percentage (%) of n-grams in test abstracts generated by system/human which appeared in training data. <COT> Looking at the "CAP" tag, we can see that the table is about the percentage of n-grams in test abstracts generated by a system and a human that appeared in the training data.
<R> <C> [ITALIC] n <C> [BOLD] 1 <C> [BOLD] 2 <C> [BOLD] 3 <C> [BOLD] 4 <C> [BOLD] 5 <C> [BOLD] 6 <R> <C> [BOLD] System <C> 100 <C> 94.4 <C> 67.3 <C> 35.0 <C> 15.9 <C> 6.6 <R> <C> [BOLD] Human <C> 98.2 <C> 78.5 <C> 42.2 <C> 17.9 <C> 7.7 <C> 4.1 <CAP> Table 3: Plagiarism Check: Percentage (%) of n-grams in test abstracts generated by system/human which appeared in training data. <COT> Looking at the "System" row, we can see that the percentage of n-grams in the test abstracts generated by the system decreases as the value of n increases.
<R> <C> [ITALIC] n <C> [BOLD] 1 <C> [BOLD] 2 <C> [BOLD] 3 <C> [BOLD] 4 <C> [BOLD] 5 <C> [BOLD] 6 <R> <C> METEOR <C> 13.3 <C> [BOLD] 14.0 <C> 13.6 <C> 13.9 <C> 13.8 <C> 13.5 <R> <C> ROUGE-L <C> [BOLD] 20.3 <C> 19.8 <C> 18.6 <C> 19.2 <C> 18.9 <C> 18.8 <CAP> Table 5: Iteration comparison (%) <COT> Looking at the "ROUGE-L" column, we can see that the highest score is in iteration 1.
<R> <C> [EMPTY] <C> [BOLD] # Tests <C> [BOLD] # Choices  [BOLD] per Test <C> [BOLD] Non-expert  [BOLD] Non-CS <C> [BOLD] Non-expert  [BOLD] CS <C> [BOLD] NLP Expert  [BOLD] Junior <C> [BOLD] NLP Expert  [BOLD] Senior <R> <C> Different Titles <C> 50 <C> 2 <C> 30% <C> 15% <C> 12% <C> 0% <R> <C> Different Titles <C> 20 <C> 5 <C> 60% <C> 20% <C> 30% <C> 20% <R> <C> Different Titles <C> 10 <C> 10 <C> 80% <C> 30% <C> 30% <C> 20% <R> <C> Same Title <C> 50 <C> 2 <C> 54% <C> 10% <C> 4% <C> 0% <R> <C> Same Title <C> 20 <C> 5 <C> 75% <C> 25% <C> 5% <C> 5% <CAP> Table 4: Turing Test Passing Rates. <COT> Looking at the table, we can see that the "Non-expert CS" category consistently has the lowest passing rates across different titles.
<R> <C> [EMPTY] <C> [BOLD] # Tests <C> [BOLD] # Choices  [BOLD] per Test <C> [BOLD] Non-expert  [BOLD] Non-CS <C> [BOLD] Non-expert  [BOLD] CS <C> [BOLD] NLP Expert  [BOLD] Junior <C> [BOLD] NLP Expert  [BOLD] Senior <R> <C> Different Titles <C> 50 <C> 2 <C> 30% <C> 15% <C> 12% <C> 0% <R> <C> Different Titles <C> 20 <C> 5 <C> 60% <C> 20% <C> 30% <C> 20% <R> <C> Different Titles <C> 10 <C> 10 <C> 80% <C> 30% <C> 30% <C> 20% <R> <C> Same Title <C> 50 <C> 2 <C> 54% <C> 10% <C> 4% <C> 0% <R> <C> Same Title <C> 20 <C> 5 <C> 75% <C> 25% <C> 5% <C> 5% <CAP> Table 4: Turing Test Passing Rates. <COT> Looking at the table, we can observe that the passing rates for the "Same Title" category are generally higher compared to the passing rates for the "Different Titles" category.
<R> <C> [BOLD] CC <C> [BOLD] S <C> [BOLD] S+P <C> [BOLD] S+I <C> [BOLD] S+P+I <R> <C> Random <C> – <C> – <C> – <C> – <R> <C> Vocal <C> 0.125 <C> 0.149 <C> 0.119 <C> [BOLD] 0.153 <R> <C> Visual <C> 0.092 <C> 0.109 <C> [BOLD] 0.116 <C> 0.106 <R> <C> Verbal <C> 0.404 <C> [BOLD] 0.455 <C> 0.434 <C> 0.417 <R> <C> Human <C> 0.820 <C> – <C> – <C> – <R> <C> [BOLD] MAE <C> [BOLD] S <C> [BOLD] S+P <C> [BOLD] S+I <C> [BOLD] S+P+I <R> <C> Random <C> 1.880 <C> – <C> – <C> – <R> <C> Vocal <C> 1.456 <C> 1.471 <C> 1.444 <C> [BOLD] 1.431 <R> <C> Visual <C> 1.442 <C> [BOLD] 1.439 <C> 1.453 <C> 1.460 <R> <C> Verbal <C> 1.196 <C> [BOLD] 1.156 <C> 1.181 <C> 1.206 <R> <C> Human <C> 0.710 <C> – <C> – <C> – <CAP> Table 2: Unimodal sentiment analysis results on the CMU-MOSI test set. Numbers in bold are the best results on each modality. <COT> looking at the "Table 2" caption, we can see that the table presents the results of unimodal sentiment analysis on the CMU-MOSI test set.
<R> <C> [BOLD] CC <C> [BOLD] S <C> [BOLD] S+P <C> [BOLD] S+I <C> [BOLD] S+P+I <R> <C> Random <C> – <C> – <C> – <C> – <R> <C> Vocal <C> 0.125 <C> 0.149 <C> 0.119 <C> [BOLD] 0.153 <R> <C> Visual <C> 0.092 <C> 0.109 <C> [BOLD] 0.116 <C> 0.106 <R> <C> Verbal <C> 0.404 <C> [BOLD] 0.455 <C> 0.434 <C> 0.417 <R> <C> Human <C> 0.820 <C> – <C> – <C> – <R> <C> [BOLD] MAE <C> [BOLD] S <C> [BOLD] S+P <C> [BOLD] S+I <C> [BOLD] S+P+I <R> <C> Random <C> 1.880 <C> – <C> – <C> – <R> <C> Vocal <C> 1.456 <C> 1.471 <C> 1.444 <C> [BOLD] 1.431 <R> <C> Visual <C> 1.442 <C> [BOLD] 1.439 <C> 1.453 <C> 1.460 <R> <C> Verbal <C> 1.196 <C> [BOLD] 1.156 <C> 1.181 <C> 1.206 <R> <C> Human <C> 0.710 <C> – <C> – <C> – <CAP> Table 2: Unimodal sentiment analysis results on the CMU-MOSI test set. Numbers in bold are the best results on each modality. <COT> looking at the "S+I" column, the "[BOLD] 0.153" cell, the "Visual" row, and the "MAE" column, the "[BOLD] 1.431" cell, we can see that the "S+I" modality has the highest sentiment analysis result among all modalities for the "Visual" and "MAE" categories.
<R> <C> [BOLD] CC <C> [BOLD] S <C> [BOLD] S+P <C> [BOLD] S+I <C> [BOLD] S+P+I <R> <C> Random <C> – <C> – <C> – <C> – <R> <C> EF <C> 0.471 <C> 0.472 <C> 0.476 <C> [BOLD] 0.482 <R> <C> TFN <C> 0.448 <C> [BOLD] 0.461 <C> 0.446 <C> 0.429 <R> <C> LF <C> [BOLD] 0.454 <C> 0.413 <C> 0.428 <C> 0.428 <R> <C> HF <C> [BOLD] 0.469 <C> 0.424 <C> 0.458 <C> 0.432 <R> <C> Human <C> 0.820 <C> – <C> – <C> – <R> <C> [BOLD] MAE <C> [BOLD] S <C> [BOLD] S+P <C> [BOLD] S+I <C> [BOLD] S+P+I <R> <C> Random <C> 1.880 <C> – <C> – <C> – <R> <C> EF <C> 1.197 <C> 1.181 <C> 1.193 <C> [BOLD] 1.172 <R> <C> TFN <C> 1.186 <C> 1.181 <C> [BOLD] 1.178 <C> 1.205 <R> <C> LF <C> [BOLD] 1.179 <C> 1.211 <C> 1.204 <C> 1.201 <R> <C> HF <C> [BOLD] 1.155 <C> 1.211 <C> 1.164 <C> 1.187 <R> <C> Human <C> 0.710 <C> – <C> – <C> – <CAP> Table 3: Multimodal sentiment analysis results on the CMU-MOSI test set. Numbers in bold are the best results for each fusion strategy in each row. <COT> Looking at "MAE" cell, "[BOLD] S+I" cell, "HF" cell, finding the highest MAE score is achieved with the fusion strategy S+I using the HF feature.
<R> <C> [BOLD] CC <C> [BOLD] S <C> [BOLD] S+P <C> [BOLD] S+I <C> [BOLD] S+P+I <R> <C> Random <C> – <C> – <C> – <C> – <R> <C> EF <C> 0.471 <C> 0.472 <C> 0.476 <C> [BOLD] 0.482 <R> <C> TFN <C> 0.448 <C> [BOLD] 0.461 <C> 0.446 <C> 0.429 <R> <C> LF <C> [BOLD] 0.454 <C> 0.413 <C> 0.428 <C> 0.428 <R> <C> HF <C> [BOLD] 0.469 <C> 0.424 <C> 0.458 <C> 0.432 <R> <C> Human <C> 0.820 <C> – <C> – <C> – <R> <C> [BOLD] MAE <C> [BOLD] S <C> [BOLD] S+P <C> [BOLD] S+I <C> [BOLD] S+P+I <R> <C> Random <C> 1.880 <C> – <C> – <C> – <R> <C> EF <C> 1.197 <C> 1.181 <C> 1.193 <C> [BOLD] 1.172 <R> <C> TFN <C> 1.186 <C> 1.181 <C> [BOLD] 1.178 <C> 1.205 <R> <C> LF <C> [BOLD] 1.179 <C> 1.211 <C> 1.204 <C> 1.201 <R> <C> HF <C> [BOLD] 1.155 <C> 1.211 <C> 1.164 <C> 1.187 <R> <C> Human <C> 0.710 <C> – <C> – <C> – <CAP> Table 3: Multimodal sentiment analysis results on the CMU-MOSI test set. Numbers in bold are the best results for each fusion strategy in each row. <COT> Looking at the "CC" cell, "[BOLD] 0.482" cell, "S+P+I" cell, finding the highest CC score is achieved with the fusion strategy S+P+I.
<R> <C> [BOLD] Dataset <C> [BOLD] Accuracy dev <C> [BOLD] Accuracy test <R> <C> +PMC <C> 80.50 <C> 78.97 <R> <C> +PubMedd <C> 81.14 <C> 78.83 <R> <C> +PubMed+PMC <C> [BOLD] 82.15 <C> [BOLD] 79.04 <CAP> Table 2: The BioBERT performance on the MedNLI task. Each model is trained on three different combinations of PMC and PubMed datasets (top score marked as bold). <COT> Looking at the "CAP" tag, we can see that the table provides the BioBERT performance on the MedNLI task for three different combinations of PMC and PubMed datasets.
<R> <C> [BOLD] Dataset <C> [BOLD] Accuracy dev <C> [BOLD] Accuracy test <R> <C> +PMC <C> 80.50 <C> 78.97 <R> <C> +PubMedd <C> 81.14 <C> 78.83 <R> <C> +PubMed+PMC <C> [BOLD] 82.15 <C> [BOLD] 79.04 <CAP> Table 2: The BioBERT performance on the MedNLI task. Each model is trained on three different combinations of PMC and PubMed datasets (top score marked as bold). <COT> Looking at the "+PubMed+PMC" row, we can see that the accuracy for the test set is marked as [BOLD] 79.04. Comparing this with the other rows, we can observe that the "+PubMed+PMC" combination has the highest accuracy for both the dev and test sets.
<R> <C> Topic <C> BiLSTM <C> DeAT <C> BiMPM <C> RCN (Our) <R> <C> CC <C> 68.1±0.6 <C> 70.9±0.7 <C> 71.5±0.6 <C> [BOLD] 73.0±0.5∗ <R> <C> HC <C> 52.5±0.6 <C> 56.9±0.4 <C> 56.4±0.7 <C> [BOLD] 58.6±0.4∗∗ <R> <C> FM <C> 58.3±0.6 <C> 60.6±0,7 <C> 59.8±0.7 <C> [BOLD] 64.4±0.5∗∗ <R> <C> AT <C> 67.5±0.4 <C> 69.5±0.5 <C> 70.3±0.6 <C> [BOLD] 72.2±0.4∗ <R> <C> LA <C> 61.3±0.3 <C> 63.2±0.6 <C> 62.4±0.4 <C> [BOLD] 64.5±0.4∗∗ <R> <C> Two tailed t-test: ∗∗  [ITALIC] p<0.01; ∗  [ITALIC] p<0.05 <C> Two tailed t-test: ∗∗  [ITALIC] p<0.01; ∗  [ITALIC] p<0.05 <C> Two tailed t-test: ∗∗  [ITALIC] p<0.01; ∗  [ITALIC] p<0.05 <C> Two tailed t-test: ∗∗  [ITALIC] p<0.01; ∗  [ITALIC] p<0.05 <C> Two tailed t-test: ∗∗  [ITALIC] p<0.01; ∗  [ITALIC] p<0.05 <CAP> Table 2: Classification performance of the compared methods on various topics, measured by the averaged macro F1-score over ten runs on the test data. <COT> Looking at the "CAP" tag, we can see that Table 2 provides the classification performance of compared methods on various topics, measured by the averaged macro F1-score over ten runs on the test data.
<R> <C> [BOLD] Model <C> [BOLD] Accuracy (±0.01) <R> <C> LSTM+Feed-Forward <C> 0.518 <R> <C> LSTM+Gated CNN+Feed-Forward <C> 0.524 <R> <C> BERT Features(512 tokens)+Feed-Forward <C> 0.639 <R> <C> BERT Classifier(30 tokens / 15 tokens from each paragraph) <C> 0.681 <R> <C> BERT Classifier(128 tokens / 64 tokens from each paragraph) <C> 0.717 <R> <C> BERT Classifier(256 tokens / 128 tokens from each paragraph) <C> 0.843 <CAP> Table 3: Accuracy on Test set. <COT> Looking at the "Model" column, we can see that the accuracy increases as we increase the number of tokens used in the BERT Classifier.
<R> <C> [BOLD] Model <C> [BOLD] Accuracy (±0.01) <R> <C> LSTM+Feed-Forward <C> 0.518 <R> <C> LSTM+Gated CNN+Feed-Forward <C> 0.524 <R> <C> BERT Features(512 tokens)+Feed-Forward <C> 0.639 <R> <C> BERT Classifier(30 tokens / 15 tokens from each paragraph) <C> 0.681 <R> <C> BERT Classifier(128 tokens / 64 tokens from each paragraph) <C> 0.717 <R> <C> BERT Classifier(256 tokens / 128 tokens from each paragraph) <C> 0.843 <CAP> Table 3: Accuracy on Test set. <COT> Comparing the accuracy values of the LSTM models and the BERT models, we can see that the BERT models consistently outperform the LSTM models.
<R> <C> [BOLD] Window position <C> [BOLD] SimLex999 <C> [BOLD] Analogies <R> <C> OS left <C> 0.40 <C> 0.35 <R> <C> OS right <C> 0.43 <C> 0.35 <R> <C> OS symmetric <C> 0.43 <C> [BOLD] 0.45 <R> <C> GW left <C> 0.43 <C> 0.64 <R> <C> GW right <C> 0.44 <C> 0.65 <R> <C> GW symmetric <C> 0.45 <C> [BOLD] 0.68 <CAP> Table 2: Average performance across all models depending on the window position. <COT> Looking at the "Window position" column, we can see that there are three different window positions: left, right, and symmetric. 
<R> <C> [BOLD] Window position <C> [BOLD] SimLex999 <C> [BOLD] Analogies <R> <C> OS left <C> 0.40 <C> 0.35 <R> <C> OS right <C> 0.43 <C> 0.35 <R> <C> OS symmetric <C> 0.43 <C> [BOLD] 0.45 <R> <C> GW left <C> 0.43 <C> 0.64 <R> <C> GW right <C> 0.44 <C> 0.65 <R> <C> GW symmetric <C> 0.45 <C> [BOLD] 0.68 <CAP> Table 2: Average performance across all models depending on the window position. <COT> Looking at the "Analogies" column, we can see that the highest score is in the "GW symmetric" row with a value of [BOLD] 0.68.
<R> <C> [BOLD] Cross-sentential <C> [BOLD] SimLex999 <C> [BOLD] Analogies <R> <C> OS False <C> [BOLD] 0.44 <C> 0.34 <R> <C> OS True <C> 0.40 <C> [BOLD] 0.43 <R> <C> GW False <C> 0.44 <C> 0.66 <R> <C> GW True <C> 0.44 <C> 0.65 <CAP> Table 3: Average performance across all models with and without cross-sentential contexts. <COT> Looking at the "Analogies" column, we can see that the performance is higher for both OS and GW when cross-sentential contexts are included.
<R> <C> [BOLD] Stop words removal <C> [BOLD] SimLex999 <C> [BOLD] Analogies <R> <C> OS no removal <C> 0.41 <C> 0.34 <R> <C> OS with removal <C> 0.42 <C> [BOLD] 0.43 <R> <C> GW no removal <C> 0.44 <C> 0.64 <R> <C> GW with removal <C> 0.44 <C> [BOLD] 0.68 <CAP> Table 4: Average performance across all models depending on the removal of stop words. <COT> Looking at the "SimLex999" column, we can see that the performance of the models with stop words removal is higher than the performance of the models without stop words removal.
<R> <C> [BOLD] Genre <C> [BOLD] CBOW <C> [BOLD] ESIM <C> [BOLD] InnerAtt <R> <C> Fiction <C> 67.5 <C> 73.0 <C> 73.2 <R> <C> Government <C> 67.5 <C> 74.8 <C> 75.2 <R> <C> Slate <C> 60.6 <C> 67.9 <C> 67.2 <R> <C> Telephone <C> 63.7 <C> 72.2 <C> 73.0 <R> <C> Travel <C> 64.6 <C> 73.7 <C> 72.8 <R> <C> 9/11 <C> 63.2 <C> 71.9 <C> 70.5 <R> <C> Face-to-face <C> 66.3 <C> 71.2 <C> 74.5 <R> <C> Letters <C> 68.3 <C> 74.7 <C> 75.4 <R> <C> Oup <C> 62.8 <C> 71.7 <C> 71.5 <R> <C> Verbatim <C> 62.7 <C> 71.9 <C> 69.5 <R> <C> [BOLD] MultiNLI Overall <C> [BOLD] 64.7 <C> [BOLD] 72.2 <C> [BOLD] 72.3 <CAP> Table 3: Validation accuracies (%) for our best model broken down by genre. Both CBOW and ESIM results are reported as in (Williams et al., 2017). <COT> Looking at the "Genre" column, we can see that there are different genres mentioned in the table.
<R> <C> [BOLD] Dataset <C> [BOLD] Pre-trained <C> [BOLD] Fine-tuned, initialized with  [BOLD] normal distr. <C> [BOLD] Fine-tuned, initialized with  [BOLD] pre-trained <C> [BOLD] Metric <C> [BOLD] Size <R> <C> MRPC <C> 0/31.6 <C> 81.2/68.3 <C> 87.9/82.3 <C> F1/Acc <C> 5.8K <R> <C> STS-B <C> 33.1 <C> 2.9 <C> 82.7 <C> Acc <C> 8.6K <R> <C> SST-2 <C> 49.1 <C> 80.5 <C> 92 <C> Acc <C> 70K <R> <C> QQP <C> 0/60.9 <C> 0/63.2 <C> 65.2/78.6 <C> F1/Acc <C> 400K <R> <C> RTE <C> 52.7 <C> 52.7 <C> 64.6 <C> Acc <C> 2.7K <R> <C> QNLI <C> 52.8 <C> 49.5 <C> 84.4 <C> Acc <C> 130K <R> <C> MNLI-m <C> 31.7 <C> 61.0 <C> 78.6 <C> Acc <C> 440K <CAP> Table 1: GLUE task performance of BERT models with different initialization. We report the scores on the validation, rather than test data, so these results differ from the original BERT paper. <COT> Looking at the "Dataset" column, we can see that the table shows the performance of BERT models on different GLUE tasks.
<R> <C> [BOLD] Dataset <C> [BOLD] Pre-trained <C> [BOLD] Fine-tuned, initialized with  [BOLD] normal distr. <C> [BOLD] Fine-tuned, initialized with  [BOLD] pre-trained <C> [BOLD] Metric <C> [BOLD] Size <R> <C> MRPC <C> 0/31.6 <C> 81.2/68.3 <C> 87.9/82.3 <C> F1/Acc <C> 5.8K <R> <C> STS-B <C> 33.1 <C> 2.9 <C> 82.7 <C> Acc <C> 8.6K <R> <C> SST-2 <C> 49.1 <C> 80.5 <C> 92 <C> Acc <C> 70K <R> <C> QQP <C> 0/60.9 <C> 0/63.2 <C> 65.2/78.6 <C> F1/Acc <C> 400K <R> <C> RTE <C> 52.7 <C> 52.7 <C> 64.6 <C> Acc <C> 2.7K <R> <C> QNLI <C> 52.8 <C> 49.5 <C> 84.4 <C> Acc <C> 130K <R> <C> MNLI-m <C> 31.7 <C> 61.0 <C> 78.6 <C> Acc <C> 440K <CAP> Table 1: GLUE task performance of BERT models with different initialization. We report the scores on the validation, rather than test data, so these results differ from the original BERT paper. <COT> Looking at the "Fine-tuned, initialized with pre-trained" column, we can see that the BERT models initialized with pre-trained weights perform better than the ones initialized with normal distribution.
<R> <C> Comparison Point Chen2018 <C> Comparison Point BPE <C> Comparison Point EnFr <C> Ref 41.0 <C> Ours 38.8 <R> <C> Wu2016 <C> BPE <C> EnFr <C> 39.0 <C> 38.8 <R> <C> Lee2017 <C> Char <C> CsEn <C> 22.5 <C> 25.9 <R> <C> [EMPTY] <C> [EMPTY] <C> DeEn <C> 25.8 <C> 31.6 <R> <C> [EMPTY] <C> [EMPTY] <C> FiEn <C> 13.1 <C> 19.3 <CAP> Table 3: Comparisons with some recent points in the literature. Scores are tokenized BLEU. <COT> Looking at the "Comparison Point Chen2018" cell and the "Comparison Point BPE" cell, we can see that our score of 38.8 is lower than the reference score of 41.0.
<R> <C> Comparison Point Chen2018 <C> Comparison Point BPE <C> Comparison Point EnFr <C> Ref 41.0 <C> Ours 38.8 <R> <C> Wu2016 <C> BPE <C> EnFr <C> 39.0 <C> 38.8 <R> <C> Lee2017 <C> Char <C> CsEn <C> 22.5 <C> 25.9 <R> <C> [EMPTY] <C> [EMPTY] <C> DeEn <C> 25.8 <C> 31.6 <R> <C> [EMPTY] <C> [EMPTY] <C> FiEn <C> 13.1 <C> 19.3 <CAP> Table 3: Comparisons with some recent points in the literature. Scores are tokenized BLEU. <COT> Looking at the "Comparison Point EnFr" cell and the "Ref 41.0" cell, we can see that the reference score of 41.0 is higher than our score of 38.8.
<R> <C> Language <C> Tokenized BLEU BPE <C> Tokenized BLEU Char <C> Tokenized BLEU Delta <C> SacreBLEU Char <R> <C> EnFr <C> 38.8 <C> 39.2 <C> 0.4 <C> 38.1 <R> <C> CsEn <C> 24.8 <C> 25.9 <C> 1.1 <C> 25.6 <R> <C> DeEn <C> 29.7 <C> 31.6 <C> 1.9 <C> 31.6 <R> <C> FiEn <C> 17.5 <C> 19.3 <C> 1.8 <C> 19.5 <CAP> Table 2: Character versus BPE translation. <COT> Looking at the "Language" column, we can see different language pairs such as EnFr, CsEn, DeEn, and FiEn.
<R> <C> Error Type <C> BPE <C> Char <R> <C> Lexical Choice <C> 19 <C> 8 <R> <C> Compounds <C> 13 <C> 1 <R> <C> Proper Names <C> 2 <C> 1 <R> <C> Morphological <C> 2 <C> 2 <R> <C> Other lexical <C> 2 <C> 4 <R> <C> Dropped Content <C> 7 <C> 0 <CAP> Table 4: Error counts out of 100 randomly sampled examples from the DeEn test set. <COT> Looking at the "Error Type" column, we can see that there are different types of errors mentioned in the table, such as Lexical Choice, Compounds, Proper Names, Morphological, Other lexical, and Dropped Content.
<R> <C> Error Type <C> BPE <C> Char <R> <C> Lexical Choice <C> 19 <C> 8 <R> <C> Compounds <C> 13 <C> 1 <R> <C> Proper Names <C> 2 <C> 1 <R> <C> Morphological <C> 2 <C> 2 <R> <C> Other lexical <C> 2 <C> 4 <R> <C> Dropped Content <C> 7 <C> 0 <CAP> Table 4: Error counts out of 100 randomly sampled examples from the DeEn test set. <COT> Looking at the "BPE" and "Char" columns, we can see that there are counts mentioned for each error type.
<R> <C> Encoder <C> BPE Size <C> BLEU <C> Comp. <R> <C> BiLSTM <C> Char <C> 31.6 <C> 1.00 <R> <C> BiLSTM <C> 1k <C> 30.5 <C> 0.44 <R> <C> BiLSTM <C> 2k <C> 30.4 <C> 0.35 <R> <C> BiLSTM <C> 4k <C> 30.0 <C> 0.29 <R> <C> BiLSTM <C> 8k <C> 29.6 <C> 0.25 <R> <C> BiLSTM <C> 16k <C> 30.0 <C> 0.22 <R> <C> BiLSTM <C> 32k <C> 29.7 <C> 0.20 <R> <C> Lee et. al. reimpl <C> Char <C> 28.0 <C> 0.20 <R> <C> BiLSTM + pooling <C> Char <C> 30.0 <C> 0.47 <R> <C> HM, 3-layer <C> Char <C> 31.2 <C> 0.77 <R> <C> HM, 2-layer <C> Char <C> 30.9 <C> 0.89 <CAP> Table 6: Compression results on WMT15 DeEn. The Comp. column shows the ratio of total computations carried out in the encoder. <COT> Looking at the "Encoder" column, we can see that the BiLSTM encoder is used in multiple rows.
<R> <C> Encoder <C> BPE Size <C> BLEU <C> Comp. <R> <C> BiLSTM <C> Char <C> 31.6 <C> 1.00 <R> <C> BiLSTM <C> 1k <C> 30.5 <C> 0.44 <R> <C> BiLSTM <C> 2k <C> 30.4 <C> 0.35 <R> <C> BiLSTM <C> 4k <C> 30.0 <C> 0.29 <R> <C> BiLSTM <C> 8k <C> 29.6 <C> 0.25 <R> <C> BiLSTM <C> 16k <C> 30.0 <C> 0.22 <R> <C> BiLSTM <C> 32k <C> 29.7 <C> 0.20 <R> <C> Lee et. al. reimpl <C> Char <C> 28.0 <C> 0.20 <R> <C> BiLSTM + pooling <C> Char <C> 30.0 <C> 0.47 <R> <C> HM, 3-layer <C> Char <C> 31.2 <C> 0.77 <R> <C> HM, 2-layer <C> Char <C> 30.9 <C> 0.89 <CAP> Table 6: Compression results on WMT15 DeEn. The Comp. column shows the ratio of total computations carried out in the encoder. <COT> Looking at the "BLEU" column, we can see that the highest BLEU score is achieved by the HM, 2-layer encoder.
<R> <C> Method <C> [ITALIC] μP <C> [ITALIC] μR <C> [ITALIC] μF1 <C> eP <C> eR <C> eF1 <R> <C> Zubiaga et al.  2017 <C> 0.667 <C> 0.556 <C> 0.607 <C> - <C> - <C> - <R> <C> BiLSTM <C> 0.623 <C> 0.564 <C> 0.590 <C> - <C> - <C> - <R> <C> BERT <C> 0.699 ± 0.0165 <C> 0.608 ± 0.0257 <C> 0.650 ± 0.0134 <C> 0.713 <C> 0.619 <C> 0.663 <R> <C> BERT + Wiki <C> 0.693 ± 0.0159 <C> 0.614 ± 0.0263 <C> 0.651 ± 0.0121 <C> 0.707 <C> 0.622 <C> 0.662 <R> <C> BERT + Wiki + PU <C> 0.699 ± 0.0133 <C> [BOLD] 0.625 ± 0.0164 <C> [BOLD] 0.660 ± 0.0109 <C> [BOLD] 0.722 <C> [BOLD] 0.646 <C> [BOLD] 0.682 <R> <C> BERT + Wiki +  [ITALIC] PUC <C> [BOLD] 0.701 ± 0.0108 <C> 0.618 ± 0.0176 <C> 0.657 ± 0.0097 <C> 0.715 <C> 0.627 <C> 0.668 <CAP> Table 2: micro-F1 (μF1) and ensembled F1 (eF1) performance of each system on the PHEME dataset. Performance is averaged across the five splits of Zubiaga et al. (2017). Results show the mean, standard deviation, and ensembled score across 15 seeds. Bold indicates best performance, underline indicates second best. <COT> Looking at the "Method" column, we can see that BERT + Wiki + PU has the highest micro-F1 (μF1) score compared to other methods.
<R> <C> Method <C> [ITALIC] μP <C> [ITALIC] μR <C> [ITALIC] μF1 <C> eP <C> eR <C> eF1 <R> <C> Zubiaga et al.  2017 <C> 0.667 <C> 0.556 <C> 0.607 <C> - <C> - <C> - <R> <C> BiLSTM <C> 0.623 <C> 0.564 <C> 0.590 <C> - <C> - <C> - <R> <C> BERT <C> 0.699 ± 0.0165 <C> 0.608 ± 0.0257 <C> 0.650 ± 0.0134 <C> 0.713 <C> 0.619 <C> 0.663 <R> <C> BERT + Wiki <C> 0.693 ± 0.0159 <C> 0.614 ± 0.0263 <C> 0.651 ± 0.0121 <C> 0.707 <C> 0.622 <C> 0.662 <R> <C> BERT + Wiki + PU <C> 0.699 ± 0.0133 <C> [BOLD] 0.625 ± 0.0164 <C> [BOLD] 0.660 ± 0.0109 <C> [BOLD] 0.722 <C> [BOLD] 0.646 <C> [BOLD] 0.682 <R> <C> BERT + Wiki +  [ITALIC] PUC <C> [BOLD] 0.701 ± 0.0108 <C> 0.618 ± 0.0176 <C> 0.657 ± 0.0097 <C> 0.715 <C> 0.627 <C> 0.668 <CAP> Table 2: micro-F1 (μF1) and ensembled F1 (eF1) performance of each system on the PHEME dataset. Performance is averaged across the five splits of Zubiaga et al. (2017). Results show the mean, standard deviation, and ensembled score across 15 seeds. Bold indicates best performance, underline indicates second best. <COT> Looking at the "eF1" column, we can see that BERT + Wiki + PUC has the highest ensembled F1 (eF1) score compared to other methods.
<R> <C> Method <C> MAP <R> <C> Konstantinovskiy et al.  2018 <C> 0.267 <R> <C> Hansen et al.  2019 <C> 0.302 <R> <C> BERT <C> [BOLD] 0.346 ± 0.024 <R> <C> BERT + Wiki <C> 0.339 ± 0.025 <R> <C> BERT + Wiki + PU <C> 0.328 ± 0.027 <R> <C> BERT + Wiki +  [ITALIC] PUC <C> 0.321 ± 0.031 <CAP> Table 3: Mean average precision (MAP) of models on political speeches. Bold indicates best performance, underline indicates second best. <COT> Looking at the "Method" column, we can see that BERT has the highest MAP score among all the methods listed in the table.
<R> <C> Method <C> MAP <R> <C> Konstantinovskiy et al.  2018 <C> 0.267 <R> <C> Hansen et al.  2019 <C> 0.302 <R> <C> BERT <C> [BOLD] 0.346 ± 0.024 <R> <C> BERT + Wiki <C> 0.339 ± 0.025 <R> <C> BERT + Wiki + PU <C> 0.328 ± 0.027 <R> <C> BERT + Wiki +  [ITALIC] PUC <C> 0.321 ± 0.031 <CAP> Table 3: Mean average precision (MAP) of models on political speeches. Bold indicates best performance, underline indicates second best. <COT> Looking at the "BERT" row, we can see that the MAP score for BERT is 0.346 ± 0.024.
<R> <C> Dataset <C> P <C> R <C> F1 <R> <C> [EMPTY] <C> 0.817 <C> 0.870 <C> 0.843 <R> <C> Wikipedia <C> 0.848 <C> 0.870 <C> 0.859 <R> <C> [EMPTY] <C> [ITALIC] 0.833 <C> [ITALIC] 0.870 <C> [ITALIC] 0.851 <R> <C> [EMPTY] <C> 0.875 <C> 0.824 <C> 0.848 <R> <C> Twitter <C> 0.863 <C> 0.812 <C> 0.836 <R> <C> [EMPTY] <C> [ITALIC] 0.869 <C> [ITALIC] 0.818 <C> [ITALIC] 0.842 <R> <C> [EMPTY] <C> 0.338 <C> 0.893 <C> 0.490 <R> <C> Politics <C> 0.311 <C> 1.0 <C> 0.475 <R> <C> [EMPTY] <C> [ITALIC] 0.325 <C> [ITALIC] 0.947 <C> [ITALIC] 0.483 <CAP> Table 4: F1 score comparing manual relabelling of the top 100 predictions by PUC model with the original labels in each dataset by two different annotators. Italics are average value between the two annotators. <COT> Looking at the "Dataset" column, we can see that there are four different datasets: Wikipedia, Twitter, Politics. Each dataset has corresponding F1 scores for P, R, and F1 metrics.
<R> <C> Dataset <C> P <C> R <C> F1 <R> <C> [EMPTY] <C> 0.817 <C> 0.870 <C> 0.843 <R> <C> Wikipedia <C> 0.848 <C> 0.870 <C> 0.859 <R> <C> [EMPTY] <C> [ITALIC] 0.833 <C> [ITALIC] 0.870 <C> [ITALIC] 0.851 <R> <C> [EMPTY] <C> 0.875 <C> 0.824 <C> 0.848 <R> <C> Twitter <C> 0.863 <C> 0.812 <C> 0.836 <R> <C> [EMPTY] <C> [ITALIC] 0.869 <C> [ITALIC] 0.818 <C> [ITALIC] 0.842 <R> <C> [EMPTY] <C> 0.338 <C> 0.893 <C> 0.490 <R> <C> Politics <C> 0.311 <C> 1.0 <C> 0.475 <R> <C> [EMPTY] <C> [ITALIC] 0.325 <C> [ITALIC] 0.947 <C> [ITALIC] 0.483 <CAP> Table 4: F1 score comparing manual relabelling of the top 100 predictions by PUC model with the original labels in each dataset by two different annotators. Italics are average value between the two annotators. <COT> Looking at the "Politics" row, we can see that the F1 score for the PUC model on the Politics dataset is 0.475.
<R> <C> [BOLD] dataset <C> [BOLD] tool <C> [BOLD] # correct prediction <C> [BOLD] positive  [BOLD] precision <C> [BOLD] positive  [BOLD] recall <C> [BOLD] positive  [BOLD] F1 <C> [BOLD] neutral  [BOLD] precision <C> [BOLD] neutral  [BOLD] recall <C> [BOLD] neutral  [BOLD] F1 <C> [BOLD] negative  [BOLD] precision <C> [BOLD] negative  [BOLD] recall <C> [BOLD] negative  [BOLD] F1 <R> <C> [BOLD] Stack Overflow <C> SentiStrength <C> 1043 <C> 0.200 <C> [BOLD] 0.359 <C> 0.257 <C> 0.858 <C> 0.772 <C> 0.813 <C> 0.397 <C> 0.433 <C> 0.414 <R> <C> positive: 178 <C> NLTK <C> 1168 <C> 0.317 <C> 0.244 <C> 0.276 <C> 0.815 <C> [BOLD] 0.941 <C> 0.873 <C> [BOLD] 0.625 <C> 0.084 <C> 0.148 <R> <C> neutral: 1,191 <C> Standford CoreNLP <C> 604 <C> 0.231 <C> 0.344 <C> 0.276 <C> [BOLD] 0.884 <C> 0.344 <C> 0.495 <C> 0.177 <C> [BOLD] 0.837 <C> 0.292 <R> <C> negative: 131 <C> SentiStrength-SE <C> 1170 <C> 0.312 <C> 0.221 <C> 0.259 <C> 0.826 <C> 0.930 <C> 0.875 <C> 0.500 <C> 0.185 <C> 0.270 <R> <C> sum: 1,500 <C> Stanford CoreNLP SO <C> 1139 <C> 0.317 <C> 0.145 <C> 0.199 <C> 0.836 <C> 0.886 <C> 0.860 <C> 0.365 <C> 0.365 <C> 0.365 <R> <C> [EMPTY] <C> N-gram auto-sklearn <C> [BOLD] 1317 <C> [BOLD] 0.667 <C> 0.316 <C> [BOLD] 0.418 <C> 0.871 <C> 0.939 <C> [BOLD] 0.904 <C> 0.600 <C> 0.472 <C> [BOLD] 0.514 <R> <C> [EMPTY] <C> N-gram auto-sklearn with SMOTE† <C> - <C> 0.680 <C> 0.005 <C> 0.009 <C> 0.344 <C> 0.930 <C> 0.499 <C> 0.657 <C> 0.160 <C> 0.251 <R> <C> [BOLD] App reviews <C> SentiStrength <C> 213 <C> 0.745 <C> 0.866 <C> 0.801 <C> 0.113 <C> 0.320 <C> 0.167 <C> 0.815 <C> 0.338 <C> 0.478 <R> <C> positive: 186 <C> NLTK <C> 184 <C> 0.751 <C> 0.812 <C> 0.780 <C> 0.093 <C> [BOLD] 0.440 <C> 0.154 <C> [BOLD] 1.000 <C> 0.169 <C> 0.289 <R> <C> neutral: 25 <C> Standford CoreNLP <C> 237 <C> 0.831 <C> 0.715 <C> 0.769 <C> [BOLD] 0.176 <C> 0.240 <C> [BOLD] 0.203 <C> 0.667 <C> 0.754 <C> 0.708 <R> <C> negative: 130 <C> SentiStrength-SE <C> 201 <C> 0.741 <C> 0.817 <C> 0.777 <C> 0.106 <C> 0.400 <C> 0.168 <C> 0.929 <C> 0.300 <C> 0.454 <R> <C> sum: 341 <C> Stanford CoreNLP SO <C> 142 <C> 0.770 <C> 0.253 <C> 0.381 <C> 0.084 <C> 0.320 <C> 0.133 <C> 0.470 <C> 0.669 <C> 0.552 <R> <C> [EMPTY] <C> N-gram auto-sklearn <C> [BOLD] 293 <C> [BOLD] 0.822 <C> [BOLD] 0.894 <C> [BOLD] 0.853 <C> 0.083 <C> 0.066 <C> 0.073 <C> 0.823 <C> [BOLD] 0.808 <C> [BOLD] 0.807 <R> <C> [EMPTY] <C> N-gram auto-sklearn with SMOTE† <C> - <C> 0.520 <C> 0.885 <C> 0.641 <C> 0.100 <C> 0.058 <C> 0.073 <C> 0.648 <C> 0.622 <C> 0.607 <R> <C> [BOLD] Jira issues <C> SentiStrength <C> 714 <C> 0.850 <C> [BOLD] 0.921 <C> 0.884 <C> - <C> - <C> - <C> 0.993 <C> 0.703 <C> 0.823 <R> <C> positive: 290 <C> NLTK <C> 276 <C> 0.840 <C> 0.362 <C> 0.506 <C> - <C> - <C> - <C> [BOLD] 1.000 <C> 0.269 <C> 0.424 <R> <C> neutral: 0 <C> Standford CoreNLP <C> 626 <C> 0.726 <C> 0.621 <C> 0.669 <C> - <C> - <C> - <C> 0.945 <C> 0.701 <C> 0.805 <R> <C> negative: 636 <C> SentiStrength-SE <C> 704 <C> 0.948 <C> 0.883 <C> 0.914 <C> - <C> - <C> - <C> 0.996 <C> 0.704 <C> 0.825 <R> <C> sum: 926 <C> Stanford CoreNLP SO <C> 333 <C> 0.635 <C> 0.252 <C> 0.361 <C> - <C> - <C> - <C> 0.724 <C> 0.409 <C> 0.523 <R> <C> [EMPTY] <C> N-gram auto-sklearn <C> [BOLD] 884 <C> [BOLD] 0.960 <C> 0.839 <C> [BOLD] 0.893 <C> - <C> - <C> - <C> 0.932 <C> [BOLD] 0.982 <C> [BOLD] 0.956 <R> <C> [EMPTY] <C> N-gram auto-sklearn with SMOTE† <C> - <C> 0.986 <C> 0.704 <C> 0.809 <C> - <C> - <C> - <C> 0.781 <C> 0.988 <C> 0.872 <R> <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <CAP> TABLE I: The comparison result of the number of corrected prediction, precision, recall, and f1-score <COT> Looking at the "dataset" column, we can see that there are four different datasets: Stack Overflow, App reviews, Jira issues, and an empty dataset.
<R> <C> [BOLD] dataset <C> [BOLD] tool <C> [BOLD] # correct prediction <C> [BOLD] positive  [BOLD] precision <C> [BOLD] positive  [BOLD] recall <C> [BOLD] positive  [BOLD] F1 <C> [BOLD] neutral  [BOLD] precision <C> [BOLD] neutral  [BOLD] recall <C> [BOLD] neutral  [BOLD] F1 <C> [BOLD] negative  [BOLD] precision <C> [BOLD] negative  [BOLD] recall <C> [BOLD] negative  [BOLD] F1 <R> <C> [BOLD] Stack Overflow <C> SentiStrength <C> 1043 <C> 0.200 <C> [BOLD] 0.359 <C> 0.257 <C> 0.858 <C> 0.772 <C> 0.813 <C> 0.397 <C> 0.433 <C> 0.414 <R> <C> positive: 178 <C> NLTK <C> 1168 <C> 0.317 <C> 0.244 <C> 0.276 <C> 0.815 <C> [BOLD] 0.941 <C> 0.873 <C> [BOLD] 0.625 <C> 0.084 <C> 0.148 <R> <C> neutral: 1,191 <C> Standford CoreNLP <C> 604 <C> 0.231 <C> 0.344 <C> 0.276 <C> [BOLD] 0.884 <C> 0.344 <C> 0.495 <C> 0.177 <C> [BOLD] 0.837 <C> 0.292 <R> <C> negative: 131 <C> SentiStrength-SE <C> 1170 <C> 0.312 <C> 0.221 <C> 0.259 <C> 0.826 <C> 0.930 <C> 0.875 <C> 0.500 <C> 0.185 <C> 0.270 <R> <C> sum: 1,500 <C> Stanford CoreNLP SO <C> 1139 <C> 0.317 <C> 0.145 <C> 0.199 <C> 0.836 <C> 0.886 <C> 0.860 <C> 0.365 <C> 0.365 <C> 0.365 <R> <C> [EMPTY] <C> N-gram auto-sklearn <C> [BOLD] 1317 <C> [BOLD] 0.667 <C> 0.316 <C> [BOLD] 0.418 <C> 0.871 <C> 0.939 <C> [BOLD] 0.904 <C> 0.600 <C> 0.472 <C> [BOLD] 0.514 <R> <C> [EMPTY] <C> N-gram auto-sklearn with SMOTE† <C> - <C> 0.680 <C> 0.005 <C> 0.009 <C> 0.344 <C> 0.930 <C> 0.499 <C> 0.657 <C> 0.160 <C> 0.251 <R> <C> [BOLD] App reviews <C> SentiStrength <C> 213 <C> 0.745 <C> 0.866 <C> 0.801 <C> 0.113 <C> 0.320 <C> 0.167 <C> 0.815 <C> 0.338 <C> 0.478 <R> <C> positive: 186 <C> NLTK <C> 184 <C> 0.751 <C> 0.812 <C> 0.780 <C> 0.093 <C> [BOLD] 0.440 <C> 0.154 <C> [BOLD] 1.000 <C> 0.169 <C> 0.289 <R> <C> neutral: 25 <C> Standford CoreNLP <C> 237 <C> 0.831 <C> 0.715 <C> 0.769 <C> [BOLD] 0.176 <C> 0.240 <C> [BOLD] 0.203 <C> 0.667 <C> 0.754 <C> 0.708 <R> <C> negative: 130 <C> SentiStrength-SE <C> 201 <C> 0.741 <C> 0.817 <C> 0.777 <C> 0.106 <C> 0.400 <C> 0.168 <C> 0.929 <C> 0.300 <C> 0.454 <R> <C> sum: 341 <C> Stanford CoreNLP SO <C> 142 <C> 0.770 <C> 0.253 <C> 0.381 <C> 0.084 <C> 0.320 <C> 0.133 <C> 0.470 <C> 0.669 <C> 0.552 <R> <C> [EMPTY] <C> N-gram auto-sklearn <C> [BOLD] 293 <C> [BOLD] 0.822 <C> [BOLD] 0.894 <C> [BOLD] 0.853 <C> 0.083 <C> 0.066 <C> 0.073 <C> 0.823 <C> [BOLD] 0.808 <C> [BOLD] 0.807 <R> <C> [EMPTY] <C> N-gram auto-sklearn with SMOTE† <C> - <C> 0.520 <C> 0.885 <C> 0.641 <C> 0.100 <C> 0.058 <C> 0.073 <C> 0.648 <C> 0.622 <C> 0.607 <R> <C> [BOLD] Jira issues <C> SentiStrength <C> 714 <C> 0.850 <C> [BOLD] 0.921 <C> 0.884 <C> - <C> - <C> - <C> 0.993 <C> 0.703 <C> 0.823 <R> <C> positive: 290 <C> NLTK <C> 276 <C> 0.840 <C> 0.362 <C> 0.506 <C> - <C> - <C> - <C> [BOLD] 1.000 <C> 0.269 <C> 0.424 <R> <C> neutral: 0 <C> Standford CoreNLP <C> 626 <C> 0.726 <C> 0.621 <C> 0.669 <C> - <C> - <C> - <C> 0.945 <C> 0.701 <C> 0.805 <R> <C> negative: 636 <C> SentiStrength-SE <C> 704 <C> 0.948 <C> 0.883 <C> 0.914 <C> - <C> - <C> - <C> 0.996 <C> 0.704 <C> 0.825 <R> <C> sum: 926 <C> Stanford CoreNLP SO <C> 333 <C> 0.635 <C> 0.252 <C> 0.361 <C> - <C> - <C> - <C> 0.724 <C> 0.409 <C> 0.523 <R> <C> [EMPTY] <C> N-gram auto-sklearn <C> [BOLD] 884 <C> [BOLD] 0.960 <C> 0.839 <C> [BOLD] 0.893 <C> - <C> - <C> - <C> 0.932 <C> [BOLD] 0.982 <C> [BOLD] 0.956 <R> <C> [EMPTY] <C> N-gram auto-sklearn with SMOTE† <C> - <C> 0.986 <C> 0.704 <C> 0.809 <C> - <C> - <C> - <C> 0.781 <C> 0.988 <C> 0.872 <R> <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <C> † Applying SMOTE, a oversampling technique, for our method. <CAP> TABLE I: The comparison result of the number of corrected prediction, precision, recall, and f1-score <COT> Looking at the "positive precision" column, we can see that NLTK has the highest precision for positive predictions.
<R> <C> [EMPTY] <C> rg  [BOLD] p <C> rg  [BOLD] s <C> rg  [BOLD] avg. <C> wordsim  [BOLD] p <C> wordsim  [BOLD] s <C> wordsim  [BOLD] avg. <C> mc  [BOLD] p <C> mc  [BOLD] s <C> mc  [BOLD] avg. <C> semeval17  [BOLD] p <C> semeval17  [BOLD] s <C> semeval17  [BOLD] avg. <R> <C> cosine <C> 77.2 <C> 76.0 <C> 76.6 <C> 64.9 <C> 69.4 <C> 67.1 <C> 79.2 <C> 80.0 <C> 79.6 <C> 69.4 <C> 70.0 <C> 69.7 <R> <C> 10rv [ITALIC] w <C> 78.1 <C> 77.0 <C> 77.5 <C> 66.0 <C> 69.6 <C> 67.8 <C> 79.7 <C> 80.7 <C> [BOLD] 80.2 <C> 70.2 <C> 70.8 <C> 70.5 <R> <C> 10rv [ITALIC] r <C> 77.4 <C> 75.5 <C> 76.4 <C> 65.8 <C> 69.5 <C> 67.6 <C> 78.8 <C> 77.9 <C> 78.3 <C> 70.0 <C> 70.7 <C> 70.3 <R> <C> 1800rv [ITALIC] w <C> 79.5 <C> 80.6 <C> [BOLD] 80.0 <C> 67.4 <C> 69.8 <C> 68.6 <C> 79.4 <C> 79.0 <C> 79.2 <C> 71.4 <C> 71.8 <C> 71.6 <R> <C> 1800rv [ITALIC] r <C> 78.9 <C> 80.2 <C> 79.5 <C> 68.1 <C> 70.1 <C> [BOLD] 69.1 <C> 79.2 <C> 79.7 <C> 79.4 <C> 72.2 <C> 73.0 <C> [BOLD] 72.6 <CAP> Table 3: Correlation results for different configurations of our proposed approach and a competitor baseline based on cosine similarity of word embeddings. <COT> Looking at the "Table 3" caption, we can see that it provides correlation results for different configurations of the proposed approach and a competitor baseline based on cosine similarity of word embeddings.
<R> <C> [EMPTY] <C> rg  [BOLD] p <C> rg  [BOLD] s <C> rg  [BOLD] avg. <C> wordsim  [BOLD] p <C> wordsim  [BOLD] s <C> wordsim  [BOLD] avg. <C> mc  [BOLD] p <C> mc  [BOLD] s <C> mc  [BOLD] avg. <C> semeval17  [BOLD] p <C> semeval17  [BOLD] s <C> semeval17  [BOLD] avg. <R> <C> cosine <C> 77.2 <C> 76.0 <C> 76.6 <C> 64.9 <C> 69.4 <C> 67.1 <C> 79.2 <C> 80.0 <C> 79.6 <C> 69.4 <C> 70.0 <C> 69.7 <R> <C> 10rv [ITALIC] w <C> 78.1 <C> 77.0 <C> 77.5 <C> 66.0 <C> 69.6 <C> 67.8 <C> 79.7 <C> 80.7 <C> [BOLD] 80.2 <C> 70.2 <C> 70.8 <C> 70.5 <R> <C> 10rv [ITALIC] r <C> 77.4 <C> 75.5 <C> 76.4 <C> 65.8 <C> 69.5 <C> 67.6 <C> 78.8 <C> 77.9 <C> 78.3 <C> 70.0 <C> 70.7 <C> 70.3 <R> <C> 1800rv [ITALIC] w <C> 79.5 <C> 80.6 <C> [BOLD] 80.0 <C> 67.4 <C> 69.8 <C> 68.6 <C> 79.4 <C> 79.0 <C> 79.2 <C> 71.4 <C> 71.8 <C> 71.6 <R> <C> 1800rv [ITALIC] r <C> 78.9 <C> 80.2 <C> 79.5 <C> 68.1 <C> 70.1 <C> [BOLD] 69.1 <C> 79.2 <C> 79.7 <C> 79.4 <C> 72.2 <C> 73.0 <C> [BOLD] 72.6 <CAP> Table 3: Correlation results for different configurations of our proposed approach and a competitor baseline based on cosine similarity of word embeddings. <COT> Looking at the "1800rv [ITALIC] w" row, we can see that the average similarity score for "wordsim" is 80.0.
<R> <C> Architecture <C> Zh⇒En <C> Params <C> Emb. <C> Red. <C> Dev. <C> MT02 <C> MT03 <C> MT04 <C> MT08 <C> All <R> <C> SMT* <C> - <C> - <C> - <C> - <C> 34.00 <C> 35.81 <C> 34.70 <C> 37.15 <C> 25.28 <C> 33.39 <R> <C> RNNsearch* <C> Vanilla <C> 74.8M <C> 55.8M <C> 0% <C> 35.92 <C> 37.88 <C> 36.21 <C> 38.83 <C> 26.30 <C> 34.81 <R> <C> RNNsearch* <C> Source bridging <C> 78.5M <C> 55.8M <C> 0% <C> 36.79 <C> 38.71 <C> 37.24 <C> 40.28 <C> 27.40 <C> 35.91 <R> <C> RNNsearch* <C> Target bridging <C> 76.6M <C> 55.8M <C> 0% <C> 36.69 <C> 39.04 <C> 37.63 <C> 40.41 <C> 27.98 <C> 36.27 <R> <C> RNNsearch* <C> Direct bridging <C> 78.9M <C> 55.8M <C> 0% <C> 36.97 <C> 39.77 <C> 38.02 <C> 40.83 <C> 27.85 <C> 36.62 <R> <C> Transformer <C> Vanilla <C> 90.2M <C> 46.1M <C> 0% <C> 41.37 <C> 42.53 <C> 40.25 <C> 43.58 <C> 32.89 <C> 40.33 <R> <C> Transformer <C> Direct bridging <C> 90.5M <C> 46.1M <C> 0% <C> 41.67 <C> 42.89 <C> 41.34 <C> 43.56 <C> 32.69 <C> 40.54 <R> <C> Transformer <C> Decoder WT <C> 74.9M <C> 30.7M <C> 33.4% <C> 41.90 <C> 43.02 <C> 41.89 <C> 43.87 <C> 32.62 <C> 40.82 <R> <C> Transformer <C> [ITALIC] Shared-private <C> 62.8M <C> 18.7M <C> 59.4% <C> 42.57↑ <C> 43.73↑ <C> 41.99↑ <C> 44.53↑ <C> 33.81⇑ <C> 41.61⇑ <CAP> Table 1: Results on the NIST Chinese-English translation task. “Params” denotes the number of model parameters. “Emb.” represents the number of parameters used for word representation. “Red.” represents the reduction rate of the standard size. The results of SMT* and RNNsearch* are reported by Kuang et al. Kuang et al. (2018) with the same datasets and vocabulary settings. “↑” indicates the result is significantly better than that of the vanilla Transformer (p<0.01), while “⇑” indicates the result is significantly better than that of all other Transformer models (p<0.01). All significance tests are measured by paired bootstrap resampling Koehn (2004). <COT> Looking at the "Architecture" column, we can see that there are different models used for translation, such as SMT*, RNNsearch*, and Transformer.
<R> <C> Architecture <C> Zh⇒En <C> Params <C> Emb. <C> Red. <C> Dev. <C> MT02 <C> MT03 <C> MT04 <C> MT08 <C> All <R> <C> SMT* <C> - <C> - <C> - <C> - <C> 34.00 <C> 35.81 <C> 34.70 <C> 37.15 <C> 25.28 <C> 33.39 <R> <C> RNNsearch* <C> Vanilla <C> 74.8M <C> 55.8M <C> 0% <C> 35.92 <C> 37.88 <C> 36.21 <C> 38.83 <C> 26.30 <C> 34.81 <R> <C> RNNsearch* <C> Source bridging <C> 78.5M <C> 55.8M <C> 0% <C> 36.79 <C> 38.71 <C> 37.24 <C> 40.28 <C> 27.40 <C> 35.91 <R> <C> RNNsearch* <C> Target bridging <C> 76.6M <C> 55.8M <C> 0% <C> 36.69 <C> 39.04 <C> 37.63 <C> 40.41 <C> 27.98 <C> 36.27 <R> <C> RNNsearch* <C> Direct bridging <C> 78.9M <C> 55.8M <C> 0% <C> 36.97 <C> 39.77 <C> 38.02 <C> 40.83 <C> 27.85 <C> 36.62 <R> <C> Transformer <C> Vanilla <C> 90.2M <C> 46.1M <C> 0% <C> 41.37 <C> 42.53 <C> 40.25 <C> 43.58 <C> 32.89 <C> 40.33 <R> <C> Transformer <C> Direct bridging <C> 90.5M <C> 46.1M <C> 0% <C> 41.67 <C> 42.89 <C> 41.34 <C> 43.56 <C> 32.69 <C> 40.54 <R> <C> Transformer <C> Decoder WT <C> 74.9M <C> 30.7M <C> 33.4% <C> 41.90 <C> 43.02 <C> 41.89 <C> 43.87 <C> 32.62 <C> 40.82 <R> <C> Transformer <C> [ITALIC] Shared-private <C> 62.8M <C> 18.7M <C> 59.4% <C> 42.57↑ <C> 43.73↑ <C> 41.99↑ <C> 44.53↑ <C> 33.81⇑ <C> 41.61⇑ <CAP> Table 1: Results on the NIST Chinese-English translation task. “Params” denotes the number of model parameters. “Emb.” represents the number of parameters used for word representation. “Red.” represents the reduction rate of the standard size. The results of SMT* and RNNsearch* are reported by Kuang et al. Kuang et al. (2018) with the same datasets and vocabulary settings. “↑” indicates the result is significantly better than that of the vanilla Transformer (p<0.01), while “⇑” indicates the result is significantly better than that of all other Transformer models (p<0.01). All significance tests are measured by paired bootstrap resampling Koehn (2004). <COT> Looking at the "Params" column, we can see the number of model parameters for each translation model.
<R> <C> En⇒De <C> Params <C> Emb. <C> Red. <C> BLEU <R> <C> Vanilla <C> 98.7M <C> 54.5M <C> 0% <C> 27.62 <R> <C> Direct bridging <C> 98.9M <C> 54.5M <C> 0% <C> 27.79 <R> <C> Decoder WT <C> 80.4M <C> 36.2M <C> 33.6% <C> 27.51 <R> <C> Three-way WT <C> 63.1M <C> 18.9M <C> 65.3% <C> 27.39 <R> <C> [ITALIC] Shared-private <C> 65.0M <C> 20.9M <C> 63.1% <C> 28.06‡ <CAP> Table 2: Results on the WMT English-German translation task. “‡” indicates the result is significantly better than the vanilla Transformer model (p<0.05). <COT> Looking at the "BLEU" column, we can see that the "Shared-private" model has the highest BLEU score of 28.06, which is significantly better than the vanilla Transformer model.
<R> <C> En⇒De <C> Params <C> Emb. <C> Red. <C> BLEU <R> <C> Vanilla <C> 98.7M <C> 54.5M <C> 0% <C> 27.62 <R> <C> Direct bridging <C> 98.9M <C> 54.5M <C> 0% <C> 27.79 <R> <C> Decoder WT <C> 80.4M <C> 36.2M <C> 33.6% <C> 27.51 <R> <C> Three-way WT <C> 63.1M <C> 18.9M <C> 65.3% <C> 27.39 <R> <C> [ITALIC] Shared-private <C> 65.0M <C> 20.9M <C> 63.1% <C> 28.06‡ <CAP> Table 2: Results on the WMT English-German translation task. “‡” indicates the result is significantly better than the vanilla Transformer model (p<0.05). <COT> Looking at the "Red." column, we can see that the "Three-way WT" model has the highest reduction percentage of 65.3%.
<R> <C> [EMPTY] <C> Model <C> Emb. <C> Red. <C> BLEU <R> <C> Ar⇒ En <C> Vanilla <C> 23.6M <C> 0% <C> 28.36 <R> <C> Ar⇒ En <C> [ITALIC] Shared-private <C> 11.8M <C> 50% <C> 29.71↑ <R> <C> Ja⇒ En <C> Vanilla <C> 25.6M <C> 0% <C> 10.94 <R> <C> Ja⇒ En <C> [ITALIC] Shared-private <C> 13.3M <C> 48.0% <C> 12.35↑ <R> <C> Ko⇒ En <C> Vanilla <C> 25.1M <C> 0% <C> 16.48 <R> <C> Ko⇒ En <C> [ITALIC] Shared-private <C> 13.2M <C> 47.4% <C> 17.84↑ <R> <C> Zh⇒ En <C> Vanilla <C> 27.4M <C> 0% <C> 19.36 <R> <C> Zh⇒ En <C> [ITALIC] Shared-private <C> 13.8M <C> 49.6% <C> 21.00↑ <CAP> Table 3: Results on the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks. These distant language pairs belonging to 5 different language families and written in 5 different alphabets.“↑” indicates the result is significantly better than that of the vanilla Transformer (p<0.01). <COT> Looking at the "CAP" tag, we can see that the table shows results on the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks.
<R> <C> [EMPTY] <C> Model <C> Emb. <C> Red. <C> BLEU <R> <C> Ar⇒ En <C> Vanilla <C> 23.6M <C> 0% <C> 28.36 <R> <C> Ar⇒ En <C> [ITALIC] Shared-private <C> 11.8M <C> 50% <C> 29.71↑ <R> <C> Ja⇒ En <C> Vanilla <C> 25.6M <C> 0% <C> 10.94 <R> <C> Ja⇒ En <C> [ITALIC] Shared-private <C> 13.3M <C> 48.0% <C> 12.35↑ <R> <C> Ko⇒ En <C> Vanilla <C> 25.1M <C> 0% <C> 16.48 <R> <C> Ko⇒ En <C> [ITALIC] Shared-private <C> 13.2M <C> 47.4% <C> 17.84↑ <R> <C> Zh⇒ En <C> Vanilla <C> 27.4M <C> 0% <C> 19.36 <R> <C> Zh⇒ En <C> [ITALIC] Shared-private <C> 13.8M <C> 49.6% <C> 21.00↑ <CAP> Table 3: Results on the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks. These distant language pairs belonging to 5 different language families and written in 5 different alphabets.“↑” indicates the result is significantly better than that of the vanilla Transformer (p<0.01). <COT> Looking at the "Vanilla" column, we can see that the "BLEU" scores for the Vanilla model are lower compared to the Shared-private model for all language pairs.
<R> <C> Zh-En <C> [ITALIC] λlm <C> [ITALIC] λwf <C> [ITALIC] λur <C> Emb. <C> BLEU <R> <C> Vanilla <C> - <C> - <C> - <C> 46.1M <C> 41.37 <R> <C> Decoder WT <C> 0 <C> 0 <C> 0 <C> 30.7M <C> 41.90 <R> <C> [ITALIC] Shared-private <C> 0.5 <C> 0.7 <C> 0.9 <C> 21.2M <C> 41.98 <R> <C> [ITALIC] Shared-private <C> 0.5 <C> 0.5 <C> 0.5 <C> 23.0M <C> 42.26 <R> <C> [ITALIC] Shared-private <C> 0.9 <C> 0.7 <C> 0 <C> 21.0M <C> 42.27 <R> <C> [ITALIC] Shared-private <C> 1 <C> 1 <C> 1 <C> 15.3M <C> 42.36 <R> <C> [ITALIC] Shared-private <C> 0.9 <C> 0.7 <C> 0.5 <C> 18.7M <C> 42.57 <CAP> Table 4: Performance of models using different sharing coefficients on the validation set of the NIST Chinese-English translation task. <COT> Looking at the "BLEU" column, we can see that the highest BLEU score is achieved by the model with the sharing coefficients [ITALIC] Shared-private 0.9, 0.7, 0.5.
<R> <C> Zh-En <C> [ITALIC] λlm <C> [ITALIC] λwf <C> [ITALIC] λur <C> Emb. <C> BLEU <R> <C> Vanilla <C> - <C> - <C> - <C> 46.1M <C> 41.37 <R> <C> Decoder WT <C> 0 <C> 0 <C> 0 <C> 30.7M <C> 41.90 <R> <C> [ITALIC] Shared-private <C> 0.5 <C> 0.7 <C> 0.9 <C> 21.2M <C> 41.98 <R> <C> [ITALIC] Shared-private <C> 0.5 <C> 0.5 <C> 0.5 <C> 23.0M <C> 42.26 <R> <C> [ITALIC] Shared-private <C> 0.9 <C> 0.7 <C> 0 <C> 21.0M <C> 42.27 <R> <C> [ITALIC] Shared-private <C> 1 <C> 1 <C> 1 <C> 15.3M <C> 42.36 <R> <C> [ITALIC] Shared-private <C> 0.9 <C> 0.7 <C> 0.5 <C> 18.7M <C> 42.57 <CAP> Table 4: Performance of models using different sharing coefficients on the validation set of the NIST Chinese-English translation task. <COT> Looking at the "Emb." column, we can see that the model with the smallest embedding size is the one with the sharing coefficients [ITALIC] Shared-private 1, 1, 1.
<R> <C> Tool <C> Time (minutes) Ubuntu <C> Time (minutes) macOS <R> <C> slate <C> 10 <C> 16 <R> <C> YEDDA <C> 14 <C> 14 <R> <C> GATE <C> 21 <C> 22 <R> <C> brat <C> - <C> - <CAP> Table 2: Average time for users to set up the tool and identify verbs in a 623 word news article. Only one participant managed to install and use brat, taking 18 minutes on Ubuntu. The differences between GATE and either slate or YEDDA are significant at the 0.01 level according to a t-test. <COT> Looking at the "CAP" tag, we can see that Table 2 provides information about the average time for users to set up a tool and identify verbs in a 623 word news article.
<R> <C> Tool <C> Time (minutes) Ubuntu <C> Time (minutes) macOS <R> <C> slate <C> 10 <C> 16 <R> <C> YEDDA <C> 14 <C> 14 <R> <C> GATE <C> 21 <C> 22 <R> <C> brat <C> - <C> - <CAP> Table 2: Average time for users to set up the tool and identify verbs in a 623 word news article. Only one participant managed to install and use brat, taking 18 minutes on Ubuntu. The differences between GATE and either slate or YEDDA are significant at the 0.01 level according to a t-test. <COT> Looking at the "brat" row, we can see that there is no time recorded for both Ubuntu and macOS.
<R> <C> No. of sentences <C> node2vec <C> syntree2vec <C> word2vec <R> <C> 0.01MB : 73 <C> 24.93 <C> 20.11 <C> 22 <R> <C> 0.03MB : 220 <C> 28.44 <C> 28.24 <C> 28.44 <R> <C> 0.07MB : 508 <C> 43.59 <C> 43.41 <C> 43.41 <R> <C> 0.15MB : 1070 <C> 71.20 <C> 71.01 <C> 71.01 <CAP> Table 1: Perplexity Scores <COT> Looking at the table, we can see that the perplexity scores for node2vec, syntree2vec, and word2vec increase as the size of the dataset increases.
<R> <C> No. of sentences <C> node2vec <C> syntree2vec <C> word2vec <R> <C> 0.01MB : 73 <C> 24.93 <C> 20.11 <C> 22 <R> <C> 0.03MB : 220 <C> 28.44 <C> 28.24 <C> 28.44 <R> <C> 0.07MB : 508 <C> 43.59 <C> 43.41 <C> 43.41 <R> <C> 0.15MB : 1070 <C> 71.20 <C> 71.01 <C> 71.01 <CAP> Table 1: Perplexity Scores <COT> Comparing the perplexity scores for each dataset size, we can see that the scores for node2vec and syntree2vec are very close, while the score for word2vec is consistently higher.
<R> <C> Label <C> S ∈ LA <C> SA ∈ S <C> # Train <C> # Dev <R> <C> 1 <C> No <C> No <C> 19,446,120 <C> 870,404 <R> <C> 2 <C> No <C> Yes <C> 428,122 <C> 25,814 <R> <C> 3 <C> Yes <C> No <C> 442,140 <C> 29,558 <R> <C> 4 <C> Yes <C> Yes <C> 61,186 <C> 4,286 <CAP> Table 1: Label description for ASNQ. Here S, LA, SA refer to answer sentence, long answer passage and short answer phrase respectively. <COT> Looking at the "Label" column, we can see that there are two labels: "Yes" and "No". 
<R> <C> Label <C> S ∈ LA <C> SA ∈ S <C> # Train <C> # Dev <R> <C> 1 <C> No <C> No <C> 19,446,120 <C> 870,404 <R> <C> 2 <C> No <C> Yes <C> 428,122 <C> 25,814 <R> <C> 3 <C> Yes <C> No <C> 442,140 <C> 29,558 <R> <C> 4 <C> Yes <C> Yes <C> 61,186 <C> 4,286 <CAP> Table 1: Label description for ASNQ. Here S, LA, SA refer to answer sentence, long answer passage and short answer phrase respectively. <COT> Looking at the "# Train" column, we can see that the number of training examples varies for each label.
<R> <C> [BOLD] Model <C> [BOLD] MAP <C> [BOLD] MRR <R> <C> Comp-Agg + LM + LC <C> 0.764 <C> 0.784 <R> <C> Comp-Agg + LM + LC+ TL(QNLI) <C> 0.834 <C> 0.848 <R> <C> BERT-B FT WikiQA <C> 0.813 <C> 0.828 <R> <C> BERT-B FT ASNQ <C> 0.884 <C> 0.898 <R> <C> BERT-B TandA (ASNQ → WikiQA ) <C> 0.893 <C> 0.903 <R> <C> BERT-L FT WikiQA <C> 0.836 <C> 0.853 <R> <C> BERT-L FT ASNQ <C> 0.892 <C> 0.904 <R> <C> BERT-L TandA (ASNQ → WikiQA) <C> 0.904 <C> 0.912 <R> <C> RoBERTa-B FT ASNQ <C> 0.882 <C> 0.894 <R> <C> RoBERTa-B TandA (ASNQ → WikiQA) <C> 0.889 <C> 0.901 <R> <C> RoBERTa-L FT ASNQ <C> 0.910 <C> 0.919 <R> <C> RoBERTa-L TandA (ASNQ → WikiQA ) <C> [BOLD] 0.920 <C> [BOLD] 0.933 <CAP> Table 3: Performance of different models on WikiQA dataset. Here Comp-Agg + LM + LC refers to a Compare-Aggregate model with Language Modeling and Latent Clustering as proposed by Yoon et al. DBLP:journals/corr/abs-1905-12897. TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively. <COT> Looking at the "Model" column and the corresponding cells, we can see that the RoBERTa-L TandA (ASNQ → WikiQA) model achieves the highest MAP and MRR scores among all the models listed in the table.
<R> <C> [BOLD] Model <C> [BOLD] MAP <C> [BOLD] MRR <R> <C> Comp-Agg + LM + LC <C> 0.764 <C> 0.784 <R> <C> Comp-Agg + LM + LC+ TL(QNLI) <C> 0.834 <C> 0.848 <R> <C> BERT-B FT WikiQA <C> 0.813 <C> 0.828 <R> <C> BERT-B FT ASNQ <C> 0.884 <C> 0.898 <R> <C> BERT-B TandA (ASNQ → WikiQA ) <C> 0.893 <C> 0.903 <R> <C> BERT-L FT WikiQA <C> 0.836 <C> 0.853 <R> <C> BERT-L FT ASNQ <C> 0.892 <C> 0.904 <R> <C> BERT-L TandA (ASNQ → WikiQA) <C> 0.904 <C> 0.912 <R> <C> RoBERTa-B FT ASNQ <C> 0.882 <C> 0.894 <R> <C> RoBERTa-B TandA (ASNQ → WikiQA) <C> 0.889 <C> 0.901 <R> <C> RoBERTa-L FT ASNQ <C> 0.910 <C> 0.919 <R> <C> RoBERTa-L TandA (ASNQ → WikiQA ) <C> [BOLD] 0.920 <C> [BOLD] 0.933 <CAP> Table 3: Performance of different models on WikiQA dataset. Here Comp-Agg + LM + LC refers to a Compare-Aggregate model with Language Modeling and Latent Clustering as proposed by Yoon et al. DBLP:journals/corr/abs-1905-12897. TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively. <COT> Looking at the "Model" column and the corresponding cells, we can see that the BERT-B TandA (ASNQ → WikiQA) model achieves a higher MAP and MRR score than the BERT-L TandA (ASNQ → WikiQA) model.
<R> <C> [BOLD] Model <C> [BOLD] MAP <C> [BOLD] MRR <R> <C> Comp-Agg + LM + LC <C> 0.868 <C> 0.928 <R> <C> Comp-Agg + LM + LC + TL(QNLI) <C> 0.875 <C> 0.940 <R> <C> BERT-B FT TREC-QA <C> 0.857 <C> 0.937 <R> <C> BERT-B FT ASNQ <C> 0.823 <C> 0.872 <R> <C> BERT-B TandA (ASNQ → TREC-QA) <C> 0.912 <C> 0.951 <R> <C> BERT-L FT TREC-QA <C> 0.904 <C> 0.946 <R> <C> BERT-L FT ASNQ <C> 0.824 <C> 0.872 <R> <C> BERT-L TandA (ASNQ → TREC-QA ) <C> 0.912 <C> 0.967 <R> <C> RoBERTa-B FT ASNQ <C> 0.849 <C> 0.907 <R> <C> RoBERTa-B TandA (ASNQ →TREC-QA ) <C> 0.914 <C> 0.952 <R> <C> RoBERTa-L FT ASNQ <C> 0.880 <C> 0.928 <R> <C> RoBERTa-L TandA (ASNQ → TREC-QA) <C> [BOLD] 0.943 <C> [BOLD] 0.974 <CAP> Table 4: Performance of different models on TREC-QA dataset. Here Comp-Agg + LM + LC refers to a Compare-Aggregate model with Language Modeling and Latent Clustering as proposed in [25]. TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively. <COT> Looking at the "Model" column, we can see that there are different models listed, including Comp-Agg + LM + LC, BERT-B, BERT-L, RoBERTa-B, and RoBERTa-L. The MAP and MRR scores are provided for each model.
<R> <C> [BOLD] Model <C> [BOLD] MAP <C> [BOLD] MRR <R> <C> Comp-Agg + LM + LC <C> 0.868 <C> 0.928 <R> <C> Comp-Agg + LM + LC + TL(QNLI) <C> 0.875 <C> 0.940 <R> <C> BERT-B FT TREC-QA <C> 0.857 <C> 0.937 <R> <C> BERT-B FT ASNQ <C> 0.823 <C> 0.872 <R> <C> BERT-B TandA (ASNQ → TREC-QA) <C> 0.912 <C> 0.951 <R> <C> BERT-L FT TREC-QA <C> 0.904 <C> 0.946 <R> <C> BERT-L FT ASNQ <C> 0.824 <C> 0.872 <R> <C> BERT-L TandA (ASNQ → TREC-QA ) <C> 0.912 <C> 0.967 <R> <C> RoBERTa-B FT ASNQ <C> 0.849 <C> 0.907 <R> <C> RoBERTa-B TandA (ASNQ →TREC-QA ) <C> 0.914 <C> 0.952 <R> <C> RoBERTa-L FT ASNQ <C> 0.880 <C> 0.928 <R> <C> RoBERTa-L TandA (ASNQ → TREC-QA) <C> [BOLD] 0.943 <C> [BOLD] 0.974 <CAP> Table 4: Performance of different models on TREC-QA dataset. Here Comp-Agg + LM + LC refers to a Compare-Aggregate model with Language Modeling and Latent Clustering as proposed in [25]. TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively. <COT> Looking at the "RoBERTa-L TandA (ASNQ → TREC-QA)" row, we can see that the MAP score is [BOLD] 0.943 and the MRR score is [BOLD] 0.974.
<R> <C> [BOLD] BERT-base <C> WikiQA MAP <C> WikiQA % Drop <C> WikiQA MRR <C> WikiQA % Drop <C> TREC-QA MAP <C> TREC-QA % Drop <C> TREC-QA MRR <C> TREC-QA % Drop <R> <C> No noise Fine-tuning <C> 0.813 <C> - <C> 0.828 <C> - <C> 0.857 <C> - <C> 0.937 <C> - <R> <C> 10% noise Fine-tuning <C> 0.775 <C> 4.67% <C> 0.793 <C> 4.22% <C> 0.826 <C> 3.62% <C> 0.902 <C> 3.73% <R> <C> 20% noise Fine-tuning <C> 0.629 <C> [BOLD] 22.63% <C> 0.645 <C> 22.10% <C> 0.738 <C> [BOLD] 13.88% <C> 0.843 <C> 10.03% <R> <C> No noise TandA (ASNQ → *) <C> 0.893 <C> - <C> 0.903 <C> - <C> 0.912 <C> - <C> 0.951 <C> - <R> <C> 10% noise TandA (ASNQ → *) <C> 0.876 <C> 1.90% <C> 0.889 <C> 1.55% <C> 0.896 <C> 1.75% <C> 0.941 <C> 1.05% <R> <C> 20% noise TandA (ASNQ → *) <C> 0.870 <C> [BOLD] 2.57% <C> 0.886 <C> 1.88% <C> 0.891 <C> [BOLD] 2.30% <C> 0.937 <C> 1.47% <CAP> Table 5: Model accuracy when noise is injected into WikiQA and TREC-QA datasets. ∗ indicates the target dataset for the second step of fine-tuning (adapt step). <COT> Looking at the "TREC-QA % Drop" column, we can see that the percentage drop in performance increases as the noise level increases for both fine-tuning and TandA (ASNQ → *) approaches.
<R> <C> [BOLD] BERT-base <C> WikiQA MAP <C> WikiQA % Drop <C> WikiQA MRR <C> WikiQA % Drop <C> TREC-QA MAP <C> TREC-QA % Drop <C> TREC-QA MRR <C> TREC-QA % Drop <R> <C> No noise Fine-tuning <C> 0.813 <C> - <C> 0.828 <C> - <C> 0.857 <C> - <C> 0.937 <C> - <R> <C> 10% noise Fine-tuning <C> 0.775 <C> 4.67% <C> 0.793 <C> 4.22% <C> 0.826 <C> 3.62% <C> 0.902 <C> 3.73% <R> <C> 20% noise Fine-tuning <C> 0.629 <C> [BOLD] 22.63% <C> 0.645 <C> 22.10% <C> 0.738 <C> [BOLD] 13.88% <C> 0.843 <C> 10.03% <R> <C> No noise TandA (ASNQ → *) <C> 0.893 <C> - <C> 0.903 <C> - <C> 0.912 <C> - <C> 0.951 <C> - <R> <C> 10% noise TandA (ASNQ → *) <C> 0.876 <C> 1.90% <C> 0.889 <C> 1.55% <C> 0.896 <C> 1.75% <C> 0.941 <C> 1.05% <R> <C> 20% noise TandA (ASNQ → *) <C> 0.870 <C> [BOLD] 2.57% <C> 0.886 <C> 1.88% <C> 0.891 <C> [BOLD] 2.30% <C> 0.937 <C> 1.47% <CAP> Table 5: Model accuracy when noise is injected into WikiQA and TREC-QA datasets. ∗ indicates the target dataset for the second step of fine-tuning (adapt step). <COT> Comparing the "20% noise Fine-tuning" row with the "No noise Fine-tuning" row, we can see that the percentage drop in performance is higher for the WikiQA MAP and WikiQA MRR metrics compared to the TREC-QA MAP and TREC-QA MRR metrics.
<R> <C> Model <C> WikiQA MAP <C> WikiQA MRR <C> TREC-QA MAP <C> TREC-QA MRR <R> <C> Neg: 1 Pos: 4 <C> 0.870 <C> 0.880 <C> 0.808 <C> 0.847 <R> <C> Neg: 2 Pos: 4 <C> 0.751 <C> 0.763 <C> 0.662 <C> 0.751 <R> <C> Neg: 3 Pos: 4 <C> 0.881 <C> 0.895 <C> 0.821 <C> 0.869 <R> <C> Neg: 2,3 Pos: 4 <C> 0.883 <C> 0.898 <C> 0.823 <C> 0.871 <R> <C> Neg: 1,2,3 Pos: 4 <C> 0.884 <C> 0.898 <C> 0.823 <C> 0.872 <CAP> Table 6: Impact of different labels of ASNQ on fine-tuning BERT for answer sentence selection. Neg and Pos refers to question-answer (QA) pairs of that particular label being chosen for fine-tuning. <COT> Looking at the "Neg: 1 Pos: 4" row and the "WikiQA MAP" column, we can see that the MAP score for WikiQA is 0.870.
<R> <C> Model <C> WikiQA MAP <C> WikiQA MRR <C> TREC-QA MAP <C> TREC-QA MRR <R> <C> Neg: 1 Pos: 4 <C> 0.870 <C> 0.880 <C> 0.808 <C> 0.847 <R> <C> Neg: 2 Pos: 4 <C> 0.751 <C> 0.763 <C> 0.662 <C> 0.751 <R> <C> Neg: 3 Pos: 4 <C> 0.881 <C> 0.895 <C> 0.821 <C> 0.869 <R> <C> Neg: 2,3 Pos: 4 <C> 0.883 <C> 0.898 <C> 0.823 <C> 0.871 <R> <C> Neg: 1,2,3 Pos: 4 <C> 0.884 <C> 0.898 <C> 0.823 <C> 0.872 <CAP> Table 6: Impact of different labels of ASNQ on fine-tuning BERT for answer sentence selection. Neg and Pos refers to question-answer (QA) pairs of that particular label being chosen for fine-tuning. <COT> Looking at the "Neg: 2,3 Pos: 4" row and the "TREC-QA MRR" column, we can see that the MRR score for TREC-QA is 0.871.
<R> <C> [BOLD] BERT-Base <C> [BOLD] WikiQA MAP <C> [BOLD] WikiQA MRR <C> [BOLD] TREC-QA MAP <C> [BOLD] TREC-QA MRR <R> <C> FT QNLI <C> 0.760 <C> 0.766 <C> 0.820 <C> 0.890 <R> <C> FT ASNQ <C> 0.884 <C> 0.898 <C> 0.823 <C> 0.872 <R> <C> TandA (QNLI →) <C> 0.832 <C> 0.852 <C> 0.863 <C> 0.906 <R> <C> TandA (ASNQ →) <C> 0.893 <C> 0.903 <C> 0.912 <C> 0.951 <CAP> Table 7: Comparison of TandA with ASNQ and QNLI <COT> Looking at the "CAP" tag, we can see that Table 7 compares the performance of TandA with ASNQ and QNLI.
<R> <C> [BOLD] BERT-Base <C> [BOLD] WikiQA MAP <C> [BOLD] WikiQA MRR <C> [BOLD] TREC-QA MAP <C> [BOLD] TREC-QA MRR <R> <C> FT QNLI <C> 0.760 <C> 0.766 <C> 0.820 <C> 0.890 <R> <C> FT ASNQ <C> 0.884 <C> 0.898 <C> 0.823 <C> 0.872 <R> <C> TandA (QNLI →) <C> 0.832 <C> 0.852 <C> 0.863 <C> 0.906 <R> <C> TandA (ASNQ →) <C> 0.893 <C> 0.903 <C> 0.912 <C> 0.951 <CAP> Table 7: Comparison of TandA with ASNQ and QNLI <COT> Looking at the "FT QNLI" row, we can see that the MAP score for TREC-QA is higher than the MAP score for WikiQA.
<R> <C> MODEL <C> MODEL <C> MODEL <C> Sample 1 Prec@1 <C> Sample 1 MAP <C> Sample 1 MRR <C> Sample 2 Prec@1 <C> Sample 2 MAP <C> Sample 2 MRR <C> Sample 3 Prec@1 <C> Sample 3 MAP <C> Sample 3 MRR <R> <C> BERT <C> Base <C> NAD <C> 49.80 <C> 0.506 <C> 0.638 <C> 52.69 <C> 0.432 <C> 0.629 <C> 41.86 <C> 0.352 <C> 0.543 <R> <C> BERT <C> Base <C> ASNQ <C> 55.06 <C> 0.557 <C> 0.677 <C> 44.31 <C> 0.395 <C> 0.567 <C> 44.19 <C> 0.369 <C> 0.561 <R> <C> BERT <C> Base <C> TANDA (ASNQ → NAD) <C> 58.70 <C> 0.585 <C> 0.703 <C> 58.68 <C> 0.474 <C> 0.683 <C> 49.42 <C> 0.391 <C> 0.613 <R> <C> BERT <C> Large <C> NAD <C> 53.85 <C> 0.537 <C> 0.671 <C> 53.29 <C> 0.469 <C> 0.629 <C> 43.61 <C> 0.395 <C> 0.558 <R> <C> BERT <C> Large <C> ASNQ <C> 57.49 <C> 0.552 <C> 0.686 <C> 50.89 <C> 0.440 <C> 0.630 <C> 45.93 <C> 0.399 <C> 0.585 <R> <C> BERT <C> Large <C> TANDA (ASNQ → NAD) <C> 61.54 <C> 0.607 <C> 0.725 <C> 63.47 <C> 0.514 <C> 0.727 <C> 51.16 <C> 0.439 <C> 0.616 <R> <C> RoBERTa <C> Base <C> NAD <C> 59.11 <C> 0.563 <C> 0.699 <C> 56.29 <C> 0.511 <C> 0.670 <C> 48.26 <C> 0.430 <C> 0.612 <R> <C> RoBERTa <C> Base <C> ASNQ <C> 58.70 <C> 0.587 <C> 0.707 <C> 54.50 <C> 0.473 <C> 0.656 <C> 45.35 <C> 0.437 <C> 0.608 <R> <C> RoBERTa <C> Base <C> TANDA (ASNQ → NAD) <C> 65.59 <C> 0.623 <C> 0.757 <C> 62.87 <C> 0.537 <C> 0.714 <C> 56.98 <C> 0.473 <C> 0.679 <R> <C> RoBERTa <C> Large <C> NAD <C> 70.81 <C> 0.654 <C> 0.796 <C> 63.47 <C> 0.581 <C> 0.734 <C> 52.91 <C> 0.490 <C> 0.651 <R> <C> RoBERTa <C> Large <C> ASNQ <C> 64.37 <C> 0.627 <C> 0.750 <C> 59.88 <C> 0.526 <C> 0.705 <C> 54.65 <C> 0.478 <C> 0.674 <R> <C> RoBERTa <C> Large <C> TANDA (ASNQ → NAD) <C> 71.26 <C> 0.680 <C> 0.805 <C> 74.85 <C> 0.625 <C> 0.821 <C> 58.14 <C> 0.514 <C> 0.699 <CAP> Table 8: Comparison between FT and TandA on real-world datasets derived from Alexa Virtual Assistant traffic <COT> Looking at the "MODEL" column, finding that there are three different models: BERT, RoBERTa Base, and RoBERTa Large.
<R> <C> MODEL <C> MODEL <C> MODEL <C> Sample 1 Prec@1 <C> Sample 1 MAP <C> Sample 1 MRR <C> Sample 2 Prec@1 <C> Sample 2 MAP <C> Sample 2 MRR <C> Sample 3 Prec@1 <C> Sample 3 MAP <C> Sample 3 MRR <R> <C> BERT <C> Base <C> NAD <C> 49.80 <C> 0.506 <C> 0.638 <C> 52.69 <C> 0.432 <C> 0.629 <C> 41.86 <C> 0.352 <C> 0.543 <R> <C> BERT <C> Base <C> ASNQ <C> 55.06 <C> 0.557 <C> 0.677 <C> 44.31 <C> 0.395 <C> 0.567 <C> 44.19 <C> 0.369 <C> 0.561 <R> <C> BERT <C> Base <C> TANDA (ASNQ → NAD) <C> 58.70 <C> 0.585 <C> 0.703 <C> 58.68 <C> 0.474 <C> 0.683 <C> 49.42 <C> 0.391 <C> 0.613 <R> <C> BERT <C> Large <C> NAD <C> 53.85 <C> 0.537 <C> 0.671 <C> 53.29 <C> 0.469 <C> 0.629 <C> 43.61 <C> 0.395 <C> 0.558 <R> <C> BERT <C> Large <C> ASNQ <C> 57.49 <C> 0.552 <C> 0.686 <C> 50.89 <C> 0.440 <C> 0.630 <C> 45.93 <C> 0.399 <C> 0.585 <R> <C> BERT <C> Large <C> TANDA (ASNQ → NAD) <C> 61.54 <C> 0.607 <C> 0.725 <C> 63.47 <C> 0.514 <C> 0.727 <C> 51.16 <C> 0.439 <C> 0.616 <R> <C> RoBERTa <C> Base <C> NAD <C> 59.11 <C> 0.563 <C> 0.699 <C> 56.29 <C> 0.511 <C> 0.670 <C> 48.26 <C> 0.430 <C> 0.612 <R> <C> RoBERTa <C> Base <C> ASNQ <C> 58.70 <C> 0.587 <C> 0.707 <C> 54.50 <C> 0.473 <C> 0.656 <C> 45.35 <C> 0.437 <C> 0.608 <R> <C> RoBERTa <C> Base <C> TANDA (ASNQ → NAD) <C> 65.59 <C> 0.623 <C> 0.757 <C> 62.87 <C> 0.537 <C> 0.714 <C> 56.98 <C> 0.473 <C> 0.679 <R> <C> RoBERTa <C> Large <C> NAD <C> 70.81 <C> 0.654 <C> 0.796 <C> 63.47 <C> 0.581 <C> 0.734 <C> 52.91 <C> 0.490 <C> 0.651 <R> <C> RoBERTa <C> Large <C> ASNQ <C> 64.37 <C> 0.627 <C> 0.750 <C> 59.88 <C> 0.526 <C> 0.705 <C> 54.65 <C> 0.478 <C> 0.674 <R> <C> RoBERTa <C> Large <C> TANDA (ASNQ → NAD) <C> 71.26 <C> 0.680 <C> 0.805 <C> 74.85 <C> 0.625 <C> 0.821 <C> 58.14 <C> 0.514 <C> 0.699 <CAP> Table 8: Comparison between FT and TandA on real-world datasets derived from Alexa Virtual Assistant traffic <COT> Looking at the "Sample 1 Prec@1" column, finding that the highest precision at 1 for Sample 1 is achieved by the model BERT Base with TANDA (ASNQ → NAD) transfer learning.
<R> <C> IC = 1 Feature <C> IC = 1 Value <C> IC = 1 Contribution <C> IC = 2 Feature <C> IC = 2 Value <C> IC = 2 Contribution <C> IC = 3 Feature <C> IC = 3 Value <C> IC = 3 Contribution <R> <C> has_diff <C> 0.0 <C> +0.953 <C> dif_too <C> 1.0 <C> +1.155 <C> Bias term <C> 1.0 <C> +0.872 <R> <C> Bias term <C> 1.0 <C> +0.419 <C> Bias term <C> 1.0 <C> +0.594 <C> has_int <C> 0.0 <C> +0.450 <R> <C> dif_but <C> 0.0 <C> +0.114 <C> dif_consider <C> 1.0 <C> +0.491 <C> dif_may <C> 1.0 <C> +0.393 <R> <C> dif_because <C> 0.0 <C> +0.058 <C> dif_however <C> 1.0 <C> +0.186 <C> dif_but <C> 1.0 <C> +0.306 <R> <C> dif_how <C> 0.0 <C> +0.033 <C> dif_how <C> 1.0 <C> +0.092 <C> dif_hope <C> 1.0 <C> +0.215 <R> <C> dif_yet <C> 0.0 <C> +0.033 <C> dif_hope <C> 0.0 <C> +0.027 <C> dif_while <C> 0.0 <C> +0.060 <R> <C> int_unity <C> 0.0 <C> +0.029 <C> dif_perhaps <C> 0.0 <C> +0.019 <C> dif_rather <C> 0.0 <C> +0.058 <R> <C> dif_depend <C> 0.0 <C> +0.024 <C> dif_almost <C> 0.0 <C> +0.018 <C> dif_too <C> 0.0 <C> +0.033 <R> <C> dif_hope <C> 0.0 <C> +0.022 <C> dif_sometimes <C> 0.0 <C> +0.012 <C> dif_seem <C> 0.0 <C> +0.030 <R> <C> dif_rather <C> 0.0 <C> +0.022 <C> dif_although <C> 0.0 <C> +0.011 <C> dif_differ <C> 0.0 <C> +0.026 <R> <C> has_int <C> 0.0 <C> -0.009 <C> dif_while <C> 0.0 <C> -0.032 <C> int_remain <C> 0.0 <C> -0.024 <R> <C> dif_close_to <C> 0.0 <C> -0.009 <C> dif_rather <C> 0.0 <C> -0.033 <C> dif_separate <C> 0.0 <C> -0.026 <R> <C> dif_seem <C> 0.0 <C> -0.010 <C> dif_different <C> 0.0 <C> -0.050 <C> int_weigh <C> 0.0 <C> -0.028 <R> <C> dif_consider <C> 0.0 <C> -0.011 <C> dif_often <C> 0.0 <C> -0.050 <C> dif_possible <C> 0.0 <C> -0.029 <R> <C> int_account <C> 0.0 <C> -0.012 <C> dif_each <C> 0.0 <C> -0.050 <C> int_unity <C> 0.0 <C> -0.031 <R> <C> dif_secret <C> 0.0 <C> -0.013 <C> dif_either <C> 0.0 <C> -0.069 <C> dif_however <C> 0.0 <C> -0.036 <R> <C> dif_differ <C> 0.0 <C> -0.018 <C> dif_about <C> 0.0 <C> -0.078 <C> dif_often <C> 0.0 <C> -0.037 <R> <C> dif_usually <C> 0.0 <C> -0.019 <C> dif_both <C> 0.0 <C> -0.079 <C> dif_about <C> 0.0 <C> -0.068 <R> <C> int_remain <C> 0.0 <C> -0.024 <C> dif_because <C> 0.0 <C> -0.284 <C> dif_though <C> 0.0 <C> -0.073 <R> <C> dif_may <C> 0.0 <C> -0.036 <C> dif_but <C> 1.0 <C> -0.575 <C> dif_because <C> 0.0 <C> -0.102 <CAP> Table 5: Top ten and bottom ten features used in successful IC classifications using vocabulary features. Differentiation and integration terms are prefixed with dif and int, while has_dif and has_int are the binary features for whether any differentiation/integration terms are present at all. The bias term is the averaged sum of the value associated with each root node in the ensemble. <COT> Looking at the "IC = 1 Contribution" column, we can see that the contribution values for all features are positive except for "has_int" which has a negative contribution value. 
<R> <C> IC = 1 Feature <C> IC = 1 Value <C> IC = 1 Contribution <C> IC = 2 Feature <C> IC = 2 Value <C> IC = 2 Contribution <C> IC = 3 Feature <C> IC = 3 Value <C> IC = 3 Contribution <R> <C> has_diff <C> 0.0 <C> +0.953 <C> dif_too <C> 1.0 <C> +1.155 <C> Bias term <C> 1.0 <C> +0.872 <R> <C> Bias term <C> 1.0 <C> +0.419 <C> Bias term <C> 1.0 <C> +0.594 <C> has_int <C> 0.0 <C> +0.450 <R> <C> dif_but <C> 0.0 <C> +0.114 <C> dif_consider <C> 1.0 <C> +0.491 <C> dif_may <C> 1.0 <C> +0.393 <R> <C> dif_because <C> 0.0 <C> +0.058 <C> dif_however <C> 1.0 <C> +0.186 <C> dif_but <C> 1.0 <C> +0.306 <R> <C> dif_how <C> 0.0 <C> +0.033 <C> dif_how <C> 1.0 <C> +0.092 <C> dif_hope <C> 1.0 <C> +0.215 <R> <C> dif_yet <C> 0.0 <C> +0.033 <C> dif_hope <C> 0.0 <C> +0.027 <C> dif_while <C> 0.0 <C> +0.060 <R> <C> int_unity <C> 0.0 <C> +0.029 <C> dif_perhaps <C> 0.0 <C> +0.019 <C> dif_rather <C> 0.0 <C> +0.058 <R> <C> dif_depend <C> 0.0 <C> +0.024 <C> dif_almost <C> 0.0 <C> +0.018 <C> dif_too <C> 0.0 <C> +0.033 <R> <C> dif_hope <C> 0.0 <C> +0.022 <C> dif_sometimes <C> 0.0 <C> +0.012 <C> dif_seem <C> 0.0 <C> +0.030 <R> <C> dif_rather <C> 0.0 <C> +0.022 <C> dif_although <C> 0.0 <C> +0.011 <C> dif_differ <C> 0.0 <C> +0.026 <R> <C> has_int <C> 0.0 <C> -0.009 <C> dif_while <C> 0.0 <C> -0.032 <C> int_remain <C> 0.0 <C> -0.024 <R> <C> dif_close_to <C> 0.0 <C> -0.009 <C> dif_rather <C> 0.0 <C> -0.033 <C> dif_separate <C> 0.0 <C> -0.026 <R> <C> dif_seem <C> 0.0 <C> -0.010 <C> dif_different <C> 0.0 <C> -0.050 <C> int_weigh <C> 0.0 <C> -0.028 <R> <C> dif_consider <C> 0.0 <C> -0.011 <C> dif_often <C> 0.0 <C> -0.050 <C> dif_possible <C> 0.0 <C> -0.029 <R> <C> int_account <C> 0.0 <C> -0.012 <C> dif_each <C> 0.0 <C> -0.050 <C> int_unity <C> 0.0 <C> -0.031 <R> <C> dif_secret <C> 0.0 <C> -0.013 <C> dif_either <C> 0.0 <C> -0.069 <C> dif_however <C> 0.0 <C> -0.036 <R> <C> dif_differ <C> 0.0 <C> -0.018 <C> dif_about <C> 0.0 <C> -0.078 <C> dif_often <C> 0.0 <C> -0.037 <R> <C> dif_usually <C> 0.0 <C> -0.019 <C> dif_both <C> 0.0 <C> -0.079 <C> dif_about <C> 0.0 <C> -0.068 <R> <C> int_remain <C> 0.0 <C> -0.024 <C> dif_because <C> 0.0 <C> -0.284 <C> dif_though <C> 0.0 <C> -0.073 <R> <C> dif_may <C> 0.0 <C> -0.036 <C> dif_but <C> 1.0 <C> -0.575 <C> dif_because <C> 0.0 <C> -0.102 <CAP> Table 5: Top ten and bottom ten features used in successful IC classifications using vocabulary features. Differentiation and integration terms are prefixed with dif and int, while has_dif and has_int are the binary features for whether any differentiation/integration terms are present at all. The bias term is the averaged sum of the value associated with each root node in the ensemble. <COT> Looking at the "IC = 2 Feature" column, we can see that the feature "Bias term" appears twice with the same value and contribution. 
<R> <C> [BOLD] Algorithm <C> [BOLD] en_ud <C> [BOLD] en_ptb <C> [BOLD] sents/s <R> <C> Eisner (generic) <C> 96.35 <C> 479.1 <C> ∼ 80 <R> <C> Eisner (ours) <C> 1.496 <C> 6.31 <C> ∼ 6009 <R> <C> CLE (generic) <C> 19.12 <C> 93.8 <C> ∼ 404 <R> <C> CLE (ours) <C> 1.764 <C> 6.98 <C> ∼ 5436 <CAP> Figure 1, Listing 1 & Table 1: (Right code snippet) Implementation of Kiperwasser and Goldberg (2016)’s neural parser in only a few lines using UniParse. (Right table and left figure) Seconds a decoder takes to decode an entire dataset, given a set of scores. Score matrix entries are generated uniformly on [0, 1]. The random generated data has an impact on CLE since worst-case performance depends on the sorting bottleneck; the figure demonstrates this by the increasingly broad standard deviation band. Experiments are run on an Ubuntu machine with an Intel Xeon E5-2660, 2.60GHz CPU. <COT> Looking at the table, we can see that the "Algorithm" column contains different algorithms for decoding an entire dataset. The "en_ud" and "en_ptb" columns show the performance of each algorithm in terms of sents/s (sentences per second).
<R> <C> Treebank ar_padt <C> Baseline form <C> Baseline 4.19 <C> Our 3.90 <C> Upp. 2.93 <C> Err.red. 23.1 <R> <C> ca_ancora <C> form <C> 4.65 <C> 4.35 <C> 3.32 <C> 22.3 <R> <C> cs_cac <C> form5 <C> 3.56 <C> 2.25 <C> 1.14 <C> 54.0 <R> <C> cs_fictree <C> form5 <C> 4.82 <C> 4.08 <C> 2.68 <C> 34.6 <R> <C> cs_pdt <C> form5 <C> 4.93 <C> 3.41 <C> 1.65 <C> 46.6 <R> <C> da_ddt <C> form <C> 2.32 <C> 2.16 <C> 1.55 <C> 21.2 <R> <C> en_ewt <C> form <C> 2.29 <C> 2.22 <C> 1.78 <C> 13.8 <R> <C> es_ancora <C> form <C> 3.99 <C> 3.38 <C> 2.25 <C> 34.7 <R> <C> et_edt <C> form5 <C> 4.78 <C> 4.31 <C> 2.54 <C> 20.9 <R> <C> fa_seraji <C> form <C> 8.99 <C> 8.76 <C> 7.44 <C> 14.8 <R> <C> fr_gsd <C> form <C> 4.12 <C> 3.81 <C> 2.70 <C> 22.0 <R> <C> hi_hdtb <C> form <C> 4.18 <C> 3.58 <C> 2.83 <C> 44.3 <R> <C> hr_set <C> form5 <C> 4.04 <C> 2.87 <C> 1.71 <C> 50.2 <R> <C> it_isdt <C> form <C> 4.27 <C> 3.71 <C> 2.78 <C> 37.8 <R> <C> it_postwita <C> form <C> 3.60 <C> 4.07 <C> 2.37 <C> -38.0 <R> <C> ja_gsd <C> form <C> 1.64 <C> 1.93 <C> 1.41 <C> -123.1 <R> <C> ko_kaist <C> form <C> 0.14 <C> 2.41 <C> 0.11 <C> -6392.8 <R> <C> la_ittb <C> form5 <C> 6.53 <C> 6.97 <C> 3.85 <C> -16.4 <R> <C> la_proiel <C> form5 <C> 6.92 <C> 7.42 <C> 4.20 <C> -18.4 <R> <C> lv_lvtb <C> form5 <C> 3.90 <C> 3.39 <C> 2.10 <C> 28.0 <R> <C> no_bokmaal <C> form <C> 2.79 <C> 2.22 <C> 1.48 <C> 43.6 <R> <C> no_nynorsk <C> form <C> 2.73 <C> 2.52 <C> 1.48 <C> 16.7 <R> <C> pl_lfg <C> form5 <C> 3.68 <C> 3.06 <C> 1.84 <C> 33.6 <R> <C> pt_bosque <C> form <C> 3.57 <C> 3.17 <C> 2.55 <C> 39.0 <R> <C> ro_nonstd <C> form5 <C> 8.13 <C> 7.95 <C> 5.64 <C> 7.2 <R> <C> sk_snk <C> form5 <C> 2.87 <C> 2.01 <C> 0.63 <C> 38.2 <R> <C> uk_iu <C> form <C> 2.66 <C> 1.94 <C> 0.88 <C> 40.7 <R> <C> ur_udtb <C> form <C> 3.95 <C> 3.79 <C> 2.65 <C> 12.3 <R> <C> Average <C> [EMPTY] <C> 4.08 <C> 3.77 <C> 2.45 <C> -210.3 <R> <C> Median <C> [EMPTY] <C> 3.97 <C> 3.40 <C> 2.31 <C> 22.7 <CAP> Table 2: Results of form clustering, measured in % of 1−vmeasure (expressing the error, i.e. lower is better). Baseline (either full form or prefix of form of length 5), our system, and oracle upper bound. Last column is error reduction on the scale from baseline to upper bound, in %. <COT> Looking at the "Average" row, "Our" cell, and "Baseline 4.19" cell, we can see that our system performs better than the baseline on average.
<R> <C> Treebank ar_padt <C> Baseline form <C> Baseline 4.19 <C> Our 3.90 <C> Upp. 2.93 <C> Err.red. 23.1 <R> <C> ca_ancora <C> form <C> 4.65 <C> 4.35 <C> 3.32 <C> 22.3 <R> <C> cs_cac <C> form5 <C> 3.56 <C> 2.25 <C> 1.14 <C> 54.0 <R> <C> cs_fictree <C> form5 <C> 4.82 <C> 4.08 <C> 2.68 <C> 34.6 <R> <C> cs_pdt <C> form5 <C> 4.93 <C> 3.41 <C> 1.65 <C> 46.6 <R> <C> da_ddt <C> form <C> 2.32 <C> 2.16 <C> 1.55 <C> 21.2 <R> <C> en_ewt <C> form <C> 2.29 <C> 2.22 <C> 1.78 <C> 13.8 <R> <C> es_ancora <C> form <C> 3.99 <C> 3.38 <C> 2.25 <C> 34.7 <R> <C> et_edt <C> form5 <C> 4.78 <C> 4.31 <C> 2.54 <C> 20.9 <R> <C> fa_seraji <C> form <C> 8.99 <C> 8.76 <C> 7.44 <C> 14.8 <R> <C> fr_gsd <C> form <C> 4.12 <C> 3.81 <C> 2.70 <C> 22.0 <R> <C> hi_hdtb <C> form <C> 4.18 <C> 3.58 <C> 2.83 <C> 44.3 <R> <C> hr_set <C> form5 <C> 4.04 <C> 2.87 <C> 1.71 <C> 50.2 <R> <C> it_isdt <C> form <C> 4.27 <C> 3.71 <C> 2.78 <C> 37.8 <R> <C> it_postwita <C> form <C> 3.60 <C> 4.07 <C> 2.37 <C> -38.0 <R> <C> ja_gsd <C> form <C> 1.64 <C> 1.93 <C> 1.41 <C> -123.1 <R> <C> ko_kaist <C> form <C> 0.14 <C> 2.41 <C> 0.11 <C> -6392.8 <R> <C> la_ittb <C> form5 <C> 6.53 <C> 6.97 <C> 3.85 <C> -16.4 <R> <C> la_proiel <C> form5 <C> 6.92 <C> 7.42 <C> 4.20 <C> -18.4 <R> <C> lv_lvtb <C> form5 <C> 3.90 <C> 3.39 <C> 2.10 <C> 28.0 <R> <C> no_bokmaal <C> form <C> 2.79 <C> 2.22 <C> 1.48 <C> 43.6 <R> <C> no_nynorsk <C> form <C> 2.73 <C> 2.52 <C> 1.48 <C> 16.7 <R> <C> pl_lfg <C> form5 <C> 3.68 <C> 3.06 <C> 1.84 <C> 33.6 <R> <C> pt_bosque <C> form <C> 3.57 <C> 3.17 <C> 2.55 <C> 39.0 <R> <C> ro_nonstd <C> form5 <C> 8.13 <C> 7.95 <C> 5.64 <C> 7.2 <R> <C> sk_snk <C> form5 <C> 2.87 <C> 2.01 <C> 0.63 <C> 38.2 <R> <C> uk_iu <C> form <C> 2.66 <C> 1.94 <C> 0.88 <C> 40.7 <R> <C> ur_udtb <C> form <C> 3.95 <C> 3.79 <C> 2.65 <C> 12.3 <R> <C> Average <C> [EMPTY] <C> 4.08 <C> 3.77 <C> 2.45 <C> -210.3 <R> <C> Median <C> [EMPTY] <C> 3.97 <C> 3.40 <C> 2.31 <C> 22.7 <CAP> Table 2: Results of form clustering, measured in % of 1−vmeasure (expressing the error, i.e. lower is better). Baseline (either full form or prefix of form of length 5), our system, and oracle upper bound. Last column is error reduction on the scale from baseline to upper bound, in %. <COT> Looking at the "cs_pdt" row, "Upp. 2.93" cell, and "Err.red. 23.1" cell, we can see that the error reduction from the baseline to the upper bound for the cs_pdt treebank is 23.1%.
<R> <C> Distance <C> Average <C> Median <R> <C> [ITALIC] JW <C> 8.17 <C> 7.92 <R> <C> [ITALIC] cos <C> 4.39 <C> 3.87 <R> <C> [ITALIC] JW⋅ [ITALIC] cos <C> 3.77 <C> 3.40 <CAP> Table 3: Comparison of the word form similarities, in % of 1−vmeasure of the clustering. Average and median over the 28 datasets. <COT> Looking at the "CAP" tag, we can see that Table 3 provides a comparison of the word form similarities, in % of 1−vmeasure of the clustering. The average and median values are calculated over 28 datasets.
<R> <C> Distance <C> Average <C> Median <R> <C> [ITALIC] JW <C> 8.17 <C> 7.92 <R> <C> [ITALIC] cos <C> 4.39 <C> 3.87 <R> <C> [ITALIC] JW⋅ [ITALIC] cos <C> 3.77 <C> 3.40 <CAP> Table 3: Comparison of the word form similarities, in % of 1−vmeasure of the clustering. Average and median over the 28 datasets. <COT> Looking at the "Median" column, we can see that the median value for the "JW⋅cos" distance measure is 3.40.
<R> <C> #of Authors <C> 6 <C> 6 <C> 8 <C> 10 <C> 12 <C> 14 <R> <C> samples/author <C> 350 <C> 1100 <C> 931 <C> 849 <C> 562 <C> 469 <R> <C> Char-CNN <C> 83 <C> 96 <C> 92 <C> 86 <C> 75 <C> 69 <R> <C> W2V(CBOW) <C> 65.3 <C> 97 <C> 82.8 <C> 83.3 <C> 76.4 <C> 71.8 <R> <C> fastText(CBOW) <C> 65 <C> 73 <C> 58 <C> 35.7 <C> 37.31 <C> 40.3 <R> <C> W2V(Skip) <C> 79 <C> 94 <C> 91.1 <C> 85.4 <C> [BOLD] 82.2 <C> 78.6 <R> <C> fastText(Skip) <C> [BOLD] 86 <C> [BOLD] 98 <C> [BOLD] 95.2 <C> [BOLD] 86.35 <C> 80.9 <C> [BOLD] 81.2 <CAP> TABLE II: Performance comparison of different models with pre-trained embedding <COT> Looking at the "samples/author" column, we can see that the values decrease as the number of authors increases.
<R> <C> [BOLD] Model <C> [BOLD] Label Accuracy ( [ITALIC] ϕ = 0.76) <C> [BOLD] Label Accuracy ( [ITALIC] ϕ = 0.67) <R> <C> HexaF - UCL <C> 80.18 <C> 80.18 <R> <C> Our Model (BERT) <C> [BOLD] 80.20 <C> [BOLD] 86.7 <CAP> Table 3: Comparison of the Label accuracy on Development set. <COT> Looking at the "Model" column, we can see that the "Our Model (BERT)" has a higher label accuracy than "HexaF - UCL" for both values of ϕ.
<R> <C> [BOLD] Model <C> [BOLD] Label Accuracy ( [ITALIC] ϕ = 0.76) <C> [BOLD] Label Accuracy ( [ITALIC] ϕ = 0.67) <R> <C> HexaF - UCL <C> 80.18 <C> 80.18 <R> <C> Our Model (BERT) <C> [BOLD] 80.20 <C> [BOLD] 86.7 <CAP> Table 3: Comparison of the Label accuracy on Development set. <COT> Comparing the label accuracy values for the two models, we can see that the label accuracy for "Our Model (BERT)" is higher when ϕ = 0.67 compared to when ϕ = 0.76.
<R> <C> [BOLD] Type of FEVER Set <C> [BOLD] Total Claims <C> [BOLD] Claims Converted to Questions <C> [BOLD] Conversion Accuracy <C> [BOLD] Total Questions <C> [BOLD] Questions per claim (Median) <R> <C> Training Set <C> 145449 <C> 131969 <C> 90.73 <C> 395717 <C> 3 <R> <C> Development Set <C> 19998 <C> 17749 <C> 88.75 <C> 54422 <C> 3 <R> <C> Test Set <C> 9999 <C> 8863 <C> 88.63 <C> 27359 <C> 3 <CAP> Table 1: Performance of the question generation system on FEVER Dataset. <COT> Looking at the "Type of FEVER Set" column, we can see that there are three different sets: Training Set, Development Set, and Test Set.
<R> <C> [BOLD] Type of FEVER Set <C> [BOLD] Total Claims <C> [BOLD] Claims Converted to Questions <C> [BOLD] Conversion Accuracy <C> [BOLD] Total Questions <C> [BOLD] Questions per claim (Median) <R> <C> Training Set <C> 145449 <C> 131969 <C> 90.73 <C> 395717 <C> 3 <R> <C> Development Set <C> 19998 <C> 17749 <C> 88.75 <C> 54422 <C> 3 <R> <C> Test Set <C> 9999 <C> 8863 <C> 88.63 <C> 27359 <C> 3 <CAP> Table 1: Performance of the question generation system on FEVER Dataset. <COT> Looking at the "Conversion Accuracy" column, we can see that the Conversion Accuracy for all three sets is above 88%.
<R> <C> [BOLD] Type of Set <C> [BOLD] Label Accuracy ( [ITALIC] ϕ = 0.76) <C> [BOLD] Label Accuracy ( [ITALIC] ϕ = 0.67) <R> <C> Training Set <C> 81.52 <C> 88.05 <R> <C> Development Set <C> 80.20 <C> 86.7 <R> <C> Test Set <C> 80.25 <C> 87.04 <CAP> Table 2: Performance of the question generation system on FEVER Dataset. <COT> Looking at the "Type of Set" column, we can see that the table shows the performance of the question generation system on the training, development, and test sets of the FEVER Dataset.
<R> <C> [BOLD] Type of Set <C> [BOLD] Label Accuracy ( [ITALIC] ϕ = 0.76) <C> [BOLD] Label Accuracy ( [ITALIC] ϕ = 0.67) <R> <C> Training Set <C> 81.52 <C> 88.05 <R> <C> Development Set <C> 80.20 <C> 86.7 <R> <C> Test Set <C> 80.25 <C> 87.04 <CAP> Table 2: Performance of the question generation system on FEVER Dataset. <COT> Looking at the "Label Accuracy (ϕ=0.76)" column, we can see that the label accuracy for the training set is 81.52%.
<R> <C> [EMPTY] <C> Transductive scenario Gap  [ITALIC] F1 <C> Transductive scenario Gap  [ITALIC] FF1 <C> Transductive scenario Gap  [ITALIC] FM1 <C> Transductive scenario Bias  [ITALIC] FF1 [ITALIC] FM1 <C> Dpr <C> Wsc <C> Wnli <C> [EMPTY] <R> <C> SOTA <C> 72.1% <C> 71.4% <C> 72.8% <C> 0.98 <C> 76.4% <C> [BOLD] 72.5%––––––– <C> [BOLD] 74.7%––––––– <C> [EMPTY] <R> <C> Bert <C> 50.0% <C> 47.2% <C> 52.7% <C> 0.90 <C> 59.8% <C> 61.9% <C> 65.8% <C> no train data <R> <C> Bert_WikiRand <C> 55.1% <C> 51.8% <C> 58.2% <C> 0.89 <C> 59.2% <C> 59.3% <C> 65.8% <C> no train data <R> <C> Bert_WikiCREM <C> [BOLD] 59.0% <C> [BOLD] 57.5% <C> [BOLD] 60.5% <C> [BOLD] 0.95 <C> [BOLD] 67.4% <C> [BOLD] 63.4% <C> [BOLD] 67.1% <C> no train data <R> <C> Bert_Gap <C> 75.2% <C> 75.1% <C> 75.3% <C> [BOLD] 1.00––––– <C> 66.8% <C> 63.0% <C> 68.5% <C> existing train data <R> <C> Bert_WikiCREM_Gap <C> [BOLD] 77.4% <C> [BOLD] 78.4% <C> [BOLD] 76.4% <C> 1.03 <C> [BOLD] 71.1% <C> [BOLD] 64.1% <C> [BOLD] 70.5% <C> existing train data <R> <C> Bert_Dpr <C> 60.9% <C> 61.3% <C> 60.6% <C> 1.01 <C> [BOLD] 83.3% <C> 67.0% <C> 71.9% <C> existing train data <R> <C> Bert_Gap_Dpr <C> [BOLD] 70.0% <C> [BOLD] 70.4% <C> [BOLD] 69.5% <C> 1.01 <C> 79.4% <C> 65.6% <C> 72.6% <C> existing train data <R> <C> Bert_WikiCREM_Dpr <C> 64.2% <C> 64.2% <C> 64.1% <C> [BOLD] 1.00––––– <C> 80.0% <C> [BOLD] 71.8% <C> [BOLD] 74.7%––––––– <C> existing train data <R> <C> Bert_all <C> 76.0% <C> 77.4% <C> 74.7% <C> [BOLD] 1.04 <C> 80.1% <C> [BOLD] 70.0% <C> 74.0% <C> existing train data <R> <C> Bert_WikiCREM_all <C> [BOLD] 78.0%––––––– <C> [BOLD] 79.4%––––––– <C> [BOLD] 76.7%––––––– <C> [BOLD] 1.04 <C> [BOLD] 84.8%––––––– <C> [BOLD] 70.0% <C> [BOLD] 74.7%––––––– <C> existing train data <CAP> Table 1: Evaluation of trained models on all test sets. Gap and WinoBias (abbreviated WB) are additionally split into subsets, as introduced in Section 5. Double lines in the table separate results from three different scenarios: when no training data is available, when additional training data exists, and the transductive scenario. The table is further split into sections separated with single horizontal lines. Each section contains a model that has been trained on WikiCREM and models that have not been. The best result in each section is in bold. The best overall result is underlined. Scores on Gap are measured as F1-scores, while the performance on other datasets is given in accuracy. The source of each SOTA is listed in Section 5. <COT> Looking at the "Bert_WikiCREM" row, the "[BOLD] 60.5%" cell, and the "Transductive scenario Gap [ITALIC] FM1" cell, we can see that the F1 score for the transductive scenario Gap subset is higher for the Bert_WikiCREM model compared to the Bert model.
<R> <C> [EMPTY] <C> Transductive scenario Gap  [ITALIC] F1 <C> Transductive scenario Gap  [ITALIC] FF1 <C> Transductive scenario Gap  [ITALIC] FM1 <C> Transductive scenario Bias  [ITALIC] FF1 [ITALIC] FM1 <C> Dpr <C> Wsc <C> Wnli <C> [EMPTY] <R> <C> SOTA <C> 72.1% <C> 71.4% <C> 72.8% <C> 0.98 <C> 76.4% <C> [BOLD] 72.5%––––––– <C> [BOLD] 74.7%––––––– <C> [EMPTY] <R> <C> Bert <C> 50.0% <C> 47.2% <C> 52.7% <C> 0.90 <C> 59.8% <C> 61.9% <C> 65.8% <C> no train data <R> <C> Bert_WikiRand <C> 55.1% <C> 51.8% <C> 58.2% <C> 0.89 <C> 59.2% <C> 59.3% <C> 65.8% <C> no train data <R> <C> Bert_WikiCREM <C> [BOLD] 59.0% <C> [BOLD] 57.5% <C> [BOLD] 60.5% <C> [BOLD] 0.95 <C> [BOLD] 67.4% <C> [BOLD] 63.4% <C> [BOLD] 67.1% <C> no train data <R> <C> Bert_Gap <C> 75.2% <C> 75.1% <C> 75.3% <C> [BOLD] 1.00––––– <C> 66.8% <C> 63.0% <C> 68.5% <C> existing train data <R> <C> Bert_WikiCREM_Gap <C> [BOLD] 77.4% <C> [BOLD] 78.4% <C> [BOLD] 76.4% <C> 1.03 <C> [BOLD] 71.1% <C> [BOLD] 64.1% <C> [BOLD] 70.5% <C> existing train data <R> <C> Bert_Dpr <C> 60.9% <C> 61.3% <C> 60.6% <C> 1.01 <C> [BOLD] 83.3% <C> 67.0% <C> 71.9% <C> existing train data <R> <C> Bert_Gap_Dpr <C> [BOLD] 70.0% <C> [BOLD] 70.4% <C> [BOLD] 69.5% <C> 1.01 <C> 79.4% <C> 65.6% <C> 72.6% <C> existing train data <R> <C> Bert_WikiCREM_Dpr <C> 64.2% <C> 64.2% <C> 64.1% <C> [BOLD] 1.00––––– <C> 80.0% <C> [BOLD] 71.8% <C> [BOLD] 74.7%––––––– <C> existing train data <R> <C> Bert_all <C> 76.0% <C> 77.4% <C> 74.7% <C> [BOLD] 1.04 <C> 80.1% <C> [BOLD] 70.0% <C> 74.0% <C> existing train data <R> <C> Bert_WikiCREM_all <C> [BOLD] 78.0%––––––– <C> [BOLD] 79.4%––––––– <C> [BOLD] 76.7%––––––– <C> [BOLD] 1.04 <C> [BOLD] 84.8%––––––– <C> [BOLD] 70.0% <C> [BOLD] 74.7%––––––– <C> existing train data <CAP> Table 1: Evaluation of trained models on all test sets. Gap and WinoBias (abbreviated WB) are additionally split into subsets, as introduced in Section 5. Double lines in the table separate results from three different scenarios: when no training data is available, when additional training data exists, and the transductive scenario. The table is further split into sections separated with single horizontal lines. Each section contains a model that has been trained on WikiCREM and models that have not been. The best result in each section is in bold. The best overall result is underlined. Scores on Gap are measured as F1-scores, while the performance on other datasets is given in accuracy. The source of each SOTA is listed in Section 5. <COT> Looking at the "Bert_Gap_Dpr" row, the "[BOLD] 70.4%" cell, and the "Transductive scenario Gap [ITALIC] FF1" cell, we can see that the FF1 score for the transductive scenario Gap subset is higher for the Bert_Gap_Dpr model compared to the Bert model.
<R> <C> #of Authors <C> 6 <C> 6 <C> 8 <C> 10 <C> 12 <C> 14 <R> <C> #of samples/class <C> 350 <C> 1100 <C> 931 <C> 849 <C> 562 <C> 469 <R> <C> Pretrained Embedding <C> 83 <C> 96 <C> 92 <C> 86 <C> 75 <C> 69 <R> <C> Not pretrained <C> 71 <C> 95 <C> 82 <C> 83 <C> 66 <C> 59.5 <CAP> TABLE III: pretrained vs non-pretrained comparison <COT> Looking at the "TABLE III" caption, we can see that the table compares the performance of pretrained and non-pretrained models based on the number of authors and the number of samples per class.
<R> <C> Model <C> bn <C> jv <C> lo <C> mr <C> pa <C> te <C> ti <C> uk <C> ug <C> Avg. <R> <C> [BOLD] Exact <C> .00 <C> .63 <C> .02 <C> .00 <C> .00 <C> .00 <C> .02 <C> .02 <C> .03 <C> .08 <R> <C> [BOLD] Trans <C> .00 <C> .63 <C> .02 <C> .17 <C> .00 <C> .00 <C> .46 <C> .02 <C> .03 <C> .15 <R> <C> [BOLD] Encode <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Manual <C> .36 (hi) <C> .70 (id) <C> .07 (th) <C> .46 (hi) <C> .31 (hi) <C> .20 (ta) <C> .44 (am) <C> .25 (ru) <C> .16 (tr) <C> .33 <R> <C> Best-53 <C> .38 (ms) <C> .70 (id) <C> .07 (th) <C> .46 (hi) <C> .36 (te) <C> .36 (pa) <C> .44 (am) <C> .41 (kk) <C> .16 (tr) <C> .37 <R> <C> [BOLD] Pbel <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Manual <C> .48 (hi) <C> .86 (id) <C> [BOLD] .28 (th) <C> [BOLD] .62 (hi) <C> [BOLD] .49 (hi) <C> .33 (ta) <C> [BOLD] .69 (am) <C> .50 (ru) <C> .32 (tr) <C> .51 <R> <C> Best-53 <C> .48 (hi) <C> .86 (id) <C> [BOLD] .28 (th) <C> [BOLD] .62 (hi) <C> [BOLD] .49 (hi) <C> [BOLD] .47 (hi) <C> [BOLD] .69 (am) <C> .54 (kk) <C> .32 (tr) <C> .53 <R> <C> Multi <C> [BOLD] .53 <C> [BOLD] .87 <C> [BOLD] .28 <C> [BOLD] .62 <C> .48 <C> .46 <C> [BOLD] .69 <C> [BOLD] .56 <C> [BOLD] .40 <C> [BOLD] .54 <CAP> Table 1: Accuracy for cross-lingual Wikipedia title linking, with the transfer HRL shown in parentheses. The best accuracy among input representations with graphemes, phonemes or articulatory features for Encode and PBEL is presented here. Complete results for each representation are in the supplementary material. <COT> Looking at the "Model" column, we can see that there are different models mentioned such as "Exact", "Trans", "Encode", "Manual", "Best-53", and "Multi".
<R> <C> Model <C> bn <C> jv <C> lo <C> mr <C> pa <C> te <C> ti <C> uk <C> ug <C> Avg. <R> <C> [BOLD] Exact <C> .00 <C> .63 <C> .02 <C> .00 <C> .00 <C> .00 <C> .02 <C> .02 <C> .03 <C> .08 <R> <C> [BOLD] Trans <C> .00 <C> .63 <C> .02 <C> .17 <C> .00 <C> .00 <C> .46 <C> .02 <C> .03 <C> .15 <R> <C> [BOLD] Encode <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Manual <C> .36 (hi) <C> .70 (id) <C> .07 (th) <C> .46 (hi) <C> .31 (hi) <C> .20 (ta) <C> .44 (am) <C> .25 (ru) <C> .16 (tr) <C> .33 <R> <C> Best-53 <C> .38 (ms) <C> .70 (id) <C> .07 (th) <C> .46 (hi) <C> .36 (te) <C> .36 (pa) <C> .44 (am) <C> .41 (kk) <C> .16 (tr) <C> .37 <R> <C> [BOLD] Pbel <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Manual <C> .48 (hi) <C> .86 (id) <C> [BOLD] .28 (th) <C> [BOLD] .62 (hi) <C> [BOLD] .49 (hi) <C> .33 (ta) <C> [BOLD] .69 (am) <C> .50 (ru) <C> .32 (tr) <C> .51 <R> <C> Best-53 <C> .48 (hi) <C> .86 (id) <C> [BOLD] .28 (th) <C> [BOLD] .62 (hi) <C> [BOLD] .49 (hi) <C> [BOLD] .47 (hi) <C> [BOLD] .69 (am) <C> .54 (kk) <C> .32 (tr) <C> .53 <R> <C> Multi <C> [BOLD] .53 <C> [BOLD] .87 <C> [BOLD] .28 <C> [BOLD] .62 <C> .48 <C> .46 <C> [BOLD] .69 <C> [BOLD] .56 <C> [BOLD] .40 <C> [BOLD] .54 <CAP> Table 1: Accuracy for cross-lingual Wikipedia title linking, with the transfer HRL shown in parentheses. The best accuracy among input representations with graphemes, phonemes or articulatory features for Encode and PBEL is presented here. Complete results for each representation are in the supplementary material. <COT> Looking at the "Encode" row, we can see that all cells in this row are empty.
<R> <C> Lang. <C> Tigrinya <C> Oromo <R> <C> Exact <C> 0.00 <C> 0.01 <R> <C> Trans Supervised <C> 0.21 <C> 0.05 <R> <C> Trans Unsupervised <C> 0.21 <C> 0.01 <R> <C> Encode <C> 0.16 <C> 0.10 <R> <C> Pbel <C> [BOLD] 0.33 <C> [BOLD] 0.11 <CAP> Table 2: Entity linking accuracy on non-Wikipedia data <COT> Looking at the "Lang." column, we can see that the table compares the entity linking accuracy for two languages, Tigrinya and Oromo.
<R> <C> Lang. <C> Tigrinya <C> Oromo <R> <C> Exact <C> 0.00 <C> 0.01 <R> <C> Trans Supervised <C> 0.21 <C> 0.05 <R> <C> Trans Unsupervised <C> 0.21 <C> 0.01 <R> <C> Encode <C> 0.16 <C> 0.10 <R> <C> Pbel <C> [BOLD] 0.33 <C> [BOLD] 0.11 <CAP> Table 2: Entity linking accuracy on non-Wikipedia data <COT> Looking at the "Pbel" cell, we can see that the entity linking accuracy for Tigrinya is higher than for Oromo.
<R> <C> Input <C> *bn (hi) <C> jv (id) <C> *lo (th) <C> mr (hi) <C> *pa (hi) <C> *te (ta) <C> ti (am) <C> *uk (ru) <C> *ug (tr) <R> <C> Grapheme <C> .00 <C> [BOLD] .86 <C> .02 <C> [BOLD] .62 <C> .00 <C> .00 <C> .61 <C> [BOLD] .50 <C> .08 <R> <C> Phoneme <C> [BOLD] .48 <C> .84 <C> .20 <C> .58 <C> .18 <C> .10 <C> [BOLD] .69 <C> .23 <C> .21 <R> <C> Articulatory <C> .45 <C> .82 <C> [BOLD] .28 <C> .56 <C> [BOLD] .49 <C> [BOLD] .33 <C> .63 <C> .42 <C> [BOLD] .32 <CAP> Table 3: Entity linking accuracy with PBEL, using Graphemes, Phonemes or Articulatory features as input. The HRL used for training and pivoting is shown in parentheses in the first row. The pairs with the different scripts are marked with a “*”. <COT> Looking at the "CAP" tag, we can see that Table 3 shows the entity linking accuracy with PBEL using different input features.
<R> <C> Input <C> *bn (hi) <C> jv (id) <C> *lo (th) <C> mr (hi) <C> *pa (hi) <C> *te (ta) <C> ti (am) <C> *uk (ru) <C> *ug (tr) <R> <C> Grapheme <C> .00 <C> [BOLD] .86 <C> .02 <C> [BOLD] .62 <C> .00 <C> .00 <C> .61 <C> [BOLD] .50 <C> .08 <R> <C> Phoneme <C> [BOLD] .48 <C> .84 <C> .20 <C> .58 <C> .18 <C> .10 <C> [BOLD] .69 <C> .23 <C> .21 <R> <C> Articulatory <C> .45 <C> .82 <C> [BOLD] .28 <C> .56 <C> [BOLD] .49 <C> [BOLD] .33 <C> .63 <C> .42 <C> [BOLD] .32 <CAP> Table 3: Entity linking accuracy with PBEL, using Graphemes, Phonemes or Articulatory features as input. The HRL used for training and pivoting is shown in parentheses in the first row. The pairs with the different scripts are marked with a “*”. <COT> Looking at the "Phoneme" row and the "[BOLD] .69" cell, we can see that Phonemes have the highest accuracy among the three input features.
<R> <C> [EMPTY] <C> Textbooks <C> Wikipedia <R> <C> IR <C> 25.24 <C> 25.14 <R> <C> PMI <C> 26.22 <C> 25.19 <R> <C> ESIM w/o scenario <C> 25.85 <C> 25.41 <R> <C> ESIM w/ scenario <C> 24.34 <C> 24.41 <R> <C> DIIN w/o scenario <C> 24.15 <C> 25.20 <R> <C> DIIN w/ scenario <C> 25.11 <C> 24.89 <R> <C> BERT [ITALIC] NLI w/o scenario <C> 24.29 <C> 24.17 <R> <C> BERT [ITALIC] NLI w/ scenario <C> 24.97 <C> 24.68 <R> <C> BiMPM w/o scenario <C> 24.13 <C> 24.51 <R> <C> BiMPM w/ scenario <C> 24.76 <C> 23.81 <R> <C> BERT [ITALIC] RC w/o scenario <C> 24.81 <C> 24.78 <R> <C> BERT [ITALIC] RC w/ scenario <C> 23.66 <C> 23.01 <CAP> Table 1: Proportions of correctly answered questions. <COT> Looking at the "PMI" row, "25.19" cell, and "Wikipedia" cell, we can see that the performance of PMI is higher on Wikipedia compared to textbooks.
<R> <C> [EMPTY] <C> Textbooks <C> Wikipedia <R> <C> IR <C> 25.24 <C> 25.14 <R> <C> PMI <C> 26.22 <C> 25.19 <R> <C> ESIM w/o scenario <C> 25.85 <C> 25.41 <R> <C> ESIM w/ scenario <C> 24.34 <C> 24.41 <R> <C> DIIN w/o scenario <C> 24.15 <C> 25.20 <R> <C> DIIN w/ scenario <C> 25.11 <C> 24.89 <R> <C> BERT [ITALIC] NLI w/o scenario <C> 24.29 <C> 24.17 <R> <C> BERT [ITALIC] NLI w/ scenario <C> 24.97 <C> 24.68 <R> <C> BiMPM w/o scenario <C> 24.13 <C> 24.51 <R> <C> BiMPM w/ scenario <C> 24.76 <C> 23.81 <R> <C> BERT [ITALIC] RC w/o scenario <C> 24.81 <C> 24.78 <R> <C> BERT [ITALIC] RC w/ scenario <C> 23.66 <C> 23.01 <CAP> Table 1: Proportions of correctly answered questions. <COT> Looking at the "DIIN w/ scenario" row, "24.89" cell, and "BERT NLI w/ scenario" cell, we can see that the performance of DIIN with scenario is higher compared to BERT NLI with scenario.
<R> <C> Embedding <C> lang. <C> r@1 <C> r@5 <C> r@10 <R> <C> vse  <C> en <C> 64.6 <C> ∅ <C> 95.7 <R> <C> dsve  <C> en <C> 69.8 <C> 91.9 <C> 96.6 <R> <C> dsve w/ w2v <C> en <C> 63.48 <C> 89.48 <C> 95.64 <R> <C> dsve w/ FastText <C> en <C> 66.08 <C> 90.7 <C> 96.2 <R> <C> Ours w/ bv <C> en <C> 65.58 <C> 90.52 <C> 96.1 <R> <C> [EMPTY] <C> en+fr <C> 67.78 <C> 91.58 <C> 96.92 <R> <C> Ours w/ muse <C> en <C> 63.1 <C> 89.58 <C> 95.56 <R> <C> [EMPTY] <C> en+fr <C> 63.88 <C> 89.2 <C> 95.24 <R> <C> [EMPTY] <C> en+fr+de <C> 62.4 <C> 89.18 <C> 95.16 <R> <C> [EMPTY] <C> all <C> 63.28 <C> 88.3 <C> 94.6 <CAP> Table 1: Experiment 1: Caption retrieval on the coco dataset. We compare the different reminders of the different methods first in English and then by adding new languages. We also evaluate variations of dsve method with different word embedding. <COT> Looking at the "Embedding" column, we can see that the "dsve" method with "w2v" word embedding has the lowest "r@1" score.
<R> <C> Embedding <C> lang. <C> r@1 <C> r@5 <C> r@10 <R> <C> vse  <C> en <C> 64.6 <C> ∅ <C> 95.7 <R> <C> dsve  <C> en <C> 69.8 <C> 91.9 <C> 96.6 <R> <C> dsve w/ w2v <C> en <C> 63.48 <C> 89.48 <C> 95.64 <R> <C> dsve w/ FastText <C> en <C> 66.08 <C> 90.7 <C> 96.2 <R> <C> Ours w/ bv <C> en <C> 65.58 <C> 90.52 <C> 96.1 <R> <C> [EMPTY] <C> en+fr <C> 67.78 <C> 91.58 <C> 96.92 <R> <C> Ours w/ muse <C> en <C> 63.1 <C> 89.58 <C> 95.56 <R> <C> [EMPTY] <C> en+fr <C> 63.88 <C> 89.2 <C> 95.24 <R> <C> [EMPTY] <C> en+fr+de <C> 62.4 <C> 89.18 <C> 95.16 <R> <C> [EMPTY] <C> all <C> 63.28 <C> 88.3 <C> 94.6 <CAP> Table 1: Experiment 1: Caption retrieval on the coco dataset. We compare the different reminders of the different methods first in English and then by adding new languages. We also evaluate variations of dsve method with different word embedding. <COT> Looking at the "Embedding" column and the "en+fr" and "en+fr+de" rows, we can see that adding more languages to the "Ours w/ muse" method decreases the "r@10" score.
<R> <C> Embedding <C> lang. <C> r@1 <C> r@5 <C> r@10 <R> <C> vse  <C> en <C> 52.00 <C> ∅ <C> 92.0 <R> <C> dsve  <C> en <C> 55.9 <C> 86.9 <C> 94.0 <R> <C> dsve w/ w2v <C> en <C> 51.87 <C> 84.31 <C> 92.48 <R> <C> dsve w/ FastText <C> en <C> 54.12 <C> 85.74 <C> 92.93 <R> <C> Ours w/ bv <C> en <C> 55.57 <C> 86.92 <C> 93.86 <R> <C> [EMPTY] <C> en+fr <C> 56.09 <C> 87.22 <C> 94.03 <R> <C> Ours w/ muse <C> en <C> 51.81 <C> 84.70 <C> 92.82 <R> <C> [EMPTY] <C> en+fr <C> 52.25 <C> 84.72 <C> 92.74 <R> <C> [EMPTY] <C> en+fr+de <C> 51.17 <C> 84.09 <C> 92.22 <R> <C> [EMPTY] <C> all <C> 50.44 <C> 83.39 <C> 91.80 <CAP> Table 2: Experiment 2: Image retrieval on the coco dataset. The methods are the same as in table 1. <COT> Looking at the "Embedding" column, we can see different types of embeddings used in the experiments.
<R> <C> train. lang. <C> en <C> fr <C> de <C> cs <C> all <R> <C> en <C> 56.60 <C> 46.05 <C> 44.18 <C> 38.75 <C> 46.40 <R> <C> en+fr <C> 50.93 <C> 43.69 <C> 41.61 <C> 34.02 <C> 42.43 <R> <C> en+fr+de <C> 54.63 <C> 46.94 <C> 45.07 <C> 38.26 <C> 46.22 <R> <C> all <C> 55.32 <C> 49.30 <C> 46.84 <C> 46.06 <C> 49.38 <CAP> Table 3: Image Recall@10 on the Multi30k dataset with different languages with muse. <COT> Looking at the "en" column, the recall@10 decreases as more languages are added to the training set.
<R> <C> train. lang. <C> en <C> fr <C> de <C> en+fr <C> en+de <R> <C> en <C> 53.35 <C> 26.13 <C> 22.96 <C> 39.74 <C> 34.57 <R> <C> en+fr <C> 59.76 <C> 55.22 <C> ∅ <C> 57.50 <C> ∅ <R> <C> en+de <C> 61.44 <C> ∅ <C> 43.59 <C> ∅ <C> 52.51 <CAP> Table 4: Image Recall@10 on Multi30k dataset with different languages with bv Embeddings. <COT> Looking at the "train. lang." column, we can see that the highest recall@10 score is achieved when training with the "en+de" language combination.
<R> <C> train. lang. <C> en <C> fr <C> de <C> en+fr <C> en+de <R> <C> en <C> 53.35 <C> 26.13 <C> 22.96 <C> 39.74 <C> 34.57 <R> <C> en+fr <C> 59.76 <C> 55.22 <C> ∅ <C> 57.50 <C> ∅ <R> <C> en+de <C> 61.44 <C> ∅ <C> 43.59 <C> ∅ <C> 52.51 <CAP> Table 4: Image Recall@10 on Multi30k dataset with different languages with bv Embeddings. <COT> Looking at the "en+fr" row, we can see that there are missing values (∅) in the "fr" column and the "en+de" column.
<R> <C> [BOLD] Resource <C> [ITALIC] Dev  [ITALIC] EM <C> [ITALIC] Dev  [ITALIC] F1 <C> [ITALIC] Test  [ITALIC] EM <C> [ITALIC] Test  [ITALIC] F1 <R> <C> DBiDAF <C> 63.0 <C> 76.9 <C> 62.6 <C> 78.5 <R> <C> DBERT <C> 59.2 <C> 74.3 <C> 63.9 <C> 76.9 <R> <C> DRoBERTa <C> 58.1 <C> 72.0 <C> 58.7 <C> 73.7 <CAP> Table 1: Non-expert human performance results for a randomly-selected validator per question. <COT> Comparing the Dev EM scores of DBiDAF, DBERT, and DRoBERTa, we can see that DBERT has the highest EM score.
<R> <C> [BOLD] Method <C> [BOLD] F1 R-1 <C> [BOLD] F1 R-2 <C> [BOLD] F1 R-L <C> [BOLD] Recall R-1 <C> [BOLD] Recall R-2 <C> [BOLD] Recall R-L <C> [BOLD] Precision R-1 <C> [BOLD] Precision R-2 <C> [BOLD] Precision R-L <R> <C> PG <C> 36.82 <C> 15.92 <C> 33.57 <C> 37.36 <C> 16.10 <C> 34.05 <C> 38.72 <C> 16.86 <C> 35.32 <R> <C> +M1-latent <C> 37.76 <C> 16.51 <C> 34.48 <C> [BOLD] 40.15 <C> [BOLD] 17.52 <C> 36.65 <C> 37.90 <C> 16.64 <C> 34.61 <R> <C> +M1-shallow <C> 37.45 <C> 16.23 <C> 34.22 <C> [BOLD] 40.15 <C> 17.38 <C> [BOLD] 36.68 <C> 37.34 <C> 16.24 <C> 34.13 <R> <C> +M2-latent <C> [BOLD] 38.04 <C> [BOLD] 16.73 <C> [BOLD] 34.83 <C> 38.92 <C> 17.05 <C> 35.62 <C> [BOLD] 39.54 <C> [BOLD] 17.51 <C> [BOLD] 36.23 <R> <C> +M2-shallow <C> 37.15 <C> 16.13 <C> 33.96 <C> 38.52 <C> 16.68 <C> 35.21 <C> 38.19 <C> 16.67 <C> 34.91 <R> <C> +M3-latent <C> 37.04 <C> 16.05 <C> 33.86 <C> 37.52 <C> 16.22 <C> 34.29 <C> 38.95 <C> 16.98 <C> 35.63 <R> <C> +M3-shallow <C> 37.09 <C> 16.15 <C> 33.95 <C> 39.05 <C> 16.97 <C> 35.73 <C> 37.62 <C> 16.46 <C> 34.45 <R> <C> PG+Cov <C> 39.32 <C> 17.22 <C> 36.02 <C> 40.33 <C> 17.61 <C> 36.93 <C> [BOLD] 40.82 <C> [BOLD] 17.99 <C> [BOLD] 37.42 <R> <C> +M1-latent <C> [BOLD] 40.06 <C> [BOLD] 17.63 <C> 36.70 <C> [BOLD] 44.44 <C> [BOLD] 19.53 <C> [BOLD] 40.69 <C> 38.60 <C> 17.05 <C> 35.39 <R> <C> +M1-shallow <C> 39.78 <C> 17.50 <C> 36.50 <C> 43.50 <C> 19.08 <C> 39.89 <C> 38.94 <C> 17.22 <C> 35.75 <R> <C> +M2-latent <C> 40.00 <C> 17.62 <C> [BOLD] 36.72 <C> 43.53 <C> 19.17 <C> 39.94 <C> 39.28 <C> 17.37 <C> 36.09 <R> <C> +M2-shallow <C> 39.58 <C> 17.30 <C> 36.36 <C> 44.00 <C> 19.19 <C> 40.38 <C> 38.40 <C> 16.87 <C> 35.31 <R> <C> +M3-latent <C> 39.23 <C> 17.00 <C> 36.00 <C> 42.95 <C> 18.54 <C> 39.37 <C> 38.29 <C> 16.69 <C> 35.16 <R> <C> +M3-shallow <C> 39.57 <C> 17.31 <C> 36.28 <C> 43.85 <C> 19.14 <C> 40.17 <C> 38.37 <C> 168.6 <C> 35.20 <CAP> Table 1: Abstractive summarization results. <COT> Looking at the "Method" column, we can see that there are different methods listed for abstractive summarization.
<R> <C> [BOLD] Method <C> [BOLD] F1 R-1 <C> [BOLD] F1 R-2 <C> [BOLD] F1 R-L <C> [BOLD] Recall R-1 <C> [BOLD] Recall R-2 <C> [BOLD] Recall R-L <C> [BOLD] Precision R-1 <C> [BOLD] Precision R-2 <C> [BOLD] Precision R-L <R> <C> PG <C> 36.82 <C> 15.92 <C> 33.57 <C> 37.36 <C> 16.10 <C> 34.05 <C> 38.72 <C> 16.86 <C> 35.32 <R> <C> +M1-latent <C> 37.76 <C> 16.51 <C> 34.48 <C> [BOLD] 40.15 <C> [BOLD] 17.52 <C> 36.65 <C> 37.90 <C> 16.64 <C> 34.61 <R> <C> +M1-shallow <C> 37.45 <C> 16.23 <C> 34.22 <C> [BOLD] 40.15 <C> 17.38 <C> [BOLD] 36.68 <C> 37.34 <C> 16.24 <C> 34.13 <R> <C> +M2-latent <C> [BOLD] 38.04 <C> [BOLD] 16.73 <C> [BOLD] 34.83 <C> 38.92 <C> 17.05 <C> 35.62 <C> [BOLD] 39.54 <C> [BOLD] 17.51 <C> [BOLD] 36.23 <R> <C> +M2-shallow <C> 37.15 <C> 16.13 <C> 33.96 <C> 38.52 <C> 16.68 <C> 35.21 <C> 38.19 <C> 16.67 <C> 34.91 <R> <C> +M3-latent <C> 37.04 <C> 16.05 <C> 33.86 <C> 37.52 <C> 16.22 <C> 34.29 <C> 38.95 <C> 16.98 <C> 35.63 <R> <C> +M3-shallow <C> 37.09 <C> 16.15 <C> 33.95 <C> 39.05 <C> 16.97 <C> 35.73 <C> 37.62 <C> 16.46 <C> 34.45 <R> <C> PG+Cov <C> 39.32 <C> 17.22 <C> 36.02 <C> 40.33 <C> 17.61 <C> 36.93 <C> [BOLD] 40.82 <C> [BOLD] 17.99 <C> [BOLD] 37.42 <R> <C> +M1-latent <C> [BOLD] 40.06 <C> [BOLD] 17.63 <C> 36.70 <C> [BOLD] 44.44 <C> [BOLD] 19.53 <C> [BOLD] 40.69 <C> 38.60 <C> 17.05 <C> 35.39 <R> <C> +M1-shallow <C> 39.78 <C> 17.50 <C> 36.50 <C> 43.50 <C> 19.08 <C> 39.89 <C> 38.94 <C> 17.22 <C> 35.75 <R> <C> +M2-latent <C> 40.00 <C> 17.62 <C> [BOLD] 36.72 <C> 43.53 <C> 19.17 <C> 39.94 <C> 39.28 <C> 17.37 <C> 36.09 <R> <C> +M2-shallow <C> 39.58 <C> 17.30 <C> 36.36 <C> 44.00 <C> 19.19 <C> 40.38 <C> 38.40 <C> 16.87 <C> 35.31 <R> <C> +M3-latent <C> 39.23 <C> 17.00 <C> 36.00 <C> 42.95 <C> 18.54 <C> 39.37 <C> 38.29 <C> 16.69 <C> 35.16 <R> <C> +M3-shallow <C> 39.57 <C> 17.31 <C> 36.28 <C> 43.85 <C> 19.14 <C> 40.17 <C> 38.37 <C> 168.6 <C> 35.20 <CAP> Table 1: Abstractive summarization results. <COT> Looking at the "Recall R-1" column, we can compare the recall scores for different methods.
<R> <C> [BOLD] Model <C> [BOLD] MAE <C> [BOLD] MAPE <R> <C> CNN w/ GloVe <C> 1.16 <C> 14.38 <R> <C> + M1-latent <C> 1.15 <C> 14.66 <R> <C> + M1-shallow <C> [BOLD] 1.12(1) 14.19 <C> [EMPTY] <R> <C> Bi-LSTM w/ GloVe <C> 1.14 <C> 14.57 <R> <C> + M1-latent <C> 1.13 <C> 14.39 <R> <C> + M1-shallow <C> 1.13 <C> 14.25 <R> <C> + M2-latent <C> 1.12 <C> 14.02 <R> <C> + M2-shallow <C> 1.13 <C> 14.20 <R> <C> Bi-LSTM w/ latent <C> [BOLD] 1.11(2) 13.91 <C> [EMPTY] <R> <C> Bi-LSTM w/ shallow <C> 1.15 <C> 14.67 <CAP> Table 2: Average petition regression performance over 3 runs (noting that lower is better for both MAE and MAPE). One-sided t-tests show that both (1) and (2) are significantly better than the baseline (p<0.05 and p<0.005, resp.). <COT> Looking at the "Model" column, we can see that the models are divided into three categories: CNN w/ GloVe, Bi-LSTM w/ GloVe, and Bi-LSTM w/ latent. The models in each category have different variations (+ M1-latent, + M1-shallow, + M2-latent, + M2-shallow). 
<R> <C> [BOLD] Model <C> [BOLD] MAE <C> [BOLD] MAPE <R> <C> CNN w/ GloVe <C> 1.16 <C> 14.38 <R> <C> + M1-latent <C> 1.15 <C> 14.66 <R> <C> + M1-shallow <C> [BOLD] 1.12(1) 14.19 <C> [EMPTY] <R> <C> Bi-LSTM w/ GloVe <C> 1.14 <C> 14.57 <R> <C> + M1-latent <C> 1.13 <C> 14.39 <R> <C> + M1-shallow <C> 1.13 <C> 14.25 <R> <C> + M2-latent <C> 1.12 <C> 14.02 <R> <C> + M2-shallow <C> 1.13 <C> 14.20 <R> <C> Bi-LSTM w/ latent <C> [BOLD] 1.11(2) 13.91 <C> [EMPTY] <R> <C> Bi-LSTM w/ shallow <C> 1.15 <C> 14.67 <CAP> Table 2: Average petition regression performance over 3 runs (noting that lower is better for both MAE and MAPE). One-sided t-tests show that both (1) and (2) are significantly better than the baseline (p<0.05 and p<0.005, resp.). <COT> Looking at the "MAE" column, we can compare the MAE values across different models and variations. By comparing the values within each model category, we can determine if a variation performs better or worse than the baseline.
<R> <C> Model <C> [ITALIC] K <C> IMDb <C> AG’s <C> SNLI <R> <C> Model <C> [ITALIC] K <C> IMDb <C> News <C> SNLI <R> <C> BERTSDV <C> 1 <C> 5.39 <C> [BOLD] 5.38 <C> [BOLD] 91.2 <R> <C> BERTSDV <C> 2 <C> 5.44 <C> 5.39 <C> 91.1 <R> <C> BERTSDV <C> 3 <C> 5.40 <C> 5.50 <C> [BOLD] 91.2 <R> <C> BERTSDV <C> 4 <C> 5.47 <C> 5.49 <C> [BOLD] 91.2 <R> <C> [EMPTY] <C> 5 <C> [BOLD] 5.35 <C> 5.55 <C> 91.1 <R> <C> BERTSDA <C> [ITALIC] T−1 <C> 5.41 <C> [BOLD] 5.29 <C> 91.0 <R> <C> BERTSDA <C> 2 <C> 5.46 <C> 5.49 <C> [BOLD] 91.2 <R> <C> BERTSDA <C> 3 <C> 5.48 <C> 5.55 <C> 91.1 <R> <C> BERTSDA <C> 4 <C> 5.44 <C> 5.52 <C> 91.1 <R> <C> [EMPTY] <C> 5 <C> [BOLD] 5.29 <C> 5.41 <C> 91.1 <CAP> Table 2: Results on IMDb dataset over different teacher sizes. BERTSDV(K=1) is same as BERTSDA(K=1). For IMDb and AG’s News, we report test error rate (%). For SNLI, we report accuracy (%). T denotes the total number of iterations. <COT> Looking at the "Model" column and the "IMDb" column, we can see that BERTSDA with teacher size T-1 has a lower test error rate on IMDb compared to BERTSDV with teacher size 1.
<R> <C> Model <C> [ITALIC] K <C> IMDb <C> AG’s <C> SNLI <R> <C> Model <C> [ITALIC] K <C> IMDb <C> News <C> SNLI <R> <C> BERTSDV <C> 1 <C> 5.39 <C> [BOLD] 5.38 <C> [BOLD] 91.2 <R> <C> BERTSDV <C> 2 <C> 5.44 <C> 5.39 <C> 91.1 <R> <C> BERTSDV <C> 3 <C> 5.40 <C> 5.50 <C> [BOLD] 91.2 <R> <C> BERTSDV <C> 4 <C> 5.47 <C> 5.49 <C> [BOLD] 91.2 <R> <C> [EMPTY] <C> 5 <C> [BOLD] 5.35 <C> 5.55 <C> 91.1 <R> <C> BERTSDA <C> [ITALIC] T−1 <C> 5.41 <C> [BOLD] 5.29 <C> 91.0 <R> <C> BERTSDA <C> 2 <C> 5.46 <C> 5.49 <C> [BOLD] 91.2 <R> <C> BERTSDA <C> 3 <C> 5.48 <C> 5.55 <C> 91.1 <R> <C> BERTSDA <C> 4 <C> 5.44 <C> 5.52 <C> 91.1 <R> <C> [EMPTY] <C> 5 <C> [BOLD] 5.29 <C> 5.41 <C> 91.1 <CAP> Table 2: Results on IMDb dataset over different teacher sizes. BERTSDV(K=1) is same as BERTSDA(K=1). For IMDb and AG’s News, we report test error rate (%). For SNLI, we report accuracy (%). T denotes the total number of iterations. <COT> Looking at the "Model" column and the "SNLI" column, we can see that BERTSDV with teacher size 4 has a higher accuracy on SNLI compared to BERTSDA with teacher size 3.
<R> <C> Model <C> IMDb <C> AG’s News <C> DBPedia <C> Yelp P. <C> Yelp F. <C> Avg. Δ <C> SNLI <C> MNLI (m/mm) <C> Avg. Δ <R> <C> Model <C> Test Error Rate (%) <C> Test Error Rate (%) <C> Test Error Rate (%) <C> Test Error Rate (%) <C> Test Error Rate (%) <C> Avg. Δ <C> Accuracy (%) <C> Accuracy (%) <C> Avg. Δ <R> <C> ULMFiT  <C> 4.60 <C> 5.01 <C> 0.80 <C> 2.16 <C> 29.98 <C> / <C> / <C> / <C> / <R> <C> BERTBASE * <C> 5.40 <C> 5.25 <C> 0.71 <C> 2.28 <C> 30.06 <C> / <C> / <C> / <C> / <R> <C> BERTBASE <C> 5.80 <C> 5.71 <C> 0.71 <C> 2.25 <C> 30.37 <C> - <C> 90.7 <C> 84.6/83.3 <C> - <R> <C> BERTVOTE ( [ITALIC] K=4) <C> 5.60 <C> 5.41 <C> 0.67 <C> 2.03 <C> 29.44 <C> 5.44% <C> 91.2 <C> 85.3/84.4 <C> 5.50% <R> <C> BERTAVG ( [ITALIC] K=4) <C> 5.68 <C> 5.53 <C> 0.68 <C> 2.03 <C> 30.03 <C> 4.07% <C> 90.8 <C> 85.1/84.2 <C> 3.24% <R> <C> BERTSE (ours) <C> 5.82 <C> 5.59 <C> 0.65 <C> 2.19 <C> 30.48 <C> 2.50% <C> 90.8 <C> 84.2/83.3 <C> -0.51% <R> <C> BERTSDV (ours) <C> 5.35 <C> 5.38 <C> [BOLD] 0.68 <C> 2.05 <C> [BOLD] 29.88 <C> 5.65% <C> [BOLD] 91.2 <C> [BOLD] 85.3/84.3 <C> [BOLD] 5.30% <R> <C> BERTSDA (ours) <C> [BOLD] 5.29 <C> [BOLD] 5.29 <C> [BOLD] 0.68 <C> [BOLD] 2.04 <C> [BOLD] 29.88 <C> [BOLD] 6.26% <C> 91.2 <C> 85.0/84.3 <C> 4.65% <CAP> Table 3: Effects on fine-tuning the BERT-base model (BERTBASE). ‘*’ indicates using extra fine-tuning strategies and data preprocessing. ‘/’ means no available reported result. We implemented a “BERTBASE” without any extra fine-tuning strategy as our baseline. “BERTVOTE” and “BERTAVG” means ensemble BERT (See section 3.1). “BERTSE” means self-ensemble BERT (See section 3.2). “BERTSDV” and “BERTSDA” means self-distillation BERT (See section 3.3). ‘Avg. Δ’ means the average of relative change, respectively. We bold the better self-distillation results. <COT> Looking at the "Model" column, we can see different models such as ULMFiT, BERTBASE, BERTVOTE, BERTAVG, BERTSE, BERTSDV, and BERTSDA.
<R> <C> Model <C> IMDb <C> AG’s News <C> DBPedia <C> Yelp P. <C> Yelp F. <C> Avg. Δ <C> SNLI <C> MNLI (m/mm) <C> Avg. Δ <R> <C> Model <C> Test Error Rate (%) <C> Test Error Rate (%) <C> Test Error Rate (%) <C> Test Error Rate (%) <C> Test Error Rate (%) <C> Avg. Δ <C> Accuracy (%) <C> Accuracy (%) <C> Avg. Δ <R> <C> ULMFiT  <C> 4.60 <C> 5.01 <C> 0.80 <C> 2.16 <C> 29.98 <C> / <C> / <C> / <C> / <R> <C> BERTBASE * <C> 5.40 <C> 5.25 <C> 0.71 <C> 2.28 <C> 30.06 <C> / <C> / <C> / <C> / <R> <C> BERTBASE <C> 5.80 <C> 5.71 <C> 0.71 <C> 2.25 <C> 30.37 <C> - <C> 90.7 <C> 84.6/83.3 <C> - <R> <C> BERTVOTE ( [ITALIC] K=4) <C> 5.60 <C> 5.41 <C> 0.67 <C> 2.03 <C> 29.44 <C> 5.44% <C> 91.2 <C> 85.3/84.4 <C> 5.50% <R> <C> BERTAVG ( [ITALIC] K=4) <C> 5.68 <C> 5.53 <C> 0.68 <C> 2.03 <C> 30.03 <C> 4.07% <C> 90.8 <C> 85.1/84.2 <C> 3.24% <R> <C> BERTSE (ours) <C> 5.82 <C> 5.59 <C> 0.65 <C> 2.19 <C> 30.48 <C> 2.50% <C> 90.8 <C> 84.2/83.3 <C> -0.51% <R> <C> BERTSDV (ours) <C> 5.35 <C> 5.38 <C> [BOLD] 0.68 <C> 2.05 <C> [BOLD] 29.88 <C> 5.65% <C> [BOLD] 91.2 <C> [BOLD] 85.3/84.3 <C> [BOLD] 5.30% <R> <C> BERTSDA (ours) <C> [BOLD] 5.29 <C> [BOLD] 5.29 <C> [BOLD] 0.68 <C> [BOLD] 2.04 <C> [BOLD] 29.88 <C> [BOLD] 6.26% <C> 91.2 <C> 85.0/84.3 <C> 4.65% <CAP> Table 3: Effects on fine-tuning the BERT-base model (BERTBASE). ‘*’ indicates using extra fine-tuning strategies and data preprocessing. ‘/’ means no available reported result. We implemented a “BERTBASE” without any extra fine-tuning strategy as our baseline. “BERTVOTE” and “BERTAVG” means ensemble BERT (See section 3.1). “BERTSE” means self-ensemble BERT (See section 3.2). “BERTSDV” and “BERTSDA” means self-distillation BERT (See section 3.3). ‘Avg. Δ’ means the average of relative change, respectively. We bold the better self-distillation results. <COT> Looking at the "Avg. Δ" column, we can see the average relative change for each model.
<R> <C> Model <C> IMDb <C> AG’s <C> Avg. Δ <C> SNLI <C> Δ <R> <C> Model <C> IMDb <C> News <C> Avg. Δ <C> SNLI <C> Δ <R> <C> MT-DNN  <C> / <C> / <C> / <C> 91.6 <C> / <R> <C> BERT-L <C> 4.98 <C> 5.45 <C> - <C> 90.9 <C> - <R> <C> (our implementation) <C> 4.98 <C> 5.45 <C> - <C> 90.9 <C> - <R> <C> BERT-LSDA( [ITALIC] K=1) <C> 4.66 <C> 5.21 <C> 5.62% <C> [BOLD] 91.5 <C> [BOLD] 6.59% <R> <C> BERT-LSDA( [ITALIC] K= [ITALIC] T−1) <C> [BOLD] 4.58 <C> [BOLD] 5.15 <C> [BOLD] 7.02% <C> 91.4 <C> 5.49% <CAP> Table 4: Effects on fine-tuning the BERT-large model (BERT-L). For IMDb and AG’s News, we report test error rate (%). For SNLI, we report accuracy (%). MT-DNN fine-tunes BERT with multi-task learning. <COT> Looking at the "Table 4" caption, we can see that the table shows the effects of fine-tuning the BERT-large model (BERT-L) on different datasets.
<R> <C> Model <C> Overall <C> High <C> Medium <C> Low <C> OOV <R> <C> Seq2Seq <C> 47.02 <C> 42.41 <C> 47.25 <C> 48.61 <C> 49.96 <R> <C> MemNet <C> 46.85 <C> 41.93 <C> 47.32 <C> 48.86 <C> 49.52 <R> <C> CopyNet <C> 40.27 <C> 36.26 <C> 40.99 <C> 42.09 <C> 42.24 <R> <C> CCM <C> 39.18 <C> 35.36 <C> 39.64 <C> 40.67 <C> 40.87 <R> <C> PostKS <C> 43.56 <C> 40.65 <C> 44.06 <C> 46.36 <C> 49.32 <R> <C> TransDG <C> [BOLD] 37.53 <C> [BOLD] 32.18 <C> [BOLD] 36.12 <C> [BOLD] 38.46 <C> [BOLD] 40.75 <CAP> Table 2: Automatic evaluation with perplexity. <COT> Looking at the "Model" column, we can see that TransDG has the lowest overall perplexity score compared to other models.
<R> <C> Model <C> Overall <C> High <C> Medium <C> Low <C> OOV <R> <C> Seq2Seq <C> 47.02 <C> 42.41 <C> 47.25 <C> 48.61 <C> 49.96 <R> <C> MemNet <C> 46.85 <C> 41.93 <C> 47.32 <C> 48.86 <C> 49.52 <R> <C> CopyNet <C> 40.27 <C> 36.26 <C> 40.99 <C> 42.09 <C> 42.24 <R> <C> CCM <C> 39.18 <C> 35.36 <C> 39.64 <C> 40.67 <C> 40.87 <R> <C> PostKS <C> 43.56 <C> 40.65 <C> 44.06 <C> 46.36 <C> 49.32 <R> <C> TransDG <C> [BOLD] 37.53 <C> [BOLD] 32.18 <C> [BOLD] 36.12 <C> [BOLD] 38.46 <C> [BOLD] 40.75 <CAP> Table 2: Automatic evaluation with perplexity. <COT> Looking at the "Overall" column, we can see that CopyNet has the highest overall perplexity score.
<R> <C> Model <C> Overall <C> High <C> Medium <C> Low <C> OOV <R> <C> Seq2Seq <C> 0.717 <C> 0.713 <C> 0.740 <C> 0.721 <C> 0.669 <R> <C> MemNet <C> 0.761 <C> 0.764 <C> 0.788 <C> 0.760 <C> 0.706 <R> <C> CopyNet <C> 0.960 <C> 0.910 <C> 0.970 <C> 0.960 <C> 0.960 <R> <C> CCM <C> 1.180 <C> 1.156 <C> 1.191 <C> 1.196 <C> 1.162 <R> <C> PostKS <C> 1.041 <C> 1.007 <C> 1.028 <C> 0.993 <C> 0.978 <R> <C> TransDG <C> [BOLD] 1.207 <C> [BOLD] 1.195 <C> [BOLD] 1.204 <C> [BOLD] 1.232 <C> [BOLD] 1.182 <CAP> Table 3: Automatic evaluation with entity score. <COT> By looking at the "Model" column, we can see that the "TransDG" model has the highest scores in all categories (Overall, High, Medium, Low, OOV) compared to other models.
<R> <C> Model <C> Overall <C> High <C> Medium <C> Low <C> OOV <R> <C> Seq2Seq <C> 0.717 <C> 0.713 <C> 0.740 <C> 0.721 <C> 0.669 <R> <C> MemNet <C> 0.761 <C> 0.764 <C> 0.788 <C> 0.760 <C> 0.706 <R> <C> CopyNet <C> 0.960 <C> 0.910 <C> 0.970 <C> 0.960 <C> 0.960 <R> <C> CCM <C> 1.180 <C> 1.156 <C> 1.191 <C> 1.196 <C> 1.162 <R> <C> PostKS <C> 1.041 <C> 1.007 <C> 1.028 <C> 0.993 <C> 0.978 <R> <C> TransDG <C> [BOLD] 1.207 <C> [BOLD] 1.195 <C> [BOLD] 1.204 <C> [BOLD] 1.232 <C> [BOLD] 1.182 <CAP> Table 3: Automatic evaluation with entity score. <COT> By looking at the "Overall" column, we can see that the "CopyNet" model has the highest overall score compared to other models.
<R> <C> Model <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <R> <C> Seq2Seq <C> 0.0977 <C> 0.0098 <C> 0.0012 <C> 0.0002 <R> <C> MemNet <C> 0.1652 <C> 0.0174 <C> 0.0028 <C> 0.0004 <R> <C> CopyNet <C> 0.1715 <C> [BOLD] 0.0181 <C> 0.0029 <C> 0.0005 <R> <C> CCM <C> 0.1625 <C> 0.0175 <C> 0.0030 <C> 0.0005 <R> <C> PostKS <C> 0.1683 <C> 0.0165 <C> 0.0029 <C> 0.0004 <R> <C> TransDG <C> [BOLD] 0.1807 <C> 0.0178 <C> [BOLD] 0.0031 <C> [BOLD] 0.0006 <CAP> Table 4: Automatic evaluation with BLEU. <COT> Looking at the "Model" column, we can see that the Seq2Seq model has the lowest BLEU-1 score of 0.0977.
<R> <C> Model <C> Fluency <C> Relevance <C> Correctness <R> <C> Seq2Seq <C> 1.67 <C> 0.68 <C> 0.80 <R> <C> MemNet <C> 1.83 <C> 0.89 <C> 1.32 <R> <C> CopyNet <C> 2.36 <C> 1.13 <C> 1.08 <R> <C> CCM <C> 2.27 <C> 1.35 <C> 1.22 <R> <C> PostKS <C> 2.32 <C> 1.36 <C> 1.31 <R> <C> TransDG <C> [BOLD] 2.41 <C> [BOLD] 1.52 <C> [BOLD] 1.34 <CAP> Table 5: Human evaluation result. <COT> Looking at the "CAP" tag, we can see that Table 5 represents the human evaluation results of different models.
<R> <C> Model <C> Fluency <C> Relevance <C> Correctness <R> <C> Seq2Seq <C> 1.67 <C> 0.68 <C> 0.80 <R> <C> MemNet <C> 1.83 <C> 0.89 <C> 1.32 <R> <C> CopyNet <C> 2.36 <C> 1.13 <C> 1.08 <R> <C> CCM <C> 2.27 <C> 1.35 <C> 1.22 <R> <C> PostKS <C> 2.32 <C> 1.36 <C> 1.31 <R> <C> TransDG <C> [BOLD] 2.41 <C> [BOLD] 1.52 <C> [BOLD] 1.34 <CAP> Table 5: Human evaluation result. <COT> Looking at the "TransDG" row, we can see that it has the highest scores for fluency, relevance, and correctness.
<R> <C> Model <C> Perplexity <C> Entity <C> BLEU-1 <C> BLEU-2 <R> <C> TransDG <C> 37.53 <C> 1.207 <C> 0.1807 <C> 0.0178 <R> <C> w/o QRT <C> 42.17 <C> 1.076 <C> 0.1604 <C> 0.0171 <R> <C> w/o KST <C> 43.05 <C> 0.774 <C> 0.1643 <C> 0.0158 <R> <C> w/o QRT+KST <C> 44.15 <C> 0.772 <C> 0.1612 <C> 0.0170 <R> <C> w/o RGA <C> 38.62 <C> 1.106 <C> 0.1712 <C> 0.0170 <R> <C> w/o SSD <C> 38.18 <C> 1.114 <C> 0.1804 <C> 0.0178 <CAP> Table 7: Ablation results of TransDG on the test set. Here, Entity represents entity score. <COT> Looking at the "CAP" tag, we can see that Table 7 shows the ablation results of TransDG on the test set.
<R> <C> Model <C> Perplexity <C> Entity <C> BLEU-1 <C> BLEU-2 <R> <C> TransDG <C> 37.53 <C> 1.207 <C> 0.1807 <C> 0.0178 <R> <C> w/o QRT <C> 42.17 <C> 1.076 <C> 0.1604 <C> 0.0171 <R> <C> w/o KST <C> 43.05 <C> 0.774 <C> 0.1643 <C> 0.0158 <R> <C> w/o QRT+KST <C> 44.15 <C> 0.772 <C> 0.1612 <C> 0.0170 <R> <C> w/o RGA <C> 38.62 <C> 1.106 <C> 0.1712 <C> 0.0170 <R> <C> w/o SSD <C> 38.18 <C> 1.114 <C> 0.1804 <C> 0.0178 <CAP> Table 7: Ablation results of TransDG on the test set. Here, Entity represents entity score. <COT> Looking at the "BLEU-1" and "BLEU-2" cells, we can see that the BLEU-1 score is higher than the BLEU-2 score for all models.
<R> <C> [BOLD] Method <C> Bas. <C> Blo. <C> Cal. <C> Hou. <C> Pub. <C> Rec. <C> Res. <C> Soc. <C> [BOLD] Avg. <R> <C> [BOLD] Previous Methods <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Han ( 2018 ) <C> 88.2 <C> 61.4 <C> 81.5 <C> 74.1 <C> 80.7 <C> 82.9 <C> 80.7 <C> 82.1 <C> 79.0 <R> <C> Su and Yan ( 2017 ) <C> 88.2 <C> 62.2 <C> 82.1 <C> 78.8 <C> 80.1 <C> 86.1 <C> 83.7 <C> 83.1 <C> 80.6 <R> <C> Herzig and Berant ( 2017 ) <C> 86.2 <C> 62.7 <C> 82.1 <C> 78.3 <C> 80.7 <C> 82.9 <C> 82.2 <C> 81.7 <C> 79.6 <R> <C> [BOLD] Our Methods <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Shaw et al. ( 2019 ) <C> [BOLD] 89.3 <C> 63.7 <C> 81.5 <C> 82.0 <C> 80.7 <C> 85.6 <C> 89.5 <C> 84.8 <C> 82.1 <R> <C> G-R (Beam-10) <C> 88.7 <C> 66.4 <C> 83.3 <C> 82.5 <C> 78.9 <C> 86.6 <C> 89.8 <C> 83.7 <C> 82.5 <R> <C> G-R (Beam-10 & pQ) <C> 89.0 <C> 65.2 <C> 83.3 <C> 83.6 <C> 78.3 <C> 87.5 <C> 89.5 <C> 85.5 <C> 82.7 <R> <C> G-R (Beam-25) <C> 89.0 <C> [BOLD] 67.7 <C> 83.3 <C> [BOLD] 84.1 <C> [BOLD] 82.6 <C> 87.5 <C> 89.4 <C> 83.9 <C> 83.4 <R> <C> G-R (Beam-25 & pQ) <C> [BOLD] 89.3 <C> 66.7 <C> 84.5 <C> 83.6 <C> 80.1 <C> [BOLD] 88.0 <C> [BOLD] 91.0 <C> 85.2 <C> 83.5 <R> <C> G-R (Beam-25 & pQ & TH1) <C> 89.0 <C> 65.7 <C> [BOLD] 85.1 <C> 83.6 <C> 81.4 <C> [BOLD] 88.0 <C> [BOLD] 91.0 <C> [BOLD] 86.0 <C> [BOLD] 83.7 <R> <C> G-R (Beam-25 & pQ & TH2) <C> 88.7 <C> 66.4 <C> 82.7 <C> 83.1 <C> 82.0 <C> 87.0 <C> 89.8 <C> 85.8 <C> 83.2 <CAP> Table 3: Test accuracy for all models on OVERNIGHT dataset, which has eight domains: Basketball, Blocks, Calendar, Housing, Publications, Recipes, Restaurants, and Social. We use the generator-reranker (G-R) architecture with different options. Beam-n: Beam search is applied with size n, pQ: The critic is pre-trained over the Quora dataset, TH1: rerank if there is at least one score above 0.5, TH2: rerank if best score−second best score>0.001. The candidate logical forms are processed with templated expansions method (Section 3.2.3) in this experiment. <COT> Looking at the "Method" column, we can see that the "Our Methods" row contains the results of different models.
<R> <C> [BOLD] Method <C> Bas. <C> Blo. <C> Cal. <C> Hou. <C> Pub. <C> Rec. <C> Res. <C> Soc. <C> [BOLD] Avg. <R> <C> [BOLD] Previous Methods <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Han ( 2018 ) <C> 88.2 <C> 61.4 <C> 81.5 <C> 74.1 <C> 80.7 <C> 82.9 <C> 80.7 <C> 82.1 <C> 79.0 <R> <C> Su and Yan ( 2017 ) <C> 88.2 <C> 62.2 <C> 82.1 <C> 78.8 <C> 80.1 <C> 86.1 <C> 83.7 <C> 83.1 <C> 80.6 <R> <C> Herzig and Berant ( 2017 ) <C> 86.2 <C> 62.7 <C> 82.1 <C> 78.3 <C> 80.7 <C> 82.9 <C> 82.2 <C> 81.7 <C> 79.6 <R> <C> [BOLD] Our Methods <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Shaw et al. ( 2019 ) <C> [BOLD] 89.3 <C> 63.7 <C> 81.5 <C> 82.0 <C> 80.7 <C> 85.6 <C> 89.5 <C> 84.8 <C> 82.1 <R> <C> G-R (Beam-10) <C> 88.7 <C> 66.4 <C> 83.3 <C> 82.5 <C> 78.9 <C> 86.6 <C> 89.8 <C> 83.7 <C> 82.5 <R> <C> G-R (Beam-10 & pQ) <C> 89.0 <C> 65.2 <C> 83.3 <C> 83.6 <C> 78.3 <C> 87.5 <C> 89.5 <C> 85.5 <C> 82.7 <R> <C> G-R (Beam-25) <C> 89.0 <C> [BOLD] 67.7 <C> 83.3 <C> [BOLD] 84.1 <C> [BOLD] 82.6 <C> 87.5 <C> 89.4 <C> 83.9 <C> 83.4 <R> <C> G-R (Beam-25 & pQ) <C> [BOLD] 89.3 <C> 66.7 <C> 84.5 <C> 83.6 <C> 80.1 <C> [BOLD] 88.0 <C> [BOLD] 91.0 <C> 85.2 <C> 83.5 <R> <C> G-R (Beam-25 & pQ & TH1) <C> 89.0 <C> 65.7 <C> [BOLD] 85.1 <C> 83.6 <C> 81.4 <C> [BOLD] 88.0 <C> [BOLD] 91.0 <C> [BOLD] 86.0 <C> [BOLD] 83.7 <R> <C> G-R (Beam-25 & pQ & TH2) <C> 88.7 <C> 66.4 <C> 82.7 <C> 83.1 <C> 82.0 <C> 87.0 <C> 89.8 <C> 85.8 <C> 83.2 <CAP> Table 3: Test accuracy for all models on OVERNIGHT dataset, which has eight domains: Basketball, Blocks, Calendar, Housing, Publications, Recipes, Restaurants, and Social. We use the generator-reranker (G-R) architecture with different options. Beam-n: Beam search is applied with size n, pQ: The critic is pre-trained over the Quora dataset, TH1: rerank if there is at least one score above 0.5, TH2: rerank if best score−second best score>0.001. The candidate logical forms are processed with templated expansions method (Section 3.2.3) in this experiment. <COT> Looking at the "Cal." column, we can see that the highest accuracy for the "Calendar" domain is achieved by the "G-R (Beam-25 & pQ & TH2)" model.
<R> <C> Search Strategy <C> BLEU <R> <C> Greedy Search <C> 86.24 <R> <C> 2-LA <C> 86.65 <R> <C> 3-LA <C> 86.71 <R> <C> 4-LA <C> 86.77 <R> <C> 5-LA <C> [BOLD] 86.79 <R> <C> Beam Search (B=10) <C> 86.28 <CAP> Table 1: The performances of the IM2LATEX-100K Bi-LSTM model. We discover that the look-ahead improves the model from the greedy search method — noted that LA is more directly comparable to the greedy search because of their same beam size. We also show the scores of the beam search for the reference <COT> Looking at the "Search Strategy" column, we can see that the scores are increasing as the look-ahead (LA) value increases, except for the last row where the score remains the same.
<R> <C> Search Strategy <C> BLEU <R> <C> Greedy Search <C> 86.24 <R> <C> 2-LA <C> 86.65 <R> <C> 3-LA <C> 86.71 <R> <C> 4-LA <C> 86.77 <R> <C> 5-LA <C> [BOLD] 86.79 <R> <C> Beam Search (B=10) <C> 86.28 <CAP> Table 1: The performances of the IM2LATEX-100K Bi-LSTM model. We discover that the look-ahead improves the model from the greedy search method — noted that LA is more directly comparable to the greedy search because of their same beam size. We also show the scores of the beam search for the reference <COT> Looking at the "Search Strategy" column, we can see that the score for the 5-LA strategy is the highest among all the search strategies.
<R> <C> [EMPTY] <C> DSQuAD <C> DBiDAF <C> DBERT <C> DRoBERTa <R> <C> Question length <C> 10.3 <C> 9.8 <C> 9.8 <C> 10.0 <R> <C> Answer length <C> 2.6 <C> 2.9 <C> 3.0 <C> 3.2 <R> <C> N-gram overlap <C> 3.0 <C> 2.2 <C> 2.1 <C> 2.0 <CAP> Table 3: Average number of words per question and answer, and average longest n-gram overlap between passage and question. <COT> Looking at the "Question length" column, we can see that the average question length is the highest for DSQuAD and the lowest for DBiDAF.
<R> <C> [EMPTY] <C> DSQuAD <C> DBiDAF <C> DBERT <C> DRoBERTa <R> <C> Question length <C> 10.3 <C> 9.8 <C> 9.8 <C> 10.0 <R> <C> Answer length <C> 2.6 <C> 2.9 <C> 3.0 <C> 3.2 <R> <C> N-gram overlap <C> 3.0 <C> 2.2 <C> 2.1 <C> 2.0 <CAP> Table 3: Average number of words per question and answer, and average longest n-gram overlap between passage and question. <COT> Looking at the "N-gram overlap" column, we can see that the average longest n-gram overlap is the highest for DSQuAD and the lowest for DRoBERTa.
<R> <C> Search Strategy <C> BLEU <C> BLEU (Target len≥25) <R> <C> Greedy Search <C> 31.67 <C> [BOLD] 23.86 <R> <C> 2-LA <C> 32.07 <C> 21.50 <R> <C> 3-LA <C> 32.20 <C> 22.78 <R> <C> 4-LA <C> [BOLD] 32.42 <C> 22.45 <R> <C> 5-LA <C> 32.41 <C> 23.30 <R> <C> Beam Search (B=10) <C> 33.83 <C> 22.45 <CAP> Table 2: The performances of the LSTM model trained on the WMT16 multimodal translation dataset with different LA steps. We show the look-ahead module is able to improve the model on the entire testing set. However, either the LA module or the beam search method harm the models when the length of the target sentences is longer than 25 words. <COT> Looking at the "BLEU (Target len≥25)" column, we can see that the BLEU scores decrease for all search strategies when the target sentence length is longer than 25 words.
<R> <C> Search Strategy <C> BLEU <R> <C> Greedy Search <C> 27.50 <R> <C> 2-LA <C> [BOLD] 27.71 <R> <C> 3-LA <C> 27.62 <R> <C> 4-LA <C> 27.56 <R> <C> 5-LA <C> 27.35 <R> <C> Beam Search (B=10) <C> 28.21 <CAP> Table 3: We show the results of applying LA module to the transformer model trained on the WMT14 dataset. We find that the LA module slightly improves the original model but harms the performance when the LA time step is 5. We suggest one of the reasons of these results are caused by the EOS problem. <COT> Looking at the "Search Strategy" column, we can see that the BLEU score for "Greedy Search" is higher than the BLEU scores for "2-LA", "3-LA", "4-LA", and "5-LA".
<R> <C> Search Strategy <C> BLEU <R> <C> Greedy Search <C> 27.50 <R> <C> 2-LA <C> [BOLD] 27.71 <R> <C> 3-LA <C> 27.62 <R> <C> 4-LA <C> 27.56 <R> <C> 5-LA <C> 27.35 <R> <C> Beam Search (B=10) <C> 28.21 <CAP> Table 3: We show the results of applying LA module to the transformer model trained on the WMT14 dataset. We find that the LA module slightly improves the original model but harms the performance when the LA time step is 5. We suggest one of the reasons of these results are caused by the EOS problem. <COT> Looking at the "Search Strategy" column, we can see that the BLEU score for "Beam Search (B=10)" is higher than the BLEU scores for "Greedy Search", "2-LA", "3-LA", "4-LA", and "5-LA".
<R> <C> Search Strategy <C> [ITALIC] γ 0.0 <C> [ITALIC] γ 0.25 <C> [ITALIC] γ 0.50 <C> [ITALIC] γ 0.75 <C> [ITALIC] γ 1.0 <C> [ITALIC] γ 1.25 <R> <C> Greedy <C> 27.50 <C> 27.81 <C> 27.74 <C> 27.75 <C> [BOLD] 27.90 <C> 27.71 <R> <C> 2-LA <C> 27.71 <C> 28.05 <C> 27.95 <C> 27.99 <C> [BOLD] 28.20 <C> 27.85 <R> <C> 3-LA <C> 27.89 <C> 27.82 <C> 27.87 <C> 27.82 <C> [BOLD] 28.10 <C> 27.68 <R> <C> 4-LA <C> 27.56 <C> 27.81 <C> [BOLD] 27.87 <C> 27.74 <C> 27.84 <C> 27.68 <R> <C> 5-LA <C> 27.35 <C> 27.71 <C> 27.74 <C> 27.63 <C> [BOLD] 27.87 <C> 27.55 <CAP> Table 4: We show the results of integrating auxiliary EOS loss into the training state. γ is the weight of the auxiliary EOS loss. We find the EOS loss not only boosts the performance of the model when using the greedy search, the model is more robust to the larger Look-ahead steps with reasonable weights of auxiliary EOS loss. <COT> Looking at the "Greedy" row, the highest score is in the "[BOLD] 27.90" cell, and the corresponding value of γ is 1.0.
<R> <C> Search Strategy <C> [ITALIC] γ 0.0 <C> [ITALIC] γ 0.25 <C> [ITALIC] γ 0.50 <C> [ITALIC] γ 0.75 <C> [ITALIC] γ 1.0 <C> [ITALIC] γ 1.25 <R> <C> Greedy <C> 27.50 <C> 27.81 <C> 27.74 <C> 27.75 <C> [BOLD] 27.90 <C> 27.71 <R> <C> 2-LA <C> 27.71 <C> 28.05 <C> 27.95 <C> 27.99 <C> [BOLD] 28.20 <C> 27.85 <R> <C> 3-LA <C> 27.89 <C> 27.82 <C> 27.87 <C> 27.82 <C> [BOLD] 28.10 <C> 27.68 <R> <C> 4-LA <C> 27.56 <C> 27.81 <C> [BOLD] 27.87 <C> 27.74 <C> 27.84 <C> 27.68 <R> <C> 5-LA <C> 27.35 <C> 27.71 <C> 27.74 <C> 27.63 <C> [BOLD] 27.87 <C> 27.55 <CAP> Table 4: We show the results of integrating auxiliary EOS loss into the training state. γ is the weight of the auxiliary EOS loss. We find the EOS loss not only boosts the performance of the model when using the greedy search, the model is more robust to the larger Look-ahead steps with reasonable weights of auxiliary EOS loss. <COT> Looking at the "3-LA" row, the highest score is in the "[BOLD] 28.10" cell, and the corresponding value of γ is 1.0.
<R> <C> Model <C> WMT’14 EN-DE <C> WMT’14 EN-FR <C> IWSLT’14 DE-EN <R> <C> vaswani2017transformer <C> 28.4 <C> 41.0 <C> 34.4 <R> <C> ahmed2018weighted <C> 28.9 <C> 41.4 <C> - <R> <C> chen2018combining <C> 28.5 <C> 41.0 <C> - <R> <C> shaw2018relative <C> 29.2 <C> 41.5 <C> - <R> <C> ott2018scaling <C> 29.3 <C> 43.2 <C> - <R> <C> wu2018dynconv <C> [BOLD] 29.7 <C> 43.2 <C> 35.2 <R> <C> he2018layerwise <C> 29.0 <C> - <C> 35.1 <R> <C> Joint Self-attention <C> [BOLD] 29.7 <C> 43.2 <C> 35.3 <R> <C> Local Joint Self-attention <C> [BOLD] 29.7 <C> [BOLD] 43.3 <C> [BOLD] 35.7 <CAP> Table 1: Translation quality evaluation (BLEU scores). <COT> Looking at the "Model" column, we can see that the "Joint Self-attention" model and the "Local Joint Self-attention" model have the same BLEU scores for the WMT'14 EN-DE and IWSLT'14 DE-EN datasets.
<R> <C> Model <C> WMT’14 EN-DE <C> WMT’14 EN-FR <C> IWSLT’14 DE-EN <R> <C> vaswani2017transformer <C> 28.4 <C> 41.0 <C> 34.4 <R> <C> ahmed2018weighted <C> 28.9 <C> 41.4 <C> - <R> <C> chen2018combining <C> 28.5 <C> 41.0 <C> - <R> <C> shaw2018relative <C> 29.2 <C> 41.5 <C> - <R> <C> ott2018scaling <C> 29.3 <C> 43.2 <C> - <R> <C> wu2018dynconv <C> [BOLD] 29.7 <C> 43.2 <C> 35.2 <R> <C> he2018layerwise <C> 29.0 <C> - <C> 35.1 <R> <C> Joint Self-attention <C> [BOLD] 29.7 <C> 43.2 <C> 35.3 <R> <C> Local Joint Self-attention <C> [BOLD] 29.7 <C> [BOLD] 43.3 <C> [BOLD] 35.7 <CAP> Table 1: Translation quality evaluation (BLEU scores). <COT> Looking at the "Model" column and the "WMT'14 EN-FR" column, we can see that the "Local Joint Self-attention" model has the highest BLEU score among all the models for the WMT'14 EN-FR dataset.
<R> <C> Method <C> Line iu % <C> Pixel iu % <R> <C> wavelength [seuret2017wavelength] <C> 68.58 <C> 79.13 <R> <C> Brigham Young University [simistira2017icdar2017] <C> 81.50 <C> 83.07 <R> <C> CITlab Argus LineDetect [gruuening2017robust] <C> 96.99 <C> 93.01 <R> <C> wavelength* (tight polygons) [simistira2017icdar2017] <C> 97.86 <C> 97.05 <R> <C> proposed method* <C> [BOLD] 99.42 <C> 96.11 <CAP> TABLE I: Results of text-line extraction on the DIVA-HisDB dataset (see Section III-A measured with the competition tool(see Section IV-A. Our proposed method outperforms state-of-the-art results by reducing the error by 80.7% and achieving nearly perfect results. Methods with * notation use semantic segmentation at pixel-level as pre-processing step. <COT> Looking at the "Method" column, we can see that the "proposed method*" has the highest values in both "Line iu %" and "Pixel iu %" columns.
<R> <C> Method <C> Line iu % <C> Pixel iu % <R> <C> wavelength [seuret2017wavelength] <C> 68.58 <C> 79.13 <R> <C> Brigham Young University [simistira2017icdar2017] <C> 81.50 <C> 83.07 <R> <C> CITlab Argus LineDetect [gruuening2017robust] <C> 96.99 <C> 93.01 <R> <C> wavelength* (tight polygons) [simistira2017icdar2017] <C> 97.86 <C> 97.05 <R> <C> proposed method* <C> [BOLD] 99.42 <C> 96.11 <CAP> TABLE I: Results of text-line extraction on the DIVA-HisDB dataset (see Section III-A measured with the competition tool(see Section IV-A. Our proposed method outperforms state-of-the-art results by reducing the error by 80.7% and achieving nearly perfect results. Methods with * notation use semantic segmentation at pixel-level as pre-processing step. <COT> Looking at the "Method" column, we can see that the "CITlab Argus LineDetect" method has the second-highest values in both "Line iu %" and "Pixel iu %" columns.
<R> <C> Method from gt <C> Line iu % <C> Pixel iu % <R> <C> wavelength [seuret2017wavelength] <C> 66.44 <C> 81.52 <R> <C> wavelength (tight polygons) [simistira2017icdar2017] <C> 99.25 <C> 98.95 <R> <C> proposed method <C> [BOLD] 100.0 <C> 97.22 <CAP> TABLE II: Results of the experiments shown in Table I with the difference that every method listed has received the ground truth of the semantic segmentation at pixel-level as input. Our proposed text-line extraction method is superior to state-of-the-art even if both methods run on the same perfect input. Moreover, in our experience, an algorithm which is not designed to take advantage of this pre-processing step will not benefit from it. <COT> Looking at the "CAP" tag, we can see that Table II shows the results of the experiments from Table I with the difference that every method listed has received the ground truth of the semantic segmentation at pixel-level as input.
<R> <C> Method from gt <C> Line iu % <C> Pixel iu % <R> <C> wavelength [seuret2017wavelength] <C> 66.44 <C> 81.52 <R> <C> wavelength (tight polygons) [simistira2017icdar2017] <C> 99.25 <C> 98.95 <R> <C> proposed method <C> [BOLD] 100.0 <C> 97.22 <CAP> TABLE II: Results of the experiments shown in Table I with the difference that every method listed has received the ground truth of the semantic segmentation at pixel-level as input. Our proposed text-line extraction method is superior to state-of-the-art even if both methods run on the same perfect input. Moreover, in our experience, an algorithm which is not designed to take advantage of this pre-processing step will not benefit from it. <COT> Looking at the "proposed method" cell, we can see that the proposed method achieves a pixel iu % of [BOLD] 100.0.
<R> <C> [EMPTY] <C> Task Dataset <C> Image-Sentence Retrieval Flickr30K  <C> Image-Sentence Retrieval MSCOCO  <C> Phrase Grounding Flickr30K <C> Phrase Grounding ReferIt  <C> Text-to-Clip DiDeMo  <C> Image Captioning MSCOCO  <C> Image Captioning MSCOCO  <C> VQA VQA  <R> <C> [EMPTY] <C> Dataset <C> Flickr30K  <C> MSCOCO  <C> Entities  <C> ReferIt  <C> DiDeMo  <C> [EMPTY] <C> [EMPTY] <C> VQA  <R> <C> [EMPTY] <C> Method <C> Embedding Network  <C> Embedding Network  <C> CITE  <C> CITE  <C> CITE  <C> ARNet  <C> ARNet  <C> EtEMN  <R> <C> [EMPTY] <C> Metric <C> Mean Recall <C> Mean Recall <C> Accuracy <C> Accuracy <C> Average <C> BLEU-4 <C> CIDEr <C> Accuracy <R> <C> [BOLD] (a) <C> [BOLD] Training from scratch <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding <C> 44.3 <C> 73.7 <C> 70.46 <C> 51.70 <C> 33.02 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention <C> 44.6 <C> 77.6 <C> 70.68 <C> 52.39 <C> 33.48 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM <C> 60.0 <C> 77.5 <C> 70.47 <C> 51.57 <C> 32.83 <C> 26.7 <C> 89.7 <C> 60.95 <R> <C> [BOLD] (b) <C> [BOLD] Word2Vec  <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding <C> 62.5 <C> 75.0 <C> 70.03 <C> 52.51 <C> 32.95 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Average Embedding + ft <C> 71.5 <C> 78.2 <C> 70.85 <C> 53.29 <C> 32.58 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention <C> 63.6 <C> 75.6 <C> 70.19 <C> 52.41 <C> 33.23 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention + ft <C> 71.9 <C> 79.9 <C> 70.94 <C> 53.54 <C> 33.26 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM <C> 68.5 <C> 72.5 <C> 69.83 <C> 52.86 <C> 33.73 <C> [BOLD] 28.5 <C> 92.7 <C> 61.40 <R> <C> [EMPTY] <C> LSTM + ft <C> 69.0 <C> 78.2 <C> 70.55 <C> 53.58 <C> [BOLD] 33.94 <C> [BOLD] 28.5 <C> [BOLD] 94.0 <C> 61.35 <R> <C> [BOLD] (c) <C> [BOLD] FastText  <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding <C> 69.2 <C> 78.5 <C> 69.75 <C> 51.27 <C> 32.45 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Average Embedding + ft <C> 73.0 <C> [BOLD] 80.7 <C> 70.62 <C> 53.24 <C> 32.01 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention <C> 69.5 <C> 78.6 <C> 69.87 <C> 52.49 <C> 33.31 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention + ft <C> [BOLD] 73.1 <C> 80.6 <C> [BOLD] 71.23 <C> 53.87 <C> 33.17 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM <C> 69.1 <C> 76.9 <C> 69.76 <C> 52.21 <C> 33.06 <C> [BOLD] 28.5 <C> 92.7 <C> [BOLD] 61.86 <R> <C> [EMPTY] <C> LSTM + ft <C> 68.5 <C> 80.1 <C> 71.09 <C> [BOLD] 53.95 <C> 32.51 <C> 28.3 <C> 93.2 <C> 61.66 <R> <C> [BOLD] (d) <C> [BOLD] Sentence-Level <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> InferSent  <C> 71.2 <C> 76.4 <C> 57.83 <C> 52.29 <C> 31.87 <C> – <C> – <C> – <R> <C> [EMPTY] <C> BERT  <C> 71.8 <C> 75.4 <C> 69.38 <C> 50.37 <C> 32.46 <C> – <C> – <C> – <CAP> Table 1: Word Embedding Comparison Across Vision Language Tasks. (a) contains the results of learning an embedding from scratch random initialization with fine-tuning during training. The remaining sections compare (b) Word2Vec, (c) FastText, and (d) sentence level embeddings InferSent and BERT. All experiments show three model variants: Average Embedding, Self-Attention, and LSTM, with and without fine-tuning during training. Average Embedding and Self-Attention are not used in generation tasks for Image Captioning and VQA as they are known to show worse performance; sentence level embeddings are not applicable for these tasks. See text for discussion. <COT> Looking at the table, focusing on the "Image Captioning MSCOCO" column, we can see that the highest BLEU-4 score is achieved by the "ARNet" method.
<R> <C> [EMPTY] <C> Task Dataset <C> Image-Sentence Retrieval Flickr30K  <C> Image-Sentence Retrieval MSCOCO  <C> Phrase Grounding Flickr30K <C> Phrase Grounding ReferIt  <C> Text-to-Clip DiDeMo  <C> Image Captioning MSCOCO  <C> Image Captioning MSCOCO  <C> VQA VQA  <R> <C> [EMPTY] <C> Dataset <C> Flickr30K  <C> MSCOCO  <C> Entities  <C> ReferIt  <C> DiDeMo  <C> [EMPTY] <C> [EMPTY] <C> VQA  <R> <C> [EMPTY] <C> Method <C> Embedding Network  <C> Embedding Network  <C> CITE  <C> CITE  <C> CITE  <C> ARNet  <C> ARNet  <C> EtEMN  <R> <C> [EMPTY] <C> Metric <C> Mean Recall <C> Mean Recall <C> Accuracy <C> Accuracy <C> Average <C> BLEU-4 <C> CIDEr <C> Accuracy <R> <C> [BOLD] (a) <C> [BOLD] Training from scratch <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding <C> 44.3 <C> 73.7 <C> 70.46 <C> 51.70 <C> 33.02 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention <C> 44.6 <C> 77.6 <C> 70.68 <C> 52.39 <C> 33.48 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM <C> 60.0 <C> 77.5 <C> 70.47 <C> 51.57 <C> 32.83 <C> 26.7 <C> 89.7 <C> 60.95 <R> <C> [BOLD] (b) <C> [BOLD] Word2Vec  <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding <C> 62.5 <C> 75.0 <C> 70.03 <C> 52.51 <C> 32.95 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Average Embedding + ft <C> 71.5 <C> 78.2 <C> 70.85 <C> 53.29 <C> 32.58 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention <C> 63.6 <C> 75.6 <C> 70.19 <C> 52.41 <C> 33.23 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention + ft <C> 71.9 <C> 79.9 <C> 70.94 <C> 53.54 <C> 33.26 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM <C> 68.5 <C> 72.5 <C> 69.83 <C> 52.86 <C> 33.73 <C> [BOLD] 28.5 <C> 92.7 <C> 61.40 <R> <C> [EMPTY] <C> LSTM + ft <C> 69.0 <C> 78.2 <C> 70.55 <C> 53.58 <C> [BOLD] 33.94 <C> [BOLD] 28.5 <C> [BOLD] 94.0 <C> 61.35 <R> <C> [BOLD] (c) <C> [BOLD] FastText  <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding <C> 69.2 <C> 78.5 <C> 69.75 <C> 51.27 <C> 32.45 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Average Embedding + ft <C> 73.0 <C> [BOLD] 80.7 <C> 70.62 <C> 53.24 <C> 32.01 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention <C> 69.5 <C> 78.6 <C> 69.87 <C> 52.49 <C> 33.31 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention + ft <C> [BOLD] 73.1 <C> 80.6 <C> [BOLD] 71.23 <C> 53.87 <C> 33.17 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM <C> 69.1 <C> 76.9 <C> 69.76 <C> 52.21 <C> 33.06 <C> [BOLD] 28.5 <C> 92.7 <C> [BOLD] 61.86 <R> <C> [EMPTY] <C> LSTM + ft <C> 68.5 <C> 80.1 <C> 71.09 <C> [BOLD] 53.95 <C> 32.51 <C> 28.3 <C> 93.2 <C> 61.66 <R> <C> [BOLD] (d) <C> [BOLD] Sentence-Level <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> InferSent  <C> 71.2 <C> 76.4 <C> 57.83 <C> 52.29 <C> 31.87 <C> – <C> – <C> – <R> <C> [EMPTY] <C> BERT  <C> 71.8 <C> 75.4 <C> 69.38 <C> 50.37 <C> 32.46 <C> – <C> – <C> – <CAP> Table 1: Word Embedding Comparison Across Vision Language Tasks. (a) contains the results of learning an embedding from scratch random initialization with fine-tuning during training. The remaining sections compare (b) Word2Vec, (c) FastText, and (d) sentence level embeddings InferSent and BERT. All experiments show three model variants: Average Embedding, Self-Attention, and LSTM, with and without fine-tuning during training. Average Embedding and Self-Attention are not used in generation tasks for Image Captioning and VQA as they are known to show worse performance; sentence level embeddings are not applicable for these tasks. See text for discussion. <COT> Looking at the table, focusing on the "VQA" column, we can see that the highest accuracy is achieved by the "EtEMN" method.
<R> <C> [EMPTY] <C> Task Dataset <C> Image-Sentence Retrieval Flickr30K <C> Image-Sentence Retrieval MSCOCO <C> Phrase Grounding Flickr30K <C> Phrase Grounding ReferIt <C> Text-to-Clip DiDeMo <C> Image Captioning MSCOCO <C> Image Captioning MSCOCO <C> VQA <R> <C> [EMPTY] <C> Dataset <C> Flickr30K <C> MSCOCO <C> Entities <C> ReferIt <C> DiDeMo <C> [EMPTY] <C> [EMPTY] <C> VQA <R> <C> [EMPTY] <C> Metric <C> Mean Recall <C> Mean Recall <C> Accuracy <C> Accuracy <C> Average <C> BLEU-4 <C> CIDEr <C> Accuracy <R> <C> [BOLD] (a) <C> [BOLD] Word2Vec + wn  <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding + ft <C> 72.0 <C> 79.2 <C> 70.51 <C> 53.93 <C> 33.24 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention + ft <C> 72.4 <C> 80.0 <C> 70.70 <C> 53.81 <C> 33.65 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM + ft <C> 69.3 <C> 78.9 <C> 70.80 <C> 53.67 <C> 34.16 <C> 28.6 <C> 93.3 <C> 61.06 <R> <C> [BOLD] (b) <C> [BOLD] GrOVLE <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding + ft <C> 72.3 <C> 80.2 <C> 70.77 <C> [BOLD] 53.99 <C> 33.71 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention + ft <C> 72.1 <C> 80.5 <C> 70.95 <C> 53.75 <C> 33.14 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM + ft <C> 69.7 <C> 78.8 <C> 70.18 <C> [BOLD] 53.99 <C> 34.47 <C> 28.3 <C> 92.5 <C> 61.22 <R> <C> [BOLD] (c) <C> [BOLD] Visual Word2Vec  <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding + ft <C> 66.8 <C> 78.7 <C> 70.61 <C> 53.14 <C> 31.73 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention + ft <C> 68.8 <C> 79.2 <C> [BOLD] 71.07 <C> 53.26 <C> 31.15 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM + ft <C> 66.7 <C> 74.5 <C> 70.70 <C> 53.19 <C> 32.29 <C> [BOLD] 28.8 <C> [BOLD] 94.0 <C> 61.15 <R> <C> [BOLD] (d) <C> [BOLD] HGLMM (300-D)  <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding + ft <C> 71.0 <C> 79.8 <C> 70.64 <C> 53.71 <C> 32.62 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention + ft <C> 71.8 <C> 80.4 <C> 70.51 <C> 53.83 <C> 33.44 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM + ft <C> 69.5 <C> 77.9 <C> 70.37 <C> 53.10 <C> 33.85 <C> 28.7 <C> [BOLD] 94.0 <C> [BOLD] 61.44 <R> <C> [BOLD] (e) <C> [BOLD] HGLMM (6K-D)  <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Average Embedding + ft <C> 73.5 <C> [BOLD] 80.9 <C> 70.83 <C> 53.36 <C> 32.66 <C> – <C> – <C> – <R> <C> [EMPTY] <C> Self-Attention + ft <C> [BOLD] 75.1 <C> 80.6 <C> 71.02 <C> 53.43 <C> 33.57 <C> – <C> – <C> – <R> <C> [EMPTY] <C> LSTM + ft <C> 68.0 <C> 79.4 <C> 70.38 <C> 53.89 <C> [BOLD] 34.62 <C> 28.0 <C> 92.8 <C> 60.58 <CAP> Table 2: Modifications of Word2Vec. (a) contains Word2Vec retrofitted results using only the WordNet (wn) lexicon from [14]. Next, (b) is our baseline embedding which includes the new Visual Genome relational graph. Visual Word2Vec results are provided in (c), and (d), (e) are Fisher vectors on top of Word2Vec. See text for discussion. <COT> looking at the "Phrase Grounding ReferIt" cell, "LSTM + ft" cell, and "Accuracy" cell, we can see that the accuracy of LSTM + ft on the Phrase Grounding task for the ReferIt dataset is 53.67.
<R> <C> Task Additional Models <C> Image-Sentence Retrieval SCAN  <C> Image-Sentence Retrieval SCAN  <C> Phrase Grounding QA R-CNN  <C> Phrase Grounding QA R-CNN  <C> Text-to-Clip TGN  <C> Image Captioning BUTD  <C> Image Captioning BUTD  <C> VQA BAN <R> <C> Metric <C> Mean Recall <C> Mean Recall <C> Accuracy <C> Accuracy <C> Average <C> BLEU-4 <C> CIDEr <C> Accuracy <R> <C> Training from scratch <C> 72.8 <C> 83.2 <C> 68.56 <C> 50.23 <C> 43.91 <C> 35.2 <C> 109.8 <C> 68.98 <R> <C> FastText + ft <C> 72.5 <C> 83.8 <C> 69.27 <C> 53.01 <C> 44.21 <C> 35.2 <C> 110.3 <C> 69.91 <R> <C> GrOVLE (w/o multi-task pretraining) + ft <C> 72.7 <C> 84.1 <C> 70.03 <C> 53.88 <C> [BOLD] 45.26 <C> 35.1 <C> 110.4 <C> 69.36 <R> <C> + multi-task pretraining w/ target task + ft <C> [BOLD] 76.2 <C> [BOLD] 84.7 <C> [BOLD] 71.08 <C> [BOLD] 54.10 <C> 43.61 <C> [BOLD] 35.7 <C> [BOLD] 111.6 <C> [BOLD] 69.97 <CAP> Table 4: We include results with additional models to verify trends. See text for discussion and the appendix for more. <COT> Looking at the "Additional Models" column, we can see that the "+ multi-task pretraining w/ target task + ft" model has the highest scores for all metrics except for BLEU-4 and CIDEr.
<R> <C> Task Additional Models <C> Image-Sentence Retrieval SCAN  <C> Image-Sentence Retrieval SCAN  <C> Phrase Grounding QA R-CNN  <C> Phrase Grounding QA R-CNN  <C> Text-to-Clip TGN  <C> Image Captioning BUTD  <C> Image Captioning BUTD  <C> VQA BAN <R> <C> Metric <C> Mean Recall <C> Mean Recall <C> Accuracy <C> Accuracy <C> Average <C> BLEU-4 <C> CIDEr <C> Accuracy <R> <C> Training from scratch <C> 72.8 <C> 83.2 <C> 68.56 <C> 50.23 <C> 43.91 <C> 35.2 <C> 109.8 <C> 68.98 <R> <C> FastText + ft <C> 72.5 <C> 83.8 <C> 69.27 <C> 53.01 <C> 44.21 <C> 35.2 <C> 110.3 <C> 69.91 <R> <C> GrOVLE (w/o multi-task pretraining) + ft <C> 72.7 <C> 84.1 <C> 70.03 <C> 53.88 <C> [BOLD] 45.26 <C> 35.1 <C> 110.4 <C> 69.36 <R> <C> + multi-task pretraining w/ target task + ft <C> [BOLD] 76.2 <C> [BOLD] 84.7 <C> [BOLD] 71.08 <C> [BOLD] 54.10 <C> 43.61 <C> [BOLD] 35.7 <C> [BOLD] 111.6 <C> [BOLD] 69.97 <CAP> Table 4: We include results with additional models to verify trends. See text for discussion and the appendix for more. <COT> Looking at the "Phrase Grounding QA R-CNN" column, we can see that the "GrOVLE (w/o multi-task pretraining) + ft" model has a higher accuracy score compared to the "FastText + ft" model.
<R> <C> Task Metric <C> Image-Sentence Retrieval Mean Recall <C> Image-Sentence Retrieval Mean Recall <C> Phrase Grounding Accuracy <C> Phrase Grounding Accuracy <C> Text-to-Clip Average <C> Image Captioning BLEU-4 <C> Image Captioning CIDEr <C> VQA Accuracy <R> <C> GrOVLE w/o multi-task pretraining <C> 64.7 <C> 75.0 <C> 70.53 <C> 52.15 <C> 34.45 <C> 28.5 <C> 92.7 <C> 61.46 <R> <C> + multi-task pretraining w/o target task <C> 65.8 <C> 76.4 <C> 70.82 <C> 52.21 <C> 34.57 <C> [BOLD] 28.8 <C> [BOLD] 93.3 <C> 61.47 <R> <C> + multi-task pretraining w/ target task <C> 66.2 <C> 80.2 <C> 70.87 <C> 52.64 <C> 34.82 <C> 28.5 <C> 92.7 <C> [BOLD] 61.53 <R> <C> + multi-task pretraining w/ target task + ft <C> [BOLD] 72.6 <C> [BOLD] 81.3 <C> [BOLD] 71.57 <C> [BOLD] 54.51 <C> [BOLD] 35.09 <C> 28.7 <C> 93.2 <C> 61.46 <CAP> Table 3: Comparison of training our word embeddings on four tasks and testing on the fifth, as well as training on all five tasks. <COT> Looking at the "Table 3" caption, we can see that the table compares the performance of training word embeddings on four tasks and testing on the fifth, as well as training on all five tasks.
<R> <C> Task Metric <C> Image-Sentence Retrieval Mean Recall <C> Image-Sentence Retrieval Mean Recall <C> Phrase Grounding Accuracy <C> Phrase Grounding Accuracy <C> Text-to-Clip Average <C> Image Captioning BLEU-4 <C> Image Captioning CIDEr <C> VQA Accuracy <R> <C> GrOVLE w/o multi-task pretraining <C> 64.7 <C> 75.0 <C> 70.53 <C> 52.15 <C> 34.45 <C> 28.5 <C> 92.7 <C> 61.46 <R> <C> + multi-task pretraining w/o target task <C> 65.8 <C> 76.4 <C> 70.82 <C> 52.21 <C> 34.57 <C> [BOLD] 28.8 <C> [BOLD] 93.3 <C> 61.47 <R> <C> + multi-task pretraining w/ target task <C> 66.2 <C> 80.2 <C> 70.87 <C> 52.64 <C> 34.82 <C> 28.5 <C> 92.7 <C> [BOLD] 61.53 <R> <C> + multi-task pretraining w/ target task + ft <C> [BOLD] 72.6 <C> [BOLD] 81.3 <C> [BOLD] 71.57 <C> [BOLD] 54.51 <C> [BOLD] 35.09 <C> 28.7 <C> 93.2 <C> 61.46 <CAP> Table 3: Comparison of training our word embeddings on four tasks and testing on the fifth, as well as training on all five tasks. <COT> Looking at the "GrOVLE w/o multi-task pretraining" row, we can see that the "Image Captioning BLEU-4" and "Image Captioning CIDEr" metrics have lower values compared to the other rows.
<R> <C> [BOLD] Model <C> [BOLD] Resource <C> [ITALIC] Seed 1  [ITALIC] EM <C> [ITALIC] Seed 1  [ITALIC] F1 <C> [ITALIC] Seed 2  [ITALIC] EM <C> [ITALIC] Seed 2  [ITALIC] F1 <R> <C> BiDAF <C> DBiDAF [ITALIC] -dev <C> 0.0 <C> 5.3 <C> 10.3 <C> 19.4 <R> <C> BERT <C> DBERT [ITALIC] -dev <C> 0.0 <C> 4.9 <C> 20.5 <C> 30.3 <R> <C> RoBERTa <C> DRoBERTa [ITALIC] -dev <C> 0.0 <C> 6.1 <C> 16.5 <C> 26.4 <R> <C> BiDAF <C> DBiDAF [ITALIC] -test <C> 0.0 <C> 5.5 <C> 12.2 <C> 21.7 <R> <C> BERT <C> DBERT [ITALIC] -test <C> 0.0 <C> 5.3 <C> 18.6 <C> 29.6 <R> <C> RoBERTa <C> DRoBERTa [ITALIC] -test <C> 0.0 <C> 5.9 <C> 16.2 <C> 27.3 <CAP> Table 4: Consistency of the adversarial effect (or lack thereof) for different models in the loop when retraining the model on the same data again, but with a new random seed. <COT> Looking at the table, we can see that the BiDAF model consistently performs better than the BERT and RoBERTa models on both the dev and test sets.
<R> <C> [BOLD] Method <C> [BOLD] Perplexity <C> [BOLD] C Score <C> [BOLD] BLEU <C> [BOLD] Distinct-1 <C> [BOLD] Diff Score <C> Δ [BOLD]  Score <R> <C> Seq2seq <C> 37.90 <C> -0.16 <C> 1.29 <C> 0.0018 <C> 0.00 <C> 0.00 <R> <C> Seq2seq-F <C> [BOLD] 34.42 <C> -0.13 <C> 1.47 <C> 0.0033 <C> 63.44 <C> 32.73 <R> <C> Speaker <C> 40.17 <C> -0.14 <C> 1.14 <C> 0.0039 <C> 0.00 <C> 0.00 <R> <C> Speaker-F <C> 36.33 <C> -0.05 <C> 1.36 <C> 0.0048 <C> 53.36 <C> 27.92 <R> <C> PAML <C> 37.60 <C> 0.07 <C> 1.52 <C> 0.0067 <C> 36.59 <C> 37.36 <R> <C> ATAML <C> 40.27 <C> [BOLD] 0.12 <C> 0.52 <C> 0.0074 <C> 105.87 <C> 50.35 <R> <C> CMAML− <C> 37.41 <C> [BOLD] 0.13 <C> [BOLD] 1.59 <C> 0.0076 <C> 379.72 <C> 193.23 <R> <C> CMAML <C> 35.88 <C> [BOLD] 0.12 <C> [BOLD] 1.57 <C> [BOLD] 0.0089 <C> [BOLD] 412.77 <C> [BOLD] 210.07 <CAP> Table 1: Overall performance in terms of personality (C Score), quality (Perplexity, BLEU), diversity (Distinct-1), structure similarity of different users (Diff Score (×10−10)), model change after adaptation (Δ score (×10−10)) . <COT> Looking at the "Method" column, we can see that "CMAML" has the lowest values for Perplexity, C Score, BLEU, Distinct-1, Diff Score, and Δ Score, indicating that it performs the best overall.
<R> <C> [BOLD] Method <C> [BOLD] Perplexity <C> [BOLD] C Score <C> [BOLD] BLEU <C> [BOLD] Distinct-1 <C> [BOLD] Diff Score <C> Δ [BOLD]  Score <R> <C> Seq2seq <C> 37.90 <C> -0.16 <C> 1.29 <C> 0.0018 <C> 0.00 <C> 0.00 <R> <C> Seq2seq-F <C> [BOLD] 34.42 <C> -0.13 <C> 1.47 <C> 0.0033 <C> 63.44 <C> 32.73 <R> <C> Speaker <C> 40.17 <C> -0.14 <C> 1.14 <C> 0.0039 <C> 0.00 <C> 0.00 <R> <C> Speaker-F <C> 36.33 <C> -0.05 <C> 1.36 <C> 0.0048 <C> 53.36 <C> 27.92 <R> <C> PAML <C> 37.60 <C> 0.07 <C> 1.52 <C> 0.0067 <C> 36.59 <C> 37.36 <R> <C> ATAML <C> 40.27 <C> [BOLD] 0.12 <C> 0.52 <C> 0.0074 <C> 105.87 <C> 50.35 <R> <C> CMAML− <C> 37.41 <C> [BOLD] 0.13 <C> [BOLD] 1.59 <C> 0.0076 <C> 379.72 <C> 193.23 <R> <C> CMAML <C> 35.88 <C> [BOLD] 0.12 <C> [BOLD] 1.57 <C> [BOLD] 0.0089 <C> [BOLD] 412.77 <C> [BOLD] 210.07 <CAP> Table 1: Overall performance in terms of personality (C Score), quality (Perplexity, BLEU), diversity (Distinct-1), structure similarity of different users (Diff Score (×10−10)), model change after adaptation (Δ score (×10−10)) . <COT> Looking at the "Diff Score" column, we can see that "CMAML" has the highest value of 412.77, indicating that it has the highest structure similarity of different users.
<R> <C> [BOLD] Model <C> [BOLD] Trained On <C> [BOLD] Evaluation (Test) Dataset DSQuAD <C> [BOLD] Evaluation (Test) Dataset DSQuAD <C> [BOLD] Evaluation (Test) Dataset DBiDAF <C> [BOLD] Evaluation (Test) Dataset DBiDAF <C> [BOLD] Evaluation (Test) Dataset DBERT <C> [BOLD] Evaluation (Test) Dataset DBERT <C> [BOLD] Evaluation (Test) Dataset DRoBERTa <C> [BOLD] Evaluation (Test) Dataset DRoBERTa <C> [BOLD] Evaluation (Test) Dataset DDROP <C> [BOLD] Evaluation (Test) Dataset DDROP <C> [BOLD] Evaluation (Test) Dataset DNQ <C> [BOLD] Evaluation (Test) Dataset DNQ <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD(10K) <C> [BOLD] 40.6 <C> [BOLD] 54.6 <C> [BOLD] 7.0 <C> [BOLD] 15.1 <C> 5.3 <C> 12.8 <C> 5.7 <C> 13.2 <C> 4.5 <C> 9.3 <C> [BOLD] 26.7 <C> [BOLD] 40.6 <R> <C> [ITALIC] BiDAF <C> [ITALIC] DBiDAF <C> 12.1 <C> 22.1 <C> 5.7 <C> 12.9 <C> 6.4 <C> 13.6 <C> 6.0 <C> 13.2 <C> 6.1 <C> 12.0 <C> 14.1 <C> 26.7 <R> <C> [EMPTY] <C> [ITALIC] DBERT <C> 9.9 <C> 18.8 <C> 6.4 <C> 13.3 <C> 8.5 <C> 15.6 <C> 8.8 <C> 15.7 <C> 8.3 <C> 14.5 <C> 14.9 <C> 27.5 <R> <C> [EMPTY] <C> [ITALIC] DRoBERTa <C> 10.9 <C> 20.8 <C> 6.6 <C> 13.8 <C> [BOLD] 10.1 <C> [BOLD] 18.0 <C> [BOLD] 9.7 <C> [BOLD] 16.7 <C> [BOLD] 14.8 <C> [BOLD] 23.3 <C> 13.3 <C> 26.0 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD(10K) <C> [BOLD] 70.5 <C> [BOLD] 83.6 <C> 36.4 <C> 50.3 <C> 15.0 <C> 26.5 <C> 10.6 <C> 21.2 <C> 20.0 <C> 31.3 <C> 54.9 <C> 69.5 <R> <C> [ITALIC] BERT <C> [ITALIC] DBiDAF <C> 67.9 <C> 81.6 <C> [BOLD] 46.5 <C> [BOLD] 62.4 <C> [BOLD] 37.5 <C> [BOLD] 49.0 <C> [BOLD] 32.3 <C> [BOLD] 44.2 <C> [BOLD] 41.1 <C> [BOLD] 51.5 <C> [BOLD] 55.8 <C> [BOLD] 71.0 <R> <C> [EMPTY] <C> [ITALIC] DBERT <C> 60.9 <C> 75.2 <C> 42.2 <C> 57.8 <C> 36.4 <C> 46.6 <C> 28.3 <C> 39.6 <C> 35.7 <C> 44.4 <C> 50.7 <C> 65.4 <R> <C> [EMPTY] <C> [ITALIC] DRoBERTa <C> 57.6 <C> 71.8 <C> 36.8 <C> 50.9 <C> 34.1 <C> 44.9 <C> 31.0 <C> 41.7 <C> 37.6 <C> 45.9 <C> 48.2 <C> 63.8 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD(10K) <C> [BOLD] 70.0 <C> [BOLD] 83.7 <C> 39.4 <C> 55.4 <C> 21.5 <C> 33.7 <C> 11.1 <C> 22.1 <C> 20.3 <C> 30.9 <C> [BOLD] 48.0 <C> 64.8 <R> <C> [ITALIC] RoBERTa <C> [ITALIC] DBiDAF <C> 65.0 <C> 80.4 <C> [BOLD] 46.6 <C> [BOLD] 62.3 <C> [BOLD] 38.9 <C> [BOLD] 50.8 <C> 25.1 <C> 36.0 <C> [BOLD] 40.0 <C> [BOLD] 51.3 <C> 46.9 <C> [BOLD] 65.3 <R> <C> [EMPTY] <C> [ITALIC] DBERT <C> 58.7 <C> 74.1 <C> 42.5 <C> 58.0 <C> 34.8 <C> 45.6 <C> 24.7 <C> 34.6 <C> 37.8 <C> 48.5 <C> 42.7 <C> 60.4 <R> <C> [EMPTY] <C> [ITALIC] DRoBERTa <C> 55.4 <C> 71.4 <C> 37.9 <C> 53.5 <C> 37.5 <C> 48.6 <C> [BOLD] 28.2 <C> [BOLD] 38.9 <C> 39.5 <C> 49.0 <C> 38.8 <C> 57.9 <CAP> Table 5: Training models on various datasets, each with 10,000 samples, and measuring their generalisation to different evaluation datasets. Results in bold indicate the best result per model. <COT> Looking at the "Model" column, we can see that the models used for training are BiDAF, BERT, RoBERTa, and DRoBERTa.
<R> <C> [BOLD] Model <C> [BOLD] Trained On <C> [BOLD] Evaluation (Test) Dataset DSQuAD <C> [BOLD] Evaluation (Test) Dataset DSQuAD <C> [BOLD] Evaluation (Test) Dataset DBiDAF <C> [BOLD] Evaluation (Test) Dataset DBiDAF <C> [BOLD] Evaluation (Test) Dataset DBERT <C> [BOLD] Evaluation (Test) Dataset DBERT <C> [BOLD] Evaluation (Test) Dataset DRoBERTa <C> [BOLD] Evaluation (Test) Dataset DRoBERTa <C> [BOLD] Evaluation (Test) Dataset DDROP <C> [BOLD] Evaluation (Test) Dataset DDROP <C> [BOLD] Evaluation (Test) Dataset DNQ <C> [BOLD] Evaluation (Test) Dataset DNQ <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD(10K) <C> [BOLD] 40.6 <C> [BOLD] 54.6 <C> [BOLD] 7.0 <C> [BOLD] 15.1 <C> 5.3 <C> 12.8 <C> 5.7 <C> 13.2 <C> 4.5 <C> 9.3 <C> [BOLD] 26.7 <C> [BOLD] 40.6 <R> <C> [ITALIC] BiDAF <C> [ITALIC] DBiDAF <C> 12.1 <C> 22.1 <C> 5.7 <C> 12.9 <C> 6.4 <C> 13.6 <C> 6.0 <C> 13.2 <C> 6.1 <C> 12.0 <C> 14.1 <C> 26.7 <R> <C> [EMPTY] <C> [ITALIC] DBERT <C> 9.9 <C> 18.8 <C> 6.4 <C> 13.3 <C> 8.5 <C> 15.6 <C> 8.8 <C> 15.7 <C> 8.3 <C> 14.5 <C> 14.9 <C> 27.5 <R> <C> [EMPTY] <C> [ITALIC] DRoBERTa <C> 10.9 <C> 20.8 <C> 6.6 <C> 13.8 <C> [BOLD] 10.1 <C> [BOLD] 18.0 <C> [BOLD] 9.7 <C> [BOLD] 16.7 <C> [BOLD] 14.8 <C> [BOLD] 23.3 <C> 13.3 <C> 26.0 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD(10K) <C> [BOLD] 70.5 <C> [BOLD] 83.6 <C> 36.4 <C> 50.3 <C> 15.0 <C> 26.5 <C> 10.6 <C> 21.2 <C> 20.0 <C> 31.3 <C> 54.9 <C> 69.5 <R> <C> [ITALIC] BERT <C> [ITALIC] DBiDAF <C> 67.9 <C> 81.6 <C> [BOLD] 46.5 <C> [BOLD] 62.4 <C> [BOLD] 37.5 <C> [BOLD] 49.0 <C> [BOLD] 32.3 <C> [BOLD] 44.2 <C> [BOLD] 41.1 <C> [BOLD] 51.5 <C> [BOLD] 55.8 <C> [BOLD] 71.0 <R> <C> [EMPTY] <C> [ITALIC] DBERT <C> 60.9 <C> 75.2 <C> 42.2 <C> 57.8 <C> 36.4 <C> 46.6 <C> 28.3 <C> 39.6 <C> 35.7 <C> 44.4 <C> 50.7 <C> 65.4 <R> <C> [EMPTY] <C> [ITALIC] DRoBERTa <C> 57.6 <C> 71.8 <C> 36.8 <C> 50.9 <C> 34.1 <C> 44.9 <C> 31.0 <C> 41.7 <C> 37.6 <C> 45.9 <C> 48.2 <C> 63.8 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD(10K) <C> [BOLD] 70.0 <C> [BOLD] 83.7 <C> 39.4 <C> 55.4 <C> 21.5 <C> 33.7 <C> 11.1 <C> 22.1 <C> 20.3 <C> 30.9 <C> [BOLD] 48.0 <C> 64.8 <R> <C> [ITALIC] RoBERTa <C> [ITALIC] DBiDAF <C> 65.0 <C> 80.4 <C> [BOLD] 46.6 <C> [BOLD] 62.3 <C> [BOLD] 38.9 <C> [BOLD] 50.8 <C> 25.1 <C> 36.0 <C> [BOLD] 40.0 <C> [BOLD] 51.3 <C> 46.9 <C> [BOLD] 65.3 <R> <C> [EMPTY] <C> [ITALIC] DBERT <C> 58.7 <C> 74.1 <C> 42.5 <C> 58.0 <C> 34.8 <C> 45.6 <C> 24.7 <C> 34.6 <C> 37.8 <C> 48.5 <C> 42.7 <C> 60.4 <R> <C> [EMPTY] <C> [ITALIC] DRoBERTa <C> 55.4 <C> 71.4 <C> 37.9 <C> 53.5 <C> 37.5 <C> 48.6 <C> [BOLD] 28.2 <C> [BOLD] 38.9 <C> 39.5 <C> 49.0 <C> 38.8 <C> 57.9 <CAP> Table 5: Training models on various datasets, each with 10,000 samples, and measuring their generalisation to different evaluation datasets. Results in bold indicate the best result per model. <COT> Looking at the "Evaluation (Test) Dataset DSQuAD" column, we can see that the F1 scores for DBERT, DRoBERTa, and DNQ are higher than the F1 scores for BiDAF and BERT.
<R> <C> ↓train/eval→ <C> WebSplit 1.0 <C> WikiSplit <R> <C> Source <C> 58.0 <C> 73.4 <R> <C> SplitHalf <C> 54.9 <C> 71.7 <R> <C> WebSplit <C> 35.3 <C> 4.2 <R> <C> WikiSplit <C> 59.4 <C> 76.0 <R> <C> Both <C> [BOLD] 61.4 <C> [BOLD] 76.1 <CAP> Table 4: Corpus-level BLEU scores on the validation sets for the same model architecture trained on different data. <COT> Looking at the "CAP" tag, we can see that Table 4 provides corpus-level BLEU scores on the validation sets for the same model architecture trained on different data.
<R> <C> ↓train/eval→ <C> WebSplit 1.0 <C> WikiSplit <R> <C> Source <C> 58.0 <C> 73.4 <R> <C> SplitHalf <C> 54.9 <C> 71.7 <R> <C> WebSplit <C> 35.3 <C> 4.2 <R> <C> WikiSplit <C> 59.4 <C> 76.0 <R> <C> Both <C> [BOLD] 61.4 <C> [BOLD] 76.1 <CAP> Table 4: Corpus-level BLEU scores on the validation sets for the same model architecture trained on different data. <COT> Looking at the "Both" cell and the corresponding bold values, we can see that the model trained on both WebSplit 1.0 and WikiSplit data achieves a BLEU score of 61.4 on the WebSplit 1.0 validation set and a BLEU score of 76.1 on the WikiSplit validation set.
<R> <C> [ITALIC] Training <C> Unsupported <C> Missing <C> Repeated <C> Correct <R> <C> AG18 <C> 82 <C> 45 <C> 12 <C> 26/119 (22%) <R> <C> WebSplit <C> 58 <C> 47 <C> 13 <C> 32/100 (32%) <R> <C> WikiSplit <C> 8 <C> 5 <C> 0 <C> 91/100 (91%) <R> <C> Both <C> [BOLD] 4 <C> [BOLD] 4 <C> [BOLD] 0 <C> [BOLD] 95/100 (95%) <CAP> Table 6: Manual evaluation results, as counts over the simple sentences predicted by each model for a random sample of 50 inputs from WebSplit 1.0 validation set. <COT> Looking at the "CAP" tag, we can see that Table 6 shows the manual evaluation results for a random sample of 50 inputs from the WebSplit 1.0 validation set.
<R> <C> [ITALIC] Training <C> Unsupported <C> Missing <C> Repeated <C> Correct <R> <C> AG18 <C> 82 <C> 45 <C> 12 <C> 26/119 (22%) <R> <C> WebSplit <C> 58 <C> 47 <C> 13 <C> 32/100 (32%) <R> <C> WikiSplit <C> 8 <C> 5 <C> 0 <C> 91/100 (91%) <R> <C> Both <C> [BOLD] 4 <C> [BOLD] 4 <C> [BOLD] 0 <C> [BOLD] 95/100 (95%) <CAP> Table 6: Manual evaluation results, as counts over the simple sentences predicted by each model for a random sample of 50 inputs from WebSplit 1.0 validation set. <COT> Looking at the "Both" row and the "Correct" column, we can see that the model trained on both AG18 and WebSplit has a correct prediction rate of 95/100 (95%).
<R> <C> [EMPTY] <C> BLEU <C> sBLEU <C> #S/C <C> #T/S <R> <C> Reference <C> [EMPTY] <C> – <C> 2.5 <C> 10.9 <R> <C> Source <C> 58.7 <C> 56.1 <C> 1.0 <C> 20.5 <R> <C> SplitHalf <C> 55.7 <C> 53.0 <C> 2.0 <C> 10.8 <R> <C> AG18 <C> 30.5 <C> 25.5 <C> 2.3 <C> 11.8 <R> <C> WebSplit <C> 34.2 <C> 30.5 <C> 2.0 <C> 8.8 <R> <C> WikiSplit <C> 60.4 <C> 58.0 <C> 2.0 <C> 11.2 <R> <C> Both <C> [BOLD] 62.4 <C> [BOLD] 60.1 <C> 2.0 <C> 11.0 <CAP> Table 5: Results on the WebSplit v1.0 test set when varying the training data while holding model architecture fixed: corpus-level BLEU, sentence-level BLEU (to match past work), simple sentences per complex sentence, and tokens per simple sentence (micro-average). AG18 is the previous best model by aharoni:2018, which used the full WebSplit training set, whereas we downsampled it. <COT> Looking at the table caption, "Results on the WebSplit v1.0 test set when varying the training data while holding model architecture fixed", we can see that the table shows the results of different models on the WebSplit test set.
<R> <C> [EMPTY] <C> BLEU <C> sBLEU <C> #S/C <C> #T/S <R> <C> Reference <C> [EMPTY] <C> – <C> 2.5 <C> 10.9 <R> <C> Source <C> 58.7 <C> 56.1 <C> 1.0 <C> 20.5 <R> <C> SplitHalf <C> 55.7 <C> 53.0 <C> 2.0 <C> 10.8 <R> <C> AG18 <C> 30.5 <C> 25.5 <C> 2.3 <C> 11.8 <R> <C> WebSplit <C> 34.2 <C> 30.5 <C> 2.0 <C> 8.8 <R> <C> WikiSplit <C> 60.4 <C> 58.0 <C> 2.0 <C> 11.2 <R> <C> Both <C> [BOLD] 62.4 <C> [BOLD] 60.1 <C> 2.0 <C> 11.0 <CAP> Table 5: Results on the WebSplit v1.0 test set when varying the training data while holding model architecture fixed: corpus-level BLEU, sentence-level BLEU (to match past work), simple sentences per complex sentence, and tokens per simple sentence (micro-average). AG18 is the previous best model by aharoni:2018, which used the full WebSplit training set, whereas we downsampled it. <COT> Looking at the "Both" row, "[BOLD] 62.4" cell, and "BLEU" column, we can see that the model trained on both the Reference and Source data achieves a BLEU score of 62.4.
<R> <C> [EMPTY] <C> Basic: no walks MA <C> Basic: no walks MR <C> Basic: no walks MC <C> Basic: no walks Avg <C> EmbDI walks MA <C> EmbDI walks MR <C> EmbDI walks MC <C> EmbDI walks Avg <R> <C> Movie <C> [BOLD] .80 <C> .54 <C> .41 <C> .59 <C> .55 <C> [BOLD] .80 <C> [BOLD] .90 <C> [BOLD] .75 <R> <C> RefS <C> .39 <C> .40 <C> .24 <C> .34 <C> [BOLD] .90 <C> [BOLD] .59 <C> [BOLD] .62 <C> [BOLD] .70 <R> <C> RefL <C> [BOLD] .63 <C> .50 <C> .29 <C> .47 <C> [BOLD] .63 <C> [BOLD] .69 <C> [BOLD] .78 <C> [BOLD] .70 <CAP> Table 2: Quality results for local embeddings. <COT> Looking at the "RefS" row, "RefS" cell, and "EmbDI walks MA" cell, we can see that the quality result for local embeddings is better for the "EmbDI walks MA" method compared to the "RefS" method.
<R> <C> [EMPTY] <C> Basic: no walks MA <C> Basic: no walks MR <C> Basic: no walks MC <C> Basic: no walks Avg <C> EmbDI walks MA <C> EmbDI walks MR <C> EmbDI walks MC <C> EmbDI walks Avg <R> <C> Movie <C> [BOLD] .80 <C> .54 <C> .41 <C> .59 <C> .55 <C> [BOLD] .80 <C> [BOLD] .90 <C> [BOLD] .75 <R> <C> RefS <C> .39 <C> .40 <C> .24 <C> .34 <C> [BOLD] .90 <C> [BOLD] .59 <C> [BOLD] .62 <C> [BOLD] .70 <R> <C> RefL <C> [BOLD] .63 <C> .50 <C> .29 <C> .47 <C> [BOLD] .63 <C> [BOLD] .69 <C> [BOLD] .78 <C> [BOLD] .70 <CAP> Table 2: Quality results for local embeddings. <COT> Looking at the "RefL" row, "RefL" cell, and "Basic: no walks MC" cell, we can see that the quality result for local embeddings is better for the "RefL" method compared to the "Basic: no walks MC" method.
<R> <C> [EMPTY] <C> EmbDI P <C> EmbDI R <C> EmbDI F <C> Seep [ITALIC] P P <C> Seep [ITALIC] P R <C> Seep [ITALIC] P F <C> Seep [ITALIC] L P <C> Seep [ITALIC] L R <C> Seep [ITALIC] L F <R> <C> Movie <C> [BOLD] .75 <C> [BOLD] .86 <C> [BOLD] .80 <C> [BOLD] .75 <C> .62 <C> .68 <C> [BOLD] .75 <C> .75 <C> .75 <R> <C> RefS <C> [BOLD] 1 <C> [BOLD] 1 <C> [BOLD] 1 <C> .75 <C> .75 <C> .75 <C> .88 <C> .75 <C> .81 <R> <C> RefL <C> [BOLD] .71 <C> .83 <C> [BOLD] .77 <C> .50 <C> .75 <C> .60 <C> .63 <C> [BOLD] .88 <C> .73 <CAP> Table 3: Quality results for SM. <COT> Looking at the "RefS" column, we can see that the values for "RefS" Precision, Recall, and F-score are all 1.
<R> <C> [EMPTY] <C> EmbDI P <C> EmbDI R <C> EmbDI F <C> Seep [ITALIC] P P <C> Seep [ITALIC] P R <C> Seep [ITALIC] P F <C> Seep [ITALIC] L P <C> Seep [ITALIC] L R <C> Seep [ITALIC] L F <R> <C> Movie <C> [BOLD] .75 <C> [BOLD] .86 <C> [BOLD] .80 <C> [BOLD] .75 <C> .62 <C> .68 <C> [BOLD] .75 <C> .75 <C> .75 <R> <C> RefS <C> [BOLD] 1 <C> [BOLD] 1 <C> [BOLD] 1 <C> .75 <C> .75 <C> .75 <C> .88 <C> .75 <C> .81 <R> <C> RefL <C> [BOLD] .71 <C> .83 <C> [BOLD] .77 <C> .50 <C> .75 <C> .60 <C> .63 <C> [BOLD] .88 <C> .73 <CAP> Table 3: Quality results for SM. <COT> Looking at the "Seep [ITALIC] L P" column, we can see that the Precision for "Seep [ITALIC] L P" is .63 and the Precision for "Seep [ITALIC] L R" is .88.
<R> <C> [EMPTY] <C> Unsupervised. Basic <C> Unsupervised. Glove <C> Unsupervised. EmbDI <C> Supervised DeepER [ITALIC] P <C> Supervised DeepER [ITALIC] L <R> <C> Movie <C> 0 <C> .43 <C> [BOLD] .78 <C> .82 <C> [BOLD] .88 <R> <C> RefS <C> 0 <C> .84 <C> [BOLD] .95 <C> .84 <C> [BOLD] .89 <R> <C> RefL <C> 0 <C> .73 <C> [BOLD] .80 <C> .80 <C> [BOLD] .87 <CAP> Table 4: F-Measure results for ER. <COT> Looking at the table, we can see that the F-Measure results for ER are provided for different methods: Unsupervised (Basic, Glove, EmbDI) and Supervised DeepER (P, L).
<R> <C> [EMPTY] <C> Unsupervised. Basic <C> Unsupervised. Glove <C> Unsupervised. EmbDI <C> Supervised DeepER [ITALIC] P <C> Supervised DeepER [ITALIC] L <R> <C> Movie <C> 0 <C> .43 <C> [BOLD] .78 <C> .82 <C> [BOLD] .88 <R> <C> RefS <C> 0 <C> .84 <C> [BOLD] .95 <C> .84 <C> [BOLD] .89 <R> <C> RefL <C> 0 <C> .73 <C> [BOLD] .80 <C> .80 <C> [BOLD] .87 <CAP> Table 4: F-Measure results for ER. <COT> Looking at the table, we can see that the highest F-Measure results are marked as bold for each method.
<R> <C> [BOLD] Model <C> [BOLD] Training Dataset <C> [BOLD] Evaluation (Test) Dataset DSQuAD <C> [BOLD] Evaluation (Test) Dataset DSQuAD <C> [BOLD] Evaluation (Test) Dataset DBiDAF <C> [BOLD] Evaluation (Test) Dataset DBiDAF <C> [BOLD] Evaluation (Test) Dataset DBERT <C> [BOLD] Evaluation (Test) Dataset DBERT <C> [BOLD] Evaluation (Test) Dataset DRoBERTa <C> [BOLD] Evaluation (Test) Dataset DRoBERTa <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD <C> [BOLD] 58.7 <C> [BOLD] 71.9 <C> 0.0 <C> 5.5 <C> 8.9 <C> 17.6 <C> 8.3 <C> 17.0 <R> <C> [ITALIC] BiDAF <C> [ITALIC] DSQuAD + DBiDAF <C> 57.3 <C> 70.6 <C> 14.9 <C> 25.8 <C> 16.9 <C> 25.5 <C> 15.3 <C> 24.2 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DBERT <C> 57.0 <C> 70.4 <C> [BOLD] 16.3 <C> [BOLD] 26.5 <C> 14.5 <C> 24.1 <C> 14.7 <C> 24.1 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DRoBERTa <C> 55.9 <C> 69.6 <C> 16.2 <C> 25.6 <C> [BOLD] 17.3 <C> [BOLD] 26.2 <C> [BOLD] 15.6 <C> [BOLD] 25.0 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD <C> 70.7 <C> 84.0 <C> 36.7 <C> 50.2 <C> 0.0 <C> 5.3 <C> 15.2 <C> 25.8 <R> <C> [ITALIC] BERT <C> [ITALIC] DSQuAD + DBiDAF <C> [BOLD] 74.5 <C> [BOLD] 85.9 <C> 47.2 <C> [BOLD] 61.1 <C> 33.7 <C> 43.6 <C> 29.1 <C> 39.4 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DBERT <C> 74.3 <C> 85.8 <C> [BOLD] 48.1 <C> [BOLD] 61.1 <C> [BOLD] 37.8 <C> [BOLD] 47.3 <C> [BOLD] 31.1 <C> [BOLD] 41.5 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DRoBERTa <C> 73.2 <C> 85.2 <C> 47.3 <C> 60.5 <C> 36.8 <C> 46.2 <C> 30.1 <C> 39.7 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD <C> 74.1 <C> 86.8 <C> 50.4 <C> 64.9 <C> 31.9 <C> 44.1 <C> 0.0 <C> 5.9 <R> <C> [ITALIC] RoBERTa <C> [ITALIC] DSQuAD + DBiDAF <C> 75.2 <C> 87.6 <C> 56.3 <C> 71.2 <C> 47.8 <C> 58.0 <C> 31.3 <C> 42.8 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DBERT <C> [BOLD] 76.2 <C> [BOLD] 88.0 <C> 56.3 <C> 70.8 <C> 48.3 <C> 58.2 <C> 33.4 <C> 44.4 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DRoBERTa <C> 75.1 <C> 87.5 <C> [BOLD] 58.2 <C> [BOLD] 73.2 <C> [BOLD] 52.8 <C> [BOLD] 62.7 <C> [BOLD] 36.4 <C> [BOLD] 47.2 <CAP> Table 6: Training models on SQuAD, as well as SQuAD combined with different adversarially created datasets. Results in bold indicate the best result per model. <COT> Looking at the "Model" column and the "Evaluation (Test) Dataset DSQuAD" column, we can see that the BERT model achieves the highest F1 score on the DSQuAD dataset.
<R> <C> [BOLD] Model <C> [BOLD] Training Dataset <C> [BOLD] Evaluation (Test) Dataset DSQuAD <C> [BOLD] Evaluation (Test) Dataset DSQuAD <C> [BOLD] Evaluation (Test) Dataset DBiDAF <C> [BOLD] Evaluation (Test) Dataset DBiDAF <C> [BOLD] Evaluation (Test) Dataset DBERT <C> [BOLD] Evaluation (Test) Dataset DBERT <C> [BOLD] Evaluation (Test) Dataset DRoBERTa <C> [BOLD] Evaluation (Test) Dataset DRoBERTa <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <C> [ITALIC] EM <C> [ITALIC] F1 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD <C> [BOLD] 58.7 <C> [BOLD] 71.9 <C> 0.0 <C> 5.5 <C> 8.9 <C> 17.6 <C> 8.3 <C> 17.0 <R> <C> [ITALIC] BiDAF <C> [ITALIC] DSQuAD + DBiDAF <C> 57.3 <C> 70.6 <C> 14.9 <C> 25.8 <C> 16.9 <C> 25.5 <C> 15.3 <C> 24.2 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DBERT <C> 57.0 <C> 70.4 <C> [BOLD] 16.3 <C> [BOLD] 26.5 <C> 14.5 <C> 24.1 <C> 14.7 <C> 24.1 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DRoBERTa <C> 55.9 <C> 69.6 <C> 16.2 <C> 25.6 <C> [BOLD] 17.3 <C> [BOLD] 26.2 <C> [BOLD] 15.6 <C> [BOLD] 25.0 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD <C> 70.7 <C> 84.0 <C> 36.7 <C> 50.2 <C> 0.0 <C> 5.3 <C> 15.2 <C> 25.8 <R> <C> [ITALIC] BERT <C> [ITALIC] DSQuAD + DBiDAF <C> [BOLD] 74.5 <C> [BOLD] 85.9 <C> 47.2 <C> [BOLD] 61.1 <C> 33.7 <C> 43.6 <C> 29.1 <C> 39.4 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DBERT <C> 74.3 <C> 85.8 <C> [BOLD] 48.1 <C> [BOLD] 61.1 <C> [BOLD] 37.8 <C> [BOLD] 47.3 <C> [BOLD] 31.1 <C> [BOLD] 41.5 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DRoBERTa <C> 73.2 <C> 85.2 <C> 47.3 <C> 60.5 <C> 36.8 <C> 46.2 <C> 30.1 <C> 39.7 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD <C> 74.1 <C> 86.8 <C> 50.4 <C> 64.9 <C> 31.9 <C> 44.1 <C> 0.0 <C> 5.9 <R> <C> [ITALIC] RoBERTa <C> [ITALIC] DSQuAD + DBiDAF <C> 75.2 <C> 87.6 <C> 56.3 <C> 71.2 <C> 47.8 <C> 58.0 <C> 31.3 <C> 42.8 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DBERT <C> [BOLD] 76.2 <C> [BOLD] 88.0 <C> 56.3 <C> 70.8 <C> 48.3 <C> 58.2 <C> 33.4 <C> 44.4 <R> <C> [EMPTY] <C> [ITALIC] DSQuAD + DRoBERTa <C> 75.1 <C> 87.5 <C> [BOLD] 58.2 <C> [BOLD] 73.2 <C> [BOLD] 52.8 <C> [BOLD] 62.7 <C> [BOLD] 36.4 <C> [BOLD] 47.2 <CAP> Table 6: Training models on SQuAD, as well as SQuAD combined with different adversarially created datasets. Results in bold indicate the best result per model. <COT> Looking at the "Evaluation (Test) Dataset DRoBERTa" column, we can see that the DRoBERTa model achieves a higher EM score than the DBERT model on the DSQuAD + DRoBERTa dataset.
<R> <C> [EMPTY] <C> #T/S <C> #S/C <C> % SAME <C> LDSC <C> SAMSA <C> SAMSAabl <R> <C> Complex <C> 30.75 <C> 1.18 <C> 100 <C> 0.00 <C> 0.36 <C> 0.94 <R> <C> MinWikiSplit <C> 12.12 <C> 3.84 <C> 0.00 <C> 17.73 <C> 0.40 <C> 0.48 <CAP> Table 4: Results of the automatic evaluation procedure on a random sample of 1000 sentences. <COT> looking at the "Complex" row, the "% SAME" cell value is 0.00, indicating that the Complex dataset has no duplicate sentences.
<R> <C> [EMPTY] <C> #T/S <C> #S/C <C> % SAME <C> LDSC <C> SAMSA <C> SAMSAabl <R> <C> Complex <C> 30.75 <C> 1.18 <C> 100 <C> 0.00 <C> 0.36 <C> 0.94 <R> <C> MinWikiSplit <C> 12.12 <C> 3.84 <C> 0.00 <C> 17.73 <C> 0.40 <C> 0.48 <CAP> Table 4: Results of the automatic evaluation procedure on a random sample of 1000 sentences. <COT> looking at the "MinWikiSplit" row, the "LDSC" cell value is 0.40, indicating a low density of similar sentences in the MinWikiSplit dataset.
<R> <C> G <C> M <C> S <R> <C> [BOLD] 4.36 <C> [BOLD] 4.10 <C> [BOLD] 3.43 <CAP> Table 6: Averaged human evaluation ratings on a random sample of 300 sentences from MinWikiSplit. Grammaticality (G), meaning preservation (M) and structural simplicity (S) are measured using a 1 (very bad) to 5 (very good) scale. <COT> Looking at the "CAP" tag, we can see that Table 6 provides the averaged human evaluation ratings for grammaticality, meaning preservation, and structural simplicity on a random sample of 300 sentences from MinWikiSplit.
<R> <C> G <C> M <C> S <R> <C> [BOLD] 4.36 <C> [BOLD] 4.10 <C> [BOLD] 3.43 <CAP> Table 6: Averaged human evaluation ratings on a random sample of 300 sentences from MinWikiSplit. Grammaticality (G), meaning preservation (M) and structural simplicity (S) are measured using a 1 (very bad) to 5 (very good) scale. <COT> Looking at the "G", "M", and "S" cells, we can see that the average human evaluation ratings for grammaticality, meaning preservation, and structural simplicity are 4.36, 4.10, and 3.43 respectively.
<R> <C> Dataset <C> # Tweets <C> Labels <C> Annotators/Tweet <R> <C> Chatzakou:2017:MBD:3091478.3091487 <C> 9,484 <C> aggressive, bullying, spam, normal <C> 5 <R> <C> DBLP:conf/naacl/WaseemH16 <C> 16, 914 <C> racist, sexist, normal <C> 1 <R> <C> DavidsonWMW17 <C> 24, 802 <C> hateful, offensive (but not hateful), neither <C> 3 or more <R> <C> Golbeck2017 <C> 35,000 <C> the worst, threats, hate speech, direct <C> 2 to 3 <R> <C> Golbeck2017 <C> 35,000 <C> harassment, potentially offensive, non-harassment <C> 2 to 3 <R> <C> FountaDCLBSVSK18 <C> 80, 000 <C> offensive, abusive, hateful speech, <C> 5 to 20 <R> <C> FountaDCLBSVSK18 <C> 80, 000 <C> aggressive, cyberbullying, spam, normal <C> 5 to 20 <R> <C> hatelingo <C> 28,608 <C> directed, generalized + target = archaic, class, disability, <C> 3 <R> <C> hatelingo <C> 28,608 <C> ethnicity, gender, nationality, religion, sexual orientation <C> 3 <R> <C> Ours <C> 13,000 <C> Labels for five different aspects <C> 5 <CAP> Table 1: Comparative table of some of the available hate speech and abusive language corpora in terms of labels and sizes. <COT> Looking at the "Dataset" column, we can see that there are multiple datasets listed, each with different labels and sizes.
<R> <C> Dataset <C> # Tweets <C> Labels <C> Annotators/Tweet <R> <C> Chatzakou:2017:MBD:3091478.3091487 <C> 9,484 <C> aggressive, bullying, spam, normal <C> 5 <R> <C> DBLP:conf/naacl/WaseemH16 <C> 16, 914 <C> racist, sexist, normal <C> 1 <R> <C> DavidsonWMW17 <C> 24, 802 <C> hateful, offensive (but not hateful), neither <C> 3 or more <R> <C> Golbeck2017 <C> 35,000 <C> the worst, threats, hate speech, direct <C> 2 to 3 <R> <C> Golbeck2017 <C> 35,000 <C> harassment, potentially offensive, non-harassment <C> 2 to 3 <R> <C> FountaDCLBSVSK18 <C> 80, 000 <C> offensive, abusive, hateful speech, <C> 5 to 20 <R> <C> FountaDCLBSVSK18 <C> 80, 000 <C> aggressive, cyberbullying, spam, normal <C> 5 to 20 <R> <C> hatelingo <C> 28,608 <C> directed, generalized + target = archaic, class, disability, <C> 3 <R> <C> hatelingo <C> 28,608 <C> ethnicity, gender, nationality, religion, sexual orientation <C> 3 <R> <C> Ours <C> 13,000 <C> Labels for five different aspects <C> 5 <CAP> Table 1: Comparative table of some of the available hate speech and abusive language corpora in terms of labels and sizes. <COT> Looking at the "Annotators/Tweet" column, we can see the number of annotators per tweet for each dataset.
<R> <C> [BOLD] Model <C> [BOLD] Precision(%) <C> [BOLD] Recall@5(%) <C> [BOLD] F1(%) <R> <C> UNC [unc] <C> 36.39 <C> 86.79 <C> 51.38 <R> <C> UCL [ucl] <C> 22.74** <C> 84.54 <C> 35.84 <R> <C> UKP-Athene [athene] <C> 23.67* <C> 85.81* <C> 37.11* <R> <C> DREAM-XLNet [xlnetgraph] <C> 26.60 <C> 87.33 <C> 40.79 <R> <C> DREAM-RoBERTa [xlnetgraph] <C> 26.67 <C> 87.64 <C> 40.90 <R> <C> Pointwise <C> 25.14 <C> 88.25 <C> 39.13 <R> <C> Pointwise + Threshold <C> [BOLD] 38.18 <C> 88.00 <C> [BOLD] 53.25 <R> <C> Pointwise + HNM <C> 25.13 <C> 88.29 <C> 39.13 <R> <C> Pairwise Ranknet <C> 24.97 <C> 88.20 <C> 38.93 <R> <C> Pairwise Ranknet + HNM <C> 24.97 <C> [BOLD] 88.32 <C> 38.93 <R> <C> Pairwise Hinge <C> 24.94 <C> 88.07 <C> 38.88 <R> <C> Pairwise Hinge + HNM <C> 25.01 <C> 88.28 <C> 38.98 <CAP> Table 1: Development set sentence retrieval performance. * We calculated the scores using the official code, and for ** we used the F1 formula to calculate the score. <COT> Looking at the "Model" column, we can see that there are different models listed, each with corresponding precision, recall@5, and F1 scores.
<R> <C> [BOLD] Model <C> [BOLD] Precision(%) <C> [BOLD] Recall@5(%) <C> [BOLD] F1(%) <R> <C> UNC [unc] <C> 36.39 <C> 86.79 <C> 51.38 <R> <C> UCL [ucl] <C> 22.74** <C> 84.54 <C> 35.84 <R> <C> UKP-Athene [athene] <C> 23.67* <C> 85.81* <C> 37.11* <R> <C> DREAM-XLNet [xlnetgraph] <C> 26.60 <C> 87.33 <C> 40.79 <R> <C> DREAM-RoBERTa [xlnetgraph] <C> 26.67 <C> 87.64 <C> 40.90 <R> <C> Pointwise <C> 25.14 <C> 88.25 <C> 39.13 <R> <C> Pointwise + Threshold <C> [BOLD] 38.18 <C> 88.00 <C> [BOLD] 53.25 <R> <C> Pointwise + HNM <C> 25.13 <C> 88.29 <C> 39.13 <R> <C> Pairwise Ranknet <C> 24.97 <C> 88.20 <C> 38.93 <R> <C> Pairwise Ranknet + HNM <C> 24.97 <C> [BOLD] 88.32 <C> 38.93 <R> <C> Pairwise Hinge <C> 24.94 <C> 88.07 <C> 38.88 <R> <C> Pairwise Hinge + HNM <C> 25.01 <C> 88.28 <C> 38.98 <CAP> Table 1: Development set sentence retrieval performance. * We calculated the scores using the official code, and for ** we used the F1 formula to calculate the score. <COT> Looking at the "Pointwise + Threshold" row, we can see that the precision and F1 scores are marked as bold, indicating that they are the highest in their respective columns.
<R> <C> Attribute <C> Model <C> Macro-F1 EN <C> Macro-F1 FR <C> Macro-F1 AR <C> Macro-F1 Avg <C> Micro-F1 EN <C> Micro-F1 FR <C> Micro-F1 AR <C> Micro-F1 Avg <R> <C> Directness <C> Majority <C> 0.50 <C> 0.11 <C> 0.50 <C> 0.47 <C> 0.79 <C> 0.41 <C> 0.54 <C> 0.58 <R> <C> Directness <C> LR <C> 0.52 <C> 0.50 <C> 0.53 <C> 0.52 <C> 0.79 <C> 0.50 <C> 0.56 <C> 0.62 <R> <C> Directness <C> STSL <C> [BOLD] 0.94 <C> [BOLD] 0.80 <C> [BOLD] 0.84 <C> [BOLD] 0.86 <C> [BOLD] 0.89 <C> [BOLD] 0.69 <C> [BOLD] 0.72 <C> [BOLD] 0.76 <R> <C> Directness <C> MTSL <C> [BOLD] 0.94 <C> 0.65 <C> 0.76 <C> 0.78 <C> [BOLD] 0.89 <C> 0.58 <C> 0.65 <C> 0.70 <R> <C> Directness <C> STML <C> [BOLD] 0.94 <C> 0.79 <C> 0.83 <C> 0.85 <C> 0.88 <C> 0.66 <C> [BOLD] 0.72 <C> 0.75 <R> <C> Directness <C> MTML <C> [BOLD] 0.94 <C> 0.78 <C> 0.74 <C> 0.82 <C> 0.88 <C> 0.66 <C> 0.65 <C> 0.73 <CAP> Table 3: Full evaluation scores of the only binary classification task where the single task single language model consistently outperforms multilingual multitask models. <COT> Looking at the "Macro-F1 Avg" column, we can see that the "STSL" model has the highest average macro-F1 score across all languages.
<R> <C> Attribute <C> Model <C> Macro-F1 EN <C> Macro-F1 FR <C> Macro-F1 AR <C> Macro-F1 Avg <C> Micro-F1 EN <C> Micro-F1 FR <C> Micro-F1 AR <C> Micro-F1 Avg <R> <C> Directness <C> Majority <C> 0.50 <C> 0.11 <C> 0.50 <C> 0.47 <C> 0.79 <C> 0.41 <C> 0.54 <C> 0.58 <R> <C> Directness <C> LR <C> 0.52 <C> 0.50 <C> 0.53 <C> 0.52 <C> 0.79 <C> 0.50 <C> 0.56 <C> 0.62 <R> <C> Directness <C> STSL <C> [BOLD] 0.94 <C> [BOLD] 0.80 <C> [BOLD] 0.84 <C> [BOLD] 0.86 <C> [BOLD] 0.89 <C> [BOLD] 0.69 <C> [BOLD] 0.72 <C> [BOLD] 0.76 <R> <C> Directness <C> MTSL <C> [BOLD] 0.94 <C> 0.65 <C> 0.76 <C> 0.78 <C> [BOLD] 0.89 <C> 0.58 <C> 0.65 <C> 0.70 <R> <C> Directness <C> STML <C> [BOLD] 0.94 <C> 0.79 <C> 0.83 <C> 0.85 <C> 0.88 <C> 0.66 <C> [BOLD] 0.72 <C> 0.75 <R> <C> Directness <C> MTML <C> [BOLD] 0.94 <C> 0.78 <C> 0.74 <C> 0.82 <C> 0.88 <C> 0.66 <C> 0.65 <C> 0.73 <CAP> Table 3: Full evaluation scores of the only binary classification task where the single task single language model consistently outperforms multilingual multitask models. <COT> Looking at the "Micro-F1 Avg" column, we can see that the "MTML" model has the lowest average micro-F1 score across all languages.
<R> <C> Attribute <C> Model <C> Macro-F1 EN <C> Macro-F1 FR <C> Macro-F1 AR <C> Macro-F1 Avg <C> Micro-F1 EN <C> Micro-F1 FR <C> Micro-F1 AR <C> Micro-F1 Avg <R> <C> Tweet <C> Majority <C> 0.24 <C> 0.19 <C> 0.20 <C> 0.21 <C> 0.41 <C> 0.27 <C> 0.27 <C> 0.32 <R> <C> Tweet <C> LR <C> 0.14 <C> 0.20 <C> 0.25 <C> 0.20 <C> 0.54 <C> 0.56 <C> [BOLD] 0.48 <C> 0.53 <R> <C> Tweet <C> STSL <C> 0.24 <C> 0.12 <C> 0.31 <C> 0.23 <C> 0.49 <C> 0.51 <C> 0.47 <C> 0.49 <R> <C> Tweet <C> MTSL <C> 0.09 <C> 0.20 <C> 0.33 <C> 0.21 <C> [BOLD] 0.55 <C> [BOLD] 0.59 <C> 0.46 <C> [BOLD] 0.54 <R> <C> Tweet <C> STML <C> 0.04 <C> 0.07 <C> [BOLD] 0.35 <C> 0.16 <C> 0.54 <C> 0.47 <C> 0.37 <C> 0.46 <R> <C> Tweet <C> MTML <C> [BOLD] 0.30 <C> [BOLD] 0.28 <C> [BOLD] 0.35 <C> [BOLD] 0.31 <C> 0.45 <C> 0.48 <C> 0.44 <C> 0.46 <R> <C> Target Attribute <C> Majority <C> 0.15 <C> 0.13 <C> 0.28 <C> 0.19 <C> 0.25 <C> 0.32 <C> 0.40 <C> 0.32 <R> <C> Target Attribute <C> LR <C> 0.41 <C> 0.35 <C> 0.47 <C> 0.41 <C> 0.52 <C> 0.55 <C> 0.53 <C> 0.53 <R> <C> Target Attribute <C> STSL <C> 0.42 <C> 0.18 <C> [BOLD] 0.63 <C> 0.41 <C> [BOLD] 0.68 <C> 0.71 <C> 0.50 <C> 0.63 <R> <C> Target Attribute <C> MTSL <C> 0.41 <C> [BOLD] 0.43 <C> 0.41 <C> [BOLD] 0.42 <C> [BOLD] 0.68 <C> 0.67 <C> [BOLD] 0.56 <C> [BOLD] 0.64 <R> <C> Target Attribute <C> STML <C> 0.39 <C> 0.09 <C> 0.24 <C> 0.24 <C> 0.67 <C> 0.62 <C> 0.53 <C> 0.61 <R> <C> Target Attribute <C> MTML <C> [BOLD] 0.43 <C> 0.24 <C> 0.16 <C> 0.28 <C> 0.66 <C> [BOLD] 0.72 <C> 0.51 <C> 0.63 <R> <C> Target Group <C> Majority <C> 0.07 <C> 0.06 <C> 0.08 <C> 0.07 <C> 0.18 <C> 0.14 <C> 0.35 <C> 0.22 <R> <C> Target Group <C> LR <C> [BOLD] 0.18 <C> 0.33 <C> [BOLD] 0.40 <C> [BOLD] 0.30 <C> 0.34 <C> 0.40 <C> 0.62 <C> 0.46 <R> <C> Target Group <C> STSL <C> 0.04 <C> 0.21 <C> 0.04 <C> 0.10 <C> 0.48 <C> [BOLD] 0.59 <C> 0.58 <C> 0.55 <R> <C> Target Group <C> MTSL <C> 0.04 <C> 0.27 <C> 0.15 <C> 0.15 <C> [BOLD] 0.50 <C> 0.54 <C> 0.55 <C> 0.53 <R> <C> Target Group <C> STML <C> 0.11 <C> [BOLD] 0.37 <C> 0.13 <C> 0.20 <C> 0.49 <C> 0.57 <C> [BOLD] 0.64 <C> [BOLD] 0.56 <R> <C> Target Group <C> MTML <C> 0.06 <C> 0.19 <C> 0.10 <C> 0.11 <C> [BOLD] 0.50 <C> 0.54 <C> 0.56 <C> 0.53 <R> <C> Annotator’s Sentiment <C> Majority <C> 0.42 <C> 0.21 <C> 0.17 <C> 0.27 <C> 0.46 <C> 0.31 <C> 0.32 <C> 0.39 <R> <C> Annotator’s Sentiment <C> LR <C> 0.29 <C> 0.15 <C> 0.14 <C> 0.19 <C> 0.45 <C> 0.30 <C> 0.46 <C> 0.40 <R> <C> Annotator’s Sentiment <C> STSL <C> [BOLD] 0.57 <C> [BOLD] 0.30 <C> 0.12 <C> [BOLD] 0.33 <C> 0.57 <C> 0.39 <C> [BOLD] 0.48 <C> 0.48 <R> <C> Annotator’s Sentiment <C> MTSL <C> [BOLD] 0.57 <C> 0.17 <C> 0.17 <C> 0.30 <C> 0.57 <C> [BOLD] 0.50 <C> 0.45 <C> 0.51 <R> <C> Annotator’s Sentiment <C> STML <C> 0.47 <C> 0.22 <C> 0.13 <C> 0.27 <C> [BOLD] 0.59 <C> 0.49 <C> [BOLD] 0.48 <C> [BOLD] 0.52 <R> <C> Annotator’s Sentiment <C> MTML <C> 0.55 <C> 0.20 <C> [BOLD] 0.21 <C> 0.32 <C> 0.58 <C> 0.45 <C> 0.45 <C> 0.49 <CAP> Table 4: Full evaluation of tasks where multilingual and multitask models outperform on average single task single language model on four different tasks. <COT> Looking at the "Micro-F1 Avg" column, we can see that the MTSL model has the highest average micro-F1 score across all languages.
<R> <C> Attribute <C> Model <C> Macro-F1 EN <C> Macro-F1 FR <C> Macro-F1 AR <C> Macro-F1 Avg <C> Micro-F1 EN <C> Micro-F1 FR <C> Micro-F1 AR <C> Micro-F1 Avg <R> <C> Tweet <C> Majority <C> 0.24 <C> 0.19 <C> 0.20 <C> 0.21 <C> 0.41 <C> 0.27 <C> 0.27 <C> 0.32 <R> <C> Tweet <C> LR <C> 0.14 <C> 0.20 <C> 0.25 <C> 0.20 <C> 0.54 <C> 0.56 <C> [BOLD] 0.48 <C> 0.53 <R> <C> Tweet <C> STSL <C> 0.24 <C> 0.12 <C> 0.31 <C> 0.23 <C> 0.49 <C> 0.51 <C> 0.47 <C> 0.49 <R> <C> Tweet <C> MTSL <C> 0.09 <C> 0.20 <C> 0.33 <C> 0.21 <C> [BOLD] 0.55 <C> [BOLD] 0.59 <C> 0.46 <C> [BOLD] 0.54 <R> <C> Tweet <C> STML <C> 0.04 <C> 0.07 <C> [BOLD] 0.35 <C> 0.16 <C> 0.54 <C> 0.47 <C> 0.37 <C> 0.46 <R> <C> Tweet <C> MTML <C> [BOLD] 0.30 <C> [BOLD] 0.28 <C> [BOLD] 0.35 <C> [BOLD] 0.31 <C> 0.45 <C> 0.48 <C> 0.44 <C> 0.46 <R> <C> Target Attribute <C> Majority <C> 0.15 <C> 0.13 <C> 0.28 <C> 0.19 <C> 0.25 <C> 0.32 <C> 0.40 <C> 0.32 <R> <C> Target Attribute <C> LR <C> 0.41 <C> 0.35 <C> 0.47 <C> 0.41 <C> 0.52 <C> 0.55 <C> 0.53 <C> 0.53 <R> <C> Target Attribute <C> STSL <C> 0.42 <C> 0.18 <C> [BOLD] 0.63 <C> 0.41 <C> [BOLD] 0.68 <C> 0.71 <C> 0.50 <C> 0.63 <R> <C> Target Attribute <C> MTSL <C> 0.41 <C> [BOLD] 0.43 <C> 0.41 <C> [BOLD] 0.42 <C> [BOLD] 0.68 <C> 0.67 <C> [BOLD] 0.56 <C> [BOLD] 0.64 <R> <C> Target Attribute <C> STML <C> 0.39 <C> 0.09 <C> 0.24 <C> 0.24 <C> 0.67 <C> 0.62 <C> 0.53 <C> 0.61 <R> <C> Target Attribute <C> MTML <C> [BOLD] 0.43 <C> 0.24 <C> 0.16 <C> 0.28 <C> 0.66 <C> [BOLD] 0.72 <C> 0.51 <C> 0.63 <R> <C> Target Group <C> Majority <C> 0.07 <C> 0.06 <C> 0.08 <C> 0.07 <C> 0.18 <C> 0.14 <C> 0.35 <C> 0.22 <R> <C> Target Group <C> LR <C> [BOLD] 0.18 <C> 0.33 <C> [BOLD] 0.40 <C> [BOLD] 0.30 <C> 0.34 <C> 0.40 <C> 0.62 <C> 0.46 <R> <C> Target Group <C> STSL <C> 0.04 <C> 0.21 <C> 0.04 <C> 0.10 <C> 0.48 <C> [BOLD] 0.59 <C> 0.58 <C> 0.55 <R> <C> Target Group <C> MTSL <C> 0.04 <C> 0.27 <C> 0.15 <C> 0.15 <C> [BOLD] 0.50 <C> 0.54 <C> 0.55 <C> 0.53 <R> <C> Target Group <C> STML <C> 0.11 <C> [BOLD] 0.37 <C> 0.13 <C> 0.20 <C> 0.49 <C> 0.57 <C> [BOLD] 0.64 <C> [BOLD] 0.56 <R> <C> Target Group <C> MTML <C> 0.06 <C> 0.19 <C> 0.10 <C> 0.11 <C> [BOLD] 0.50 <C> 0.54 <C> 0.56 <C> 0.53 <R> <C> Annotator’s Sentiment <C> Majority <C> 0.42 <C> 0.21 <C> 0.17 <C> 0.27 <C> 0.46 <C> 0.31 <C> 0.32 <C> 0.39 <R> <C> Annotator’s Sentiment <C> LR <C> 0.29 <C> 0.15 <C> 0.14 <C> 0.19 <C> 0.45 <C> 0.30 <C> 0.46 <C> 0.40 <R> <C> Annotator’s Sentiment <C> STSL <C> [BOLD] 0.57 <C> [BOLD] 0.30 <C> 0.12 <C> [BOLD] 0.33 <C> 0.57 <C> 0.39 <C> [BOLD] 0.48 <C> 0.48 <R> <C> Annotator’s Sentiment <C> MTSL <C> [BOLD] 0.57 <C> 0.17 <C> 0.17 <C> 0.30 <C> 0.57 <C> [BOLD] 0.50 <C> 0.45 <C> 0.51 <R> <C> Annotator’s Sentiment <C> STML <C> 0.47 <C> 0.22 <C> 0.13 <C> 0.27 <C> [BOLD] 0.59 <C> 0.49 <C> [BOLD] 0.48 <C> [BOLD] 0.52 <R> <C> Annotator’s Sentiment <C> MTML <C> 0.55 <C> 0.20 <C> [BOLD] 0.21 <C> 0.32 <C> 0.58 <C> 0.45 <C> 0.45 <C> 0.49 <CAP> Table 4: Full evaluation of tasks where multilingual and multitask models outperform on average single task single language model on four different tasks. <COT> Looking at the "Macro-F1 Avg" column, we can see that the STSL model has the highest average macro-F1 score across all languages.
<R> <C> [EMPTY] <C> French F1 <C> French EM <C> Japanese F1 <C> Japanese EM <R> <C> Baseline <C> 61.88 <C> 40.67 <C> 52.19 <C> 37.00 <R> <C> Multilingual BERT <C> [BOLD] 76.65 <C> [BOLD] 61.77 <C> [BOLD] 61.83 <C> [BOLD] 59.94 <CAP> Table 1: Comparison of Exact Match and F1-score of multilingual BERT and the baseline on French and Japanese SQuAD. F1 and EM are the two official metrics of the SQuAD benchmark. EM measures the percentage of predictions that match exactly the ground-truth location of the answer. F1 measures the average overlap between the prediction and ground truth answer. <COT> Looking at the "Multilingual BERT" row, the "French F1" cell, and the "Japanese F1" cell, we can see that Multilingual BERT outperforms the baseline in terms of F1-score for both French and Japanese SQuAD datasets.
<R> <C> [EMPTY] <C> French F1 <C> French EM <C> Japanese F1 <C> Japanese EM <R> <C> Baseline <C> 61.88 <C> 40.67 <C> 52.19 <C> 37.00 <R> <C> Multilingual BERT <C> [BOLD] 76.65 <C> [BOLD] 61.77 <C> [BOLD] 61.83 <C> [BOLD] 59.94 <CAP> Table 1: Comparison of Exact Match and F1-score of multilingual BERT and the baseline on French and Japanese SQuAD. F1 and EM are the two official metrics of the SQuAD benchmark. EM measures the percentage of predictions that match exactly the ground-truth location of the answer. F1 measures the average overlap between the prediction and ground truth answer. <COT> Looking at the "Multilingual BERT" row, the "French EM" cell, and the "Japanese EM" cell, we can see that Multilingual BERT outperforms the baseline in terms of Exact Match (EM) score for both French and Japanese SQuAD datasets.
<R> <C> [EMPTY] <C> Question <C> En F1 <C> En EM <C> Fr F1 <C> Fr EM <C> Jap F1 <C> Jap EM <R> <C> [EMPTY] <C> En <C> 90.57 <C> [BOLD] 81.96 <C> 78.55 <C> [BOLD] 67.28 <C> 66.22 <C> 52.91 <R> <C> Context <C> Fr <C> 81.10 <C> 65.14 <C> 76.65 <C> 61.77 <C> 60.28 <C> 42.20 <R> <C> [EMPTY] <C> Jap <C> 58.95 <C> 57.49 <C> 47.19 <C> 45.26 <C> 61.83 <C> [BOLD] 59.93 <CAP> Table 2: Exact Match and F1-score of multilingual BERT on each of the cross-lingual SQuAD datasets. The row language is the one of the paragraph and the column language is the one of the question. The figures in bold are the best exact match, for each language, among the datasets where they occur. <COT> Looking at the "CAP" tag, we can see that the table shows the Exact Match and F1-score of multilingual BERT on each of the cross-lingual SQuAD datasets.
<R> <C> [EMPTY] <C> Question <C> En F1 <C> En EM <C> Fr F1 <C> Fr EM <C> Jap F1 <C> Jap EM <R> <C> [EMPTY] <C> En <C> 90.57 <C> [BOLD] 81.96 <C> 78.55 <C> [BOLD] 67.28 <C> 66.22 <C> 52.91 <R> <C> Context <C> Fr <C> 81.10 <C> 65.14 <C> 76.65 <C> 61.77 <C> 60.28 <C> 42.20 <R> <C> [EMPTY] <C> Jap <C> 58.95 <C> 57.49 <C> 47.19 <C> 45.26 <C> 61.83 <C> [BOLD] 59.93 <CAP> Table 2: Exact Match and F1-score of multilingual BERT on each of the cross-lingual SQuAD datasets. The row language is the one of the paragraph and the column language is the one of the question. The figures in bold are the best exact match, for each language, among the datasets where they occur. <COT> Looking at the "Fr F1" cell, we can see that the F1 score for French is 67.28.
<R> <C> [BOLD] Model <C> [BOLD] FEVER Score(%) <C> [BOLD] Label Accuracy(%) <R> <C> UNC [unc] <C> 66.14 <C> 69.60 <R> <C> UCL [ucl] <C> 65.41 <C> 69.66 <R> <C> UKP-Athene [athene] <C> 64.74 <C> - <R> <C> BERT & UKP-Athene <C> 69.79 <C> 71.70 <R> <C> BERT Large & UKP-Athene <C> 70.64 <C> 72.72 <R> <C> BERT & BERT (Pointwise) <C> 71.38 <C> 73.51 <R> <C> BERT & BERT (Pointwise + HNM) <C> 71.33 <C> 73.54 <R> <C> BERT (Large) & BERT (Pointwise) <C> [BOLD] 72.42 <C> 74.58 <R> <C> BERT (Large) & BERT (Pointwise + HNM) <C> [BOLD] 72.42 <C> [BOLD] 74.59 <R> <C> BERT & BERT (Pairwise Ranknet) <C> 71.02 <C> 73.22 <R> <C> BERT & BERT (Pairwise Ranknet + HNM) <C> 70.99 <C> 73.02 <R> <C> BERT & BERT (Pairwise Hinge) <C> 71.60 <C> 72.74 <R> <C> BERT & BERT (Pairwise Hinge + HNM) <C> 70.70 <C> 72.76 <CAP> Table 2: Development set verification scores. <COT> Looking at the "BERT (Large) & BERT (Pointwise)" cell, "[BOLD] 72.42" cell, and "FEVER Score(%)" column, we can see that the FEVER score is higher for the combination of BERT (Large) and BERT (Pointwise) compared to other models.
<R> <C> [BOLD] Model <C> [BOLD] FEVER Score(%) <C> [BOLD] Label Accuracy(%) <R> <C> UNC [unc] <C> 66.14 <C> 69.60 <R> <C> UCL [ucl] <C> 65.41 <C> 69.66 <R> <C> UKP-Athene [athene] <C> 64.74 <C> - <R> <C> BERT & UKP-Athene <C> 69.79 <C> 71.70 <R> <C> BERT Large & UKP-Athene <C> 70.64 <C> 72.72 <R> <C> BERT & BERT (Pointwise) <C> 71.38 <C> 73.51 <R> <C> BERT & BERT (Pointwise + HNM) <C> 71.33 <C> 73.54 <R> <C> BERT (Large) & BERT (Pointwise) <C> [BOLD] 72.42 <C> 74.58 <R> <C> BERT (Large) & BERT (Pointwise + HNM) <C> [BOLD] 72.42 <C> [BOLD] 74.59 <R> <C> BERT & BERT (Pairwise Ranknet) <C> 71.02 <C> 73.22 <R> <C> BERT & BERT (Pairwise Ranknet + HNM) <C> 70.99 <C> 73.02 <R> <C> BERT & BERT (Pairwise Hinge) <C> 71.60 <C> 72.74 <R> <C> BERT & BERT (Pairwise Hinge + HNM) <C> 70.70 <C> 72.76 <CAP> Table 2: Development set verification scores. <COT> Looking at the "BERT (Large) & BERT (Pointwise + HNM)" cell, "[BOLD] 72.42" cell, and "Label Accuracy(%)" column, we can see that the label accuracy is higher for the combination of BERT (Large) and BERT (Pointwise + HNM) compared to other models.
<R> <C> Track <C> Target <C> Constrained <C> Valid <C> Test <R> <C> NLG <C> EN <C> no <C> 23.5 <C> 20.5 <R> <C> MT <C> EN <C> yes <C> 60.2 <C> 58.2 <R> <C> MT <C> EN <C> no <C> 64.2 <C> 62.2 <R> <C> MT+NLG <C> EN <C> yes <C> 64.4 <C> 62.2 <R> <C> NLG <C> DE <C> no <C> 16.9 <C> 16.1 <R> <C> MT <C> DE <C> yes <C> 49.8 <C> 48.0 <R> <C> MT+NLG <C> DE <C> yes <C> 49.4 <C> 48.2 <CAP> Table 3: Doc-level BLEU scores on the DGT valid and test sets of our submitted models in all tracks. <COT> Looking at the "Track" column, we can see that there are three different tracks: NLG, MT, and MT+NLG.
<R> <C> Track <C> Target <C> Constrained <C> Valid <C> Test <R> <C> NLG <C> EN <C> no <C> 23.5 <C> 20.5 <R> <C> MT <C> EN <C> yes <C> 60.2 <C> 58.2 <R> <C> MT <C> EN <C> no <C> 64.2 <C> 62.2 <R> <C> MT+NLG <C> EN <C> yes <C> 64.4 <C> 62.2 <R> <C> NLG <C> DE <C> no <C> 16.9 <C> 16.1 <R> <C> MT <C> DE <C> yes <C> 49.8 <C> 48.0 <R> <C> MT+NLG <C> DE <C> yes <C> 49.4 <C> 48.2 <CAP> Table 3: Doc-level BLEU scores on the DGT valid and test sets of our submitted models in all tracks. <COT> Looking at the "Constrained" column, we can observe that some models are constrained (yes) while others are not constrained (no).
<R> <C> Model <C> Rotowire test <R> <C> Wiseman et al. ( 2017 ) <C> 14.5 <R> <C> Puduppully et al. ( 2019 ) <C> 16.5 <R> <C> Ours (4-player) <C> 22.2 <CAP> Table 6: English NLG comparison against state-of-the-art on Rotowire-test. BLEU of submitted NLG (EN) model, averaged over 3 runs. Because Rotowire tokenization is slightly different, we apply a set of fixes to the model outputs (e.g., 1-of-3 → 1 - of - 3). <COT> Looking at the "CAP" tag, we can see that Table 6 provides a comparison of English NLG models against state-of-the-art on the Rotowire-test dataset.
<R> <C> Model <C> Rotowire test <R> <C> Wiseman et al. ( 2017 ) <C> 14.5 <R> <C> Puduppully et al. ( 2019 ) <C> 16.5 <R> <C> Ours (4-player) <C> 22.2 <CAP> Table 6: English NLG comparison against state-of-the-art on Rotowire-test. BLEU of submitted NLG (EN) model, averaged over 3 runs. Because Rotowire tokenization is slightly different, we apply a set of fixes to the model outputs (e.g., 1-of-3 → 1 - of - 3). <COT> Looking at the "Ours (4-player)" row and the "Rotowire test" column, we can see that the BLEU score for the "Ours (4-player)" model on the Rotowire-test dataset is 22.2.
<R> <C> Model <C> Valid <C> Test <R> <C> Baseline (3 players, sorted) <C> [BOLD] 22.7 <C> 20.4 <R> <C> No player <C> 20.1 <C> 18.8 <R> <C> All players, sorted <C> [BOLD] 22.7 <C> 20.9 <R> <C> All players, shuffled <C> 22.0 <C> 20.0 <R> <C> (1) No next game <C> 22.0 <C> 19.9 <R> <C> (2) No week day <C> 22.2 <C> 20.5 <R> <C> (3) No player position <C> 22.6 <C> 20.5 <R> <C> (4) No team-level sums <C> 22.5 <C> 20.5 <R> <C> (5) Remove most tags <C> 22.6 <C> 20.8 <R> <C> (1) to (5) <C> 21.3 <C> 19.7 <CAP> Table 7: English NLG ablation study, starting from a 3 best player baseline (the submitted NLG model has 4 players). BLEU averages over 3 runs. Standard deviation ranges between 0.1 and 0.4. <COT> Looking at the "Baseline (3 players, sorted)" row, the "Valid" and "Test" columns show that the baseline model achieved a BLEU score of 22.7 on the validation set and 20.4 on the test set.
<R> <C> Model <C> Valid <C> Test <R> <C> Baseline (3 players, sorted) <C> [BOLD] 22.7 <C> 20.4 <R> <C> No player <C> 20.1 <C> 18.8 <R> <C> All players, sorted <C> [BOLD] 22.7 <C> 20.9 <R> <C> All players, shuffled <C> 22.0 <C> 20.0 <R> <C> (1) No next game <C> 22.0 <C> 19.9 <R> <C> (2) No week day <C> 22.2 <C> 20.5 <R> <C> (3) No player position <C> 22.6 <C> 20.5 <R> <C> (4) No team-level sums <C> 22.5 <C> 20.5 <R> <C> (5) Remove most tags <C> 22.6 <C> 20.8 <R> <C> (1) to (5) <C> 21.3 <C> 19.7 <CAP> Table 7: English NLG ablation study, starting from a 3 best player baseline (the submitted NLG model has 4 players). BLEU averages over 3 runs. Standard deviation ranges between 0.1 and 0.4. <COT> Looking at the "(1) to (5)" row, the "Valid" and "Test" columns show that when all the ablations (1) to (5) are applied to the model, the BLEU score decreases to 21.3 on the validation set and 19.7 on the test set.
<R> <C> [EMPTY] <C> TnT <C> neural in-lang. plain <C> neural in-lang. +Poly <C> neural transfer +Medium src <C> neural transfer +Large src <C> neural transfer FineTune <R> <C> zero-shot <C> — <C> — <C> — <C> 58.29 <C> 61.18 <C> — <R> <C> Tiny <C> 37.48 <C> 36.17 <C> 56.05 <C> 67.14 <C> 67.49 <C> 62.07 <R> <C> Small <C> 44.30 <C> 51.90 <C> 67.18 <C> [BOLD] 70.82 <C> 70.01 <C> 65.63 <CAP> Table 3: F1 score on the development set for low-resource training setups (none, tiny 5k or small 10k labeled Danish sentences). Transfer via multilingual embeddings from Medium (3.2k sentences, 51k tokens) or Large English source data (14k sentences/203k tokens). <COT> Looking at the caption, we can see that Table 3 shows the F1 score on the development set for low-resource training setups using different transfer methods and different sizes of labeled Danish sentences.
<R> <C> [EMPTY] <C> TnT <C> neural in-lang. plain <C> neural in-lang. +Poly <C> neural transfer +Medium src <C> neural transfer +Large src <C> neural transfer FineTune <R> <C> zero-shot <C> — <C> — <C> — <C> 58.29 <C> 61.18 <C> — <R> <C> Tiny <C> 37.48 <C> 36.17 <C> 56.05 <C> 67.14 <C> 67.49 <C> 62.07 <R> <C> Small <C> 44.30 <C> 51.90 <C> 67.18 <C> [BOLD] 70.82 <C> 70.01 <C> 65.63 <CAP> Table 3: F1 score on the development set for low-resource training setups (none, tiny 5k or small 10k labeled Danish sentences). Transfer via multilingual embeddings from Medium (3.2k sentences, 51k tokens) or Large English source data (14k sentences/203k tokens). <COT> Looking at the "neural transfer +Large src" column, we can see that the F1 score for the "Small" setup is [BOLD] 70.82.
<R> <C> Dev <C> All <C> PER <C> LOC <C> ORG <C> MISC <R> <C> Majority <C> 44.4 <C> 61.8 <C> 0.0 <C> 0.0 <C> — <R> <C> DKIE <C> 58.9 <C> 68.9 <C> 63.6 <C> 23.3 <C> — <R> <C> Polyglot <C> 64.5 <C> 73.7 <C> [BOLD] 73.4 <C> 36.8 <C> — <R> <C> Bilstm <C> [BOLD] 70.8 <C> [BOLD] 83.3 <C> 71.8 <C> [BOLD] 60.0 <C> 23.9 <R> <C> Test <C> All <C> PER <C> LOC <C> ORG <C> MISC <R> <C> Polyglot <C> 61.6 <C> 78.4 <C> [BOLD] 69.7 <C> 24.7 <C> — <R> <C> Bilstm <C> [BOLD] 66.0 <C> [BOLD] 86.6 <C> 63.6 <C> [BOLD] 42.5 <C> 24.8 <CAP> Table 4: F1 score for Danish NER. <COT> Looking at the "Dev" column, we can see that the Bilstm model achieves the highest F1 score of 70.8, and in the "Test" column, the Bilstm model also achieves the highest F1 score of 66.0.
<R> <C> Dev <C> All <C> PER <C> LOC <C> ORG <C> MISC <R> <C> Majority <C> 44.4 <C> 61.8 <C> 0.0 <C> 0.0 <C> — <R> <C> DKIE <C> 58.9 <C> 68.9 <C> 63.6 <C> 23.3 <C> — <R> <C> Polyglot <C> 64.5 <C> 73.7 <C> [BOLD] 73.4 <C> 36.8 <C> — <R> <C> Bilstm <C> [BOLD] 70.8 <C> [BOLD] 83.3 <C> 71.8 <C> [BOLD] 60.0 <C> 23.9 <R> <C> Test <C> All <C> PER <C> LOC <C> ORG <C> MISC <R> <C> Polyglot <C> 61.6 <C> 78.4 <C> [BOLD] 69.7 <C> 24.7 <C> — <R> <C> Bilstm <C> [BOLD] 66.0 <C> [BOLD] 86.6 <C> 63.6 <C> [BOLD] 42.5 <C> 24.8 <CAP> Table 4: F1 score for Danish NER. <COT> Looking at the "PER" column, we can see that the Polyglot model achieves the highest F1 score of 73.4 in the "Dev" row, and in the "Test" row, the Polyglot model achieves the highest F1 score of 69.7.
<R> <C> [BOLD] Model <C> [BOLD] Inspec  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Inspec  [ITALIC] F1@5 <C> [BOLD] Krapivin  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Krapivin  [ITALIC] F1@5 <C> [BOLD] NUS  [ITALIC] F1@ [ITALIC] M <C> [BOLD] NUS  [ITALIC] F1@5 <C> [BOLD] SemEval  [ITALIC] F1@ [ITALIC] M <C> [BOLD] SemEval  [ITALIC] F1@5 <C> [BOLD] KP20k  [ITALIC] F1@ [ITALIC] M <C> [BOLD] KP20k  [ITALIC] F1@5 <R> <C> catSeq <C> 0.262 <C> 0.225 <C> 0.354 <C> 0.269 <C> 0.397 <C> 0.323 <C> 0.283 <C> 0.242 <C> 0.367 <C> 0.291 <R> <C> catSeqD <C> 0.263 <C> 0.219 <C> 0.349 <C> 0.264 <C> 0.394 <C> 0.321 <C> 0.274 <C> 0.233 <C> 0.363 <C> 0.285 <R> <C> catSeqCorr <C> 0.269 <C> 0.227 <C> 0.349 <C> 0.265 <C> 0.390 <C> 0.319 <C> 0.290 <C> 0.246 <C> 0.365 <C> 0.289 <R> <C> catSeqTG <C> 0.270 <C> 0.229 <C> 0.366 <C> 0.282 <C> 0.393 <C> 0.325 <C> 0.290 <C> 0.246 <C> 0.366 <C> 0.292 <R> <C> catSeq-2 [ITALIC] RF1 <C> 0.300 <C> 0.250 <C> 0.362 <C> 0.287 <C> 0.426 <C> 0.364 <C> 0.327 <C> 0.285 <C> 0.383 <C> 0.310 <R> <C> catSeqD-2 [ITALIC] RF1 <C> 0.292 <C> 0.242 <C> 0.360 <C> 0.282 <C> 0.419 <C> 0.353 <C> 0.316 <C> 0.272 <C> 0.379 <C> 0.305 <R> <C> catSeqCorr-2 [ITALIC] RF1 <C> 0.291 <C> 0.240 <C> [BOLD] 0.369 <C> 0.286 <C> 0.414 <C> 0.349 <C> 0.322 <C> 0.278 <C> 0.382 <C> 0.308 <R> <C> catSeqTG-2 [ITALIC] RF1 <C> [BOLD] 0.301 <C> [BOLD] 0.253 <C> [BOLD] 0.369 <C> [BOLD] 0.300 <C> [BOLD] 0.433 <C> [BOLD] 0.375 <C> [BOLD] 0.329 <C> [BOLD] 0.287 <C> [BOLD] 0.386 <C> [BOLD] 0.321 <CAP> Table 2: Results of present keyphrase prediction on five datasets. Suffix “-2RF1” denotes that a model is trained by our reinforcement learning approach. <COT> Looking at the "catSeq-2 RF1" row, "0.300" cell, "Krapivin F1@5" cell, and "catSeqTG-2 RF1" row, "[BOLD] 0.375" cell, "Krapivin F1@5" cell, we can see that catSeqTG-2 RF1 performs better on the Krapivin dataset than catSeq-2 RF1.
<R> <C> [BOLD] Model <C> [BOLD] Present MAE <C> [BOLD] Present Avg. # <C> [BOLD] Absent MAE <C> [BOLD] Absent Avg. # <R> <C> oracle <C> 0.000 <C> 2.837 <C> 0.000 <C> 2.432 <R> <C> catSeq <C> 2.271 <C> 3.781 <C> 1.943 <C> 0.659 <R> <C> catSeqD <C> 2.225 <C> 3.694 <C> 1.961 <C> 0.629 <R> <C> catSeqCorr <C> 2.292 <C> 3.790 <C> 1.914 <C> 0.703 <R> <C> catSeqTG <C> 2.276 <C> 3.780 <C> 1.956 <C> 0.638 <R> <C> catSeq-2 [ITALIC] RF1 <C> 2.118 <C> 3.733 <C> 1.494 <C> 1.574 <R> <C> catSeqD-2 [ITALIC] RF1 <C> [BOLD] 2.087 <C> [BOLD] 3.666 <C> 1.541 <C> 1.455 <R> <C> catSeqCorr-2 [ITALIC] RF1 <C> 2.107 <C> 3.696 <C> 1.557 <C> 1.409 <R> <C> catSeqTG-2 [ITALIC] RF1 <C> 2.204 <C> 3.865 <C> [BOLD] 1.439 <C> [BOLD] 1.749 <CAP> Table 4: The abilities of predicting the correct number of keyphrases on the KP20k dataset. MAE denotes the mean absolute error (the lower the better), Avg. # denotes the average number of generated keyphrases per document. <COT> Looking at the "Model" column, "catSeqTG-2 [ITALIC] RF1" cell, and "Present MAE" column, we can see that the "catSeqTG-2 [ITALIC] RF1" model has the lowest Present MAE value.
<R> <C> [BOLD] Model <C> [BOLD] Present MAE <C> [BOLD] Present Avg. # <C> [BOLD] Absent MAE <C> [BOLD] Absent Avg. # <R> <C> oracle <C> 0.000 <C> 2.837 <C> 0.000 <C> 2.432 <R> <C> catSeq <C> 2.271 <C> 3.781 <C> 1.943 <C> 0.659 <R> <C> catSeqD <C> 2.225 <C> 3.694 <C> 1.961 <C> 0.629 <R> <C> catSeqCorr <C> 2.292 <C> 3.790 <C> 1.914 <C> 0.703 <R> <C> catSeqTG <C> 2.276 <C> 3.780 <C> 1.956 <C> 0.638 <R> <C> catSeq-2 [ITALIC] RF1 <C> 2.118 <C> 3.733 <C> 1.494 <C> 1.574 <R> <C> catSeqD-2 [ITALIC] RF1 <C> [BOLD] 2.087 <C> [BOLD] 3.666 <C> 1.541 <C> 1.455 <R> <C> catSeqCorr-2 [ITALIC] RF1 <C> 2.107 <C> 3.696 <C> 1.557 <C> 1.409 <R> <C> catSeqTG-2 [ITALIC] RF1 <C> 2.204 <C> 3.865 <C> [BOLD] 1.439 <C> [BOLD] 1.749 <CAP> Table 4: The abilities of predicting the correct number of keyphrases on the KP20k dataset. MAE denotes the mean absolute error (the lower the better), Avg. # denotes the average number of generated keyphrases per document. <COT> Looking at the "Model" column, "catSeqTG-2 [ITALIC] RF1" cell, and "Absent Avg. #" column, we can see that the "catSeqTG-2 [ITALIC] RF1" model has the highest Absent Avg. # value.
<R> <C> [BOLD] Model <C> [BOLD] Present  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Present  [ITALIC] F1@5 <C> [BOLD] Absent  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Absent  [ITALIC] F1@5 <R> <C> catSeq <C> 0.367 <C> 0.291 <C> 0.032 <C> 0.015 <R> <C> catSeq- [ITALIC] RF1 <C> 0.380 <C> 0.336 <C> 0.006 <C> 0.003 <R> <C> catSeq-2 [ITALIC] F1 <C> 0.378 <C> 0.278 <C> 0.042 <C> 0.020 <R> <C> catSeq-2 [ITALIC] RF1 <C> 0.383 <C> 0.310 <C> 0.047 <C> 0.024 <CAP> Table 5: Ablation study on the KP20k dataset. Suffix “-2RF1” denotes our full RL approach. Suffix “-2F1” denotes that we replace our adaptive RF1 reward function in the full approach by an F1 reward function. Suffix “-RF1” denotes that we replace the two separate RF1 reward signals in our full approach with only one RF1 reward signal for all the generated keyphrases. <COT> Looking at the "catSeq" row, we can see that the "Present F1@M" and "Present F1@5" scores are higher when the model is present compared to when it is absent.
<R> <C> [BOLD] Model <C> [BOLD] Present  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Present  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Absent  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Absent  [ITALIC] F1@ [ITALIC] M <R> <C> [BOLD] Model <C> old <C> new <C> old <C> new <R> <C> catSeq <C> 0.367 <C> 0.376 <C> 0.032 <C> 0.034 <R> <C> catSeqD <C> 0.363 <C> 0.372 <C> 0.031 <C> 0.033 <R> <C> catSeqCorr <C> 0.365 <C> 0.375 <C> 0.032 <C> 0.034 <R> <C> catSeqTG <C> 0.366 <C> 0.374 <C> 0.032 <C> 0.033 <R> <C> catSeq-2 [ITALIC] RF1 <C> 0.383 <C> 0.396 <C> 0.047 <C> 0.054 <R> <C> catSeqD-2 [ITALIC] RF1 <C> 0.379 <C> 0.390 <C> 0.046 <C> 0.052 <R> <C> catSeqCorr-2 [ITALIC] RF1 <C> 0.382 <C> 0.393 <C> 0.045 <C> 0.051 <R> <C> catSeqTG-2 [ITALIC] RF1 <C> 0.386 <C> 0.398 <C> 0.050 <C> 0.056 <CAP> Table 6: Keyphrase prediction results on the KP20k dataset with our new evaluation method. <COT> Looking at the "Model" column, we can see that there are different models used for keyphrase prediction.
<R> <C> [BOLD] Model <C> [BOLD] Present  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Present  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Absent  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Absent  [ITALIC] F1@ [ITALIC] M <R> <C> [BOLD] Model <C> old <C> new <C> old <C> new <R> <C> catSeq <C> 0.367 <C> 0.376 <C> 0.032 <C> 0.034 <R> <C> catSeqD <C> 0.363 <C> 0.372 <C> 0.031 <C> 0.033 <R> <C> catSeqCorr <C> 0.365 <C> 0.375 <C> 0.032 <C> 0.034 <R> <C> catSeqTG <C> 0.366 <C> 0.374 <C> 0.032 <C> 0.033 <R> <C> catSeq-2 [ITALIC] RF1 <C> 0.383 <C> 0.396 <C> 0.047 <C> 0.054 <R> <C> catSeqD-2 [ITALIC] RF1 <C> 0.379 <C> 0.390 <C> 0.046 <C> 0.052 <R> <C> catSeqCorr-2 [ITALIC] RF1 <C> 0.382 <C> 0.393 <C> 0.045 <C> 0.051 <R> <C> catSeqTG-2 [ITALIC] RF1 <C> 0.386 <C> 0.398 <C> 0.050 <C> 0.056 <CAP> Table 6: Keyphrase prediction results on the KP20k dataset with our new evaluation method. <COT> Looking at the "Present F1@M" and "Absent F1@M" columns, we can see that the F1@M scores are higher for the "new" models compared to the "old" models.
<R> <C> Model <C> BLEU <C> distinct-1 <C> distinct-2 <C> Avg.length <C> Stopword <C> adv succ <R> <C> Human <C> - <C> 16.8% <C> 58.1% <C> 14.2 <C> 69.8% <C> [EMPTY] <R> <C> AR <C> 1.64 <C> 3.7% <C> 9.5% <C> 6.4 <C> 82.3% <C> 2.7% <R> <C> AR+MMI <C> 2.10 <C> 10.6% <C> 20.5% <C> 7.2 <C> 76.4% <C> 6.3% <R> <C> AR+MMI+diverse <C> 2.16 <C> 16.0% <C> 27.3% <C> 7.5 <C> 72.1% <C> 6.4% <R> <C> AR+MMI+RL <C> 2.34 <C> 13.7% <C> 25.2% <C> 7.3 <C> 73.0% <C> 8.0% <R> <C> NonAR <C> 1.54 <C> 8.9% <C> 14.6% <C> 7.1 <C> 77.9% <C> 2.4% <R> <C> NonAR+MMI <C> 2.68 <C> 15.9% <C> 27.0% <C> 7.4 <C> 71.9% <C> 9.2% <CAP> Table 1: Automatic Metrics Evaluation for Different Models. <COT> Looking at the "Model" column, we can see different models such as AR, AR+MMI, AR+MMI+diverse, AR+MMI+RL, NonAR, and NonAR+MMI. 
<R> <C> Model <C> BLEU <C> distinct-1 <C> distinct-2 <C> Avg.length <C> Stopword <C> adv succ <R> <C> Human <C> - <C> 16.8% <C> 58.1% <C> 14.2 <C> 69.8% <C> [EMPTY] <R> <C> AR <C> 1.64 <C> 3.7% <C> 9.5% <C> 6.4 <C> 82.3% <C> 2.7% <R> <C> AR+MMI <C> 2.10 <C> 10.6% <C> 20.5% <C> 7.2 <C> 76.4% <C> 6.3% <R> <C> AR+MMI+diverse <C> 2.16 <C> 16.0% <C> 27.3% <C> 7.5 <C> 72.1% <C> 6.4% <R> <C> AR+MMI+RL <C> 2.34 <C> 13.7% <C> 25.2% <C> 7.3 <C> 73.0% <C> 8.0% <R> <C> NonAR <C> 1.54 <C> 8.9% <C> 14.6% <C> 7.1 <C> 77.9% <C> 2.4% <R> <C> NonAR+MMI <C> 2.68 <C> 15.9% <C> 27.0% <C> 7.4 <C> 71.9% <C> 9.2% <CAP> Table 1: Automatic Metrics Evaluation for Different Models. <COT> Looking at the "BLEU" column, we can compare the BLEU scores for different models.
<R> <C> Model <C> disagr (%) <C> un(%) <C> agr(%) <R> <C> Coherence <C> Coherence <C> Coherence <C> Coherence <R> <C> Human <C> 17.4 <C> 20.8 <C> 61.8 <R> <C> AR <C> 28.6 <C> 29.5 <C> 41.9 <R> <C> AR+MMI <C> 25.3 <C> 27.9 <C> 46.8 <R> <C> AR+MMI+diverse <C> 24.8 <C> 27.8 <C> 47.4 <R> <C> AR+MMI+RL <C> 24.1 <C> 26.5 <C> 49.4 <R> <C> nonAR <C> 29.9 <C> 28.7 <C> 41.4 <R> <C> nonAR+MMI <C> 23.1 <C> 24.0 <C> 52.9 <R> <C> Content Richness <C> Content Richness <C> Content Richness <C> Content Richness <R> <C> Human <C> 14.0 <C> 16.6 <C> 69.4 <R> <C> AR <C> 38.2 <C> 30.4 <C> 31.4 <R> <C> AR+MMI <C> 30.6 <C> 26.2 <C> 43.2 <R> <C> AR+MMI+diverse <C> 23.9 <C> 21.3 <C> 54.8 <R> <C> AR+MMI+RL <C> 26.4 <C> 24.9 <C> 48.7 <R> <C> NonAR <C> 31.4 <C> 25.0 <C> 44.6 <R> <C> NonAR+MMI <C> 24.2 <C> 20.5 <C> 55.3 <CAP> Table 3: Human judgments for Coherence and Content Richeness of the different models. <COT> Looking at the "Content Richness" column, we can see that the "NonAR" model has the highest percentage of disagreement (31.4%) compared to other models.
<R> <C> [EMPTY] <C> [BOLD] WMT14 En→De <C> [BOLD] WMT14 De→En <C> [BOLD] WMT16 Ro→En <R> <C> NAT (Gu et al.,  2018 ) <C> 17.69 <C> 20.62 <C> 29.79 <R> <C> iNAT (Lee et al.,  2018 ) <C> 21.54 <C> 25.43 <C> 29.32 <R> <C> FlowSeq-large (raw data) (Ma et al.,  2019 ) <C> 20.85 <C> 25.40 <C> 29.86 <R> <C> NAT (our implementation) <C> 22.32 <C> 24.83 <C> 29.93 <R> <C> NAT +MMI <C> 23.80 <C> 26.05 <C> 30.50 <R> <C> [EMPTY] <C> (+1.48) <C> (+1.22) <C> (+0.57) <CAP> Table 4: The performances of NonAR+MMI methods on WMT14 En↔De and WMT16 Ro→En. Results from Gu et al. (2018); Lee et al. (2018); Ma et al. (2019) are copied from original papers for reference purposes. <COT> looking at the "CAP" tag, the table provides the performances of NonAR+MMI methods on WMT14 En↔De and WMT16 Ro→En.
<R> <C> [EMPTY] <C> [BOLD] WMT14 En→De <C> [BOLD] WMT14 De→En <C> [BOLD] WMT16 Ro→En <R> <C> NAT (Gu et al.,  2018 ) <C> 17.69 <C> 20.62 <C> 29.79 <R> <C> iNAT (Lee et al.,  2018 ) <C> 21.54 <C> 25.43 <C> 29.32 <R> <C> FlowSeq-large (raw data) (Ma et al.,  2019 ) <C> 20.85 <C> 25.40 <C> 29.86 <R> <C> NAT (our implementation) <C> 22.32 <C> 24.83 <C> 29.93 <R> <C> NAT +MMI <C> 23.80 <C> 26.05 <C> 30.50 <R> <C> [EMPTY] <C> (+1.48) <C> (+1.22) <C> (+0.57) <CAP> Table 4: The performances of NonAR+MMI methods on WMT14 En↔De and WMT16 Ro→En. Results from Gu et al. (2018); Lee et al. (2018); Ma et al. (2019) are copied from original papers for reference purposes. <COT> looking at the "NAT +MMI" row, the numbers in the parentheses show the improvement of NAT +MMI compared to NAT in terms of performance.
<R> <C> [BOLD] Model <C> [BOLD] W param <C> [BOLD] Cos-d <C> [BOLD] Q1 <C> [BOLD] Q2 <C> [BOLD] Q3 <C> ≤ [BOLD] 5 <R> <C> TransWeight-feat <C> [ITALIC] n+ [ITALIC] n <C> 0.344 <C> 2 <C> 5 <C> 28 <C> 50.82% <R> <C> TransWeight-trans <C> [ITALIC] t+ [ITALIC] n <C> 0.338 <C> 2 <C> 5 <C> 24 <C> 52.90% <R> <C> TransWeight-mat <C> [ITALIC] tn+ [ITALIC] n <C> 0.338 <C> 2 <C> 5 <C> 25 <C> 53.24% <R> <C> [BOLD] TransWeight <C> [ITALIC] tn2+ [ITALIC] n <C> [BOLD] 0.310 <C> [BOLD] 1 <C> [BOLD] 3 <C> [BOLD] 11 <C> [BOLD] 65.21% <CAP> Table 3: Different weighting variations evaluated on the compounds dataset (32,246 nominal compounds). All variations use t=100 transformations, word representations with n=200 dimensions and the dropout rate that was observed to work best on the dev dataset (see Appendix B for details). Results on the 6442 compounds in the test set of the German compounds dataset. <COT> Looking at the "Model" column, we can see that "TransWeight" has the highest value for "Cos-d" compared to other models.
<R> <C> [BOLD] Model <C> [BOLD] W param <C> [BOLD] Cos-d <C> [BOLD] Q1 <C> [BOLD] Q2 <C> [BOLD] Q3 <C> ≤ [BOLD] 5 <R> <C> TransWeight-feat <C> [ITALIC] n+ [ITALIC] n <C> 0.344 <C> 2 <C> 5 <C> 28 <C> 50.82% <R> <C> TransWeight-trans <C> [ITALIC] t+ [ITALIC] n <C> 0.338 <C> 2 <C> 5 <C> 24 <C> 52.90% <R> <C> TransWeight-mat <C> [ITALIC] tn+ [ITALIC] n <C> 0.338 <C> 2 <C> 5 <C> 25 <C> 53.24% <R> <C> [BOLD] TransWeight <C> [ITALIC] tn2+ [ITALIC] n <C> [BOLD] 0.310 <C> [BOLD] 1 <C> [BOLD] 3 <C> [BOLD] 11 <C> [BOLD] 65.21% <CAP> Table 3: Different weighting variations evaluated on the compounds dataset (32,246 nominal compounds). All variations use t=100 transformations, word representations with n=200 dimensions and the dropout rate that was observed to work best on the dev dataset (see Appendix B for details). Results on the 6442 compounds in the test set of the German compounds dataset. <COT> Looking at the "Q3" column, we can see that "TransWeight" has the highest value for "Q3" compared to other models.
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> [ITALIC] CNN <C> [ITALIC] CNN <C> [ITALIC] CNN <C> [ITALIC] CNN <R> <C> +  [ITALIC] ENT-ONLY <C> 52.7 <C> 43.1 <C> 47.4 <R> <C> +  [ITALIC] ENT-SENT <C> 75.8 <C> 60.7 <C> 67.3 <R> <C> +  [ITALIC] ENT-DYM <C> 66.5 <C> 70.6 <C> 68.5 <R> <C> +  [ITALIC] ENT-DEP0 <C> 59.8 <C> 61.5 <C> 60.6 <R> <C> +  [ITALIC] ENT-DEP1 <C> 67.6 <C> 65.1 <C> 66.3 <R> <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <R> <C> +  [ITALIC] ENT-ONLY <C> 74.0 <C> 69.4 <C> 71.6 <R> <C> +  [ITALIC] ENT-SENT <C> 74.8 <C> 71.7 <C> 73.1 <R> <C> +  [ITALIC] ENT-DYM <C> 71.5 <C> 73.4 <C> 72.4 <R> <C> +  [ITALIC] ENT-DEP0 <C> 72.8 <C> 69.4 <C> 71.1 <R> <C> +  [ITALIC] ENT-DEP1 <C> 71.6 <C> 76.4 <C> [BOLD] 73.9 <R> <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <R> <C> +  [ITALIC] ENT-ONLY <C> 69.6 <C> 72.3 <C> 70.9 <R> <C> +  [ITALIC] ENT-SENT <C> 69.4 <C> 74.9 <C> 72.0 <R> <C> +  [ITALIC] ENT-DYM <C> 71.0 <C> 69.7 <C> 71.8 <R> <C> +  [ITALIC] ENT-DEP0 <C> 72.2 <C> 69.5 <C> 70.8 <R> <C> +  [ITALIC] ENT-DEP1 <C> 71.0 <C> 74.3 <C> 72.6 <R> <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <R> <C> +  [ITALIC] ENT-ONLY <C> 69.3 <C> 71.4 <C> 70.4 <R> <C> +  [ITALIC] ENT-SENT <C> 72.2 <C> 71.9 <C> 72.0 <R> <C> +  [ITALIC] ENT-DYM <C> 69.7 <C> 73.9 <C> 71.7 <R> <C> +  [ITALIC] ENT-DEP0 <C> 70.1 <C> 71.1 <C> 70.6 <R> <C> +  [ITALIC] ENT-DEP1 <C> 72.7 <C> 72.9 <C> 72.8 <CAP> Table 1: Results on DDI 2013 <COT> Looking at the table, we can see that the F1 score for the model "BiLSTM" with the feature "ENT-DEP1" is 73.9.
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> [ITALIC] CNN <C> [ITALIC] CNN <C> [ITALIC] CNN <C> [ITALIC] CNN <R> <C> +  [ITALIC] ENT-ONLY <C> 52.7 <C> 43.1 <C> 47.4 <R> <C> +  [ITALIC] ENT-SENT <C> 75.8 <C> 60.7 <C> 67.3 <R> <C> +  [ITALIC] ENT-DYM <C> 66.5 <C> 70.6 <C> 68.5 <R> <C> +  [ITALIC] ENT-DEP0 <C> 59.8 <C> 61.5 <C> 60.6 <R> <C> +  [ITALIC] ENT-DEP1 <C> 67.6 <C> 65.1 <C> 66.3 <R> <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <R> <C> +  [ITALIC] ENT-ONLY <C> 74.0 <C> 69.4 <C> 71.6 <R> <C> +  [ITALIC] ENT-SENT <C> 74.8 <C> 71.7 <C> 73.1 <R> <C> +  [ITALIC] ENT-DYM <C> 71.5 <C> 73.4 <C> 72.4 <R> <C> +  [ITALIC] ENT-DEP0 <C> 72.8 <C> 69.4 <C> 71.1 <R> <C> +  [ITALIC] ENT-DEP1 <C> 71.6 <C> 76.4 <C> [BOLD] 73.9 <R> <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <R> <C> +  [ITALIC] ENT-ONLY <C> 69.6 <C> 72.3 <C> 70.9 <R> <C> +  [ITALIC] ENT-SENT <C> 69.4 <C> 74.9 <C> 72.0 <R> <C> +  [ITALIC] ENT-DYM <C> 71.0 <C> 69.7 <C> 71.8 <R> <C> +  [ITALIC] ENT-DEP0 <C> 72.2 <C> 69.5 <C> 70.8 <R> <C> +  [ITALIC] ENT-DEP1 <C> 71.0 <C> 74.3 <C> 72.6 <R> <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <R> <C> +  [ITALIC] ENT-ONLY <C> 69.3 <C> 71.4 <C> 70.4 <R> <C> +  [ITALIC] ENT-SENT <C> 72.2 <C> 71.9 <C> 72.0 <R> <C> +  [ITALIC] ENT-DYM <C> 69.7 <C> 73.9 <C> 71.7 <R> <C> +  [ITALIC] ENT-DEP0 <C> 70.1 <C> 71.1 <C> 70.6 <R> <C> +  [ITALIC] ENT-DEP1 <C> 72.7 <C> 72.9 <C> 72.8 <CAP> Table 1: Results on DDI 2013 <COT> Looking at the table, we can see that the precision (P) for the model "CNN" with the feature "ENT-ONLY" is 52.7.
<R> <C> [EMPTY] <C> TrecQA MRR <C> TrecQA MAP <C> WikiQA MRR <C> WikiQA MAP <C> YahooQA MRR <C> YahooQA MAP <C> SemEvalcQA-16 MRR <C> SemEvalcQA-16 MAP <C> SemEvalcQA-17 MRR <C> SemEvalcQA-17 MAP <R> <C> epoch=3 <C> 0.927 <C> 0.877 <C> 0.770 <C> 0.753 <C> 0.942 <C> 0.942 <C> 0.872 <C> 0.810 <C> 0.951 <C> 0.909 <R> <C> epoch=5 <C> 0.944 <C> 0.883 <C> 0.784 <C> 0.769 <C> 0.942 <C> 0.942 <C> 0.890 <C> 0.816 <C> 0.953 <C> 0.908 <R> <C> SOTA <C> 0.865 <C> 0.904 <C> 0.758 <C> 0.746 <C> - <C> 0.801 <C> 0.872 <C> 0.801 <C> 0.926 <C> 0.887 <CAP> Table 2: Results of BERTbase in test set of five datasets with different epochs. The SOTA results are from Madabushi et al. (2018) (TrecQA), Sha et al. (2018) (WikiQA, SemEvalcQA-16), Tay et al. (2018b) (YahooQA), Nakov et al. (2017) (SemEvalcQA-17). <COT> Looking at the "epoch=5" row, "YahooQA MAP" cell, and "SOTA" row, "YahooQA MAP" cell, we can see that the MAP score for BERTbase in the YahooQA dataset at epoch=5 is higher than the SOTA result.
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> [ITALIC] CNN <C> [ITALIC] CNN <C> [ITALIC] CNN <C> [ITALIC] CNN <R> <C> +  [ITALIC] ENT-ONLY <C> 54.2 <C> 65.7 <C> 59.1 <R> <C> +  [ITALIC] ENT-SENT <C> 55.0 <C> 62.5 <C> 59.1 <R> <C> +  [ITALIC] ENT-DYM <C> 54.6 <C> 53.3 <C> 53.5 <R> <C> +  [ITALIC] ENT-DEP0 <C> 55.9 <C> 65.8 <C> 60.6 <R> <C> +  [ITALIC] ENT-DEP1 <C> 55.7 <C> 67.7 <C> 61.1 <R> <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <R> <C> +  [ITALIC] ENT-ONLY <C> 58.9 <C> 59.6 <C> 59.2 <R> <C> +  [ITALIC] ENT-SENT <C> 60.7 <C> 59.2 <C> 59.9 <R> <C> +  [ITALIC] ENT-DYM <C> 50.2 <C> 66.0 <C> 56.9 <R> <C> +  [ITALIC] ENT-DEP0 <C> 51.6 <C> 78.0 <C> 61.9 <R> <C> +  [ITALIC] ENT-DEP1 <C> 54.7 <C> 72.6 <C> 62.4 <R> <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <R> <C> +  [ITALIC] ENT-ONLY <C> 56.4 <C> 66.2 <C> 60.8 <R> <C> +  [ITALIC] ENT-SENT <C> 53.6 <C> 69.2 <C> 60.5 <R> <C> +  [ITALIC] ENT-DYM <C> 47.1 <C> 78.0 <C> 58.7 <R> <C> +  [ITALIC] ENT-DEP0 <C> 55.9 <C> 71.4 <C> [BOLD] 62.5 <R> <C> +  [ITALIC] ENT-DEP1 <C> 54.1 <C> 74.7 <C> 62.4 <R> <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <R> <C> +  [ITALIC] ENT-ONLY <C> 62.7 <C> 56.1 <C> 58.9 <R> <C> +  [ITALIC] ENT-SENT <C> 58.4 <C> 58.7 <C> 58.5 <R> <C> +  [ITALIC] ENT-DYM <C> 56.8 <C> 58.4 <C> 56.6 <R> <C> +  [ITALIC] ENT-DEP0 <C> 55.6 <C> 67.4 <C> 60.8 <R> <C> +  [ITALIC] ENT-DEP1 <C> 54.4 <C> 71.1 <C> 61.5 <CAP> Table 2: Results on BioNLP BB3 <COT> Looking at the "Model" column, we can see that there are four different models: CNN, BiLSTM, BiLSTM-CNN, and BiLSTM-GCNN.
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> [ITALIC] CNN <C> [ITALIC] CNN <C> [ITALIC] CNN <C> [ITALIC] CNN <R> <C> +  [ITALIC] ENT-ONLY <C> 54.2 <C> 65.7 <C> 59.1 <R> <C> +  [ITALIC] ENT-SENT <C> 55.0 <C> 62.5 <C> 59.1 <R> <C> +  [ITALIC] ENT-DYM <C> 54.6 <C> 53.3 <C> 53.5 <R> <C> +  [ITALIC] ENT-DEP0 <C> 55.9 <C> 65.8 <C> 60.6 <R> <C> +  [ITALIC] ENT-DEP1 <C> 55.7 <C> 67.7 <C> 61.1 <R> <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <C> [ITALIC] BiLSTM <R> <C> +  [ITALIC] ENT-ONLY <C> 58.9 <C> 59.6 <C> 59.2 <R> <C> +  [ITALIC] ENT-SENT <C> 60.7 <C> 59.2 <C> 59.9 <R> <C> +  [ITALIC] ENT-DYM <C> 50.2 <C> 66.0 <C> 56.9 <R> <C> +  [ITALIC] ENT-DEP0 <C> 51.6 <C> 78.0 <C> 61.9 <R> <C> +  [ITALIC] ENT-DEP1 <C> 54.7 <C> 72.6 <C> 62.4 <R> <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <C> [ITALIC] BiLSTM-CNN <R> <C> +  [ITALIC] ENT-ONLY <C> 56.4 <C> 66.2 <C> 60.8 <R> <C> +  [ITALIC] ENT-SENT <C> 53.6 <C> 69.2 <C> 60.5 <R> <C> +  [ITALIC] ENT-DYM <C> 47.1 <C> 78.0 <C> 58.7 <R> <C> +  [ITALIC] ENT-DEP0 <C> 55.9 <C> 71.4 <C> [BOLD] 62.5 <R> <C> +  [ITALIC] ENT-DEP1 <C> 54.1 <C> 74.7 <C> 62.4 <R> <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <C> [ITALIC] BiLSTM-GCNN <R> <C> +  [ITALIC] ENT-ONLY <C> 62.7 <C> 56.1 <C> 58.9 <R> <C> +  [ITALIC] ENT-SENT <C> 58.4 <C> 58.7 <C> 58.5 <R> <C> +  [ITALIC] ENT-DYM <C> 56.8 <C> 58.4 <C> 56.6 <R> <C> +  [ITALIC] ENT-DEP0 <C> 55.6 <C> 67.4 <C> 60.8 <R> <C> +  [ITALIC] ENT-DEP1 <C> 54.4 <C> 71.1 <C> 61.5 <CAP> Table 2: Results on BioNLP BB3 <COT> Looking at the "F1" column, we can see that the highest F1 score is 62.5, which corresponds to the "ENT-DEP0" configuration under the "BiLSTM-GCNN" model.
<R> <C> Total triples (millions) <C> OPIEC 341.0 <C> OPIEC <C> OPIEC-Clean 104.0 <C> OPIEC-Clean <C> OPIEC-Linked 5.8 <C> OPIEC-Linked <R> <C> Triples with semantic annotations <C> 166.3 <C> (49%) <C> 51.46 <C> (49%) <C> 3.37 <C> (58%) <R> <C> negative polarity <C> 5.3 <C> (2%) <C> 1.33 <C> (1%) <C> 0.01 <C> (0%) <R> <C> possibility modality <C> 13.9 <C> (4%) <C> 3.27 <C> (3%) <C> 0.04 <C> (1%) <R> <C> quantities <C> 59.4 <C> (17%) <C> 15.91 <C> (15%) <C> 0.45 <C> (8%) <R> <C> attribution <C> 6.4 <C> (2%) <C> 1.44 <C> (1%) <C> 0.01 <C> (0%) <R> <C> time <C> 65.3 <C> (19%) <C> 19.66 <C> (19%) <C> 0.58 <C> (1%) <R> <C> space <C> 61.5 <C> (18%) <C> 22.11 <C> (21%) <C> 2.64 <C> (45%) <R> <C> space OR time <C> 111.3 <C> (33%) <C> 37.22 <C> (36%) <C> 3.01 <C> (52%) <R> <C> space AND time <C> 15.4 <C> (5%) <C> 4.54 <C> (4%) <C> 0.20 <C> (4%) <R> <C> Triple length in tokens ( [ITALIC] μ± [ITALIC] σ) <C> 7.66±4.25 <C> 7.66±4.25 <C> 6.06±2.82 <C> 6.06±2.82 <C> 6.45±2.65 <C> 6.45±2.65 <R> <C> subject ( [ITALIC] μ± [ITALIC] σ) <C> 2.12±2.12 <C> 2.12±2.12 <C> 1.48±0.79 <C> 1.48±0.79 <C> 1.92±0.94 <C> 1.92±0.94 <R> <C> relation ( [ITALIC] μ± [ITALIC] σ) <C> 3.01±2.47 <C> 3.01±2.47 <C> 3.10±2.56 <C> 3.10±2.56 <C> 2.77±2.14 <C> 2.77±2.14 <R> <C> object ( [ITALIC] μ± [ITALIC] σ) <C> 2.52±2.69 <C> 2.52±2.69 <C> 1.48±0.79 <C> 1.48±0.79 <C> 1.76±0.94 <C> 1.76±0.94 <R> <C> Confidence score ( [ITALIC] μ± [ITALIC] σ) <C> 0.53±0.23 <C> 0.53±0.23 <C> 0.59±0.23 <C> 0.59±0.23 <C> 0.61±0.26 <C> 0.61±0.26 <CAP> Table 2: Statistics for different OPIEC corpora. All frequencies are in millions. We count triples with annotations (not annotations directly). Percentages refer to the respective subcorpus. <COT> Looking at the "Total triples (millions)" column, we can see that the OPIEC-Linked corpus has the fewest total triples.
<R> <C> Total triples (millions) <C> OPIEC 341.0 <C> OPIEC <C> OPIEC-Clean 104.0 <C> OPIEC-Clean <C> OPIEC-Linked 5.8 <C> OPIEC-Linked <R> <C> Triples with semantic annotations <C> 166.3 <C> (49%) <C> 51.46 <C> (49%) <C> 3.37 <C> (58%) <R> <C> negative polarity <C> 5.3 <C> (2%) <C> 1.33 <C> (1%) <C> 0.01 <C> (0%) <R> <C> possibility modality <C> 13.9 <C> (4%) <C> 3.27 <C> (3%) <C> 0.04 <C> (1%) <R> <C> quantities <C> 59.4 <C> (17%) <C> 15.91 <C> (15%) <C> 0.45 <C> (8%) <R> <C> attribution <C> 6.4 <C> (2%) <C> 1.44 <C> (1%) <C> 0.01 <C> (0%) <R> <C> time <C> 65.3 <C> (19%) <C> 19.66 <C> (19%) <C> 0.58 <C> (1%) <R> <C> space <C> 61.5 <C> (18%) <C> 22.11 <C> (21%) <C> 2.64 <C> (45%) <R> <C> space OR time <C> 111.3 <C> (33%) <C> 37.22 <C> (36%) <C> 3.01 <C> (52%) <R> <C> space AND time <C> 15.4 <C> (5%) <C> 4.54 <C> (4%) <C> 0.20 <C> (4%) <R> <C> Triple length in tokens ( [ITALIC] μ± [ITALIC] σ) <C> 7.66±4.25 <C> 7.66±4.25 <C> 6.06±2.82 <C> 6.06±2.82 <C> 6.45±2.65 <C> 6.45±2.65 <R> <C> subject ( [ITALIC] μ± [ITALIC] σ) <C> 2.12±2.12 <C> 2.12±2.12 <C> 1.48±0.79 <C> 1.48±0.79 <C> 1.92±0.94 <C> 1.92±0.94 <R> <C> relation ( [ITALIC] μ± [ITALIC] σ) <C> 3.01±2.47 <C> 3.01±2.47 <C> 3.10±2.56 <C> 3.10±2.56 <C> 2.77±2.14 <C> 2.77±2.14 <R> <C> object ( [ITALIC] μ± [ITALIC] σ) <C> 2.52±2.69 <C> 2.52±2.69 <C> 1.48±0.79 <C> 1.48±0.79 <C> 1.76±0.94 <C> 1.76±0.94 <R> <C> Confidence score ( [ITALIC] μ± [ITALIC] σ) <C> 0.53±0.23 <C> 0.53±0.23 <C> 0.59±0.23 <C> 0.59±0.23 <C> 0.61±0.26 <C> 0.61±0.26 <CAP> Table 2: Statistics for different OPIEC corpora. All frequencies are in millions. We count triples with annotations (not annotations directly). Percentages refer to the respective subcorpus. <COT> Looking at the "Triple length in tokens (μ±σ)" row, we can see that the average triple length in tokens is the same for all three OPIEC corpora.
<R> <C> location  [ITALIC] “be in” <C> location (43,842) <C> associatedMusicalArtist  [ITALIC] “be” <C> associatedMusicalArtist (6,273) <C> spouse  [ITALIC] “be wife of” <C> spouse (1,965) <R> <C> [ITALIC] “have” <C> (3,175) <C> [ITALIC] “have” <C> (3,600) <C> [ITALIC] “be” <C> (1,308) <R> <C> [ITALIC] “be” <C> (1,901) <C> [ITALIC] “be member of” <C> (740) <C> [ITALIC] “marry” <C> (702) <R> <C> [ITALIC] “be at” <C> (1,109) <C> [ITALIC] “be guitarist of” <C> (703) <C> [ITALIC] “be widow of” <C> (479) <R> <C> [ITALIC] “be of” <C> (706) <C> [ITALIC] “be drummer of” <C> (458) <C> [ITALIC] “have” <C> (298) <R> <C> [ITALIC] “be historic home <C> (491) <C> [ITALIC] “be feature” <C> (416) <C> [ITALIC] “be husband of” <C> (284) <R> <C> [ITALIC] located at” <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 4: The most frequent open relations aligned to the DBpedia relations location, associatedMusicalArtist, and spouse in OPEIC-Linked <COT> Looking at the table, we can see that the most frequent open relation aligned to the DBpedia relation "location" is "[ITALIC] 'be in'" with a count of 43,842.
<R> <C> location  [ITALIC] “be in” <C> location (43,842) <C> associatedMusicalArtist  [ITALIC] “be” <C> associatedMusicalArtist (6,273) <C> spouse  [ITALIC] “be wife of” <C> spouse (1,965) <R> <C> [ITALIC] “have” <C> (3,175) <C> [ITALIC] “have” <C> (3,600) <C> [ITALIC] “be” <C> (1,308) <R> <C> [ITALIC] “be” <C> (1,901) <C> [ITALIC] “be member of” <C> (740) <C> [ITALIC] “marry” <C> (702) <R> <C> [ITALIC] “be at” <C> (1,109) <C> [ITALIC] “be guitarist of” <C> (703) <C> [ITALIC] “be widow of” <C> (479) <R> <C> [ITALIC] “be of” <C> (706) <C> [ITALIC] “be drummer of” <C> (458) <C> [ITALIC] “have” <C> (298) <R> <C> [ITALIC] “be historic home <C> (491) <C> [ITALIC] “be feature” <C> (416) <C> [ITALIC] “be husband of” <C> (284) <R> <C> [ITALIC] located at” <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 4: The most frequent open relations aligned to the DBpedia relations location, associatedMusicalArtist, and spouse in OPEIC-Linked <COT> Looking at the table, we can see that the most frequent open relation aligned to the DBpedia relation "spouse" is "[ITALIC] 'be wife of'" with a count of 1,965.
<R> <C> [BOLD] Model <C> [BOLD] FrameNet 1.5  [BOLD] All <C> [BOLD] FrameNet 1.5  [BOLD] Ambiguous <C> [BOLD] FrameNet 1.7  [BOLD] All <C> [BOLD] FrameNet 1.7  [BOLD] Ambiguous <R> <C> [ITALIC] Das’s Test Set  <C> [ITALIC] Das’s Test Set  <C> [ITALIC] Das’s Test Set  <C> [ITALIC] Das’s Test Set  <C> [ITALIC] Das’s Test Set  <R> <C> SEMAFOR  <C> 83.60 <C> 69.19 <C> - <C> - <R> <C> Hermann et al.  <C> 88.73 <C> 73.67 <C> - <C> - <R> <C> Yang and Mitchell  <C> 88.20 <C> 75.70 <C> - <C> - <R> <C> Hartmann et al.  <C> 87.63 <C> 73.80 <C> - <C> - <R> <C> Botschen et al.  <C> 88.82 <C> 75.28 <C> - <C> - <R> <C> Peng et al.  <C> 90.00 <C> 78.00 <C> 89.10 <C> 77.50 <R> <C> [BOLD] PAFIBERT - Filtered by LUs <C> [BOLD] 92.22 <C> [BOLD] 82.90 <C> [BOLD] 91.44 <C> [BOLD] 82.55 <R> <C> [BOLD] PAFIBERT - Filtered by Targets <C> [BOLD] 91.39 <C> [BOLD] 82.80 <C> [BOLD] 90.15 <C> [BOLD] 81.92 <R> <C> [ITALIC] YAGS Test Set  <C> [ITALIC] YAGS Test Set  <C> [ITALIC] YAGS Test Set  <C> [ITALIC] YAGS Test Set  <C> [ITALIC] YAGS Test Set  <R> <C> SEMAFOR (Reported by ) <C> 60.01 <C> - <C> - <C> - <R> <C> Hartmann et al.  <C> 62.51 <C> - <C> - <C> - <R> <C> [BOLD] PAFIBERT - Filtered by LUs <C> [BOLD] 75.06 <C> [BOLD] 69.07 <C> [BOLD] - <C> [BOLD] - <R> <C> [BOLD] PAFIBERT - Filtered by Targets <C> [BOLD] 75.01 <C> [BOLD] 69.07 <C> [BOLD] - <C> [BOLD] - <CAP> Table 2: Results with frame filtering <COT> Looking at the "Model" column, we can see that PAFIBERT performs better than SEMAFOR, Hermann et al., Yang and Mitchell, and Hartmann et al. on both FrameNet 1.5 All and FrameNet 1.7 All.
<R> <C> [BOLD] Model <C> [BOLD] FrameNet 1.5  [BOLD] All <C> [BOLD] FrameNet 1.5  [BOLD] Ambiguous <C> [BOLD] FrameNet 1.7  [BOLD] All <C> [BOLD] FrameNet 1.7  [BOLD] Ambiguous <R> <C> [ITALIC] Das’s Test Set  <C> [ITALIC] Das’s Test Set  <C> [ITALIC] Das’s Test Set  <C> [ITALIC] Das’s Test Set  <C> [ITALIC] Das’s Test Set  <R> <C> SEMAFOR  <C> 83.60 <C> 69.19 <C> - <C> - <R> <C> Hermann et al.  <C> 88.73 <C> 73.67 <C> - <C> - <R> <C> Yang and Mitchell  <C> 88.20 <C> 75.70 <C> - <C> - <R> <C> Hartmann et al.  <C> 87.63 <C> 73.80 <C> - <C> - <R> <C> Botschen et al.  <C> 88.82 <C> 75.28 <C> - <C> - <R> <C> Peng et al.  <C> 90.00 <C> 78.00 <C> 89.10 <C> 77.50 <R> <C> [BOLD] PAFIBERT - Filtered by LUs <C> [BOLD] 92.22 <C> [BOLD] 82.90 <C> [BOLD] 91.44 <C> [BOLD] 82.55 <R> <C> [BOLD] PAFIBERT - Filtered by Targets <C> [BOLD] 91.39 <C> [BOLD] 82.80 <C> [BOLD] 90.15 <C> [BOLD] 81.92 <R> <C> [ITALIC] YAGS Test Set  <C> [ITALIC] YAGS Test Set  <C> [ITALIC] YAGS Test Set  <C> [ITALIC] YAGS Test Set  <C> [ITALIC] YAGS Test Set  <R> <C> SEMAFOR (Reported by ) <C> 60.01 <C> - <C> - <C> - <R> <C> Hartmann et al.  <C> 62.51 <C> - <C> - <C> - <R> <C> [BOLD] PAFIBERT - Filtered by LUs <C> [BOLD] 75.06 <C> [BOLD] 69.07 <C> [BOLD] - <C> [BOLD] - <R> <C> [BOLD] PAFIBERT - Filtered by Targets <C> [BOLD] 75.01 <C> [BOLD] 69.07 <C> [BOLD] - <C> [BOLD] - <CAP> Table 2: Results with frame filtering <COT> Looking at the "FrameNet 1.5 Ambiguous" column, we can see that PAFIBERT performs better than SEMAFOR, Hermann et al., Yang and Mitchell, and Hartmann et al.
<R> <C> [EMPTY] <C> TrecQA base <C> TrecQA large <C> WikiQA base <C> WikiQA large <C> YahooQA base <C> YahooQA large <C> SemEvalcQA-16 base <C> SemEvalcQA-16 large <C> SemEvalcQA-17 base <C> SemEvalcQA-17 large <R> <C> MRR <C> 0.927 <C> [BOLD] 0.961 <C> 0.770 <C> [BOLD] 0.875 <C> [BOLD] 0.942 <C> 0.938 <C> 0.872 <C> [BOLD] 0.911 <C> 0.951 <C> [BOLD] 0.958 <R> <C> MAP <C> 0.877 <C> [BOLD] 0.904 <C> 0.753 <C> [BOLD] 0.860 <C> [BOLD] 0.942 <C> 0.938 <C> 0.810 <C> [BOLD] 0.844 <C> [BOLD] 0.909 <C> 0.907 <CAP> Table 3: Results of BERTbase and BERTlarge in test set of five datasets. The number of training epochs is 3. <COT> Looking at the "MRR" column, we can see that BERTlarge performs better than BERTbase on all datasets except for WikiQA base and YahooQA large.
<R> <C> [EMPTY] <C> TrecQA base <C> TrecQA large <C> WikiQA base <C> WikiQA large <C> YahooQA base <C> YahooQA large <C> SemEvalcQA-16 base <C> SemEvalcQA-16 large <C> SemEvalcQA-17 base <C> SemEvalcQA-17 large <R> <C> MRR <C> 0.927 <C> [BOLD] 0.961 <C> 0.770 <C> [BOLD] 0.875 <C> [BOLD] 0.942 <C> 0.938 <C> 0.872 <C> [BOLD] 0.911 <C> 0.951 <C> [BOLD] 0.958 <R> <C> MAP <C> 0.877 <C> [BOLD] 0.904 <C> 0.753 <C> [BOLD] 0.860 <C> [BOLD] 0.942 <C> 0.938 <C> 0.810 <C> [BOLD] 0.844 <C> [BOLD] 0.909 <C> 0.907 <CAP> Table 3: Results of BERTbase and BERTlarge in test set of five datasets. The number of training epochs is 3. <COT> Looking at the "MAP" column, we can see that BERTlarge performs better than BERTbase on all datasets except for WikiQA base and YahooQA large.
<R> <C> [width=13em]Hyperparam TuningDataset <C> semeval <C> ddi Class <C> ddi Detect <C> i2b2 Class <C> i2b2 Detect <R> <C> Default <C> 81.55 <C> 62.55 <C> 80.29 <C> 55.15 <C> 81.98 <R> <C> Default <C> 80.85 (1.31) <C> 81.62 (1.35) <C> 87.76 (1.03) <C> 67.28 (1.83) <C> 86.57 (0.58) <R> <C> Manual Search <C> - <C> [BOLD] 65.53 <C> [BOLD] 81.74 <C> [BOLD] 59.75 <C> [BOLD] 83.17 <R> <C> Manual Search <C> [EMPTY] <C> 82.23 (0.32)• <C> 88.40 (0.48)• <C> 70.10 (0.85) <C> 86.45 (0.58)• <R> <C> Random Search <C> [BOLD] 82.2 <C> 62.29 <C> 79.04 <C> 55.0 <C> 80.77 <R> <C> Random Search <C> 81.10 (1.26)• <C> 75.43 (1.48) <C> 83.54 (0.60) <C> 60.66 (1.43) <C> 82.73 (0.49) <CAP> Table 6: Hyperparameter tuning methods with original pre-processing and fixed CRCNN model. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to Default with p<0.05 except those marked with a •. Note that hyperparameter tuning can involve much higher performance variation depending on the distribution of the data. Therefore, even though there is no statistical significance in the manual search case for the held out fold in the ddi dataset, there was statistical significance for the dev fold which drove those set of hyperparameters. For both ddi and i2b2 datasets, manual search is better than random search with p<0.05. <COT> Looking at the "Hyperparam Tuning" cell, "Random Search" row, "semeval" cell, finding that the default hyperparameter tuning method achieves a higher score on the semeval dataset compared to the random search method.
<R> <C> [width=13em]Hyperparam TuningDataset <C> semeval <C> ddi Class <C> ddi Detect <C> i2b2 Class <C> i2b2 Detect <R> <C> Default <C> 81.55 <C> 62.55 <C> 80.29 <C> 55.15 <C> 81.98 <R> <C> Default <C> 80.85 (1.31) <C> 81.62 (1.35) <C> 87.76 (1.03) <C> 67.28 (1.83) <C> 86.57 (0.58) <R> <C> Manual Search <C> - <C> [BOLD] 65.53 <C> [BOLD] 81.74 <C> [BOLD] 59.75 <C> [BOLD] 83.17 <R> <C> Manual Search <C> [EMPTY] <C> 82.23 (0.32)• <C> 88.40 (0.48)• <C> 70.10 (0.85) <C> 86.45 (0.58)• <R> <C> Random Search <C> [BOLD] 82.2 <C> 62.29 <C> 79.04 <C> 55.0 <C> 80.77 <R> <C> Random Search <C> 81.10 (1.26)• <C> 75.43 (1.48) <C> 83.54 (0.60) <C> 60.66 (1.43) <C> 82.73 (0.49) <CAP> Table 6: Hyperparameter tuning methods with original pre-processing and fixed CRCNN model. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to Default with p<0.05 except those marked with a •. Note that hyperparameter tuning can involve much higher performance variation depending on the distribution of the data. Therefore, even though there is no statistical significance in the manual search case for the held out fold in the ddi dataset, there was statistical significance for the dev fold which drove those set of hyperparameters. For both ddi and i2b2 datasets, manual search is better than random search with p<0.05. <COT> Looking at the "Hyperparam Tuning" cell, "Manual Search" row, "i2b2 Class" cell, finding that the manual search method achieves a higher score on the i2b2 Class dataset compared to the default hyperparameter tuning method.
<R> <C> [width=10em]PreprocessDataset <C> semeval <C> ddi Class <C> ddi Detect <C> i2b2 Class <C> i2b2 Detect <R> <C> Original <C> [BOLD] 81.55 <C> 65.53 <C> 81.74 <C> 59.75 <C> 83.17 <R> <C> Original <C> 80.85 (1.31) <C> 82.23 (0.32) <C> 88.40 (0.48) <C> 70.10 (0.85) <C> 86.45 (0.58) <R> <C> Entity Blinding <C> 72.73 <C> [BOLD] 67.02 <C> [BOLD] 82.37 <C> [BOLD] 68.76 <C> [BOLD] 84.37 <R> <C> Entity Blinding <C> 71.31 (1.14) <C> 83.56 (2.05)• <C> 89.45 (1.05)• <C> 76.59 (1.07) <C> 88.41 (0.37) <R> <C> Punct and Digit <C> 81.23 <C> 63.41 <C> 80.49 <C> 58.85 <C> 81.96 <R> <C> Punct and Digit <C> 80.95 (1.21)• <C> 80.44 (1.77) <C> 87.52 (0.98) <C> 69.37 (1.43)• <C> 85.82 (0.43) <R> <C> Punct, Digit and Stop <C> 72.92 <C> 55.87 <C> 76.57 <C> 56.19 <C> 80.47 <R> <C> Punct, Digit and Stop <C> 71.61 (1.25) <C> 78.52 (1.99) <C> 85.65 (1.21) <C> 68.14 (2.05)• <C> 84.84 (0.77) <R> <C> NER Blinding <C> 81.63 <C> 57.22 <C> 79.03 <C> 50.41 <C> 81.61 <R> <C> NER Blinding <C> 80.85 (1.07)• <C> 78.06 (1.45) <C> 86.79 (0.65) <C> 66.26 (2.44) <C> 86.72 (0.57)• <CAP> Table 4: Pre-processing techniques with CRCNN model. Row labels Original = simple tokenization and lower casing of words, Punct = punctuation removal, Digit = digit removal and Stop = stop word removal. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to Original pre-processing (p<0.05) using a paired t-test except those marked with a • <COT> Looking at the "PreprocessDataset" column, "Original" row, and "ddi Class" cell, we can see that the performance of the CRCNN model on the ddi Class dataset is 81.55.
<R> <C> [width=10em]PreprocessDataset <C> semeval <C> ddi Class <C> ddi Detect <C> i2b2 Class <C> i2b2 Detect <R> <C> Original <C> [BOLD] 81.55 <C> 65.53 <C> 81.74 <C> 59.75 <C> 83.17 <R> <C> Original <C> 80.85 (1.31) <C> 82.23 (0.32) <C> 88.40 (0.48) <C> 70.10 (0.85) <C> 86.45 (0.58) <R> <C> Entity Blinding <C> 72.73 <C> [BOLD] 67.02 <C> [BOLD] 82.37 <C> [BOLD] 68.76 <C> [BOLD] 84.37 <R> <C> Entity Blinding <C> 71.31 (1.14) <C> 83.56 (2.05)• <C> 89.45 (1.05)• <C> 76.59 (1.07) <C> 88.41 (0.37) <R> <C> Punct and Digit <C> 81.23 <C> 63.41 <C> 80.49 <C> 58.85 <C> 81.96 <R> <C> Punct and Digit <C> 80.95 (1.21)• <C> 80.44 (1.77) <C> 87.52 (0.98) <C> 69.37 (1.43)• <C> 85.82 (0.43) <R> <C> Punct, Digit and Stop <C> 72.92 <C> 55.87 <C> 76.57 <C> 56.19 <C> 80.47 <R> <C> Punct, Digit and Stop <C> 71.61 (1.25) <C> 78.52 (1.99) <C> 85.65 (1.21) <C> 68.14 (2.05)• <C> 84.84 (0.77) <R> <C> NER Blinding <C> 81.63 <C> 57.22 <C> 79.03 <C> 50.41 <C> 81.61 <R> <C> NER Blinding <C> 80.85 (1.07)• <C> 78.06 (1.45) <C> 86.79 (0.65) <C> 66.26 (2.44) <C> 86.72 (0.57)• <CAP> Table 4: Pre-processing techniques with CRCNN model. Row labels Original = simple tokenization and lower casing of words, Punct = punctuation removal, Digit = digit removal and Stop = stop word removal. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to Original pre-processing (p<0.05) using a paired t-test except those marked with a • <COT> Looking at the "PreprocessDataset" column, "NER Blinding" row, and "i2b2 Detect" cell, we can see that the performance of the CRCNN model on the i2b2 Detect dataset is 50.41.
<R> <C> [width=10em]ModelingDataset <C> semeval <C> ddi Class <C> ddi Detect <C> i2b2 Class <C> i2b2 Detect <R> <C> CRCNN <C> 81.55 <C> 65.53 <C> 81.74 <C> 59.75 <C> 83.17 <R> <C> CRCNN <C> 80.85 (1.31) <C> 82.23 (0.32) <C> 88.40 (0.48) <C> 70.10 (0.85) <C> 86.45 (0.58) <R> <C> Piecewise pool <C> 81.59 <C> 63.01 <C> 80.62 <C> 60.85 <C> 83.69 <R> <C> Piecewise pool <C> 80.55 (0.99)• <C> 81.99 (0.38)• <C> 88.47 (0.48)• <C> 73.79 (0.97) <C> 89.29 (0.61) <R> <C> BERT-tokens <C> 85.67 <C> [BOLD] 71.97 <C> [BOLD] 86.53 <C> 63.11 <C> [BOLD] 84.91 <R> <C> BERT-tokens <C> 85.63 (0.83) <C> 85.35 (0.53) <C> 90.70 (0.46) <C> 72.06 (1.36) <C> 87.57 (0.75) <R> <C> BERT-CLS <C> 82.42 <C> 61.3 <C> 79.63 <C> 56.79 <C> 81.91 <R> <C> BERT-CLS <C> 80.83 (1.18)• <C> 82.71 (0.68)• <C> 88.35 (0.77)• <C> 67.37 (1.08) <C> 85.43 (0.36) <R> <C> ELMo <C> [BOLD] 85.89 <C> 66.63 <C> 83.05 <C> [BOLD] 63.18 <C> 84.54 <R> <C> ELMo <C> 84.79 (1.08) <C> 84.53 (0.96) <C> 90.11 (0.56) <C> 72.53 (0.80) <C> 87.81 (0.34) <CAP> Table 5: Modeling techniques with original pre-processing. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to CRCNN model (p<0.05) using a paired t-test except those marked with a •. In terms of statistical significance, comparing contextualized embeddings with each other reveals that BERT-tokens is equivalent to ELMo for i2b2, but for semeval BERT-tokens is better than ELMo and for ddi BERT-tokens is better than ELMo only for detection. <COT> Looking at the "Modeling Dataset" column, we can see that the BERT-tokens model performs better than the CRCNN model on the semeval and ddi Class datasets.
<R> <C> [width=10em]ModelingDataset <C> semeval <C> ddi Class <C> ddi Detect <C> i2b2 Class <C> i2b2 Detect <R> <C> CRCNN <C> 81.55 <C> 65.53 <C> 81.74 <C> 59.75 <C> 83.17 <R> <C> CRCNN <C> 80.85 (1.31) <C> 82.23 (0.32) <C> 88.40 (0.48) <C> 70.10 (0.85) <C> 86.45 (0.58) <R> <C> Piecewise pool <C> 81.59 <C> 63.01 <C> 80.62 <C> 60.85 <C> 83.69 <R> <C> Piecewise pool <C> 80.55 (0.99)• <C> 81.99 (0.38)• <C> 88.47 (0.48)• <C> 73.79 (0.97) <C> 89.29 (0.61) <R> <C> BERT-tokens <C> 85.67 <C> [BOLD] 71.97 <C> [BOLD] 86.53 <C> 63.11 <C> [BOLD] 84.91 <R> <C> BERT-tokens <C> 85.63 (0.83) <C> 85.35 (0.53) <C> 90.70 (0.46) <C> 72.06 (1.36) <C> 87.57 (0.75) <R> <C> BERT-CLS <C> 82.42 <C> 61.3 <C> 79.63 <C> 56.79 <C> 81.91 <R> <C> BERT-CLS <C> 80.83 (1.18)• <C> 82.71 (0.68)• <C> 88.35 (0.77)• <C> 67.37 (1.08) <C> 85.43 (0.36) <R> <C> ELMo <C> [BOLD] 85.89 <C> 66.63 <C> 83.05 <C> [BOLD] 63.18 <C> 84.54 <R> <C> ELMo <C> 84.79 (1.08) <C> 84.53 (0.96) <C> 90.11 (0.56) <C> 72.53 (0.80) <C> 87.81 (0.34) <CAP> Table 5: Modeling techniques with original pre-processing. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to CRCNN model (p<0.05) using a paired t-test except those marked with a •. In terms of statistical significance, comparing contextualized embeddings with each other reveals that BERT-tokens is equivalent to ELMo for i2b2, but for semeval BERT-tokens is better than ELMo and for ddi BERT-tokens is better than ELMo only for detection. <COT> Looking at the "ddi Detect" column, we can see that the BERT-tokens model performs better than the CRCNN model on the ddi Detect dataset.
<R> <C> activation <C> de→en <C> ja→en <C> ro→en <C> en→de <R> <C> softmax <C> 29.79 <C> 21.57 <C> 32.70 <C> 26.02 <R> <C> 1.5-entmax <C> 29.83 <C> [BOLD] 22.13 <C> [BOLD] 33.10 <C> 25.89 <R> <C> [ITALIC] α-entmax <C> [BOLD] 29.90 <C> 21.74 <C> 32.89 <C> [BOLD] 26.93 <CAP> Table 1: Machine translation tokenized BLEU test results on IWSLT 2017 de→en, KFTT ja→en, WMT 2016 ro→en and WMT 2014 en→de, respectively. <COT> Looking at the "Table 1" caption, we can see that the table shows machine translation tokenized BLEU test results on different language pairs.
<R> <C> activation <C> de→en <C> ja→en <C> ro→en <C> en→de <R> <C> softmax <C> 29.79 <C> 21.57 <C> 32.70 <C> 26.02 <R> <C> 1.5-entmax <C> 29.83 <C> [BOLD] 22.13 <C> [BOLD] 33.10 <C> 25.89 <R> <C> [ITALIC] α-entmax <C> [BOLD] 29.90 <C> 21.74 <C> 32.89 <C> [BOLD] 26.93 <CAP> Table 1: Machine translation tokenized BLEU test results on IWSLT 2017 de→en, KFTT ja→en, WMT 2016 ro→en and WMT 2014 en→de, respectively. <COT> Looking at the "softmax" row and the "ja→en" column, we can see that the BLEU test result for the softmax activation function on the ja→en language pair is 21.57.
<R> <C> [EMPTY] <C> subtask 1.1 macro F1 <C> subtask 1.1 micro F1 <C> subtask 1.2 macro F1 <C> subtask 1.2 micro F1 <R> <C> CC <C> 54.42 <C> 67.61 <C> [BOLD] 74.42 <C> [BOLD] 78.87 <R> <C> arXiv <C> [BOLD] 67.49 <C> [BOLD] 70.96 <C> 67.02 <C> 74.37 <CAP> Table 6: Results for C-LSTM models trained with CC and arXiv embeddings on both subtasks. <COT> Looking at the "CAP" tag, we can see that Table 6 shows the results for C-LSTM models trained with CC and arXiv embeddings on both subtasks.
<R> <C> [EMPTY] <C> subtask 1.1 macro F1 <C> subtask 1.1 micro F1 <C> subtask 1.2 macro F1 <C> subtask 1.2 micro F1 <R> <C> CC <C> 54.42 <C> 67.61 <C> [BOLD] 74.42 <C> [BOLD] 78.87 <R> <C> arXiv <C> [BOLD] 67.49 <C> [BOLD] 70.96 <C> 67.02 <C> 74.37 <CAP> Table 6: Results for C-LSTM models trained with CC and arXiv embeddings on both subtasks. <COT> Looking at the "subtask 1.2 macro F1" column, we can see that the macro F1 score for subtask 1.2 is higher for CC compared to arXiv.
<R> <C> [EMPTY] <C> val <C> UNC testA <C> testB <C> val <C> UNC+ testA <C> testB <C> G-Ref val <C> ReferIt test <R> <C> LSTM-CNN  <C> - <C> - <C> - <C> - <C> - <C> - <C> 28.14 <C> 48.03 <R> <C> RMI  <C> 45.18 <C> 45.69 <C> 45.57 <C> 29.86 <C> 30.48 <C> 29.50 <C> 34.52 <C> 58.73 <R> <C> DMN  <C> 49.78 <C> 54.83 <C> 45.13 <C> 38.88 <C> 44.22 <C> 32.29 <C> 36.76 <C> 52.81 <R> <C> KWA  <C> - <C> - <C> - <C> - <C> - <C> - <C> 36.92 <C> 59.09 <R> <C> RRN  <C> 55.33 <C> 57.26 <C> 53.93 <C> 39.75 <C> 42.15 <C> 36.11 <C> 36.45 <C> 63.63 <R> <C> Ours <C> [BOLD] 58.32 <C> [BOLD] 60.61 <C> [BOLD] 55.09 <C> [BOLD] 43.76 <C> [BOLD] 47.60 <C> [BOLD] 37.89 <C> [BOLD] 39.98 <C> [BOLD] 63.80 <CAP> Table 1: Comparison of segmentation performance with the state-of-the-art methods on four evaluation datasets in terms of IoU. <COT> Looking at the "Table 1" caption, we can see that the table compares the segmentation performance of different methods on four evaluation datasets.
<R> <C> Method <C> IoU <R> <C> No attention <C> 45.63 <R> <C> Word attention <C> 47.01 <R> <C> Pixel attention <C> 47.84 <R> <C> Word-pixel pair attention <C> 47.57 <R> <C> Cross-modal self-attention <C> [BOLD] 50.12 <CAP> Table 2: Ablation study of different attention methods for multimodal features on the UNC val set. <COT> Looking at the "Method" column, we can see that the "Cross-modal self-attention" method has the highest IoU score.
<R> <C> Method <C> IoU <R> <C> No attention <C> 45.63 <R> <C> Word attention <C> 47.01 <R> <C> Pixel attention <C> 47.84 <R> <C> Word-pixel pair attention <C> 47.57 <R> <C> Cross-modal self-attention <C> [BOLD] 50.12 <CAP> Table 2: Ablation study of different attention methods for multimodal features on the UNC val set. <COT> Looking at the "Cross-modal self-attention" cell, we can see that it has a higher IoU score compared to the other attention methods.
<R> <C> Method <C> prec@0.5 <C> prec@0.6 <C> prec@0.7 <C> prec@0.8 <C> prec@0.9 <C> IoU <R> <C> RMI-LSTM  <C> 42.99 <C> 33.24 <C> 22.75 <C> 12.11 <C> 2.23 <C> 45.18 <R> <C> RRN-CNN ∗ <C> 47.59 <C> 38.76 <C> 26.53 <C> 14.79 <C> 3.17 <C> 46.95 <R> <C> CMSA-S <C> 51.19 <C> 41.31 <C> 29.57 <C> 14.99 <C> 2.61 <C> 48.53 <R> <C> CMSA-W <C> [BOLD] 51.95 <C> [BOLD] 43.11 <C> [BOLD] 32.74 <C> [BOLD] 19.28 <C> [BOLD] 4.11 <C> [BOLD] 50.12 <R> <C> CMSA+PPM <C> 58.25 <C> 49.82 <C> 39.09 <C> 24.76 <C> 5.73 <C> 53.54 <R> <C> CMSA+Deconv <C> 58.29 <C> 49.94 <C> 39.16 <C> 25.42 <C> 6.75 <C> 54.18 <R> <C> CMSA+ConvLSTM <C> 64.73 <C> 56.03 <C> 45.23 <C> 29.15 <C> 7.86 <C> 56.56 <R> <C> CMSA+Gated <C> 65.17 <C> 57.25 <C> 47.37 <C> 33.31 <C> 9.66 <C> 57.08 <R> <C> CMSA+GF(Ours) <C> [BOLD] 66.44 <C> [BOLD] 59.70 <C> [BOLD] 50.77 <C> [BOLD] 35.52 <C> [BOLD] 10.96 <C> [BOLD] 58.32 <CAP> Table 3: Ablation study on the UNC val set. The top four methods compare results of different methods for multimodal feature representations. The bottom five results show comparisons of multi-level feature fusion methods. CMSA and GF denote the proposed cross-modal self-attention and gated multi-level fusion modules. All methods use the same base model (DeepLab-101) and DenseCRF for postprocessing. ∗The numbers for [15] are slightly higher than original numbers reported in their paper which did not use DenseCRF postprocessing. <COT> Looking at the "prec@0.5" column, the values for CMSA-S, CMSA-W, CMSA+PPM, CMSA+Deconv, CMSA+ConvLSTM, CMSA+Gated, and CMSA+GF(Ours) are all higher than the values for RMI-LSTM and RRN-CNN.
<R> <C> [EMPTY] <C> [ITALIC]  [BOLD] Model 1 <C> [ITALIC]  [BOLD] Model 2 <C> [ITALIC]  [BOLD] Model 3 <R> <C> First metric <C> First metric <C> [EMPTY] <C> [EMPTY] <R> <C> 0.08 [ITALIC]  [BOLD] Disp. <C> [BOLD] 0.94 <C> 0.72 <C> 0 <R> <C> 0.08 [ITALIC]  [BOLD] Gen. perf. <C> 0.68 <C> 0.63 <C> 1 <R> <C> Second metric <C> Second metric <C> [EMPTY] <C> [EMPTY] <R> <C> 0.08 [ITALIC]  [BOLD] Disp. <C> 0.93 <C> [BOLD] 0.96 <C> 0 <R> <C> 0.08 [ITALIC]  [BOLD] Gen. perf. <C> 0.68 <C> 0.68 <C> 1 <CAP> Table 2: Experimental results of the evaluation metrics. <COT> Looking at the "Model 3" column, we can see that the value for the second metric is empty. 
<R> <C> [EMPTY] <C> [ITALIC]  [BOLD] Model 1 <C> [ITALIC]  [BOLD] Model 2 <C> [ITALIC]  [BOLD] Model 3 <R> <C> First metric <C> First metric <C> [EMPTY] <C> [EMPTY] <R> <C> 0.08 [ITALIC]  [BOLD] Disp. <C> [BOLD] 0.94 <C> 0.72 <C> 0 <R> <C> 0.08 [ITALIC]  [BOLD] Gen. perf. <C> 0.68 <C> 0.63 <C> 1 <R> <C> Second metric <C> Second metric <C> [EMPTY] <C> [EMPTY] <R> <C> 0.08 [ITALIC]  [BOLD] Disp. <C> 0.93 <C> [BOLD] 0.96 <C> 0 <R> <C> 0.08 [ITALIC]  [BOLD] Gen. perf. <C> 0.68 <C> 0.68 <C> 1 <CAP> Table 2: Experimental results of the evaluation metrics. <COT> Looking at the "Model 2" column, we can see that the value for the first metric is 0.94.
<R> <C> [BOLD] Model <C> [BOLD] SwDA <C> [BOLD] MRDA <R> <C> TF-IDF GloVe <C> 66.5 <C> 78.7 <R> <C> Kalchbrenner and Blunsom ( 2013 ) <C> 73.9 <C> - <R> <C> Lee and Dernoncourt ( 2016 ) <C> 73.9 <C> 84.6 <R> <C> Khanpour et al. ( 2016 ) <C> 75.8 <C> 86.8 <R> <C> Ji et al. ( 2016 ) <C> 77.0 <C> - <R> <C> Shen and Lee ( 2016 ) <C> 72.6 <C> - <R> <C> Li and Wu ( 2016 ) <C> 79.4 <C> - <R> <C> Ortega and Vu ( 2017 ) <C> 73.8 <C> 84.3 <R> <C> Tran et al. ( 2017 ) <C> 74.5 <C> - <R> <C> Kumar et al. ( 2018 ) <C> 79.2 <C> 90.9 <R> <C> Chen et al. ( 2018 ) <C> 81.3 <C> [BOLD] 91.7 <R> <C> [BOLD] Our Method <C> [BOLD] 82.9 <C> 91.1 <R> <C> [BOLD] Human Agreement <C> 84.0 <C> - <CAP> Table 3: DA Classification Accuracy <COT> Looking at the "Model" column, we can see that the "Our Method" model has an accuracy of 82.9 on the SwDA dataset.
<R> <C> [BOLD] Model <C> [BOLD] SwDA <C> [BOLD] MRDA <R> <C> TF-IDF GloVe <C> 66.5 <C> 78.7 <R> <C> Kalchbrenner and Blunsom ( 2013 ) <C> 73.9 <C> - <R> <C> Lee and Dernoncourt ( 2016 ) <C> 73.9 <C> 84.6 <R> <C> Khanpour et al. ( 2016 ) <C> 75.8 <C> 86.8 <R> <C> Ji et al. ( 2016 ) <C> 77.0 <C> - <R> <C> Shen and Lee ( 2016 ) <C> 72.6 <C> - <R> <C> Li and Wu ( 2016 ) <C> 79.4 <C> - <R> <C> Ortega and Vu ( 2017 ) <C> 73.8 <C> 84.3 <R> <C> Tran et al. ( 2017 ) <C> 74.5 <C> - <R> <C> Kumar et al. ( 2018 ) <C> 79.2 <C> 90.9 <R> <C> Chen et al. ( 2018 ) <C> 81.3 <C> [BOLD] 91.7 <R> <C> [BOLD] Our Method <C> [BOLD] 82.9 <C> 91.1 <R> <C> [BOLD] Human Agreement <C> 84.0 <C> - <CAP> Table 3: DA Classification Accuracy <COT> Looking at the "MRDA" column, we can see that the "Our Method" model has an accuracy of 91.1 on the MRDA dataset.
<R> <C> [BOLD] Method <C> [BOLD] Inspec (Avg=7.8)  [BOLD] F1@5 <C> [BOLD] Inspec (Avg=7.8)  [BOLD] F1@10 <C> [BOLD] Krapivin (Avg=3.4)  [BOLD] F1@5 <C> [BOLD] Krapivin (Avg=3.4)  [BOLD] F1@10 <C> [BOLD] NUS (Avg=6.1)  [BOLD] F1@5 <C> [BOLD] NUS (Avg=6.1)  [BOLD] F1@10 <C> [BOLD] SemEval (Avg=6.7)  [BOLD] F1@5 <C> [BOLD] SemEval (Avg=6.7)  [BOLD] F1@10 <C> [BOLD] Average  [BOLD] F1@5 <C> [BOLD] Average  [BOLD] F1@10 <R> <C> [BOLD] One2One <C> 0.244 <C> 0.289 <C> 0.305 <C> [BOLD] 0.266 <C> [BOLD] 0.376 <C> [BOLD] 0.352 <C> 0.318 <C> [BOLD] 0.318 <C> 0.311 <C> [BOLD] 0.306 <R> <C> [BOLD] Random <C> 0.283 <C> 0.206 <C> 0.288 <C> 0.183 <C> 0.344 <C> 0.238 <C> 0.304 <C> 0.218 <C> 0.305 <C> 0.211 <R> <C> [BOLD] Length <C> 0.298 <C> 0.224 <C> 0.321 <C> 0.206 <C> 0.364 <C> 0.259 <C> 0.311 <C> 0.222 <C> 0.324 <C> 0.228 <R> <C> [BOLD] No-Sort <C> 0.323 <C> 0.253 <C> 0.317 <C> 0.209 <C> [BOLD] 0.376 <C> 0.264 <C> 0.318 <C> 0.244 <C> 0.333 <C> 0.243 <R> <C> [BOLD] Alpha <C> 0.319 <C> 0.283 <C> [BOLD] 0.329 <C> 0.238 <C> [BOLD] 0.376 <C> 0.289 <C> [BOLD] 0.343 <C> 0.266 <C> [BOLD] 0.342 <C> 0.269 <R> <C> [BOLD] Appear-Pre <C> 0.320 <C> 0.307 <C> 0.322 <C> 0.245 <C> 0.369 <C> 0.302 <C> 0.327 <C> 0.291 <C> 0.334 <C> 0.286 <R> <C> [BOLD] Appear-Ap <C> [BOLD] 0.344 <C> [BOLD] 0.333 <C> 0.320 <C> 0.236 <C> 0.367 <C> 0.295 <C> 0.324 <C> 0.286 <C> 0.339 <C> 0.287 <CAP> Table 1: Present keyphrase prediction performance (F1-score) on four benchmark datasets. Dataset names are listed in the header followed by the average number of target phrases per document. The baseline One2One model is on the first row, followed by different One2Seq variants. Bold indicates best score in each column. <COT> Looking at the "Method" column, "No-Sort" row, and "[BOLD] 0.376" cell, we can see that the "No-Sort" method achieves the highest F1@5 score on the Krapivin dataset.
<R> <C> [BOLD] Method <C> [BOLD] Inspec (Avg=7.8)  [BOLD] F1@5 <C> [BOLD] Inspec (Avg=7.8)  [BOLD] F1@10 <C> [BOLD] Krapivin (Avg=3.4)  [BOLD] F1@5 <C> [BOLD] Krapivin (Avg=3.4)  [BOLD] F1@10 <C> [BOLD] NUS (Avg=6.1)  [BOLD] F1@5 <C> [BOLD] NUS (Avg=6.1)  [BOLD] F1@10 <C> [BOLD] SemEval (Avg=6.7)  [BOLD] F1@5 <C> [BOLD] SemEval (Avg=6.7)  [BOLD] F1@10 <C> [BOLD] Average  [BOLD] F1@5 <C> [BOLD] Average  [BOLD] F1@10 <R> <C> [BOLD] One2One <C> 0.244 <C> 0.289 <C> 0.305 <C> [BOLD] 0.266 <C> [BOLD] 0.376 <C> [BOLD] 0.352 <C> 0.318 <C> [BOLD] 0.318 <C> 0.311 <C> [BOLD] 0.306 <R> <C> [BOLD] Random <C> 0.283 <C> 0.206 <C> 0.288 <C> 0.183 <C> 0.344 <C> 0.238 <C> 0.304 <C> 0.218 <C> 0.305 <C> 0.211 <R> <C> [BOLD] Length <C> 0.298 <C> 0.224 <C> 0.321 <C> 0.206 <C> 0.364 <C> 0.259 <C> 0.311 <C> 0.222 <C> 0.324 <C> 0.228 <R> <C> [BOLD] No-Sort <C> 0.323 <C> 0.253 <C> 0.317 <C> 0.209 <C> [BOLD] 0.376 <C> 0.264 <C> 0.318 <C> 0.244 <C> 0.333 <C> 0.243 <R> <C> [BOLD] Alpha <C> 0.319 <C> 0.283 <C> [BOLD] 0.329 <C> 0.238 <C> [BOLD] 0.376 <C> 0.289 <C> [BOLD] 0.343 <C> 0.266 <C> [BOLD] 0.342 <C> 0.269 <R> <C> [BOLD] Appear-Pre <C> 0.320 <C> 0.307 <C> 0.322 <C> 0.245 <C> 0.369 <C> 0.302 <C> 0.327 <C> 0.291 <C> 0.334 <C> 0.286 <R> <C> [BOLD] Appear-Ap <C> [BOLD] 0.344 <C> [BOLD] 0.333 <C> 0.320 <C> 0.236 <C> 0.367 <C> 0.295 <C> 0.324 <C> 0.286 <C> 0.339 <C> 0.287 <CAP> Table 1: Present keyphrase prediction performance (F1-score) on four benchmark datasets. Dataset names are listed in the header followed by the average number of target phrases per document. The baseline One2One model is on the first row, followed by different One2Seq variants. Bold indicates best score in each column. <COT> Looking at the "Average F1@10" column, "Alpha" row, and "[BOLD] 0.343" cell, we can see that the "Alpha" method achieves the highest F1@10 score on average across all datasets.
<R> <C> [BOLD] Model  [BOLD] #(Param) <C> [BOLD] BaseRNN 13M <C> [BOLD] BaseRNN 13M <C> [BOLD] BigRNN 37M <C> [BOLD] BigRNN 37M <C> [BOLD] Transformer 80M <C> [BOLD] Transformer 80M <R> <C> [EMPTY] <C> [BOLD] F@5 <C> [BOLD] F@10 <C> [BOLD] F@5 <C> [BOLD] F@10 <C> [BOLD] F@5 <C> [BOLD] F@10 <R> <C> [BOLD] Random <C> 0.358 <C> 0.304 <C> 0.356 <C> 0.305 <C> 0.359 <C> 0.289 <R> <C> [BOLD] Length <C> 0.351 <C> 0.319 <C> 0.349 <C> 0.321 <C> [BOLD] 0.361 <C> 0.318 <R> <C> [BOLD] No-Sort <C> [BOLD] 0.364 <C> 0.325 <C> [BOLD] 0.361 <C> 0.329 <C> 0.358 <C> 0.329 <R> <C> [BOLD] Alpha <C> 0.354 <C> 0.341 <C> 0.358 <C> 0.341 <C> 0.353 <C> 0.336 <R> <C> [BOLD] Appear-Pre <C> 0.337 <C> 0.345 <C> 0.339 <C> 0.341 <C> 0.352 <C> 0.343 <R> <C> [BOLD] Appear-Ap <C> 0.339 <C> [BOLD] 0.347 <C> 0.344 <C> [BOLD] 0.346 <C> 0.357 <C> [BOLD] 0.345 <CAP> Table 4: F1 scores on present keyphrase generation of One2Seq models with different model complexities (BeamWidth=50). <COT> Looking at the "Model #(Param)" column, we can see that the number of parameters increases as the model complexity increases.
<R> <C> [BOLD] Model  [BOLD] #(Param) <C> [BOLD] BaseRNN 13M <C> [BOLD] BaseRNN 13M <C> [BOLD] BigRNN 37M <C> [BOLD] BigRNN 37M <C> [BOLD] Transformer 80M <C> [BOLD] Transformer 80M <R> <C> [EMPTY] <C> [BOLD] F@5 <C> [BOLD] F@10 <C> [BOLD] F@5 <C> [BOLD] F@10 <C> [BOLD] F@5 <C> [BOLD] F@10 <R> <C> [BOLD] Random <C> 0.358 <C> 0.304 <C> 0.356 <C> 0.305 <C> 0.359 <C> 0.289 <R> <C> [BOLD] Length <C> 0.351 <C> 0.319 <C> 0.349 <C> 0.321 <C> [BOLD] 0.361 <C> 0.318 <R> <C> [BOLD] No-Sort <C> [BOLD] 0.364 <C> 0.325 <C> [BOLD] 0.361 <C> 0.329 <C> 0.358 <C> 0.329 <R> <C> [BOLD] Alpha <C> 0.354 <C> 0.341 <C> 0.358 <C> 0.341 <C> 0.353 <C> 0.336 <R> <C> [BOLD] Appear-Pre <C> 0.337 <C> 0.345 <C> 0.339 <C> 0.341 <C> 0.352 <C> 0.343 <R> <C> [BOLD] Appear-Ap <C> 0.339 <C> [BOLD] 0.347 <C> 0.344 <C> [BOLD] 0.346 <C> 0.357 <C> [BOLD] 0.345 <CAP> Table 4: F1 scores on present keyphrase generation of One2Seq models with different model complexities (BeamWidth=50). <COT> Looking at the "F@5" column, we can see that the F1 score for the "No-Sort" model is higher than the F1 score for the "Appear-Pre" model.
<R> <C> [EMPTY] <C> Diagnosis <C> Prescription <C> Penn Adverse Drug <C> Chemical–Disease <C> Drug–Disease <R> <C> [EMPTY] <C> Detection <C> Reasons <C> Reactions (ADR) <C> Relations (CDR) <C> Relations <R> <C> rand-LSTM-CRF <C> 77.3 ± 0.05 <C> 69.6 ± 0.25 <C> 53.8 ± 0.88 <C> 85.1 ± 0.10 <C> 48.2 ± 1.12 <R> <C> HB-CRF <C> 82.0 ± 0.05 <C> 78.5 ± 0.01 <C> 58.8 ± 0.12 <C> 86.2 ± 0.02 <C> 42.3 ± 0.30 <R> <C> ELMo-LSTM-CRF <C> 83.9 ± 0.35 <C> 81.0 ± 0.20 <C> 65.7 ± 0.35 <C> 88.2 ± 0.34 <C> 50.6 ± 0.64 <R> <C> ELMo-LSTM-CRF-HB <C> [BOLD] 85.3 ± 0.24∗∗∗ <C> [BOLD] 82.0 ± 0.03∗∗∗ <C> [BOLD] 68.5 ± 1.67∗ <C> [BOLD] 89.9 ± 0.12∗∗∗ <C> [BOLD] 51.9 ± 0.52∗∗ <CAP> Table 2: Per-token macro-F1 scores. For ADR, the F1 scores are for chunks via approximate matching (Nikfarjam et al., 2015; Tsai et al., 2006). ‘rand-LSTM’ is an LSTM with randomly initialized word vectors. ‘ELMo-LSTM’ is an LSTM initialized with pretrained ELMo embeddings. ‘HB’ signals sparse, high-dimensional feature representations based on hand-built feature functions. The mean values and standard deviations are calculated using F1 scores of three runs of repeated experiments, as discussed in section 3. Statistical significance notation for the last two rows (two top-performing models) is ∗: p<0.05; ∗∗: p<0.01; ∗∗∗: p<0.001. <COT> Looking at the "ELMo-LSTM-CRF-HB" row, the values in the "Diagnosis" column are higher than the values in the "Prescription" column.
<R> <C> [EMPTY] <C> Diagnosis <C> Prescription <C> Penn Adverse Drug <C> Chemical–Disease <C> Drug–Disease <R> <C> [EMPTY] <C> Detection <C> Reasons <C> Reactions (ADR) <C> Relations (CDR) <C> Relations <R> <C> rand-LSTM-CRF <C> 77.3 ± 0.05 <C> 69.6 ± 0.25 <C> 53.8 ± 0.88 <C> 85.1 ± 0.10 <C> 48.2 ± 1.12 <R> <C> HB-CRF <C> 82.0 ± 0.05 <C> 78.5 ± 0.01 <C> 58.8 ± 0.12 <C> 86.2 ± 0.02 <C> 42.3 ± 0.30 <R> <C> ELMo-LSTM-CRF <C> 83.9 ± 0.35 <C> 81.0 ± 0.20 <C> 65.7 ± 0.35 <C> 88.2 ± 0.34 <C> 50.6 ± 0.64 <R> <C> ELMo-LSTM-CRF-HB <C> [BOLD] 85.3 ± 0.24∗∗∗ <C> [BOLD] 82.0 ± 0.03∗∗∗ <C> [BOLD] 68.5 ± 1.67∗ <C> [BOLD] 89.9 ± 0.12∗∗∗ <C> [BOLD] 51.9 ± 0.52∗∗ <CAP> Table 2: Per-token macro-F1 scores. For ADR, the F1 scores are for chunks via approximate matching (Nikfarjam et al., 2015; Tsai et al., 2006). ‘rand-LSTM’ is an LSTM with randomly initialized word vectors. ‘ELMo-LSTM’ is an LSTM initialized with pretrained ELMo embeddings. ‘HB’ signals sparse, high-dimensional feature representations based on hand-built feature functions. The mean values and standard deviations are calculated using F1 scores of three runs of repeated experiments, as discussed in section 3. Statistical significance notation for the last two rows (two top-performing models) is ∗: p<0.05; ∗∗: p<0.01; ∗∗∗: p<0.001. <COT> Looking at the "rand-LSTM-CRF" row, the value in the "Penn Adverse Drug" column is lower than the value in the "Chemical-Disease" column.
<R> <C> Diagnosis Detection Label <C> Diagnosis Detection Support <C> Diagnosis Detection F1 score <C> Diagnosis Detection Improvement <C> Prescription Reasons Label <C> Prescription Reasons Support <C> Prescription Reasons F1 score <C> Prescription Reasons Improvement <R> <C> Other <C> 74888 <C> 95.3 <C> 1.4% <C> Other <C> 83618 <C> 95.8 <C> 0.9% <R> <C> Positive <C> 24489 <C> 86.1 <C> 4.4% <C> Reason <C> 9114 <C> 64.7 <C> 8.6% <R> <C> Ruled-out <C> 2797 <C> 86.4 <C> 3.6% <C> Prescribed <C> 5967 <C> 84.7 <C> 4.4% <R> <C> Concern <C> 2780 <C> 72.1 <C> 5.6% <C> Discontinued <C> 2754 <C> 82.7 <C> 5.6% <R> <C> Chemical–Disease Relations (CDR) <C> Chemical–Disease Relations (CDR) <C> Chemical–Disease Relations (CDR) <C> Chemical–Disease Relations (CDR) <C> Drug–Disease Relations <C> Drug–Disease Relations <C> Drug–Disease Relations <C> Drug–Disease Relations <R> <C> Label <C> Support <C> F1 score <C> Improvement <C> Label <C> Support <C> F1 score <C> Improvement <R> <C> Other <C> 104530 <C> 98.3 <C> 0.5% <C> Other <C> 10634 <C> 90.8 <C> 2.3% <R> <C> Disease <C> 6887 <C> 84.2 <C> 6.3% <C> Treats <C> 3671 <C> 76.0 <C> 5.7% <R> <C> Chemical <C> 6270 <C> 87.0 <C> 6.7% <C> Unrelated <C> 1145 <C> 53.8 <C> 71.3% <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Prevents <C> 320 <C> 41.1 <C> 103.5% <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Contraindicated-for <C> 69 <C> 0 <C> – <CAP> Table 3: Relative F1 score improvements of different labels. For each label, we give the number of supporting examples (Support), the F1 score of our combined model, and the relative improvements over the HB-CRF model. The F1 scores of minor labels suffer from insufficient training data, and thus have lower values. However, the combined model shows the largest relative improvements in these categories. ADR results are shown in table A4. <COT> Looking at the "Diagnosis Detection F1 score" column, we can see that the F1 score for the label "Positive" is 86.1, while the F1 score for the label "Ruled-out" is 86.4. This indicates that the diagnosis detection performance for the label "Ruled-out" is slightly higher than the label "Positive".
<R> <C> Diagnosis Detection Label <C> Diagnosis Detection Support <C> Diagnosis Detection F1 score <C> Diagnosis Detection Improvement <C> Prescription Reasons Label <C> Prescription Reasons Support <C> Prescription Reasons F1 score <C> Prescription Reasons Improvement <R> <C> Other <C> 74888 <C> 95.3 <C> 1.4% <C> Other <C> 83618 <C> 95.8 <C> 0.9% <R> <C> Positive <C> 24489 <C> 86.1 <C> 4.4% <C> Reason <C> 9114 <C> 64.7 <C> 8.6% <R> <C> Ruled-out <C> 2797 <C> 86.4 <C> 3.6% <C> Prescribed <C> 5967 <C> 84.7 <C> 4.4% <R> <C> Concern <C> 2780 <C> 72.1 <C> 5.6% <C> Discontinued <C> 2754 <C> 82.7 <C> 5.6% <R> <C> Chemical–Disease Relations (CDR) <C> Chemical–Disease Relations (CDR) <C> Chemical–Disease Relations (CDR) <C> Chemical–Disease Relations (CDR) <C> Drug–Disease Relations <C> Drug–Disease Relations <C> Drug–Disease Relations <C> Drug–Disease Relations <R> <C> Label <C> Support <C> F1 score <C> Improvement <C> Label <C> Support <C> F1 score <C> Improvement <R> <C> Other <C> 104530 <C> 98.3 <C> 0.5% <C> Other <C> 10634 <C> 90.8 <C> 2.3% <R> <C> Disease <C> 6887 <C> 84.2 <C> 6.3% <C> Treats <C> 3671 <C> 76.0 <C> 5.7% <R> <C> Chemical <C> 6270 <C> 87.0 <C> 6.7% <C> Unrelated <C> 1145 <C> 53.8 <C> 71.3% <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Prevents <C> 320 <C> 41.1 <C> 103.5% <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Contraindicated-for <C> 69 <C> 0 <C> – <CAP> Table 3: Relative F1 score improvements of different labels. For each label, we give the number of supporting examples (Support), the F1 score of our combined model, and the relative improvements over the HB-CRF model. The F1 scores of minor labels suffer from insufficient training data, and thus have lower values. However, the combined model shows the largest relative improvements in these categories. ADR results are shown in table A4. <COT> Looking at the "Prescription Reasons F1 score" column, we can see that the F1 score for the label "Reason" is 64.7, while the F1 score for the label "Prescribed" is 84.7. This indicates that the prescription reasons detection performance for the label "Prescribed" is higher than the label "Reason".
<R> <C> [BOLD] Dataset Friends <C> [BOLD] Dataset Training <C> #Dialogues / #Utterances 4,000 / 58,012 <C> #Avg. utterances per dialogue 14.50 <C> #Avg. length of dialogues 160.92 <C> Neutral 45.0% <C> Joy 11.8% <C> Sadness 3.4% <C> Anger 5.2% <C> Out-Of-Domain 34.6% <R> <C> Friends <C> Test <C> 240 / 3296 <C> 13.73 <C> 156.38 <C> 31.4% <C> 15.3% <C> 3.7% <C> 4.3% <C> 45.3% <R> <C> EmotionPush <C> Training <C> 4,000 / 58,968 <C> 14.74 <C> 114.96 <C> 66.8% <C> 14.2% <C> 3.5% <C> 0.9% <C> 14.6% <R> <C> EmotionPush <C> Test <C> 240 / 3536 <C> 14.73 <C> 92.43 <C> 60.7% <C> 17.0% <C> 3.1% <C> 0.8% <C> 18.4% <CAP> Table 2: Corpus statistics and label distributions of Friends and EmotionPush datasets. <COT> Looking at the "Friends Test" row, we can see that the average length of dialogues is 156.38.
<R> <C> [BOLD] Dataset Friends <C> [BOLD] Dataset Training <C> #Dialogues / #Utterances 4,000 / 58,012 <C> #Avg. utterances per dialogue 14.50 <C> #Avg. length of dialogues 160.92 <C> Neutral 45.0% <C> Joy 11.8% <C> Sadness 3.4% <C> Anger 5.2% <C> Out-Of-Domain 34.6% <R> <C> Friends <C> Test <C> 240 / 3296 <C> 13.73 <C> 156.38 <C> 31.4% <C> 15.3% <C> 3.7% <C> 4.3% <C> 45.3% <R> <C> EmotionPush <C> Training <C> 4,000 / 58,968 <C> 14.74 <C> 114.96 <C> 66.8% <C> 14.2% <C> 3.5% <C> 0.9% <C> 14.6% <R> <C> EmotionPush <C> Test <C> 240 / 3536 <C> 14.73 <C> 92.43 <C> 60.7% <C> 17.0% <C> 3.1% <C> 0.8% <C> 18.4% <CAP> Table 2: Corpus statistics and label distributions of Friends and EmotionPush datasets. <COT> Looking at the "EmotionPush Training" row, we can see that the label distribution for Joy is 14.2%.
<R> <C> [EMPTY] <C> [BOLD] Dataset <C> [BOLD] # of extracted pairs <C> [BOLD] # of valid pairs <C> [BOLD] Precision <R> <C> Essentia <C> Snips <C> 173 <C> 84 <C> 48.55% <R> <C> Essentia <C> HotelQA <C> 2221 <C> 642 <C> 28.91% <R> <C> FSA <C> Snips <C> 18 <C> 15 <C> 83.33% <R> <C> FSA <C> HotelQA <C> 342 <C> 185 <C> 54.09 % <CAP> Table 1: Comparison between Essentia and FSA baseline on paraphrase extraction <COT> Looking at the "Precision" column, we can see that FSA has a higher precision than Essentia on both Snips and HotelQA datasets.
<R> <C> [BOLD] BR <C> [BOLD] US <C> [BOLD] BR + US <R> <C> 85% <C> 72% <C> 70% <CAP> Table 5: Classification test scores for classifying R vs U in the BR, US, and combined BR + US dataset. The baseline score is 50%. <COT> Looking at the "BR" cell, the "[BOLD] 85%" cell, and the "US" cell, we can see that the classification test score for classifying R vs U is higher in the BR dataset compared to the US dataset.
<R> <C> [BOLD] BR <C> [BOLD] US <C> [BOLD] BR + US <R> <C> 85% <C> 72% <C> 70% <CAP> Table 5: Classification test scores for classifying R vs U in the BR, US, and combined BR + US dataset. The baseline score is 50%. <COT> Looking at the "BR + US" cell and the "BR" cell, we can see that the classification test score for classifying R vs U is lower in the combined BR + US dataset compared to the BR dataset.
<R> <C> 2 Data type <C> geth <C> ethanos <C> Diff <R> <C> Headers <C> 316.68 <C> 348.16 <C> +31.48 <R> <C> Bodies <C> 11,110.00 <C> 12,750.00 <C> +1,640.00 <R> <C> Receipts <C> 3,578.97 <C> 3,605.66 <C> +26.69 <R> <C> Difficulties <C> 15.57 <C> 15.33 <C> -0.24 <R> <C> Block number -> hash <C> 39.66 <C> 39.35 <C> -0.31 <R> <C> Block hash -> number <C> 39.10 <C> 39.10 <C> 0 <R> <C> Transaction Index <C> 3,650.00 <C> 3,650.00 <C> 0 <R> <C> Bloombit index <C> 20.49 <C> 20.49 <C> 0 <R> <C> Trie nodes <C> 183,100.00 <C> 165,140.00 <C> -17,960.00 <R> <C> Trie preimages <C> 857.00 <C> 888.11 <C> +31.11 <R> <C> total <C> 202,727.47 <C> 186,496.20 <C> -16,231.27 <R> <C> 2 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: Storage size(MB) comparison of full archive sync between geth and ethanos from 7M+1 to 8M block. <COT> Looking at the "Diff" column, we can see that there are positive and negative values indicating the difference in storage size between geth and ethanos for different data types.
<R> <C> 2 Data type <C> fast sync geth <C> fast sync ethanos <C> compact sync geth <C> compact sync ethanos <R> <C> Headers <C> 276.66 <C> 303.83 <C> 276.66 <C> 303.83 <R> <C> Bodies <C> 9,260.00 <C> 10,130.00 <C> 8.12 <C> 13.02 <R> <C> Receipts <C> 2,973.66 <C> 2,979.81 <C> 10.43 <C> 10.44 <R> <C> Difficulties <C> 11.49 <C> 11.48 <C> 11.49 <C> 11.48 <R> <C> Block number -> hash <C> 31.86 <C> 31.88 <C> 31.86 <C> 31.88 <R> <C> Block hash -> number <C> 33.79 <C> 33.79 <C> 33.79 <C> 33.79 <R> <C> Transaction Index <C> 3,020.00 <C> 3,020.00 <C> 0.19 <C> 0.19 <R> <C> Bloombit index <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Trie nodes <C> 2,220.00 <C> 804.79 <C> 2,220.00 <C> 802.33 <R> <C> Trie preimages <C> 0 <C> 0.28 <C> 0 <C> 0 <R> <C> total <C> 17,827.46 <C> 17,315.86 <C> 2,592.54 <C> 1,206.97 <R> <C> 2 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 2: Storage size(MB) comparison of fast sync and compact sync between geth and ethanos from 7M+1 to 7M+864K (5th checkpoint) block. <COT> Looking at the "Headers" cell, "276.66" cell, "fast sync geth" cell, and "compact sync geth" cell, we can see that the storage size for Headers is the same for both fast sync and compact sync in geth.
<R> <C> 2 Data type <C> fast sync geth <C> fast sync ethanos <C> compact sync geth <C> compact sync ethanos <R> <C> Headers <C> 276.66 <C> 303.83 <C> 276.66 <C> 303.83 <R> <C> Bodies <C> 9,260.00 <C> 10,130.00 <C> 8.12 <C> 13.02 <R> <C> Receipts <C> 2,973.66 <C> 2,979.81 <C> 10.43 <C> 10.44 <R> <C> Difficulties <C> 11.49 <C> 11.48 <C> 11.49 <C> 11.48 <R> <C> Block number -> hash <C> 31.86 <C> 31.88 <C> 31.86 <C> 31.88 <R> <C> Block hash -> number <C> 33.79 <C> 33.79 <C> 33.79 <C> 33.79 <R> <C> Transaction Index <C> 3,020.00 <C> 3,020.00 <C> 0.19 <C> 0.19 <R> <C> Bloombit index <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Trie nodes <C> 2,220.00 <C> 804.79 <C> 2,220.00 <C> 802.33 <R> <C> Trie preimages <C> 0 <C> 0.28 <C> 0 <C> 0 <R> <C> total <C> 17,827.46 <C> 17,315.86 <C> 2,592.54 <C> 1,206.97 <R> <C> 2 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 2: Storage size(MB) comparison of fast sync and compact sync between geth and ethanos from 7M+1 to 7M+864K (5th checkpoint) block. <COT> Looking at the "Trie nodes" cell, "2,220.00" cell, "fast sync geth" cell, and "compact sync geth" cell, we can see that the storage size for Trie nodes is higher for fast sync than for compact sync in geth.
<R> <C> [EMPTY] <C> Avg MT02 – 08 BLEU <C> Avg MT02 – 08 HE <C> Avg MT02 – 08 chrF1 <C> Avg MT02 – 08 RD <R> <C> Transformer-word <C> 39.75 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Search (0.2) <C> 32.42 <C> 3.22 <C> 0.82 <C> 0.184 <R> <C> Search (0.3) <C> 28.83 <C> 2.45 <C> 0.77 <C> 0.275 <R> <C> Ours <C> [BOLD] 33.28 <C> [BOLD] 3.69 <C> [BOLD] 0.80 <C> [BOLD] 0.163 <R> <C> Transformer-BPE <C> 43.38 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Search (0.2) <C> 34.27 <C> 3.87 <C> 0.89 <C> 0.210 <R> <C> Search (0.4) <C> 27.27 <C> 2.91 <C> 0.80 <C> 0.371 <R> <C> Ours <C> [BOLD] 31.35 <C> [BOLD] 3.66 <C> [BOLD] 0.80 <C> [BOLD] 0.277 <R> <C> RNN-search-BPE <C> 39.38 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Search (0.2) <C> 31.83 <C> 3.83 <C> 0.89 <C> 0.192 <R> <C> Search (0.4) <C> 26.13 <C> 2.82 <C> 0.79 <C> 0.336 <R> <C> Ours <C> [BOLD] 31.18 <C> [BOLD] 3.60 <C> [BOLD] 0.83 <C> [BOLD] 0.208 <CAP> Table 3: Experiment results. Note that sequence length for word level system is shorter, thus we search by ratio 0.3 which shares similar chrF1 with search on subword level system with ratio 0.4. An ideal adversarial sample generation must achieve degradation with respect to higher semantic similarity with origin inputs (HE). <COT> Looking at the "Ours" row and the "Avg MT02 - 08 BLEU" column, we can see that our model performs worse in terms of BLEU score compared to the Transformer-word and Transformer-BPE models.
<R> <C> [EMPTY] <C> Avg MT02 – 08 BLEU <C> Avg MT02 – 08 HE <C> Avg MT02 – 08 chrF1 <C> Avg MT02 – 08 RD <R> <C> Transformer-word <C> 39.75 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Search (0.2) <C> 32.42 <C> 3.22 <C> 0.82 <C> 0.184 <R> <C> Search (0.3) <C> 28.83 <C> 2.45 <C> 0.77 <C> 0.275 <R> <C> Ours <C> [BOLD] 33.28 <C> [BOLD] 3.69 <C> [BOLD] 0.80 <C> [BOLD] 0.163 <R> <C> Transformer-BPE <C> 43.38 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Search (0.2) <C> 34.27 <C> 3.87 <C> 0.89 <C> 0.210 <R> <C> Search (0.4) <C> 27.27 <C> 2.91 <C> 0.80 <C> 0.371 <R> <C> Ours <C> [BOLD] 31.35 <C> [BOLD] 3.66 <C> [BOLD] 0.80 <C> [BOLD] 0.277 <R> <C> RNN-search-BPE <C> 39.38 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Search (0.2) <C> 31.83 <C> 3.83 <C> 0.89 <C> 0.192 <R> <C> Search (0.4) <C> 26.13 <C> 2.82 <C> 0.79 <C> 0.336 <R> <C> Ours <C> [BOLD] 31.18 <C> [BOLD] 3.60 <C> [BOLD] 0.83 <C> [BOLD] 0.208 <CAP> Table 3: Experiment results. Note that sequence length for word level system is shorter, thus we search by ratio 0.3 which shares similar chrF1 with search on subword level system with ratio 0.4. An ideal adversarial sample generation must achieve degradation with respect to higher semantic similarity with origin inputs (HE). <COT> Looking at the "Ours" row and the "Avg MT02 - 08 RD" column, we can see that our model has a lower RD (Reduction) score compared to the Transformer-word and Transformer-BPE models.
<R> <C> Model <C> Task SLC P <C> Task SLC R <C> Task SLC F1 <C> Task FLC P <C> Task FLC R <C> Task FLC F1 <R> <C> All-Propaganda <C> 23.92 <C> 100.0 <C> 38.61 <C> - <C> - <C> - <R> <C> BERT <C> [BOLD] 63.20 <C> 53.16 <C> 57.74 <C> 21.48 <C> [BOLD] 21.39 <C> 21.39 <R> <C> Joint <C> 62.84 <C> 55.46 <C> 58.91 <C> 20.11 <C> 19.74 <C> 19.92 <R> <C> Granu <C> 62.80 <C> 55.24 <C> 58.76 <C> 23.85 <C> 20.14 <C> 21.80 <R> <C> Multi-Granularity <C> Multi-Granularity <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> ReLU <C> 60.41 <C> [BOLD] 61.58 <C> [BOLD] 60.98 <C> 23.98 <C> 20.33 <C> 21.82 <R> <C> Sigmoid <C> 62.27 <C> 59.56 <C> 60.71 <C> [BOLD] 24.42 <C> 21.05 <C> [BOLD] 22.58 <CAP> Table 1: Sentence-level (left) and fragment-level experiments (right). All-propaganda is a baseline that always output the propaganda class. <COT> Looking at the "Model" column, we can see that the "BERT" model has the highest F1 scores for both the sentence-level classification (SLC) and fragment-level classification (FLC) tasks.
<R> <C> Model <C> Task SLC P <C> Task SLC R <C> Task SLC F1 <C> Task FLC P <C> Task FLC R <C> Task FLC F1 <R> <C> All-Propaganda <C> 23.92 <C> 100.0 <C> 38.61 <C> - <C> - <C> - <R> <C> BERT <C> [BOLD] 63.20 <C> 53.16 <C> 57.74 <C> 21.48 <C> [BOLD] 21.39 <C> 21.39 <R> <C> Joint <C> 62.84 <C> 55.46 <C> 58.91 <C> 20.11 <C> 19.74 <C> 19.92 <R> <C> Granu <C> 62.80 <C> 55.24 <C> 58.76 <C> 23.85 <C> 20.14 <C> 21.80 <R> <C> Multi-Granularity <C> Multi-Granularity <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> ReLU <C> 60.41 <C> [BOLD] 61.58 <C> [BOLD] 60.98 <C> 23.98 <C> 20.33 <C> 21.82 <R> <C> Sigmoid <C> 62.27 <C> 59.56 <C> 60.71 <C> [BOLD] 24.42 <C> 21.05 <C> [BOLD] 22.58 <CAP> Table 1: Sentence-level (left) and fragment-level experiments (right). All-propaganda is a baseline that always output the propaganda class. <COT> Looking at the "Task SLC P" column, we can see that the "ReLU" model has a higher precision score than the "Joint" model.
<R> <C> [BOLD] Method <C> [BOLD] Accuracy (%) <R> <C> BERT (baseline) <C> 63.8 <R> <C> CoS-E-open-ended <C> 65.5 <R> <C> CAGE-reasoning <C> [BOLD] 72.6 <CAP> Table 2: Results on CQA dev-random-split with CoS-E used during training. <COT> Looking at the "Method" column, we can see that the method "CAGE-reasoning" has the highest accuracy of [BOLD] 72.6%.
<R> <C> [BOLD] Method <C> [BOLD] Accuracy (%) <R> <C> BERT (baseline) <C> 63.8 <R> <C> CoS-E-open-ended <C> 65.5 <R> <C> CAGE-reasoning <C> [BOLD] 72.6 <CAP> Table 2: Results on CQA dev-random-split with CoS-E used during training. <COT> Looking at the "CAP" tag, we can see that Table 2 shows the results on CQA dev-random-split with CoS-E used during training.
<R> <C> [BOLD] Method <C> [BOLD] Accuracy (%) <R> <C> RC (Talmor et al.,  2019 ) <C> 47.7 <R> <C> GPT (Talmor et al.,  2019 ) <C> 54.8 <R> <C> CoS-E-open-ended <C> 60.2 <R> <C> CAGE-reasoning <C> [BOLD] 64.7 <R> <C> Human (Talmor et al.,  2019 ) <C> 95.3 <CAP> Table 3: Test accuracy on CQA v1.0. The addition of CoS-E-open-ended during training dramatically improves performance. Replacing CoS-E during training with CAGE reasoning during both training and inference leads to an absolute gain of 10% over the previous state-of-the-art. <COT> Looking at the "Method" column, we can see that CAGE-reasoning has the highest accuracy of 64.7%.
<R> <C> [BOLD] Method <C> [BOLD] Accuracy (%) <R> <C> RC (Talmor et al.,  2019 ) <C> 47.7 <R> <C> GPT (Talmor et al.,  2019 ) <C> 54.8 <R> <C> CoS-E-open-ended <C> 60.2 <R> <C> CAGE-reasoning <C> [BOLD] 64.7 <R> <C> Human (Talmor et al.,  2019 ) <C> 95.3 <CAP> Table 3: Test accuracy on CQA v1.0. The addition of CoS-E-open-ended during training dramatically improves performance. Replacing CoS-E during training with CAGE reasoning during both training and inference leads to an absolute gain of 10% over the previous state-of-the-art. <COT> Looking at the "Method" column, we can see that Human (Talmor et al., 2019) has the highest accuracy of 95.3%.
<R> <C> [BOLD] Method <C> [BOLD] Accuracy (%) <R> <C> CoS-E-selected w/o ques <C> 53.0 <R> <C> CoS-E-limited-open-ended <C> 67.6 <R> <C> CoS-E-selected <C> 70.0 <R> <C> CoS-E-open-ended w/o ques <C> 84.5 <R> <C> CoS-E-open-ended* <C> [BOLD] 89.8 <CAP> Table 4: Oracle results on CQA dev-random-split using different variants of CoS-E for both training and validation. * indicates CoS-E-open-ended used during both training and validation to contrast with CoS-E-open-ended used only during training in Table 2. <COT> Looking at the table, we can see that the accuracy of the "CoS-E-open-ended*" method is higher than the accuracy of all other methods.
<R> <C> [BOLD] Method <C> [BOLD] Accuracy (%) <R> <C> CoS-E-selected w/o ques <C> 53.0 <R> <C> CoS-E-limited-open-ended <C> 67.6 <R> <C> CoS-E-selected <C> 70.0 <R> <C> CoS-E-open-ended w/o ques <C> 84.5 <R> <C> CoS-E-open-ended* <C> [BOLD] 89.8 <CAP> Table 4: Oracle results on CQA dev-random-split using different variants of CoS-E for both training and validation. * indicates CoS-E-open-ended used during both training and validation to contrast with CoS-E-open-ended used only during training in Table 2. <COT> Looking at the table, we can see that the accuracy of the "CoS-E-selected" method is higher than the accuracy of the "CoS-E-limited-open-ended" method.
<R> <C> [BOLD] Method <C> [BOLD] SWAG <C> [BOLD] Story Cloze <R> <C> BERT <C> 84.2 <C> 89.8 <R> <C> + expl transfer <C> 83.6 <C> 89.5 <CAP> Table 6: Results for explanation transfer from CQA to out-of-domain SWAG and Sotry Cloze tasks. <COT> Looking at the "Method" column, we can see that there are two different methods mentioned: BERT and + expl transfer.
<R> <C> [BOLD] Method <C> [BOLD] SWAG <C> [BOLD] Story Cloze <R> <C> BERT <C> 84.2 <C> 89.8 <R> <C> + expl transfer <C> 83.6 <C> 89.5 <CAP> Table 6: Results for explanation transfer from CQA to out-of-domain SWAG and Sotry Cloze tasks. <COT> Looking at the "SWAG" column, we can see that the scores for both BERT and + expl transfer are lower than the scores for the Story Cloze task.
<R> <C> Method <C> P <C> R <C> F1 <C> eP <C> eR <C> eF1 <R> <C> Redi et al.  2019 <C> 0.753 <C> 0.709 <C> 0.730 [0.76]* <C> - <C> - <C> - <R> <C> BERT <C> 0.788 ± 0.013 <C> 0.837 ± 0.045 <C> 0.810 ± 0.015 <C> 0.790 <C> 0.853 <C> 0.820 <R> <C> BERT + PU <C> 0.788 ± 0.009 <C> 0.843 ± 0.030 <C> 0.814 ± 0.010 <C> 0.790 <C> 0.856 <C> 0.822 <R> <C> BERT +  [ITALIC] PUC <C> 0.784 ± 0.009 <C> [BOLD] 0.856 ± 0.032 <C> [BOLD] 0.818 ± 0.010 <C> 0.786 <C> [BOLD] 0.871 <C> [BOLD] 0.826 <CAP> Table 1: F1 and ensembled F1 score for citation needed detection training on the FA split and testing on the LQN split of Redi et al. (2019). The FA split contains statements with citations from featured articles and the LQN split consists of statements which were flagged as not having a citation but needing one. Listed are the mean, standard deviation, and ensembled results across 15 seeds (eP, eR, and eF1). Bold indicates best performance, underline indicates second best. *The reported value is from rerunning their released model on the test dataset. The value in brackets is the value reported in the original paper. <COT> Looking at the "Method" column, we can see that BERT + PUC has the highest precision and recall values compared to other methods.
<R> <C> Method <C> P <C> R <C> F1 <C> eP <C> eR <C> eF1 <R> <C> Redi et al.  2019 <C> 0.753 <C> 0.709 <C> 0.730 [0.76]* <C> - <C> - <C> - <R> <C> BERT <C> 0.788 ± 0.013 <C> 0.837 ± 0.045 <C> 0.810 ± 0.015 <C> 0.790 <C> 0.853 <C> 0.820 <R> <C> BERT + PU <C> 0.788 ± 0.009 <C> 0.843 ± 0.030 <C> 0.814 ± 0.010 <C> 0.790 <C> 0.856 <C> 0.822 <R> <C> BERT +  [ITALIC] PUC <C> 0.784 ± 0.009 <C> [BOLD] 0.856 ± 0.032 <C> [BOLD] 0.818 ± 0.010 <C> 0.786 <C> [BOLD] 0.871 <C> [BOLD] 0.826 <CAP> Table 1: F1 and ensembled F1 score for citation needed detection training on the FA split and testing on the LQN split of Redi et al. (2019). The FA split contains statements with citations from featured articles and the LQN split consists of statements which were flagged as not having a citation but needing one. Listed are the mean, standard deviation, and ensembled results across 15 seeds (eP, eR, and eF1). Bold indicates best performance, underline indicates second best. *The reported value is from rerunning their released model on the test dataset. The value in brackets is the value reported in the original paper. <COT> Looking at the "F1" column, we can see that BERT + PUC has a higher F1 score than BERT.
<R> <C> Top-synset <C> Size <C> Max. <C> #basic <C> [ITALIC] κ <R> <C> of domain <C> #syns. <C> depth <C> level <C> [EMPTY] <R> <C> Hand tool <C> 157 <C> 6 <C> 30 <C> 0.73 <R> <C> Edible fruit <C> 197 <C> 5 <C> 77 <C> 0.78 <R> <C> Musical Instr. <C> 164 <C> 7 <C> 54 <C> 0.64 <R> <C> All <C> 518 <C> 7 <C> 161 <C> 0.73 <CAP> Table 1: Properties of the training and test set: the number of synsets, the maximum depth of the hierarchy counted from (and including) the top synset of the domain, the number of basic level concepts, and the inter-rater agreement (κ). <COT> Looking at the "Size" column, we can see that the size of the "Edible fruit" domain is larger than the size of the "Hand tool" domain.
<R> <C> Top-synset <C> Size <C> Max. <C> #basic <C> [ITALIC] κ <R> <C> of domain <C> #syns. <C> depth <C> level <C> [EMPTY] <R> <C> Hand tool <C> 157 <C> 6 <C> 30 <C> 0.73 <R> <C> Edible fruit <C> 197 <C> 5 <C> 77 <C> 0.78 <R> <C> Musical Instr. <C> 164 <C> 7 <C> 54 <C> 0.64 <R> <C> All <C> 518 <C> 7 <C> 161 <C> 0.73 <CAP> Table 1: Properties of the training and test set: the number of synsets, the maximum depth of the hierarchy counted from (and including) the top synset of the domain, the number of basic level concepts, and the inter-rater agreement (κ). <COT> Looking at the "κ" column, we can see that the inter-rater agreement for the "Edible fruit" domain is higher than the inter-rater agreement for the "Musical Instr." domain.
<R> <C> Feature <C> All <C> Tool <C> Fruit <C> Music <R> <C> depth_in_hierarchy <C> 1 <C> 1 <C> 1 <C> 3 <R> <C> G.Ngrams_2008_max <C> 2 <C> 2 <C> 5 <C> 2 <R> <C> gloss_length <C> 3 <C> 4 <C> 7 <C> 1 <R> <C> polysemy_max <C> 4 <C> 3 <C> 4 <C> 6 <R> <C> word_length_min <C> 5 <C> 5 <C> 3 <C> 4 <R> <C> nr_of_partOfs <C> 6 <C> 8 <C> 2 <C> 8 <R> <C> nr_of_hyponyms <C> 7 <C> 6 <C> 6 <C> 5 <R> <C> nr_of_synonyms <C> 8 <C> 7 <C> 8 <C> 7 <R> <C> nr_of_direct_hypernyms <C> 9 <C> 9 <C> 9 <C> 9 <CAP> Table 2: Features ranked in order of importance. <COT> Looking at the "polysemy_max" column, we can see that the value for the "Fruit" feature is 4, which is higher than the values for the other features.
<R> <C> Normalized features: New <C> Normalized features: Trained <C> None Bal. <C> None  [ITALIC] κ <C> Structural Bal. <C> Structural  [ITALIC] κ <C> Lexical Bal. <C> Lexical  [ITALIC] κ <C> Frequency Bal. <C> Frequency  [ITALIC] κ <R> <C> domain <C> on <C> Acc. <C> [EMPTY] <C> Acc. <C> [EMPTY] <C> Acc. <C> [EMPTY] <C> Acc. <C> [EMPTY] <R> <C> Tools <C> Fruit+Music <C> 0.69 <C> 0.40 <C> 0.84 <C> 0.74 <C> 0.83 <C> 0.68 <C> 0.65 <C> 0.30 <R> <C> Fruit <C> Tools+Music <C> 0.66 <C> 0.30 <C> 0.82 <C> 0.62 <C> 0.73 <C> 0.43 <C> 0.66 <C> 0.31 <R> <C> Music <C> Tools+Fruit <C> 0.62 <C> 0.27 <C> 0.68 <C> 0.37 <C> 0.73 <C> 0.41 <C> 0.55 <C> 0.12 <CAP> Table 3: Balanced accuracy and κ of predictions made in a new domain, with or without normalization. <COT> Looking at the "Bal. Acc." column, we can see that the highest balanced accuracy is achieved with the "Structural" normalization method for all three domains.
<R> <C> Normalized features: New <C> Normalized features: Trained <C> None Bal. <C> None  [ITALIC] κ <C> Structural Bal. <C> Structural  [ITALIC] κ <C> Lexical Bal. <C> Lexical  [ITALIC] κ <C> Frequency Bal. <C> Frequency  [ITALIC] κ <R> <C> domain <C> on <C> Acc. <C> [EMPTY] <C> Acc. <C> [EMPTY] <C> Acc. <C> [EMPTY] <C> Acc. <C> [EMPTY] <R> <C> Tools <C> Fruit+Music <C> 0.69 <C> 0.40 <C> 0.84 <C> 0.74 <C> 0.83 <C> 0.68 <C> 0.65 <C> 0.30 <R> <C> Fruit <C> Tools+Music <C> 0.66 <C> 0.30 <C> 0.82 <C> 0.62 <C> 0.73 <C> 0.43 <C> 0.66 <C> 0.31 <R> <C> Music <C> Tools+Fruit <C> 0.62 <C> 0.27 <C> 0.68 <C> 0.37 <C> 0.73 <C> 0.41 <C> 0.55 <C> 0.12 <CAP> Table 3: Balanced accuracy and κ of predictions made in a new domain, with or without normalization. <COT> Looking at the "Normalized features: Trained" column, we can see that the accuracy is generally higher when the features are normalized compared to when they are not normalized.
<R> <C> POS <C> Training <C> Dev. <C> Test <R> <C> Noun <C> 6,710 <C> 111 <C> 1,499 <R> <C> Verb <C> 11,269 <C> 193 <C> 2,426 <R> <C> Adjective <C> 610 <C> 9 <C> 118 <R> <C> Preposition <C> 146 <C> 1 <C> 25 <R> <C> Overall Event Tokens <C> 18,735 <C> 314 <C> 4,068 <CAP> Table 1: Distribution of the event mentions per POS per token in all datasets of the EVENTI corpus. <COT> Looking at the "Table 1" caption, we can see that the table provides the distribution of event mentions per part-of-speech (POS) per token in all datasets of the EVENTI corpus.
<R> <C> POS <C> Training <C> Dev. <C> Test <R> <C> Noun <C> 6,710 <C> 111 <C> 1,499 <R> <C> Verb <C> 11,269 <C> 193 <C> 2,426 <R> <C> Adjective <C> 610 <C> 9 <C> 118 <R> <C> Preposition <C> 146 <C> 1 <C> 25 <R> <C> Overall Event Tokens <C> 18,735 <C> 314 <C> 4,068 <CAP> Table 1: Distribution of the event mentions per POS per token in all datasets of the EVENTI corpus. <COT> Looking at the "Noun" row and the "Training" column, we can see that there are 6,710 noun event mentions in the training dataset.
<R> <C> Class <C> Training <C> Dev. <C> Test <R> <C> OCCURRENCE <C> 9,041 <C> 162 <C> 1,949 <R> <C> ASPECTUAL <C> 446 <C> 14 <C> 107 <R> <C> I_STATE <C> 1,599 <C> 29 <C> 355 <R> <C> I_ACTION <C> 1,476 <C> 25 <C> 357 <R> <C> PERCEPTION <C> 162 <C> 2 <C> 37 <R> <C> REPORTING <C> 714 <C> 8 <C> 149 <R> <C> STATE <C> 4,090 <C> 61 <C> 843 <R> <C> Overall Events <C> 17,528 <C> 301 <C> 3,798 <CAP> Table 2: Distribution of the event mentions per class in all datasets of the EVENTI corpus. <COT> looking at the "Overall Events" row, "17,528" cell, "Training" column, finding the total number of events in the training dataset.
<R> <C> Class <C> Training <C> Dev. <C> Test <R> <C> OCCURRENCE <C> 9,041 <C> 162 <C> 1,949 <R> <C> ASPECTUAL <C> 446 <C> 14 <C> 107 <R> <C> I_STATE <C> 1,599 <C> 29 <C> 355 <R> <C> I_ACTION <C> 1,476 <C> 25 <C> 357 <R> <C> PERCEPTION <C> 162 <C> 2 <C> 37 <R> <C> REPORTING <C> 714 <C> 8 <C> 149 <R> <C> STATE <C> 4,090 <C> 61 <C> 843 <R> <C> Overall Events <C> 17,528 <C> 301 <C> 3,798 <CAP> Table 2: Distribution of the event mentions per class in all datasets of the EVENTI corpus. <COT> looking at the "ASPECTUAL" row, "107" cell, "Test" column, finding the number of events belonging to the ASPECTUAL class in the test dataset.
<R> <C> Embedding Parameter <C> Strict Evaluation R <C> Strict Evaluation P <C> Strict Evaluation F1 <C> Strict Evaluation F1-class <C> Relaxed Evaluation R <C> Relaxed Evaluation P <C> Relaxed Evaluation F1 <C> Relaxed Evaluation F1-class <R> <C> Berardi2015_w2v <C> 0.868 <C> 0.868 <C> 0.868 <C> 0.705 <C> 0.892 <C> 0.892 <C> 0.892 <C> 0.725 <R> <C> Berardi2015_Glove <C> 0.848 <C> 0.872 <C> 0.860 <C> 0.697 <C> 0.870 <C> 0.895 <C> 0.882 <C> 0.714 <R> <C> Fastext-It <C> [BOLD] 0.897 <C> 0.863 <C> [BOLD] 0.880 <C> [BOLD] 0.736 <C> [BOLD] 0.921 <C> 0.887 <C> [BOLD] 0.903 <C> [BOLD] 0.756 <R> <C> ILC-ItWack <C> 0.831 <C> [BOLD] 0.884 <C> 0.856 <C> 0.702 <C> 0.860 <C> [BOLD] 0.914 <C> 0.886 <C> 0.725 <R> <C> DH-FBK_100 <C> 0.855 <C> 0.859 <C> 0.857 <C> 0.685 <C> 0.881 <C> 0.885 <C> 0.883 <C> 0.705 <R> <C> FBK-HLT@EVENTI 2014 <C> 0.850 <C> [ITALIC] 0.884 <C> 0.867 <C> 0.671 <C> 0.868 <C> 0.902 <C> 0.884 <C> 0.685 <CAP> Table 3: Results for Bubtask B Main Task - Event detection and classification. <COT> Looking at the "Embedding Parameter" column, we can see that the embedding parameter "Fastext-It" has the highest values for both Strict Evaluation F1 and Relaxed Evaluation F1.
<R> <C> Embedding Parameter <C> Strict Evaluation R <C> Strict Evaluation P <C> Strict Evaluation F1 <C> Strict Evaluation F1-class <C> Relaxed Evaluation R <C> Relaxed Evaluation P <C> Relaxed Evaluation F1 <C> Relaxed Evaluation F1-class <R> <C> Berardi2015_w2v <C> 0.868 <C> 0.868 <C> 0.868 <C> 0.705 <C> 0.892 <C> 0.892 <C> 0.892 <C> 0.725 <R> <C> Berardi2015_Glove <C> 0.848 <C> 0.872 <C> 0.860 <C> 0.697 <C> 0.870 <C> 0.895 <C> 0.882 <C> 0.714 <R> <C> Fastext-It <C> [BOLD] 0.897 <C> 0.863 <C> [BOLD] 0.880 <C> [BOLD] 0.736 <C> [BOLD] 0.921 <C> 0.887 <C> [BOLD] 0.903 <C> [BOLD] 0.756 <R> <C> ILC-ItWack <C> 0.831 <C> [BOLD] 0.884 <C> 0.856 <C> 0.702 <C> 0.860 <C> [BOLD] 0.914 <C> 0.886 <C> 0.725 <R> <C> DH-FBK_100 <C> 0.855 <C> 0.859 <C> 0.857 <C> 0.685 <C> 0.881 <C> 0.885 <C> 0.883 <C> 0.705 <R> <C> FBK-HLT@EVENTI 2014 <C> 0.850 <C> [ITALIC] 0.884 <C> 0.867 <C> 0.671 <C> 0.868 <C> 0.902 <C> 0.884 <C> 0.685 <CAP> Table 3: Results for Bubtask B Main Task - Event detection and classification. <COT> Looking at the "Strict Evaluation P" column, we can see that the embedding parameter "ILC-ItWack" has the highest value.
<R> <C> [BOLD] Model <C> [BOLD] BLEU R <C> [BOLD] BLEU P <C> [BOLD] BLEU F1 <C> [BOLD] BOW Embedding A <C> [BOLD] BOW Embedding E <C> [BOLD] BOW Embedding G <C> [BOLD] intra-dist dist-1 <C> [BOLD] intra-dist dist-2 <C> [BOLD] inter-dist dist-1 <C> [BOLD] inter-dist dist-2 <C> [BOLD] L <R> <C> HRED <C> 0.262 <C> 0.262 <C> 0.262 <C> 0.820 <C> 0.537 <C> 0.832 <C> 0.813 <C> 0.452 <C> 0.081 <C> 0.045 <C> 12.1 <R> <C> SeqGAN <C> 0.282 <C> [BOLD] 0.282 <C> 0.282 <C> 0.817 <C> 0.515 <C> 0.748 <C> 0.705 <C> 0.521 <C> 0.070 <C> 0.052 <C> [BOLD] 17.2 <R> <C> CVAE <C> 0.295 <C> 0.258 <C> 0.275 <C> 0.836 <C> 0.572 <C> 0.846 <C> 0.803 <C> 0.415 <C> 0.112 <C> 0.102 <C> 12.4 <R> <C> CVAE-BOW <C> 0.298 <C> 0.272 <C> 0.284 <C> 0.828 <C> 0.555 <C> 0.840 <C> 0.819 <C> 0.493 <C> 0.107 <C> 0.099 <C> 12.5 <R> <C> CVAE-CO <C> 0.299 <C> 0.269 <C> 0.283 <C> 0.839 <C> 0.557 <C> 0.855 <C> 0.863 <C> 0.581 <C> 0.111 <C> 0.110 <C> 10.3 <R> <C> VHRED <C> 0.253 <C> 0.231 <C> 0.242 <C> 0.810 <C> 0.531 <C> 0.844 <C> [BOLD] 0.881 <C> 0.522 <C> 0.110 <C> 0.092 <C> 8.74 <R> <C> VHCR <C> 0.276 <C> 0.234 <C> 0.254 <C> 0.826 <C> 0.546 <C> 0.851 <C> 0.877 <C> 0.536 <C> 0.130 <C> 0.131 <C> 9.29 <R> <C> DialogWAE <C> 0.394 <C> 0.254 <C> 0.309 <C> 0.897 <C> 0.627 <C> 0.887 <C> 0.713 <C> 0.651 <C> 0.245 <C> 0.413 <C> 15.5 <R> <C> DialogWAE-GMP <C> [BOLD] 0.420 <C> 0.258 <C> [BOLD]  0.319 <C> [BOLD] 0.925 <C> [BOLD]  0.661 <C> [BOLD]  0.894 <C> 0.713 <C> [BOLD]  0.671 <C> [BOLD] 0.333 <C> [BOLD]  0.555 <C> 15.2 <CAP> Table 1: Performance comparison on the SwitchBoard dataset (P: n-gram precision, R: n-gram recall, A: Average, E: Extrema, G: Greedy, L: average length) <COT> Looking at the "Model" column, we can see that DialogWAE-GMP has the highest BLEU R, BLEU F1, BOW Embedding A, BOW Embedding E, BOW Embedding G, intra-dist dist-1, intra-dist dist-2, inter-dist dist-1, and inter-dist dist-2 scores among all the models.
<R> <C> [BOLD] Model <C> [BOLD] BLEU R <C> [BOLD] BLEU P <C> [BOLD] BLEU F1 <C> [BOLD] BOW Embedding A <C> [BOLD] BOW Embedding E <C> [BOLD] BOW Embedding G <C> [BOLD] intra-dist dist-1 <C> [BOLD] intra-dist dist-2 <C> [BOLD] inter-dist dist-1 <C> [BOLD] inter-dist dist-2 <C> [BOLD] L <R> <C> HRED <C> 0.262 <C> 0.262 <C> 0.262 <C> 0.820 <C> 0.537 <C> 0.832 <C> 0.813 <C> 0.452 <C> 0.081 <C> 0.045 <C> 12.1 <R> <C> SeqGAN <C> 0.282 <C> [BOLD] 0.282 <C> 0.282 <C> 0.817 <C> 0.515 <C> 0.748 <C> 0.705 <C> 0.521 <C> 0.070 <C> 0.052 <C> [BOLD] 17.2 <R> <C> CVAE <C> 0.295 <C> 0.258 <C> 0.275 <C> 0.836 <C> 0.572 <C> 0.846 <C> 0.803 <C> 0.415 <C> 0.112 <C> 0.102 <C> 12.4 <R> <C> CVAE-BOW <C> 0.298 <C> 0.272 <C> 0.284 <C> 0.828 <C> 0.555 <C> 0.840 <C> 0.819 <C> 0.493 <C> 0.107 <C> 0.099 <C> 12.5 <R> <C> CVAE-CO <C> 0.299 <C> 0.269 <C> 0.283 <C> 0.839 <C> 0.557 <C> 0.855 <C> 0.863 <C> 0.581 <C> 0.111 <C> 0.110 <C> 10.3 <R> <C> VHRED <C> 0.253 <C> 0.231 <C> 0.242 <C> 0.810 <C> 0.531 <C> 0.844 <C> [BOLD] 0.881 <C> 0.522 <C> 0.110 <C> 0.092 <C> 8.74 <R> <C> VHCR <C> 0.276 <C> 0.234 <C> 0.254 <C> 0.826 <C> 0.546 <C> 0.851 <C> 0.877 <C> 0.536 <C> 0.130 <C> 0.131 <C> 9.29 <R> <C> DialogWAE <C> 0.394 <C> 0.254 <C> 0.309 <C> 0.897 <C> 0.627 <C> 0.887 <C> 0.713 <C> 0.651 <C> 0.245 <C> 0.413 <C> 15.5 <R> <C> DialogWAE-GMP <C> [BOLD] 0.420 <C> 0.258 <C> [BOLD]  0.319 <C> [BOLD] 0.925 <C> [BOLD]  0.661 <C> [BOLD]  0.894 <C> 0.713 <C> [BOLD]  0.671 <C> [BOLD] 0.333 <C> [BOLD]  0.555 <C> 15.2 <CAP> Table 1: Performance comparison on the SwitchBoard dataset (P: n-gram precision, R: n-gram recall, A: Average, E: Extrema, G: Greedy, L: average length) <COT> Looking at the "Model" column, we can see that VHRED has the highest BLEU P score among all the models.
<R> <C> Model <C> Coherence <C> Diversity <C> Informative <R> <C> CVAE-CO <C> 14.4% <C> 19.2% <C> 24.8% <R> <C> VHCR <C> 26.8% <C> 22.4% <C> 20.4% <R> <C> DialogWAE <C> 27.6% <C> [BOLD] 29.2% <C> 25.6% <R> <C> DialogWAE-GMP <C> [BOLD] 31.6% <C> [BOLD] 29.2% <C> [BOLD] 29.6% <CAP> Table 5: Human judgments for models trained on the Dailydialog dataset <COT> Looking at "CAP" tag, finding the Table 5 shows human judgments for models trained on the Dailydialog dataset.
<R> <C> Model <C> Coherence <C> Diversity <C> Informative <R> <C> CVAE-CO <C> 14.4% <C> 19.2% <C> 24.8% <R> <C> VHCR <C> 26.8% <C> 22.4% <C> 20.4% <R> <C> DialogWAE <C> 27.6% <C> [BOLD] 29.2% <C> 25.6% <R> <C> DialogWAE-GMP <C> [BOLD] 31.6% <C> [BOLD] 29.2% <C> [BOLD] 29.6% <CAP> Table 5: Human judgments for models trained on the Dailydialog dataset <COT> Looking at the "Model" column, "DialogWAE-GMP" row, and the "[BOLD] 31.6%" cell, finding that DialogWAE-GMP has the highest coherence score among all models.
<R> <C> [EMPTY] <C> BLEU <C> Empathy <C> Relevance <C> Fluency <R> <C> Gold <C> - <C> 3.651 <C> 3.752 <C> 3.718 <R> <C> Seq2Seq <C> 2.29 <C> 3.013 <C> 3.173 <C> 2.967 <R> <C> MultiSeq <C> [BOLD] 2.45 <C> 2.979 <C> 3.25 <C> 2.952 <R> <C> RL Current <C> 2.31 <C> 3.18 <C> 3.187 <C> 3.14 <R> <C> RL Look-ahead <C> 2.32 <C> [BOLD] 3.327 <C> [BOLD] 3.593 <C> [BOLD] 3.327 <CAP> Table 2: Comparison between our proposed methods and baselines. RL Look-ahead model achieves the highest score for all three aspects: Empathy, Relevance, and Fluency. MultiSeq model achieves highest BLEU score, but not significant. <COT> Looking at the "RL Look-ahead" row, "[BOLD] 3.327" cell, "Empathy" cell, "Relevance" cell, and "Fluency" cell, we can see that the RL Look-ahead model achieves the highest score for all three aspects: Empathy, Relevance, and Fluency.
<R> <C> [EMPTY] <C> BLEU <C> Empathy <C> Relevance <C> Fluency <R> <C> Gold <C> - <C> 3.651 <C> 3.752 <C> 3.718 <R> <C> Seq2Seq <C> 2.29 <C> 3.013 <C> 3.173 <C> 2.967 <R> <C> MultiSeq <C> [BOLD] 2.45 <C> 2.979 <C> 3.25 <C> 2.952 <R> <C> RL Current <C> 2.31 <C> 3.18 <C> 3.187 <C> 3.14 <R> <C> RL Look-ahead <C> 2.32 <C> [BOLD] 3.327 <C> [BOLD] 3.593 <C> [BOLD] 3.327 <CAP> Table 2: Comparison between our proposed methods and baselines. RL Look-ahead model achieves the highest score for all three aspects: Empathy, Relevance, and Fluency. MultiSeq model achieves highest BLEU score, but not significant. <COT> Looking at the "MultiSeq" row, "[BOLD] 2.45" cell, and "BLEU" cell, we can see that the MultiSeq model achieves the highest BLEU score.
<R> <C> [BOLD] Model <C> [BOLD] SParC Ques.Match <C> [BOLD] SParC Int.Match <C> [BOLD] CoSQL Ques.Match <C> [BOLD] CoSQL Int.Match <R> <C> SyntaxSQL-con <C> 18.5 <C> 4.3 <C> 15.1 <C> 2.7 <R> <C> CD-Seq2Seq <C> 21.9 <C> 8.1 <C> 13.8 <C> 2.1 <R> <C> EditSQL <C> 33.0 <C> 16.4 <C> 22.2 <C> 5.8 <R> <C> Ours <C> [BOLD] 41.8 <C> [BOLD] 20.6 <C> [BOLD] 33.5 <C> [BOLD] 9.6 <R> <C> EditSQL + BERT <C> 47.2 <C> 29.5 <C> 40.0 <C> 11.0 <R> <C> Ours + BERT <C> [BOLD] 52.6 <C> [BOLD] 29.9 <C> [BOLD] 41.0 <C> [BOLD] 14.0 <CAP> Table 1: We report the best performance observed in 5 runs on the development sets of both SParC and CoSQL, since their test sets are not public. We also conduct Wilcoxon signed-rank tests between our method and the baselines, and the bold results show the improvements of our model are significant with p < 0.005. <COT> Looking at the "Model" column, we can see that the performance of "Ours" is better than the performance of "EditSQL" and "EditSQL + BERT" on all metrics (SParC Ques.Match, SParC Int.Match, CoSQL Ques.Match, CoSQL Int.Match).
<R> <C> [BOLD] Model <C> [BOLD] SParC Ques.Match <C> [BOLD] SParC Int.Match <C> [BOLD] CoSQL Ques.Match <C> [BOLD] CoSQL Int.Match <R> <C> SyntaxSQL-con <C> 18.5 <C> 4.3 <C> 15.1 <C> 2.7 <R> <C> CD-Seq2Seq <C> 21.9 <C> 8.1 <C> 13.8 <C> 2.1 <R> <C> EditSQL <C> 33.0 <C> 16.4 <C> 22.2 <C> 5.8 <R> <C> Ours <C> [BOLD] 41.8 <C> [BOLD] 20.6 <C> [BOLD] 33.5 <C> [BOLD] 9.6 <R> <C> EditSQL + BERT <C> 47.2 <C> 29.5 <C> 40.0 <C> 11.0 <R> <C> Ours + BERT <C> [BOLD] 52.6 <C> [BOLD] 29.9 <C> [BOLD] 41.0 <C> [BOLD] 14.0 <CAP> Table 1: We report the best performance observed in 5 runs on the development sets of both SParC and CoSQL, since their test sets are not public. We also conduct Wilcoxon signed-rank tests between our method and the baselines, and the bold results show the improvements of our model are significant with p < 0.005. <COT> Looking at the "CoSQL Ques.Match" column, we can see that the performance of "Ours + BERT" is better than the performance of "Ours".
<R> <C> [EMPTY] <C> Detection (Average Precision) BLESS <C> Detection (Average Precision) EVAL <C> Detection (Average Precision) LEDS <C> Detection (Average Precision) SHWARTZ <C> Detection (Average Precision) WBLESS <C> Direction (Average Accuracy) BLESS <C> Direction (Average Accuracy) WBLESS <C> Direction (Average Accuracy) BIBLESS <R> <C> Count based p(x,y) <C> .49 <C> .38 <C> .71 <C> .29 <C> .74 <C> .46 <C> .69 <C> .62 <R> <C> ppmi(x,y) <C> .45 <C> .36 <C> .70 <C> .28 <C> .72 <C> .46 <C> .68 <C> .61 <R> <C> SVD ppmi(x,y) <C> .76 <C> .48 <C> .84 <C> .44 <C> .96 <C> .96 <C> .87 <C> .85 <R> <C> HyperbolicCones <C> [BOLD] .81 <C> [BOLD] .50 <C> .89 <C> [BOLD] .50 <C> [BOLD] .98 <C> .94 <C> .90 <C> . [BOLD] 87 <R> <C> Proposed SPON <C> [BOLD] .81 <C> [BOLD] .50 <C> [BOLD] .91 <C> [BOLD] .50 <C> [BOLD] .98 <C> [BOLD] .97 <C> [BOLD] .91 <C> [BOLD] .87 <CAP> Table 3: Results on the unsupervised hypernym detection and direction prediction tasks. The first three rows of results are from Roller, Kiela, and Nickel (2018). The HyperbolicCones results were reported by Le et al. (2019). The improvements in LEDS and BLESS benchmark are statistically significant with two-tailed p values being 0.019 and ≤ 0.001 respectively. <COT> Looking at the "Detection (Average Precision)" column, we can see that the average precision for LEDS is higher than BLESS, EVAL, SHWARTZ, and WBLESS.
<R> <C> [EMPTY] <C> Detection (Average Precision) BLESS <C> Detection (Average Precision) EVAL <C> Detection (Average Precision) LEDS <C> Detection (Average Precision) SHWARTZ <C> Detection (Average Precision) WBLESS <C> Direction (Average Accuracy) BLESS <C> Direction (Average Accuracy) WBLESS <C> Direction (Average Accuracy) BIBLESS <R> <C> Count based p(x,y) <C> .49 <C> .38 <C> .71 <C> .29 <C> .74 <C> .46 <C> .69 <C> .62 <R> <C> ppmi(x,y) <C> .45 <C> .36 <C> .70 <C> .28 <C> .72 <C> .46 <C> .68 <C> .61 <R> <C> SVD ppmi(x,y) <C> .76 <C> .48 <C> .84 <C> .44 <C> .96 <C> .96 <C> .87 <C> .85 <R> <C> HyperbolicCones <C> [BOLD] .81 <C> [BOLD] .50 <C> .89 <C> [BOLD] .50 <C> [BOLD] .98 <C> .94 <C> .90 <C> . [BOLD] 87 <R> <C> Proposed SPON <C> [BOLD] .81 <C> [BOLD] .50 <C> [BOLD] .91 <C> [BOLD] .50 <C> [BOLD] .98 <C> [BOLD] .97 <C> [BOLD] .91 <C> [BOLD] .87 <CAP> Table 3: Results on the unsupervised hypernym detection and direction prediction tasks. The first three rows of results are from Roller, Kiela, and Nickel (2018). The HyperbolicCones results were reported by Le et al. (2019). The improvements in LEDS and BLESS benchmark are statistically significant with two-tailed p values being 0.019 and ≤ 0.001 respectively. <COT> Looking at the "Direction (Average Accuracy)" column, we can see that the average accuracy for BLESS is higher than WBLESS and BIBLESS.
<R> <C> [EMPTY] <C> BLESS <C> EVAL <C> LEDS <C> WBLESS <R> <C> [ITALIC] RELU+ [ITALIC] Residual <C> [BOLD] .81 <C> [BOLD] .50 <C> [BOLD] .91 <C> [BOLD] .98 <R> <C> [ITALIC] RELU Only <C> .73 <C> .49 <C> .82 <C> .96 <R> <C> [ITALIC] Tanh+ [ITALIC] Residual <C> .79 <C> .49 <C> .90 <C> [BOLD] .98 <CAP> Table 4: Ablation tests reporting Average Precision values on the unsupervised hypernym detection task, signifying the choice of layers utilized in our proposed SPON model. The first row represents SPON i.e. a RELU layer followed by a Residual connection. The second row removes the Residual connection, whereas the third row substitutes the non-negative activation layer RELU with Tanh that can take negative values. <COT> Looking at the table, we can see that the highest Average Precision values are achieved when using the RELU+ Residual combination for all evaluation metrics (BLESS, EVAL, LEDS, WBLESS).
<R> <C> [EMPTY] <C> BLESS <C> EVAL <C> LEDS <C> WBLESS <R> <C> [ITALIC] RELU+ [ITALIC] Residual <C> [BOLD] .81 <C> [BOLD] .50 <C> [BOLD] .91 <C> [BOLD] .98 <R> <C> [ITALIC] RELU Only <C> .73 <C> .49 <C> .82 <C> .96 <R> <C> [ITALIC] Tanh+ [ITALIC] Residual <C> .79 <C> .49 <C> .90 <C> [BOLD] .98 <CAP> Table 4: Ablation tests reporting Average Precision values on the unsupervised hypernym detection task, signifying the choice of layers utilized in our proposed SPON model. The first row represents SPON i.e. a RELU layer followed by a Residual connection. The second row removes the Residual connection, whereas the third row substitutes the non-negative activation layer RELU with Tanh that can take negative values. <COT> Looking at the table, we can see that the Tanh+ Residual combination achieves a higher Average Precision value than the Tanh Only combination for the WBLESS evaluation metric.
<R> <C> Method <C> Average Precision <R> <C> OE  <C> 0.761 <R> <C> Smoothed Box  <C> 0.795 <R> <C> SPON (Our Approach) <C> [BOLD] 0.811 <CAP> Table 5: Results on the unsupervised hypernym detection task for BLESS dataset. With 13,089 test instances, the improvement in Average Precision values obtained by SPON as compared against Smoothed Box model is statistically significant with two-tailed p value equals 0.00116. <COT> Looking at the "Method" column, we can see that the SPON approach has the highest Average Precision value compared to the other methods.
<R> <C> Method <C> Average Precision <R> <C> OE  <C> 0.761 <R> <C> Smoothed Box  <C> 0.795 <R> <C> SPON (Our Approach) <C> [BOLD] 0.811 <CAP> Table 5: Results on the unsupervised hypernym detection task for BLESS dataset. With 13,089 test instances, the improvement in Average Precision values obtained by SPON as compared against Smoothed Box model is statistically significant with two-tailed p value equals 0.00116. <COT> Referring to the caption, it states that the improvement in Average Precision values obtained by SPON compared to the Smoothed Box model is statistically significant with a two-tailed p-value of 0.00116.
<R> <C> Model <C> R-1 <C> R-2 <R> <C> First sentences <C> 28.6 <C> 17.3 <R> <C> First  [ITALIC] k words <C> 35.7 <C> 21.6 <R> <C> Full  <C> 42.2 <C> 24.9 <R> <C> ML+RL+intra-attn  <C> 42.94 <C> 26.02 <R> <C> Two-Stage + RL (Ours) <C> [BOLD] 45.33 <C> [BOLD] 26.53 <CAP> Table 2: Limited length ROUGE recall results on the NYT50 test set. <COT> Looking at the "Model" column, we can see that the "Two-Stage + RL (Ours)" model has the highest ROUGE-1 and ROUGE-2 scores compared to the other models.
<R> <C> Model <C> R-1 <C> R-2 <R> <C> First sentences <C> 28.6 <C> 17.3 <R> <C> First  [ITALIC] k words <C> 35.7 <C> 21.6 <R> <C> Full  <C> 42.2 <C> 24.9 <R> <C> ML+RL+intra-attn  <C> 42.94 <C> 26.02 <R> <C> Two-Stage + RL (Ours) <C> [BOLD] 45.33 <C> [BOLD] 26.53 <CAP> Table 2: Limited length ROUGE recall results on the NYT50 test set. <COT> Looking at the "First sentences" and "First k words" rows, we can observe that the ROUGE scores increase as more words are considered.
<R> <C> [EMPTY] <C> [BOLD] Music MAP <C> [BOLD] Music MRR <C> [BOLD] Music P@5 <R> <C> CRIM <C> 40.97 <C> 60.93 <C> 41.31 <R> <C> SPON <C> [BOLD] 54.70 <C> [BOLD] 71.20 <C> [BOLD] 56.30 <R> <C> [EMPTY] <C> [BOLD] Medical <C> [BOLD] Medical <C> [BOLD] Medical <R> <C> [EMPTY] <C> MAP <C> MRR <C> P@5 <R> <C> CRIM <C> [BOLD] 34.05 <C> [BOLD] 54.64 <C> [BOLD] 36.77 <R> <C> SPON <C> 33.50 <C> 50.60 <C> 35.10 <CAP> Table 7: Results on SemEval 2018 Domain-specific hypernym discovery task. CRIM is the best system on the domain specific datasets. <COT> Looking at the "Table 7" caption, we can see that the table presents the results on the SemEval 2018 Domain-specific hypernym discovery task.
<R> <C> [EMPTY] <C> [BOLD] Music MAP <C> [BOLD] Music MRR <C> [BOLD] Music P@5 <R> <C> CRIM <C> 40.97 <C> 60.93 <C> 41.31 <R> <C> SPON <C> [BOLD] 54.70 <C> [BOLD] 71.20 <C> [BOLD] 56.30 <R> <C> [EMPTY] <C> [BOLD] Medical <C> [BOLD] Medical <C> [BOLD] Medical <R> <C> [EMPTY] <C> MAP <C> MRR <C> P@5 <R> <C> CRIM <C> [BOLD] 34.05 <C> [BOLD] 54.64 <C> [BOLD] 36.77 <R> <C> SPON <C> 33.50 <C> 50.60 <C> 35.10 <CAP> Table 7: Results on SemEval 2018 Domain-specific hypernym discovery task. CRIM is the best system on the domain specific datasets. <COT> Looking at the "CRIM" row, we can see that CRIM is the best system on the domain-specific datasets.
<R> <C> [BOLD] Length <C> 1-3 <C> 4-6 <C> 7-10 <R> <C> Real Input <C> 0.439 <C> 0.518 <C> 0.566 <R> <C> Pre-trained Greedy <C> 0.446 <C> 0.529 <C> 0.559 <R> <C> RL Greedy <C> 0.486 <C> 0.560 <C> 0.588 <R> <C> RL BeamSearch(50) <C> 0.599 <C> 0.678 <C> 0.709 <R> <C> RL BeamSearch(200) <C> 0.621 <C> 0.694 <C> 0.726 <CAP> Table 1: Average embedding similarity scores between the output and the target output in terms of Real target output list. <COT> Comparing the "Real Input" and "Pre-trained Greedy" columns, we can see that the scores in the "Pre-trained Greedy" column are slightly higher than the scores in the "Real Input" column for all length ranges.
<R> <C> [BOLD] Model <C> [BOLD] ROC <C> [BOLD] SST <C> [BOLD] Amazon <R> <C> [BOLD] Hard <C> 62.2 (4K) <C> 75.5 (6K) <C> 88.5 (67K) <R> <C> [BOLD] DAN <C> 64.3 (91K) <C> 83.1 (91K) <C> 85.4 (91K) <R> <C> [BOLD] BiLSTM <C> 65.2 (844K) <C> 84.8 (1.5M) <C> [BOLD] 90.8 (844K) <R> <C> [BOLD] CNN <C> 64.3 (155K) <C> 82.2 (62K) <C> 90.2 (305K) <R> <C> [BOLD] SoPa <C> [BOLD] 66.5 (255K) <C> [BOLD] 85.6 (255K) <C> 90.5 (256K) <R> <C> [BOLD] SoPa [ITALIC] ms1 <C> 64.4 <C> 84.8 <C> 90.0 <R> <C> [BOLD] SoPa [ITALIC] ms1∖{ [ITALIC] sl} <C> 63.2 <C> 84.6 <C> 89.8 <R> <C> [BOLD] SoPa [ITALIC] ms1∖{ [ITALIC] ϵ} <C> 64.3 <C> 83.6 <C> 89.7 <R> <C> [BOLD] SoPa [ITALIC] ms1∖{ [ITALIC] sl, [ITALIC] ϵ} <C> 64.0 <C> 85.0 <C> 89.5 <CAP> Table 1: Test classification accuracy (and the number of parameters used). The bottom part shows our ablation results: SoPa: our full model. SoPams1: running with max-sum semiring (rather than max-product), with the identity function as our encoder E (see Equation 3). sl: self-loops, ϵ: ϵ transitions. The final row is equivalent to a one-layer CNN. <COT> Looking at the "Hard" row, we can see that the number of parameters used for "SoPa" is 255K.
<R> <C> [BOLD] Annotators <C> [ITALIC] Ao <C> [ITALIC] κ <R> <C> Questions <C> 0.73 <C> 0.63 <R> <C> Features <C> 0.90 <C> 0.67 <R> <C> Answers <C> 0.59 <C> 0.49 <CAP> Table 4: Cohen’s Kappa score (κ) and observed agreement (Ao) for gold standard dialogue <COT> Looking at the "Annotators" row, the "Ao" and "κ" values for "Questions" are higher than those for "Features" and "Answers".
<R> <C> [BOLD] Annotators <C> [ITALIC] Ao <C> [ITALIC] κ <R> <C> Questions <C> 0.73 <C> 0.63 <R> <C> Features <C> 0.90 <C> 0.67 <R> <C> Answers <C> 0.59 <C> 0.49 <CAP> Table 4: Cohen’s Kappa score (κ) and observed agreement (Ao) for gold standard dialogue <COT> Looking at the "Ao" and "κ" values for "Features" and "Answers", the scores for "Features" are higher than those for "Answers".
<R> <C> Wikipedia <C> Wikipedia <C> Majority 16.7% <C> Benchmark 46.7±0.34% <C> Doc2Vec 23.2±1.41% <C> Inceptionfixed 43.7±0.51 <C> biLSTM 54.1±0.47% <C> Inception 57.0±0.63% <C> Joint  [BOLD] 59.4±0.47%† <R> <C> arXiv <C> cs.ai <C> 92.2% <C> 92.6% <C> 73.3±9.81% <C> 92.3±0.29 <C> 91.5±1.03% <C> 92.8±0.79% <C> [BOLD] 93.4±1.07%† <R> <C> arXiv <C> cs.cl <C> 68.9% <C> 75.7% <C> 66.2±8.38% <C> 75.0±1.95 <C> 76.2±1.30% <C> 76.2±2.92% <C> [BOLD] 77.1±3.10% <R> <C> arXiv <C> cs.lg <C> 67.9% <C> 70.7% <C> 64.7±9.08% <C> 73.9±1.23 <C> [BOLD] 81.1±0.83% <C> 79.3±2.94% <C> 79.9±2.54% <CAP> Table 3: Experimental results. The best result for each dataset is indicated in bold, and marked with “†” if it is significantly higher than the second best result (based on a one-tailed Wilcoxon signed-rank test; p<0.05). The results of Benchmark on the arXiv dataset are from the original paper, where the standard deviation values were not reported. All neural models except for Inceptionfixed have larger standard deviation values on arXiv than Wikipedia, which can be explained by the small size of the arXiv test set. <COT> Looking at the "Wikipedia" row, we can see that the "Joint" model has the highest performance with a score of 59.4±0.47%.
<R> <C> Wikipedia <C> Wikipedia <C> Majority 16.7% <C> Benchmark 46.7±0.34% <C> Doc2Vec 23.2±1.41% <C> Inceptionfixed 43.7±0.51 <C> biLSTM 54.1±0.47% <C> Inception 57.0±0.63% <C> Joint  [BOLD] 59.4±0.47%† <R> <C> arXiv <C> cs.ai <C> 92.2% <C> 92.6% <C> 73.3±9.81% <C> 92.3±0.29 <C> 91.5±1.03% <C> 92.8±0.79% <C> [BOLD] 93.4±1.07%† <R> <C> arXiv <C> cs.cl <C> 68.9% <C> 75.7% <C> 66.2±8.38% <C> 75.0±1.95 <C> 76.2±1.30% <C> 76.2±2.92% <C> [BOLD] 77.1±3.10% <R> <C> arXiv <C> cs.lg <C> 67.9% <C> 70.7% <C> 64.7±9.08% <C> 73.9±1.23 <C> [BOLD] 81.1±0.83% <C> 79.3±2.94% <C> 79.9±2.54% <CAP> Table 3: Experimental results. The best result for each dataset is indicated in bold, and marked with “†” if it is significantly higher than the second best result (based on a one-tailed Wilcoxon signed-rank test; p<0.05). The results of Benchmark on the arXiv dataset are from the original paper, where the standard deviation values were not reported. All neural models except for Inceptionfixed have larger standard deviation values on arXiv than Wikipedia, which can be explained by the small size of the arXiv test set. <COT> Looking at the "arXiv cs.ai" row, we can see that the "Inception" model has a higher performance than the "Inceptionfixed" model.
<R> <C> Quality <C> FA <C> GA <C> B <C> C <C> Start <C> Stub <R> <C> FA <C> 397 <C> 83 <C> 20 <C> 0 <C> 0 <C> 0 <R> <C> GA <C> 112 <C> 299 <C> 65 <C> 22 <C> 2 <C> 0 <R> <C> B <C> 23 <C> 53 <C> 253 <C> 75 <C> 44 <C> 7 <R> <C> C <C> 5 <C> 33 <C> 193 <C> 124 <C> 100 <C> 12 <R> <C> Start <C> 1 <C> 6 <C> 36 <C> 85 <C> 239 <C> 84 <R> <C> Stub <C> 0 <C> 0 <C> 6 <C> 7 <C> 63 <C> 345 <CAP> Table 4: Confusion matrix of the Joint model on Wikipedia. Rows are the actual quality classes and columns are the predicted quality classes. The diagonal (gray cells) indicates correct predictions. <COT> Looking at the "Quality" column, we can see that the quality classes are ranked in the order of FA, GA, B, C, Start, and Stub.
<R> <C> Quality <C> FA <C> GA <C> B <C> C <C> Start <C> Stub <R> <C> FA <C> 397 <C> 83 <C> 20 <C> 0 <C> 0 <C> 0 <R> <C> GA <C> 112 <C> 299 <C> 65 <C> 22 <C> 2 <C> 0 <R> <C> B <C> 23 <C> 53 <C> 253 <C> 75 <C> 44 <C> 7 <R> <C> C <C> 5 <C> 33 <C> 193 <C> 124 <C> 100 <C> 12 <R> <C> Start <C> 1 <C> 6 <C> 36 <C> 85 <C> 239 <C> 84 <R> <C> Stub <C> 0 <C> 0 <C> 6 <C> 7 <C> 63 <C> 345 <CAP> Table 4: Confusion matrix of the Joint model on Wikipedia. Rows are the actual quality classes and columns are the predicted quality classes. The diagonal (gray cells) indicates correct predictions. <COT> Looking at the "Quality" column and the corresponding cells, we can see that the number of correct predictions (gray cells) increases as we move from lower quality classes to higher quality classes.
<R> <C> Data set <C> Train <C> Test <C> classes <C> Task <R> <C> AG news <C> 120k <C> 7.6k <C> 4 <C> English news categorization <R> <C> AG news <C> 5k <C> 7.6k <C> 4 <C> English news categorization <R> <C> Sogou news <C> 450k <C> 60k <C> 5 <C> Chinese news categorization <R> <C> Sogou news <C> 10k <C> 60k <C> 5 <C> Chinese news categorization <CAP> Table 1: Large-scale text classification data sets <COT> Looking at the "Data set" column, we can see that there are two different data sets mentioned - AG news and Sogou news.
<R> <C> Data set <C> Train <C> Test <C> classes <C> Task <R> <C> AG news <C> 120k <C> 7.6k <C> 4 <C> English news categorization <R> <C> AG news <C> 5k <C> 7.6k <C> 4 <C> English news categorization <R> <C> Sogou news <C> 450k <C> 60k <C> 5 <C> Chinese news categorization <R> <C> Sogou news <C> 10k <C> 60k <C> 5 <C> Chinese news categorization <CAP> Table 1: Large-scale text classification data sets <COT> Looking at the "classes" column, we can see that each data set has a different number of classes.
<R> <C> Data set <C> AG <C> Sogou <C> AG(5k) <C> Sogou(10k) <R> <C> [ITALIC] - our model <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> SeMemNN-ct <C> 9.29 <C> 4.73 <C> 16.72 <C> 10.82 <R> <C> SeMemNN-abs <C> 9.04 <C> 4.62 <C> 15.32 <C> 9.80 <R> <C> B-SeMemNN-ct <C> 9.01 <C> 4.52 <C> 15.37 <C> 9.37 <R> <C> B-SeMemNN-abs <C> 8.68 <C> 4.19 <C> 14.35 <C> 8.76 <R> <C> SAB-SeMemNN-ct <C> 8.88 <C> 4.33 <C> 14.07 <C> 7.95 <R> <C> SAB-SeMemNN-abs <C> 8.37 <C> 3.67 <C> 13.79 <C> 7.89 <R> <C> [ITALIC] - related studies <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Bow  <C> 11.19 <C> 7.15 <C> - <C> - <R> <C> Bow TFIDF  <C> 10.36 <C> 6.55 <C> - <C> - <R> <C> ngrams TFIDF  <C> 7.64 <C> 2.81 <C> - <C> - <R> <C> Bag-of-means  <C> 16.91 <C> 10.79 <C> - <C> - <R> <C> LSTM  <C> 13.94 <C> 4.82 <C> - <C> - <R> <C> char-CNN  <C> 9.51 <C> 4.39 <C> - <C> - <R> <C> VDCNN  <C> [BOLD] 8.67 <C> [BOLD] 3.18 <C> - <C> - <R> <C> VDCNN <C> 10.64 <C> 6.53 <C> 19.25 <C> unable <R> <C> XLNet  <C> 4.49 <C> - <C> - <C> - <R> <C> ULMFiT  <C> 5.01 <C> - <C> - <C> - <R> <C> CNN  <C> 6.57 <C> - <C> - <C> - <R> <C> DPCNN  <C> 6.87 <C> 1.84 <C> - <C> - <CAP> Table 2: Testing error of our model and related studies <COT> Looking at the "VDCNN" row, we can see that the VDCNN model has the lowest testing error for both the AG and Sogou datasets.
<R> <C> [BOLD] Description <C> [BOLD] Average <R> <C> All models <C> 91.51 <R> <C> All prefix models <C> [BOLD] 94.43 <R> <C> All postfix models <C> 92.37 <R> <C> All infix models <C> 87.72 <R> <C> All pre-trained models <C> 91.06 <R> <C> All non-pre-trained models <C> 91.96 <R> <C> All medium (type 1) models <C> 90.49 <R> <C> All small (type 2) models <C> 92.09 <R> <C> All minimal (type 3) models <C> 91.94 <CAP> TABLE II: Summary of BLEU scores from Table I. <COT> Looking at the "Description" column, we can see that the table summarizes different types of models and their corresponding average scores.
<R> <C> [BOLD] Description <C> [BOLD] Average <R> <C> All models <C> 91.51 <R> <C> All prefix models <C> [BOLD] 94.43 <R> <C> All postfix models <C> 92.37 <R> <C> All infix models <C> 87.72 <R> <C> All pre-trained models <C> 91.06 <R> <C> All non-pre-trained models <C> 91.96 <R> <C> All medium (type 1) models <C> 90.49 <R> <C> All small (type 2) models <C> 92.09 <R> <C> All minimal (type 3) models <C> 91.94 <CAP> TABLE II: Summary of BLEU scores from Table I. <COT> Looking at the "All prefix models" cell, we can see that the average score for prefix models is marked as [BOLD] 94.43.
<R> <C> [BOLD] (Type) Model <C> [BOLD] AI2 <C> [BOLD] CC <C> [BOLD] IL <C> [BOLD] MAWPS <C> [BOLD] Average <R> <C>  Hosseini, et.al. <C> 77.7 <C> – <C> – <C> – <C> ∗77.7 <R> <C>  Kushman, et.al. <C> 64.0 <C> 73.7 <C> 2.3 <C> – <C> ∗46.7 <R> <C>  Roy, et.al. <C> – <C> – <C> 52.7 <C> – <C> ∗52.7 <R> <C>  Robaidek, et.al. <C> – <C> – <C> – <C> 62.8 <C> ∗62.8 <R> <C>  Wang, et.al. <C> [BOLD] 78.5 <C> 75.5 <C> 73.3 <C> – <C> ∗75.4 <R> <C> [ITALIC] Pre-trained <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> (1) Prefix-Transformer <C> 70.2 <C> 91.1 <C> 95.2 <C> 82.4 <C> 84.7 <R> <C> (1) Postfix-Transformer <C> 68.4 <C> 90.0 <C> 92.9 <C> 82.7 <C> 83.5 <R> <C> (1) Infix-Transformer <C> 75.4 <C> 74.4 <C> 64.3 <C> 56.4 <C> 67.6 <R> <C> (2) Prefix-Transformer <C> 66.7 <C> 91.1 <C> [BOLD] 96.4 <C> 82.1 <C> 84.1 <R> <C> (2) Postfix-Transformer <C> 73.7 <C> 93.3 <C> 94.1 <C> 82.4 <C> 85.9 <R> <C> (2) Infix-Transformer <C> 75.4 <C> 75.6 <C> 66.7 <C> 59.0 <C> 69.2 <R> <C> (3) Prefix-Transformer <C> 70.2 <C> 91.1 <C> 95.2 <C> 82.4 <C> 84.7 <R> <C> (3) Postfix-Transformer <C> 73.7 <C> 92.2 <C> 94.1 <C> 82.1 <C> 85.5 <R> <C> (3) Infix-Transformer <C> 75.4 <C> 75.6 <C> 64.3 <C> 58.7 <C> 68.5 <R> <C> [ITALIC] Non-pre-trained <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> (1) Prefix-Transformer <C> 71.9 <C> [BOLD] 94.4 <C> 95.2 <C> 83.4 <C> 86.3 <R> <C> (1) Postfix-Transformer <C> 73.7 <C> 81.1 <C> 92.9 <C> 75.7 <C> 80.8 <R> <C> (1) Infix-Transformer <C> 77.2 <C> 73.3 <C> 61.9 <C> 56.8 <C> 67.3 <R> <C> (2) Prefix-Transformer <C> 71.9 <C> [BOLD] 94.4 <C> 94.1 <C> [BOLD] 84.7 <C> 86.3 <R> <C> (2) Postfix-Transformer <C> 77.2 <C> [BOLD] 94.4 <C> 94.1 <C> 83.1 <C> [BOLD] 87.2 <R> <C> (2) Infix-Transformer <C> 77.2 <C> 76.7 <C> 66.7 <C> 61.5 <C> 70.5 <R> <C> (3) Prefix-Transformer <C> 71.9 <C> 93.3 <C> 95.2 <C> 84.1 <C> 86.2 <R> <C> (3) Postfix-Transformer <C> 77.2 <C> 94.4 <C> 94.1 <C> 82.4 <C> 87.0 <R> <C> (3) Infix-Transformer <C> 77.2 <C> 76.7 <C> 66.7 <C> 62.4 <C> 70.7 <CAP> TABLE III: Test results for Experiment 2 (* denotes averages on present values only). <COT> Looking at the "Average" row, we can see that the average performance of the models is higher for pre-trained models compared to non-pre-trained models.
<R> <C> [BOLD] (Type) Model <C> [BOLD] AI2 <C> [BOLD] CC <C> [BOLD] IL <C> [BOLD] MAWPS <C> [BOLD] Average <R> <C>  Hosseini, et.al. <C> 77.7 <C> – <C> – <C> – <C> ∗77.7 <R> <C>  Kushman, et.al. <C> 64.0 <C> 73.7 <C> 2.3 <C> – <C> ∗46.7 <R> <C>  Roy, et.al. <C> – <C> – <C> 52.7 <C> – <C> ∗52.7 <R> <C>  Robaidek, et.al. <C> – <C> – <C> – <C> 62.8 <C> ∗62.8 <R> <C>  Wang, et.al. <C> [BOLD] 78.5 <C> 75.5 <C> 73.3 <C> – <C> ∗75.4 <R> <C> [ITALIC] Pre-trained <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> (1) Prefix-Transformer <C> 70.2 <C> 91.1 <C> 95.2 <C> 82.4 <C> 84.7 <R> <C> (1) Postfix-Transformer <C> 68.4 <C> 90.0 <C> 92.9 <C> 82.7 <C> 83.5 <R> <C> (1) Infix-Transformer <C> 75.4 <C> 74.4 <C> 64.3 <C> 56.4 <C> 67.6 <R> <C> (2) Prefix-Transformer <C> 66.7 <C> 91.1 <C> [BOLD] 96.4 <C> 82.1 <C> 84.1 <R> <C> (2) Postfix-Transformer <C> 73.7 <C> 93.3 <C> 94.1 <C> 82.4 <C> 85.9 <R> <C> (2) Infix-Transformer <C> 75.4 <C> 75.6 <C> 66.7 <C> 59.0 <C> 69.2 <R> <C> (3) Prefix-Transformer <C> 70.2 <C> 91.1 <C> 95.2 <C> 82.4 <C> 84.7 <R> <C> (3) Postfix-Transformer <C> 73.7 <C> 92.2 <C> 94.1 <C> 82.1 <C> 85.5 <R> <C> (3) Infix-Transformer <C> 75.4 <C> 75.6 <C> 64.3 <C> 58.7 <C> 68.5 <R> <C> [ITALIC] Non-pre-trained <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> (1) Prefix-Transformer <C> 71.9 <C> [BOLD] 94.4 <C> 95.2 <C> 83.4 <C> 86.3 <R> <C> (1) Postfix-Transformer <C> 73.7 <C> 81.1 <C> 92.9 <C> 75.7 <C> 80.8 <R> <C> (1) Infix-Transformer <C> 77.2 <C> 73.3 <C> 61.9 <C> 56.8 <C> 67.3 <R> <C> (2) Prefix-Transformer <C> 71.9 <C> [BOLD] 94.4 <C> 94.1 <C> [BOLD] 84.7 <C> 86.3 <R> <C> (2) Postfix-Transformer <C> 77.2 <C> [BOLD] 94.4 <C> 94.1 <C> 83.1 <C> [BOLD] 87.2 <R> <C> (2) Infix-Transformer <C> 77.2 <C> 76.7 <C> 66.7 <C> 61.5 <C> 70.5 <R> <C> (3) Prefix-Transformer <C> 71.9 <C> 93.3 <C> 95.2 <C> 84.1 <C> 86.2 <R> <C> (3) Postfix-Transformer <C> 77.2 <C> 94.4 <C> 94.1 <C> 82.4 <C> 87.0 <R> <C> (3) Infix-Transformer <C> 77.2 <C> 76.7 <C> 66.7 <C> 62.4 <C> 70.7 <CAP> TABLE III: Test results for Experiment 2 (* denotes averages on present values only). <COT> Looking at the "Infix-Transformer" column, we can see that the performance of the model is consistently higher for Experiment 1 compared to Experiment 2.
<R> <C> Model <C> Joint Accuracy <R> <C> NBT-DNN Mrkšić et al. ( 2017 ) <C> 0.844 <R> <C> BT-CNN Ramadan et al. ( 2018 ) <C> 0.855 <R> <C> GLAD Zhong et al. ( 2018 ) <C> 0.881 <R> <C> GCE Nouri and Hosseini-Asl ( 2018 ) <C> 0.885 <R> <C> StateNetPSI Ren et al. ( 2018 ) <C> 0.889 <R> <C> BERT+RNN (baseline 1) <C> 0.892 (±0.011) <R> <C> BERT+RNN+Ontology (baseline 2) <C> 0.893 (±0.013) <R> <C> Slot-dependent SUMBT (baseline 3) <C> 0.891 (±0.010) <R> <C> Slot-independent SUMBT (proposed) <C> [BOLD] 0.910 (±0.010) <CAP> Table 1: Joint goal accuracy on the evaluation dataset of WOZ 2.0 corpus. <COT> Looking at the "Model" column, we can see that the "Slot-independent SUMBT (proposed)" model has the highest joint accuracy score.
<R> <C> Model <C> Joint Accuracy <R> <C> NBT-DNN Mrkšić et al. ( 2017 ) <C> 0.844 <R> <C> BT-CNN Ramadan et al. ( 2018 ) <C> 0.855 <R> <C> GLAD Zhong et al. ( 2018 ) <C> 0.881 <R> <C> GCE Nouri and Hosseini-Asl ( 2018 ) <C> 0.885 <R> <C> StateNetPSI Ren et al. ( 2018 ) <C> 0.889 <R> <C> BERT+RNN (baseline 1) <C> 0.892 (±0.011) <R> <C> BERT+RNN+Ontology (baseline 2) <C> 0.893 (±0.013) <R> <C> Slot-dependent SUMBT (baseline 3) <C> 0.891 (±0.010) <R> <C> Slot-independent SUMBT (proposed) <C> [BOLD] 0.910 (±0.010) <CAP> Table 1: Joint goal accuracy on the evaluation dataset of WOZ 2.0 corpus. <COT> Looking at the "BERT+RNN (baseline 1)" row, we can see that the joint accuracy score is 0.892 (±0.011).
<R> <C> Model <C> Joint Accuracy <R> <C> Benchmark baseline <C> 0.2583 <R> <C> GLAD Zhong et al. ( 2018 ) <C> 0.3557 <R> <C> GCE Nouri and Hosseini-Asl ( 2018 ) <C> 0.3558 <R> <C> SUMBT <C> [BOLD] 0.4240 (±0.0187) <CAP> Table 2: Joint goal accuracy on the evaluation dataset of MultiWOZ corpus. <COT> Looking at the "Model" column, we can see that the SUMBT model has the highest joint accuracy compared to the other models listed in the table.
<R> <C> Model <C> Joint Accuracy <R> <C> Benchmark baseline <C> 0.2583 <R> <C> GLAD Zhong et al. ( 2018 ) <C> 0.3557 <R> <C> GCE Nouri and Hosseini-Asl ( 2018 ) <C> 0.3558 <R> <C> SUMBT <C> [BOLD] 0.4240 (±0.0187) <CAP> Table 2: Joint goal accuracy on the evaluation dataset of MultiWOZ corpus. <COT> Looking at the "Benchmark baseline" column, we can see that the joint accuracy of the SUMBT model is higher than that of the benchmark baseline.
<R> <C> [BOLD] Model <C> [BOLD] Target Corpus <C> [BOLD] Transfer BERT <C> [BOLD] Transfer Filler <C> [BOLD] Transfer Role <C> [BOLD] Baseline Acc. (%) <C> [BOLD] Fine-tuned Acc. (%) <C> [BOLD] Gain (%) <R> <C> BERT <C> QNLI <C> True <C> – <C> – <C> 91.60 <C> 91.27 <C> – 0.33 <R> <C> BERT <C> QQP <C> True <C> – <C> – <C> 91.45 <C> 91.12 <C> – 0.33 <R> <C> BERT <C> RTE <C> True <C> – <C> – <C> 71.12 <C> 73.65 <C> + 2.53 <R> <C> BERT <C> SNLI <C> True <C> – <C> – <C> 90.45 <C> 90.69 <C> + 0.24 <R> <C> BERT <C> SST <C> True <C> – <C> – <C> 93.23 <C> 92.78 <C> – 0.45 <R> <C> HUBERT (Transformer) <C> QNLI <C> True <C> True <C> False <C> 90.56 <C> 91.16 <C> [BOLD] + 0.60 <R> <C> HUBERT (Transformer) <C> QQP <C> False <C> False <C> True <C> 90.81 <C> 91.42 <C> [BOLD] + 0.61 <R> <C> HUBERT (Transformer) <C> RTE <C> True <C> True <C> True <C> 61.73 <C> 74.01 <C> [BOLD] + 12.28 <R> <C> HUBERT (Transformer) <C> SNLI <C> True <C> False <C> True <C> 90.66 <C> 91.36 <C> [BOLD] + 0.70 <R> <C> HUBERT (Transformer) <C> SST <C> True <C> False <C> True <C> 91.28 <C> 92.43 <C> [BOLD] + 1.15 <CAP> Table 3: Transfer learning results for GLUE tasks. The source corpus is MNLI. Baseline accuracy is when Transfer BERT, Filler, and Role are all False, equivalent to no transfer. Fine-tuned accuracy is the best accuracy among all possible transfer options. <COT> Looking at the "Baseline Acc. (%)" and "Fine-tuned Acc. (%)" columns, we can see that the fine-tuned accuracy is higher than the baseline accuracy for all models and target corpora.
<R> <C> [BOLD] Model <C> [BOLD] Target Corpus <C> [BOLD] Transfer BERT <C> [BOLD] Transfer Filler <C> [BOLD] Transfer Role <C> [BOLD] Baseline Acc. (%) <C> [BOLD] Fine-tuned Acc. (%) <C> [BOLD] Gain (%) <R> <C> BERT <C> QNLI <C> True <C> – <C> – <C> 91.60 <C> 91.27 <C> – 0.33 <R> <C> BERT <C> QQP <C> True <C> – <C> – <C> 91.45 <C> 91.12 <C> – 0.33 <R> <C> BERT <C> RTE <C> True <C> – <C> – <C> 71.12 <C> 73.65 <C> + 2.53 <R> <C> BERT <C> SNLI <C> True <C> – <C> – <C> 90.45 <C> 90.69 <C> + 0.24 <R> <C> BERT <C> SST <C> True <C> – <C> – <C> 93.23 <C> 92.78 <C> – 0.45 <R> <C> HUBERT (Transformer) <C> QNLI <C> True <C> True <C> False <C> 90.56 <C> 91.16 <C> [BOLD] + 0.60 <R> <C> HUBERT (Transformer) <C> QQP <C> False <C> False <C> True <C> 90.81 <C> 91.42 <C> [BOLD] + 0.61 <R> <C> HUBERT (Transformer) <C> RTE <C> True <C> True <C> True <C> 61.73 <C> 74.01 <C> [BOLD] + 12.28 <R> <C> HUBERT (Transformer) <C> SNLI <C> True <C> False <C> True <C> 90.66 <C> 91.36 <C> [BOLD] + 0.70 <R> <C> HUBERT (Transformer) <C> SST <C> True <C> False <C> True <C> 91.28 <C> 92.43 <C> [BOLD] + 1.15 <CAP> Table 3: Transfer learning results for GLUE tasks. The source corpus is MNLI. Baseline accuracy is when Transfer BERT, Filler, and Role are all False, equivalent to no transfer. Fine-tuned accuracy is the best accuracy among all possible transfer options. <COT> Looking at the "Gain (%)" column, we can see that the gain in accuracy for the HUBERT (Transformer) model is higher than for the BERT model for all target corpora.
<R> <C> [BOLD] Model <C> [BOLD] Target Corpus <C> [BOLD] Transfer BERT <C> [BOLD] Transfer Filler <C> [BOLD] Transfer Role <C> [BOLD] Baseline Acc. (%) <C> [BOLD] Fine-tuned Acc. (%) <C> [BOLD] Gain (%) <R> <C> BERT <C> QNLI <C> True <C> – <C> – <C> 91.60 <C> 90.96 <C> – 0.64 <R> <C> BERT <C> MNLI <C> True <C> – <C> – <C> 84.15 <C> 84.41 <C> + 0.26 <R> <C> BERT <C> RTE <C> True <C> – <C> – <C> 71.12 <C> 62.45 <C> – 8.67 <R> <C> BERT <C> SNLI <C> True <C> – <C> – <C> 90.45 <C> 90.88 <C> + 0.43 <R> <C> BERT <C> SST <C> True <C> – <C> – <C> 93.23 <C> 92.09 <C> – 1.14 <R> <C> HUBERT (Transformer) <C> QNLI <C> False <C> True <C> True <C> 88.32 <C> 90.55 <C> [BOLD] + 2.23 <R> <C> HUBERT (Transformer) <C> MNLI <C> False <C> True <C> True <C> 84.30 <C> 85.24 <C> [BOLD] + 0.94 <R> <C> HUBERT (Transformer) <C> RTE <C> False <C> True <C> False <C> 61.73 <C> 65.70 <C> [BOLD] + 3.97 <R> <C> HUBERT (Transformer) <C> SNLI <C> False <C> False <C> True <C> 90.63 <C> 91.20 <C> [BOLD] + 0.57 <R> <C> HUBERT (Transformer) <C> SST <C> True <C> True <C> True <C> 86.12 <C> 91.06 <C> [BOLD] + 4.94 <CAP> Table 4: Transfer learning results for GLUE tasks. The source corpus is QQP. Baseline accuracy is for when Transfer BERT, Filler, and Role are all False, which is equivalent to no transfer. Fine-tuned accuracy is the best accuracy among all possible transfer options. <COT> Looking at the "Baseline Acc. (%)" and "Fine-tuned Acc. (%)" columns, we can see that for the BERT model, the fine-tuned accuracy is higher than the baseline accuracy for all target corpora except for SST.
<R> <C> [BOLD] Model <C> [BOLD] Target Corpus <C> [BOLD] Transfer BERT <C> [BOLD] Transfer Filler <C> [BOLD] Transfer Role <C> [BOLD] Baseline Acc. (%) <C> [BOLD] Fine-tuned Acc. (%) <C> [BOLD] Gain (%) <R> <C> BERT <C> QNLI <C> True <C> – <C> – <C> 91.60 <C> 90.96 <C> – 0.64 <R> <C> BERT <C> MNLI <C> True <C> – <C> – <C> 84.15 <C> 84.41 <C> + 0.26 <R> <C> BERT <C> RTE <C> True <C> – <C> – <C> 71.12 <C> 62.45 <C> – 8.67 <R> <C> BERT <C> SNLI <C> True <C> – <C> – <C> 90.45 <C> 90.88 <C> + 0.43 <R> <C> BERT <C> SST <C> True <C> – <C> – <C> 93.23 <C> 92.09 <C> – 1.14 <R> <C> HUBERT (Transformer) <C> QNLI <C> False <C> True <C> True <C> 88.32 <C> 90.55 <C> [BOLD] + 2.23 <R> <C> HUBERT (Transformer) <C> MNLI <C> False <C> True <C> True <C> 84.30 <C> 85.24 <C> [BOLD] + 0.94 <R> <C> HUBERT (Transformer) <C> RTE <C> False <C> True <C> False <C> 61.73 <C> 65.70 <C> [BOLD] + 3.97 <R> <C> HUBERT (Transformer) <C> SNLI <C> False <C> False <C> True <C> 90.63 <C> 91.20 <C> [BOLD] + 0.57 <R> <C> HUBERT (Transformer) <C> SST <C> True <C> True <C> True <C> 86.12 <C> 91.06 <C> [BOLD] + 4.94 <CAP> Table 4: Transfer learning results for GLUE tasks. The source corpus is QQP. Baseline accuracy is for when Transfer BERT, Filler, and Role are all False, which is equivalent to no transfer. Fine-tuned accuracy is the best accuracy among all possible transfer options. <COT> Looking at the "Gain (%)" column, we can see that for the HUBERT (Transformer) model, the gain in accuracy after fine-tuning is positive for all target corpora.
<R> <C> [BOLD] Source Corpus <C> [BOLD] Target Corpus <C> [BOLD] HUBERT Transfer BERT <C> [BOLD] HUBERT Transfer Filler <C> [BOLD] HUBERT Transfer Role <C> [BOLD] BERT Acc. (%) <C> [BOLD] HUBERT Acc. (%) <R> <C> MNLI <C> QNLI <C> True <C> True <C> False <C> [BOLD] 90.50 <C> [BOLD] 90.50 <R> <C> MNLI <C> QQP <C> False <C> False <C> True <C> 89.20 <C> [BOLD] 89.30 <R> <C> MNLI <C> RTE <C> True <C> True <C> True <C> 66.40 <C> [BOLD] 69.30 <R> <C> MNLI <C> SNLI <C> True <C> False <C> True <C> 89.20 <C> [BOLD] 90.35 <R> <C> MNLI <C> SST <C> True <C> False <C> True <C> [BOLD] 93.50 <C> 92.60 <R> <C> QQP <C> QNLI <C> False <C> True <C> True <C> 90.50 <C> [BOLD] 90.70 <R> <C> QQP <C> MNLI <C> False <C> True <C> True <C> 84.60 <C> [BOLD] 84.70 <R> <C> QQP <C> RTE <C> False <C> True <C> False <C> [BOLD] 66.40 <C> 63.20 <R> <C> QQP <C> SNLI <C> False <C> False <C> True <C> 89.20 <C> [BOLD] 90.36 <R> <C> QQP <C> SST <C> True <C> True <C> True <C> [BOLD] 93.50 <C> 91.00 <CAP> Table 5: Test set results for HUBERT (Transformer) and BERT. BERT accuracy indicates test results on target corpus (without transfer) for bert-base-uncased which are directly taken from the GLUE leaderboard. Fine-tuned accuracy are the test results for best performing HUBERT (Transformer) model on target dev set after transfer (see Tables 3 and 4). <COT> Looking at the "HUBERT Transfer BERT" and "HUBERT Transfer Filler" cells, we can see that the HUBERT model performs better than BERT on both MNLI and QNLI tasks.
<R> <C> [BOLD] Source Corpus <C> [BOLD] Target Corpus <C> [BOLD] HUBERT Transfer BERT <C> [BOLD] HUBERT Transfer Filler <C> [BOLD] HUBERT Transfer Role <C> [BOLD] BERT Acc. (%) <C> [BOLD] HUBERT Acc. (%) <R> <C> MNLI <C> QNLI <C> True <C> True <C> False <C> [BOLD] 90.50 <C> [BOLD] 90.50 <R> <C> MNLI <C> QQP <C> False <C> False <C> True <C> 89.20 <C> [BOLD] 89.30 <R> <C> MNLI <C> RTE <C> True <C> True <C> True <C> 66.40 <C> [BOLD] 69.30 <R> <C> MNLI <C> SNLI <C> True <C> False <C> True <C> 89.20 <C> [BOLD] 90.35 <R> <C> MNLI <C> SST <C> True <C> False <C> True <C> [BOLD] 93.50 <C> 92.60 <R> <C> QQP <C> QNLI <C> False <C> True <C> True <C> 90.50 <C> [BOLD] 90.70 <R> <C> QQP <C> MNLI <C> False <C> True <C> True <C> 84.60 <C> [BOLD] 84.70 <R> <C> QQP <C> RTE <C> False <C> True <C> False <C> [BOLD] 66.40 <C> 63.20 <R> <C> QQP <C> SNLI <C> False <C> False <C> True <C> 89.20 <C> [BOLD] 90.36 <R> <C> QQP <C> SST <C> True <C> True <C> True <C> [BOLD] 93.50 <C> 91.00 <CAP> Table 5: Test set results for HUBERT (Transformer) and BERT. BERT accuracy indicates test results on target corpus (without transfer) for bert-base-uncased which are directly taken from the GLUE leaderboard. Fine-tuned accuracy are the test results for best performing HUBERT (Transformer) model on target dev set after transfer (see Tables 3 and 4). <COT> Looking at the "HUBERT Transfer Role" column, we can see that the HUBERT model performs better than BERT on all tasks except for RTE.
<R> <C> Model <C> Model <C> Mortality AUPRC <C> Mortality AUROC <C> Primary CCS Top-1 Recall <C> Primary CCS Top-5 Recall <C> All ICD-9 AUPRC <C> All ICD-9 AUROC, weighted <R> <C> No notes <C> - <C> 0.449 (0.006) <C> 0.869 (0.001) <C> 0.526 (0.006) <C> 0.796 (0.003) <C> 0.305 (0.001) <C> 0.873 (<0.001) <R> <C> Bag-of-words <C> Unigrams (notes only) <C> 0.383 (0.004) <C> 0.832 (0.003) <C> 0.591 (0.004) <C> 0.849 (0.002) <C> 0.328 (0.002) <C> 0.880 (0.001) <R> <C> [EMPTY] <C> Unigrams (all features) <C> [BOLD] 0.479 (0.008) <C> 0.880 (0.001) <C> 0.592 (0.003) <C> 0.842 (0.003) <C> 0.331 (0.001) <C> 0.883 (0.001) <R> <C> [EMPTY] <C> Unigrams and bigrams (all features) <C> 0.460 (0.005) <C> 0.872 (0.002) <C> 0.587 (0.008) <C> 0.829 (0.005) <C> 0.325 (0.002) <C> 0.881 (<0.001) <R> <C> Hierarchical (without pretraining) <C> Notes only <C> 0.351 (0.003) <C> 0.825 (0.003) <C> 0.606 (0.003) <C> 0.850 (0.001) <C> 0.345 (0.005) <C> 0.887 (0.002) <R> <C> [EMPTY] <C> All features <C> 0.471 (0.006) <C> 0.876 (0.003) <C> 0.591 (0.008) <C> 0.833 (0.006) <C> 0.301 (0.004) <C> 0.868 (0.001) <R> <C> SHiP <C> Notes only <C> 0.353 (0.005) <C> 0.825 (0.004) <C> 0.667 (0.006) <C> [BOLD] 0.897*† (0.003) <C> [BOLD] 0.352† (0.001) <C> [BOLD] 0.891† (0.001) <R> <C> [EMPTY] <C> All features <C> [BOLD] 0.479 (0.007) <C> [BOLD] 0.882 (0.001) <C> [BOLD] 0.671*† (0.004) <C> 0.890 (0.001) <C> 0.345 (0.005) <C> 0.889 (0.002) <CAP> Table 1: Model performance results on the tasks of interest. Best values for each metric are bolded. <COT> Looking at the "Model" column, "SHiP" row, and "Primary CCS Top-1 Recall" cell, we can see that the Primary CCS Top-1 Recall for the SHiP model with notes only is 0.667.
<R> <C> Model <C> Model <C> Mortality AUPRC <C> Mortality AUROC <C> Primary CCS Top-1 Recall <C> Primary CCS Top-5 Recall <C> All ICD-9 AUPRC <C> All ICD-9 AUROC, weighted <R> <C> No notes <C> - <C> 0.449 (0.006) <C> 0.869 (0.001) <C> 0.526 (0.006) <C> 0.796 (0.003) <C> 0.305 (0.001) <C> 0.873 (<0.001) <R> <C> Bag-of-words <C> Unigrams (notes only) <C> 0.383 (0.004) <C> 0.832 (0.003) <C> 0.591 (0.004) <C> 0.849 (0.002) <C> 0.328 (0.002) <C> 0.880 (0.001) <R> <C> [EMPTY] <C> Unigrams (all features) <C> [BOLD] 0.479 (0.008) <C> 0.880 (0.001) <C> 0.592 (0.003) <C> 0.842 (0.003) <C> 0.331 (0.001) <C> 0.883 (0.001) <R> <C> [EMPTY] <C> Unigrams and bigrams (all features) <C> 0.460 (0.005) <C> 0.872 (0.002) <C> 0.587 (0.008) <C> 0.829 (0.005) <C> 0.325 (0.002) <C> 0.881 (<0.001) <R> <C> Hierarchical (without pretraining) <C> Notes only <C> 0.351 (0.003) <C> 0.825 (0.003) <C> 0.606 (0.003) <C> 0.850 (0.001) <C> 0.345 (0.005) <C> 0.887 (0.002) <R> <C> [EMPTY] <C> All features <C> 0.471 (0.006) <C> 0.876 (0.003) <C> 0.591 (0.008) <C> 0.833 (0.006) <C> 0.301 (0.004) <C> 0.868 (0.001) <R> <C> SHiP <C> Notes only <C> 0.353 (0.005) <C> 0.825 (0.004) <C> 0.667 (0.006) <C> [BOLD] 0.897*† (0.003) <C> [BOLD] 0.352† (0.001) <C> [BOLD] 0.891† (0.001) <R> <C> [EMPTY] <C> All features <C> [BOLD] 0.479 (0.007) <C> [BOLD] 0.882 (0.001) <C> [BOLD] 0.671*† (0.004) <C> 0.890 (0.001) <C> 0.345 (0.005) <C> 0.889 (0.002) <CAP> Table 1: Model performance results on the tasks of interest. Best values for each metric are bolded. <COT> Looking at the "Model" column, "Hierarchical (without pretraining)" row, and "All features" cell, we can see that the Mortality AUROC for the Hierarchical model without pretraining with all features is 0.876.
<R> <C> Method <C> # Params <C> Inference <C> SST-2 <C> MRPC <C> QQP <C> MNLI <C> QNLI <C> RTE <C> Average <R> <C> Method <C> # Params <C> Speedup <C> SST-2 <C> MRPC <C> QQP <C> MNLI <C> QNLI <C> RTE <C> Average <R> <C> BERT12 <C> 109M <C> 1x <C> 93.5 <C> 88.9 <C> 71.2 <C> 84.6 <C> 90.5 <C> 66.4 <C> 82.5 <R> <C> BERT12-T <C> 109M <C> 1x <C> 93.3 <C> 88.7 <C> 71.1 <C> 84.8 <C> 90.4 <C> 66.1 <C> 82.4 <R> <C> BERT6-PKD <C> 67.0M <C> 1.9x <C> 92.0 <C> 85.0 <C> 70.7 <C> 81.5 <C> [BOLD] 89.0 <C> [BOLD] 65.5 <C> [BOLD] 80.6 <R> <C> BERT3-PKD <C> 45.7M <C> 3.7x <C> 87.5 <C> 80.7 <C> 68.1 <C> 76.7 <C> 84.7 <C> 58.2 <C> 76.0 <R> <C> DistilBERT4 <C> 52.2M <C> 3.0x <C> 91.4 <C> 82.4 <C> 68.5 <C> 78.9 <C> 85.2 <C> 54.1 <C> 76.8 <R> <C> TinyBert4 <C> 14.5M <C> 9.4x <C> [BOLD] 92.6 <C> [BOLD] 86.4 <C> [BOLD] 71.3 <C> [BOLD] 82.5 <C> 87.7 <C> 62.9 <C> [BOLD] 80.6 <R> <C> BiLSTM [ITALIC] SOFT <C> 10.1M <C> 7.6x <C> 90.7 <C> - <C> 68.2 <C> 73.0 <C> - <C> - <C> - <R> <C> AdaBERT <C> [BOLD] 6.4M ∼ 9.5M <C> [BOLD] 12.7x ∼ 29.3x <C> 91.8 <C> 85.1 <C> 70.7 <C> 81.6 <C> 86.8 <C> 64.4 <C> 80.1 <CAP> Table 1: The compression results including model efficiency and accuracy from the GLUE test server, and the MNLI result is evaluated for matched-accuracy (MNLI-m). BERT12 indicates the results of the fine-tuned BERT-base from Devlin et al. (2019) and BERT12-T indicates the results of the fine-tuned BERT-base in our implementation. The results of BERT-PKD are from Sun et al. (2019), the results of DistilBERT4 and TinyBERT4 are from Jiao et al. (2019), and the results of BiLSTMSOFT is from Tang et al. (2019). The number of model parameters includes the embedding size, and the inference time is tested with a batch size of 128 over 50,000 samples. The bold numbers and underlined numbers indicate the best and the second-best performance respectively. <COT> Looking at the "Method" column, we can see that there are different methods listed such as BERT12, BERT12-T, BERT6-PKD, BERT3-PKD, DistilBERT4, TinyBert4, BiLSTM SOFT, and AdaBERT. 
<R> <C> Method <C> # Params <C> Inference <C> SST-2 <C> MRPC <C> QQP <C> MNLI <C> QNLI <C> RTE <C> Average <R> <C> Method <C> # Params <C> Speedup <C> SST-2 <C> MRPC <C> QQP <C> MNLI <C> QNLI <C> RTE <C> Average <R> <C> BERT12 <C> 109M <C> 1x <C> 93.5 <C> 88.9 <C> 71.2 <C> 84.6 <C> 90.5 <C> 66.4 <C> 82.5 <R> <C> BERT12-T <C> 109M <C> 1x <C> 93.3 <C> 88.7 <C> 71.1 <C> 84.8 <C> 90.4 <C> 66.1 <C> 82.4 <R> <C> BERT6-PKD <C> 67.0M <C> 1.9x <C> 92.0 <C> 85.0 <C> 70.7 <C> 81.5 <C> [BOLD] 89.0 <C> [BOLD] 65.5 <C> [BOLD] 80.6 <R> <C> BERT3-PKD <C> 45.7M <C> 3.7x <C> 87.5 <C> 80.7 <C> 68.1 <C> 76.7 <C> 84.7 <C> 58.2 <C> 76.0 <R> <C> DistilBERT4 <C> 52.2M <C> 3.0x <C> 91.4 <C> 82.4 <C> 68.5 <C> 78.9 <C> 85.2 <C> 54.1 <C> 76.8 <R> <C> TinyBert4 <C> 14.5M <C> 9.4x <C> [BOLD] 92.6 <C> [BOLD] 86.4 <C> [BOLD] 71.3 <C> [BOLD] 82.5 <C> 87.7 <C> 62.9 <C> [BOLD] 80.6 <R> <C> BiLSTM [ITALIC] SOFT <C> 10.1M <C> 7.6x <C> 90.7 <C> - <C> 68.2 <C> 73.0 <C> - <C> - <C> - <R> <C> AdaBERT <C> [BOLD] 6.4M ∼ 9.5M <C> [BOLD] 12.7x ∼ 29.3x <C> 91.8 <C> 85.1 <C> 70.7 <C> 81.6 <C> 86.8 <C> 64.4 <C> 80.1 <CAP> Table 1: The compression results including model efficiency and accuracy from the GLUE test server, and the MNLI result is evaluated for matched-accuracy (MNLI-m). BERT12 indicates the results of the fine-tuned BERT-base from Devlin et al. (2019) and BERT12-T indicates the results of the fine-tuned BERT-base in our implementation. The results of BERT-PKD are from Sun et al. (2019), the results of DistilBERT4 and TinyBERT4 are from Jiao et al. (2019), and the results of BiLSTMSOFT is from Tang et al. (2019). The number of model parameters includes the embedding size, and the inference time is tested with a batch size of 128 over 50,000 samples. The bold numbers and underlined numbers indicate the best and the second-best performance respectively. <COT> Looking at the "Average" row, we can see that the average performance of each method is calculated by taking the average of the accuracy scores for different tasks (SST-2, MRPC, QQP, MNLI, QNLI, and RTE).
<R> <C> StructureTask <C> SST-2 <C> MRPC <C> QQP <C> MNLI <C> QNLI <C> RTE <R> <C> AdaBERT-SST-2 <C> [BOLD] 91.9 <C> 78.1 <C> 58.6 <C> 64.0 <C> 74.1 <C> 53.8 <R> <C> AdaBERT-MRPC <C> 81.5 <C> [BOLD] 84.7 <C> 68.9 <C> 75.9 <C> 82.2 <C> 60.3 <R> <C> AdaBERT-QQP <C> 81.9 <C> 84.1 <C> [BOLD] 70.5 <C> 76.3 <C> 82.5 <C> 60.5 <R> <C> AdaBERT-MNLI <C> 82.1 <C> 81.5 <C> 66.8 <C> [BOLD] 81.3 <C> 86.1 <C> 63.2 <R> <C> AdaBERT-QNLI <C> 81.6 <C> 82.3 <C> 67.7 <C> 79.2 <C> [BOLD] 87.2 <C> 62.9 <R> <C> AdaBERT-RTE <C> 82.9 <C> 81.1 <C> 66.5 <C> 79.8 <C> 86.0 <C> [BOLD] 64.1 <R> <C> Random <C> 80.4 ± 4.3 <C> 79.2 ± 2.8 <C> 61.8 ± 4.9 <C> 69.7 ± 6.7 <C> 78.2 ± 5.5 <C> 55.3 ± 4.1 <CAP> Table 3: Accuracy comparison on the dev sets with the searched compression structures applying to different tasks. For Random, 5-times averaging results with standard deviations are reported. <COT> Looking at the "AdaBERT-MNLI" row, we can see that the highest accuracy score is in the "MNLI" column, indicating that AdaBERT performs best on the MNLI task.
<R> <C> StructureTask <C> SST-2 <C> MRPC <C> QQP <C> MNLI <C> QNLI <C> RTE <R> <C> AdaBERT-SST-2 <C> [BOLD] 91.9 <C> 78.1 <C> 58.6 <C> 64.0 <C> 74.1 <C> 53.8 <R> <C> AdaBERT-MRPC <C> 81.5 <C> [BOLD] 84.7 <C> 68.9 <C> 75.9 <C> 82.2 <C> 60.3 <R> <C> AdaBERT-QQP <C> 81.9 <C> 84.1 <C> [BOLD] 70.5 <C> 76.3 <C> 82.5 <C> 60.5 <R> <C> AdaBERT-MNLI <C> 82.1 <C> 81.5 <C> 66.8 <C> [BOLD] 81.3 <C> 86.1 <C> 63.2 <R> <C> AdaBERT-QNLI <C> 81.6 <C> 82.3 <C> 67.7 <C> 79.2 <C> [BOLD] 87.2 <C> 62.9 <R> <C> AdaBERT-RTE <C> 82.9 <C> 81.1 <C> 66.5 <C> 79.8 <C> 86.0 <C> [BOLD] 64.1 <R> <C> Random <C> 80.4 ± 4.3 <C> 79.2 ± 2.8 <C> 61.8 ± 4.9 <C> 69.7 ± 6.7 <C> 78.2 ± 5.5 <C> 55.3 ± 4.1 <CAP> Table 3: Accuracy comparison on the dev sets with the searched compression structures applying to different tasks. For Random, 5-times averaging results with standard deviations are reported. <COT> Comparing the "AdaBERT-SST-2" row with the "Random" row, we can see that the accuracy score for AdaBERT is higher than the average accuracy score for the Random model in all tasks.
<R> <C> [EMPTY] <C> SST-2 <C> MRPC <C> QNLI <C> RTE <R> <C> [ITALIC] β = 0 <C> 91.8 <C> 84.5 <C> 87.1 <C> 63.9 <R> <C> [ITALIC] β = 0 <C> (7.5M) <C> (7.8M) <C> (8.3M) <C> (9.1M) <R> <C> [ITALIC] β = 4 <C> [BOLD] 91.9 <C> [BOLD] 84.7 <C> [BOLD] 87.2 <C> [BOLD] 64.1 <R> <C> [ITALIC] β = 4 <C> (6.4M) <C> (7.5M) <C> (7.9M) <C> (8.6M) <R> <C> [ITALIC] β = 8 <C> 91.3 <C> 84.2 <C> 86.4 <C> 63.3 <R> <C> [ITALIC] β = 8 <C> (5.3M) <C> (6.4M) <C> (7.1M) <C> (7.8M) <CAP> Table 5: The effect of efficiency loss term. <COT> Looking at the "β = 0" row, the values in the SST-2, MRPC, QNLI, and RTE columns are all lower than the corresponding values in the "β = 4" row. 
<R> <C> [EMPTY] <C> SST-2 <C> MRPC <C> QNLI <C> RTE <R> <C> [ITALIC] β = 0 <C> 91.8 <C> 84.5 <C> 87.1 <C> 63.9 <R> <C> [ITALIC] β = 0 <C> (7.5M) <C> (7.8M) <C> (8.3M) <C> (9.1M) <R> <C> [ITALIC] β = 4 <C> [BOLD] 91.9 <C> [BOLD] 84.7 <C> [BOLD] 87.2 <C> [BOLD] 64.1 <R> <C> [ITALIC] β = 4 <C> (6.4M) <C> (7.5M) <C> (7.9M) <C> (8.6M) <R> <C> [ITALIC] β = 8 <C> 91.3 <C> 84.2 <C> 86.4 <C> 63.3 <R> <C> [ITALIC] β = 8 <C> (5.3M) <C> (6.4M) <C> (7.1M) <C> (7.8M) <CAP> Table 5: The effect of efficiency loss term. <COT> Looking at the "β = 8" row, the values in the SST-2, MRPC, QNLI, and RTE columns are all lower than the corresponding values in the "β = 4" row.
<R> <C> [EMPTY] <C> SST-2 <C> MRPC <C> QNLI <C> RTE <R> <C> Base-KD <C> 86.6 <C> 77.2 <C> 82.0 <C> 56.7 <R> <C> + Probe <C> 88.4 <C> 78.7 <C> 83.3 <C> 58.1 <R> <C> + DA <C> 91.4 <C> 83.9 <C> 86.5 <C> 63.2 <R> <C> + L [ITALIC] CE (All) <C> 91.9 <C> 84.7 <C> 87.2 <C> 64.1 <CAP> Table 4: The effect of knowledge loss terms. <COT> Looking at the table, we can see that the performance of the models improves as more knowledge loss terms are added.
<R> <C> [EMPTY] <C> SST-2 <C> MRPC <C> QNLI <C> RTE <R> <C> Base-KD <C> 86.6 <C> 77.2 <C> 82.0 <C> 56.7 <R> <C> + Probe <C> 88.4 <C> 78.7 <C> 83.3 <C> 58.1 <R> <C> + DA <C> 91.4 <C> 83.9 <C> 86.5 <C> 63.2 <R> <C> + L [ITALIC] CE (All) <C> 91.9 <C> 84.7 <C> 87.2 <C> 64.1 <CAP> Table 4: The effect of knowledge loss terms. <COT> Looking at the table, we can see that the model with the highest performance is the one with the knowledge loss term "LCE (All)" added.
<R> <C> Task Metric <C> BA↑ <C> F1↑ <C> MAE↓ <C> Corr↑ <R> <C> SOTA3 <C> 77.1 <C> 77.0 <C> 0.968 <C> 0.625 <R> <C> SOTA2 <C> 77.4 <C> 77.3 <C> 0.965 <C> 0.632 <R> <C> SOTA1 <C> 78.4 <C> 78.0 <C> 0.922 <C> 0.681 <R> <C> BERT <C> 83.36 <C> 85.53 <C> 0.736 <C> 0.777 <R> <C> M-BERT <C> [BOLD] 84.38 <C> [BOLD] 86.34 <C> [BOLD] 0.732 <C> [BOLD] 0.790 <R> <C> Δ [ITALIC] SOTA <C> ↑  [BOLD] 5.98 <C> ↑  [BOLD] 8.34 <C> ↓  [BOLD] 0.19 <C> ↑ [BOLD] 0.11 <CAP> Table 1: Sentiment prediction results on CMU-MOSI. SOTA1, SOTA2 and SOTA3 refer to the previous best, second best and third best state of the art models respectively. Best results are highlighted in bold and ΔSOTA represents the change in performance of M-BERT model over SOTA1. Our model significantly outperforms the current SOTA across all evaluation metrics. <COT> Looking at the "Δ [ITALIC] SOTA" row, we can see that the M-BERT model outperforms SOTA1 in all evaluation metrics.
<R> <C> Task Metric <C> BA↑ <C> F1↑ <C> MAE↓ <C> Corr↑ <R> <C> SOTA3 <C> 77.1 <C> 77.0 <C> 0.968 <C> 0.625 <R> <C> SOTA2 <C> 77.4 <C> 77.3 <C> 0.965 <C> 0.632 <R> <C> SOTA1 <C> 78.4 <C> 78.0 <C> 0.922 <C> 0.681 <R> <C> BERT <C> 83.36 <C> 85.53 <C> 0.736 <C> 0.777 <R> <C> M-BERT <C> [BOLD] 84.38 <C> [BOLD] 86.34 <C> [BOLD] 0.732 <C> [BOLD] 0.790 <R> <C> Δ [ITALIC] SOTA <C> ↑  [BOLD] 5.98 <C> ↑  [BOLD] 8.34 <C> ↓  [BOLD] 0.19 <C> ↑ [BOLD] 0.11 <CAP> Table 1: Sentiment prediction results on CMU-MOSI. SOTA1, SOTA2 and SOTA3 refer to the previous best, second best and third best state of the art models respectively. Best results are highlighted in bold and ΔSOTA represents the change in performance of M-BERT model over SOTA1. Our model significantly outperforms the current SOTA across all evaluation metrics. <COT> Looking at the "BERT" row, we can see that the F1 score is higher than the SOTA models.
<R> <C> [BOLD] Length of sentence <C> [BOLD] Attention <C> [BOLD] Gaussian Mask only <C> [BOLD] Gaussian Mask + RL model <R> <C> 4-7 <C> 1.46 <C> 1.13 <C> 1.33 <R> <C> 8-11 <C> 2.32 <C> 1.96 <C> 2.12 <R> <C> 12-15 <C> 2.86 <C> 2.43 <C> 2.61 <CAP> Table 1: Inference times in ms <COT> Looking at the "Length of sentence" column, we can see that as the length of the sentence increases, the inference time also increases for all three attention methods.
<R> <C> [BOLD] Length of sentence <C> [BOLD] Attention <C> [BOLD] Gaussian Mask only <C> [BOLD] Gaussian Mask + RL model <R> <C> 4-7 <C> 1.46 <C> 1.13 <C> 1.33 <R> <C> 8-11 <C> 2.32 <C> 1.96 <C> 2.12 <R> <C> 12-15 <C> 2.86 <C> 2.43 <C> 2.61 <CAP> Table 1: Inference times in ms <COT> Comparing the "Gaussian Mask only" and "Gaussian Mask + RL model" columns, we can see that the addition of the RL model improves the inference time for all sentence lengths.
<R> <C> [BOLD] Score <C> Omniglot <C> Omniglot Classifier <C> Imagenet <C> Imagenet Classifier <R> <C> 0.2 <C> 24,304 <C> 2,144 <C> 1,280 <C> 2,048 <R> <C> 0.1 <C> 99,248 <C> 22,912 <C> 8,448 <C> 3,072 <R> <C> 0.05 <C> 160,608 <C> 43,328 <C> 111,872 <C> 8,960 <CAP> Table 3: Number of images after which the ME Score falls below threshold. <COT> Looking at the "Score" column, we can see that as the score threshold decreases, the number of images decreases for both Omniglot and Imagenet.
<R> <C> [BOLD] Score <C> Omniglot <C> Omniglot Classifier <C> Imagenet <C> Imagenet Classifier <R> <C> 0.2 <C> 24,304 <C> 2,144 <C> 1,280 <C> 2,048 <R> <C> 0.1 <C> 99,248 <C> 22,912 <C> 8,448 <C> 3,072 <R> <C> 0.05 <C> 160,608 <C> 43,328 <C> 111,872 <C> 8,960 <CAP> Table 3: Number of images after which the ME Score falls below threshold. <COT> Looking at the "Omniglot Classifier" column, we can see that the number of images for Omniglot decreases as the score threshold decreases.
<R> <C> [BOLD] Transformer <C> [BOLD] Method <C> src <C> [ITALIC] l1 <C> [ITALIC] l2 <C> [ITALIC] lblstm1 <C> [ITALIC] lblstm2 <R> <C> [BOLD] en-de <C> random + HotFlip <C> 51.04 <C> 80.49 <C> 47.53 <C> 36.42 <C> 43.66 <R> <C> [BOLD] en-de <C> Min-Grad + HotFlip <C> 53.23 <C> 83.13 <C> 49.15 <C> 36.51 <C> 44.76 <R> <C> [BOLD] en-de <C> random + Soft-Att <C> 32.01 <C> 84.79 <C> [BOLD] 29.72 <C> [BOLD] 20.62 <C> 27.85 <R> <C> [BOLD] en-de <C> Min-Grad + Soft-Att <C> [BOLD] 31.17 <C> [BOLD] 88.55 <C> 31.09 <C> 20.63 <C> [BOLD] 27.43 <R> <C> [BOLD] en-fr <C> random + HotFlip <C> 55.51 <C> 85.18 <C> 40.35 <C> 52.00 <C> 36.18 <R> <C> [BOLD] en-fr <C> Min-Grad + HotFlip <C> 57.92 <C> 88.40 <C> 41.98 <C> 54.39 <C> 37.68 <R> <C> [BOLD] en-fr <C> random + Soft-Att <C> [BOLD] 33.61 <C> 89.77 <C> [BOLD] 21.59 <C> [BOLD] 32.37 <C> [BOLD] 19.09 <R> <C> [BOLD] en-fr <C> Min-Grad + Soft-Att <C> 35.40 <C> [BOLD] 91.99 <C> 23.28 <C> 34.32 <C> 20.29 <CAP> TABLE V: BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. l1 denotes the model under attack, l2 denotes the other Transformer model. lblstm1,lblstm2 are the BLSTM counterparts of l1 and l2. <COT> Looking at the "en-de" row and the "Soft-Att" column, we can see that the BLEU score for the original/adversarial sentence translation by the Min-Grad + Soft-Att model is 31.17/88.55.
<R> <C> [BOLD] Transformer <C> [BOLD] Method <C> src <C> [ITALIC] l1 <C> [ITALIC] l2 <C> [ITALIC] lblstm1 <C> [ITALIC] lblstm2 <R> <C> [BOLD] en-de <C> random + HotFlip <C> 51.04 <C> 80.49 <C> 47.53 <C> 36.42 <C> 43.66 <R> <C> [BOLD] en-de <C> Min-Grad + HotFlip <C> 53.23 <C> 83.13 <C> 49.15 <C> 36.51 <C> 44.76 <R> <C> [BOLD] en-de <C> random + Soft-Att <C> 32.01 <C> 84.79 <C> [BOLD] 29.72 <C> [BOLD] 20.62 <C> 27.85 <R> <C> [BOLD] en-de <C> Min-Grad + Soft-Att <C> [BOLD] 31.17 <C> [BOLD] 88.55 <C> 31.09 <C> 20.63 <C> [BOLD] 27.43 <R> <C> [BOLD] en-fr <C> random + HotFlip <C> 55.51 <C> 85.18 <C> 40.35 <C> 52.00 <C> 36.18 <R> <C> [BOLD] en-fr <C> Min-Grad + HotFlip <C> 57.92 <C> 88.40 <C> 41.98 <C> 54.39 <C> 37.68 <R> <C> [BOLD] en-fr <C> random + Soft-Att <C> [BOLD] 33.61 <C> 89.77 <C> [BOLD] 21.59 <C> [BOLD] 32.37 <C> [BOLD] 19.09 <R> <C> [BOLD] en-fr <C> Min-Grad + Soft-Att <C> 35.40 <C> [BOLD] 91.99 <C> 23.28 <C> 34.32 <C> 20.29 <CAP> TABLE V: BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. l1 denotes the model under attack, l2 denotes the other Transformer model. lblstm1,lblstm2 are the BLSTM counterparts of l1 and l2. <COT> Looking at the "en-fr" row and the "random + Soft-Att" column, we can see that the BLEU score for the original/adversarial sentence translation by the random + Soft-Att model is 33.61/89.77.
<R> <C> [BOLD] BLSTM <C> [BOLD] Method <C> src <C> [ITALIC] l1 <C> [ITALIC] l2 <C> [ITALIC] ltrans1 <C> [ITALIC] ltrans2 <R> <C> [BOLD] en-de <C> random + HotFlip <C> 57.09 <C> 71.35 <C> 48.84 <C> 43.90 <C> 49.42 <R> <C> [BOLD] en-de <C> Min-Grad + HotFlip <C> 59.28 <C> 75.55 <C> 50.38 <C> 45.96 <C> 52.26 <R> <C> [BOLD] en-de <C> random + Soft-Att <C> [BOLD] 13.77 <C> 87.14 <C> [BOLD] 19.20 <C> [BOLD] 18.36 <C> [BOLD] 21.62 <R> <C> [BOLD] en-de <C> Min-Grad + Soft-Att <C> 14.49 <C> [BOLD] 89.86 <C> 19.74 <C> 18.51 <C> 21.98 <R> <C> [BOLD] en-fr <C> random + HotFlip <C> 60.87 <C> 79.62 <C> 39.28 <C> 58.60 <C> 41.73 <R> <C> [BOLD] en-fr <C> Min-Grad + HotFlip <C> 63.97 <C> 84.87 <C> 41.16 <C> 61.80 <C> 44.94 <R> <C> [BOLD] en-fr <C> random + Soft-Att <C> 12.99 <C> 92.44 <C> 10.62 <C> 28.34 <C> 12.12 <R> <C> [BOLD] en-fr <C> Min-Grad + Soft-Att <C> [BOLD] 12.66 <C> [BOLD] 93.87 <C> [BOLD] 9.95 <C> [BOLD] 27.21 <C> [BOLD] 11.92 <CAP> TABLE VI: BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. l1 denotes the model under attack, l2 denotes the other BLSTM model. ltrans1,ltrans2 are the Transformer counterparts of l1 and l2. <COT> Looking at the "en-de" row, "random + Soft-Att" cell, "[BOLD] 13.77" cell, "src" cell, and "l1" cell, we can see that the BLEU score for the original sentence translated by the "random + Soft-Att" model is 13.77.
<R> <C> [BOLD] BLSTM <C> [BOLD] Method <C> src <C> [ITALIC] l1 <C> [ITALIC] l2 <C> [ITALIC] ltrans1 <C> [ITALIC] ltrans2 <R> <C> [BOLD] en-de <C> random + HotFlip <C> 57.09 <C> 71.35 <C> 48.84 <C> 43.90 <C> 49.42 <R> <C> [BOLD] en-de <C> Min-Grad + HotFlip <C> 59.28 <C> 75.55 <C> 50.38 <C> 45.96 <C> 52.26 <R> <C> [BOLD] en-de <C> random + Soft-Att <C> [BOLD] 13.77 <C> 87.14 <C> [BOLD] 19.20 <C> [BOLD] 18.36 <C> [BOLD] 21.62 <R> <C> [BOLD] en-de <C> Min-Grad + Soft-Att <C> 14.49 <C> [BOLD] 89.86 <C> 19.74 <C> 18.51 <C> 21.98 <R> <C> [BOLD] en-fr <C> random + HotFlip <C> 60.87 <C> 79.62 <C> 39.28 <C> 58.60 <C> 41.73 <R> <C> [BOLD] en-fr <C> Min-Grad + HotFlip <C> 63.97 <C> 84.87 <C> 41.16 <C> 61.80 <C> 44.94 <R> <C> [BOLD] en-fr <C> random + Soft-Att <C> 12.99 <C> 92.44 <C> 10.62 <C> 28.34 <C> 12.12 <R> <C> [BOLD] en-fr <C> Min-Grad + Soft-Att <C> [BOLD] 12.66 <C> [BOLD] 93.87 <C> [BOLD] 9.95 <C> [BOLD] 27.21 <C> [BOLD] 11.92 <CAP> TABLE VI: BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. l1 denotes the model under attack, l2 denotes the other BLSTM model. ltrans1,ltrans2 are the Transformer counterparts of l1 and l2. <COT> Looking at the "en-de" row, "random + HotFlip" cell, "src" cell, and "l2" cell, we can see that the BLEU score for the original sentence translated by the "random + HotFlip" model is 43.90.
<R> <C> % noised <C> SacreBleu Newstest ’12 <C> SacreBleu Newstest ’17 <R> <C> 0% <C> 22.4 <C> 28.1 <R> <C> 20% <C> 22.4 <C> 27.9 <R> <C> 80% <C> 21.5 <C> 27.0 <R> <C> 100% <C> 21.2 <C> 25.6 <CAP> Table 2: SacreBleu degradation as a function of the proportion of bitext data that is noised. <COT> Looking at the "SacreBleu Newstest '12" column, we can observe that as the percentage of noised data increases, the SacreBleu score decreases.
<R> <C> % noised <C> SacreBleu Newstest ’12 <C> SacreBleu Newstest ’17 <R> <C> 0% <C> 22.4 <C> 28.1 <R> <C> 20% <C> 22.4 <C> 27.9 <R> <C> 80% <C> 21.5 <C> 27.0 <R> <C> 100% <C> 21.2 <C> 25.6 <CAP> Table 2: SacreBleu degradation as a function of the proportion of bitext data that is noised. <COT> Comparing the "SacreBleu Newstest '17" column for different percentages of noised data, we can see that the SacreBleu score is higher for 0% noised data compared to 100% noised data.
<R> <C> a. Forward models (EnRo) Model <C> a. Forward models (EnRo) dev <C> a. Forward models (EnRo) test <R> <C> Gehring et al. ( 2017 ) <C> [EMPTY] <C> [ITALIC] 29.9 <R> <C> Sennrich 2016 (BT) <C> [ITALIC] 29.3 <C> [ITALIC] 28.1 <R> <C> bitext <C> 26.5 <C> 28.3 <R> <C> BT <C> 31.6 <C> 32.6 <R> <C> NoisedBT <C> 29.9 <C> 32.0 <R> <C> TaggedBT <C> 30.5 <C> 33.0 <R> <C> It.-3 BT <C> 31.3 <C> 32.8 <R> <C> It.-3 NoisedBT <C> 31.2 <C> 32.6 <R> <C> It.-3 TaggedBT <C> 31.4 <C> [BOLD] 33.4 <R> <C> b. Reverse models (RoEn) <C> b. Reverse models (RoEn) <C> b. Reverse models (RoEn) <R> <C> Model <C> dev <C> test <R> <C> bitext <C> 32.9 <C> 31.9 <R> <C> It.-2 BT <C> 39.5 <C> [BOLD] 37.3 <CAP> Table 4: Comparing SacreBleu scores for different flavors of BT for WMT16 EnRo. Previous works’ scores are reported in italics as they use detok.multi-bleu instead of SacreBleu, so are not guaranteed to be comparable. In this case, however, we do see identical Bleu on our systems when we score them with detok.multi-bleu, so we believe it to be a fair comparison. <COT> Looking at the "a. Forward models (EnRo) Model" column, the "a. Forward models (EnRo) dev" column, and the "a. Forward models (EnRo) test" column, we can see that the Bitext model performs better on the dev set compared to the NoisedBT model, but the NoisedBT model performs better on the test set compared to the Bitext model.
<R> <C> a. Forward models (EnRo) Model <C> a. Forward models (EnRo) dev <C> a. Forward models (EnRo) test <R> <C> Gehring et al. ( 2017 ) <C> [EMPTY] <C> [ITALIC] 29.9 <R> <C> Sennrich 2016 (BT) <C> [ITALIC] 29.3 <C> [ITALIC] 28.1 <R> <C> bitext <C> 26.5 <C> 28.3 <R> <C> BT <C> 31.6 <C> 32.6 <R> <C> NoisedBT <C> 29.9 <C> 32.0 <R> <C> TaggedBT <C> 30.5 <C> 33.0 <R> <C> It.-3 BT <C> 31.3 <C> 32.8 <R> <C> It.-3 NoisedBT <C> 31.2 <C> 32.6 <R> <C> It.-3 TaggedBT <C> 31.4 <C> [BOLD] 33.4 <R> <C> b. Reverse models (RoEn) <C> b. Reverse models (RoEn) <C> b. Reverse models (RoEn) <R> <C> Model <C> dev <C> test <R> <C> bitext <C> 32.9 <C> 31.9 <R> <C> It.-2 BT <C> 39.5 <C> [BOLD] 37.3 <CAP> Table 4: Comparing SacreBleu scores for different flavors of BT for WMT16 EnRo. Previous works’ scores are reported in italics as they use detok.multi-bleu instead of SacreBleu, so are not guaranteed to be comparable. In this case, however, we do see identical Bleu on our systems when we score them with detok.multi-bleu, so we believe it to be a fair comparison. <COT> Looking at the "b. Reverse models (RoEn) Model" column, the "b. Reverse models (RoEn) dev" column, and the "b. Reverse models (RoEn) test" column, we can see that the It.-2 BT model performs better on the dev set compared to the It.-2 NoisedBT model, but the It.-2 NoisedBT model performs better on the test set compared to the It.-2 BT model.
<R> <C> Model <C> Avg <C> 2008 <C> 2009 <C> 2010 <C> 2011 <C> 2012 <C> 2013 <C> 2014 <C> 2015 <R> <C> Bitext <C> 32.8 <C> 26.3 <C> 28.8 <C> 32.0 <C> 32.9 <C> 30.1 <C> 33.5 <C> 40.6 <C> 38.4 <R> <C> BT <C> 29.2 <C> 22.2 <C> 27.3 <C> 28.8 <C> 29.3 <C> 27.9 <C> 30.7 <C> 32.6 <C> 34.8 <R> <C> NoisedBT <C> 33.8 <C> 26.8 <C> 29.9 <C> 33.4 <C> [BOLD] 33.9 <C> [BOLD] 31.3 <C> 34.3 <C> 42.3 <C> 38.8 <R> <C> TaggedBT <C> [BOLD] 34.1 <C> [BOLD] 27.0 <C> [BOLD] 30.0 <C> [BOLD] 33.6 <C> [BOLD] 33.9 <C> 31.2 <C> [BOLD] 34.4 <C> [BOLD] 42.7 <C> [BOLD] 39.8 <CAP> Table 5: Results on WMT15 EnFr, with bitext, BT, NoisedBT, and TaggedBT. <COT> Looking at the "Model" column, finding that the table shows the results on WMT15 EnFr with different models including Bitext, BT, NoisedBT, and TaggedBT.
<R> <C> Model <C> Avg <C> 2008 <C> 2009 <C> 2010 <C> 2011 <C> 2012 <C> 2013 <C> 2014 <C> 2015 <R> <C> Bitext <C> 32.8 <C> 26.3 <C> 28.8 <C> 32.0 <C> 32.9 <C> 30.1 <C> 33.5 <C> 40.6 <C> 38.4 <R> <C> BT <C> 29.2 <C> 22.2 <C> 27.3 <C> 28.8 <C> 29.3 <C> 27.9 <C> 30.7 <C> 32.6 <C> 34.8 <R> <C> NoisedBT <C> 33.8 <C> 26.8 <C> 29.9 <C> 33.4 <C> [BOLD] 33.9 <C> [BOLD] 31.3 <C> 34.3 <C> 42.3 <C> 38.8 <R> <C> TaggedBT <C> [BOLD] 34.1 <C> [BOLD] 27.0 <C> [BOLD] 30.0 <C> [BOLD] 33.6 <C> [BOLD] 33.9 <C> 31.2 <C> [BOLD] 34.4 <C> [BOLD] 42.7 <C> [BOLD] 39.8 <CAP> Table 5: Results on WMT15 EnFr, with bitext, BT, NoisedBT, and TaggedBT. <COT> Looking at the "TaggedBT" row, finding that the "2011" cell, "2012" cell, "2014" cell, and "2015" cell are marked as [BOLD], indicating the highest scores in those years.
<R> <C> Model <C> ASR0 <C> ASR| [ITALIC] x| <C> ~H <R> <C> Bitext baseline <C> 0.31 <C> 10.21 <C> 0.504 <R> <C> BT <C> 0.28 <C> 10.98 <C> [BOLD] 0.455 <R> <C> P3BT <C> 0.37 <C> 7.66 <C> 0.558 <R> <C> NoisedBT <C> 1.01 <C> 3.96 <C> 0.619 <R> <C> TaggedBT <C> [BOLD] 5.31 <C> 5.31 <C> 0.597 <R> <C> TaggedNoisedBT <C> [BOLD] 7.33 <C> 7.33 <C> 0.491 <CAP> Table 6: Attention sink ratio on the first and last token and entropy (at decoder layer 5) for the models in Table 3.a, averaged over all sentences in newstest14. For ASR, data is treated as if it were BT (noised and/or tagged, resp.), whereas for entropy the natural text is used. Outliers discussed in the text are bolded. <COT> Looking at the "Model" column, we can see that the "TaggedBT" model has the highest attention sink ratio on the first and last token.
<R> <C> Model <C> ASR0 <C> ASR| [ITALIC] x| <C> ~H <R> <C> Bitext baseline <C> 0.31 <C> 10.21 <C> 0.504 <R> <C> BT <C> 0.28 <C> 10.98 <C> [BOLD] 0.455 <R> <C> P3BT <C> 0.37 <C> 7.66 <C> 0.558 <R> <C> NoisedBT <C> 1.01 <C> 3.96 <C> 0.619 <R> <C> TaggedBT <C> [BOLD] 5.31 <C> 5.31 <C> 0.597 <R> <C> TaggedNoisedBT <C> [BOLD] 7.33 <C> 7.33 <C> 0.491 <CAP> Table 6: Attention sink ratio on the first and last token and entropy (at decoder layer 5) for the models in Table 3.a, averaged over all sentences in newstest14. For ASR, data is treated as if it were BT (noised and/or tagged, resp.), whereas for entropy the natural text is used. Outliers discussed in the text are bolded. <COT> Looking at the "Model" column and the "BT" row, we can see that the attention sink ratio for the "BT" model is lower than the attention sink ratio for the "NoisedBT" model.
<R> <C> Model <C> Decode type <C> AVG 13-17 <C> 2010 <C> 2011 <C> 2012 <C> 2013 <C> 2014 <C> 2015 <C> 2016 <C> 2017 <R> <C> TaggedBT <C> standard <C> [BOLD] 33.24 <C> 26.5 <C> [BOLD] 24.2 <C> [BOLD] 25.2 <C> [BOLD] 28.7 <C> [BOLD] 32.8 <C> [BOLD] 34.5 <C> [BOLD] 38.1 <C> [BOLD] 32.4 <R> <C> [EMPTY] <C> as BT (tagged) <C> 30.30 <C> 24.3 <C> 22.2 <C> 23.4 <C> 26.6 <C> 30.0 <C> 30.5 <C> 34.2 <C> 30.2 <R> <C> NoisedBT <C> standard <C> 33.06 <C> [BOLD] 26.7 <C> 24.0 <C> [BOLD] 25.2 <C> 28.6 <C> 32.6 <C> 33.9 <C> 38.0 <C> 32.2 <R> <C> [EMPTY] <C> as BT (noised) <C> 10.66 <C> 8.1 <C> 6.5 <C> 7.5 <C> 8.2 <C> 11.1 <C> 10.0 <C> 12.7 <C> 11.3 <CAP> Table 7: Comparing standard decoding with decoding as if the input were back-translated data, meaning that it is tagged (for the TaggedBT model) or noised (for the NoisedBT model) . <COT> Looking at the "Model" column, we can see that there are two models: TaggedBT and NoisedBT.
<R> <C> Model <C> Decode type <C> AVG 13-17 <C> 2010 <C> 2011 <C> 2012 <C> 2013 <C> 2014 <C> 2015 <C> 2016 <C> 2017 <R> <C> TaggedBT <C> standard <C> [BOLD] 33.24 <C> 26.5 <C> [BOLD] 24.2 <C> [BOLD] 25.2 <C> [BOLD] 28.7 <C> [BOLD] 32.8 <C> [BOLD] 34.5 <C> [BOLD] 38.1 <C> [BOLD] 32.4 <R> <C> [EMPTY] <C> as BT (tagged) <C> 30.30 <C> 24.3 <C> 22.2 <C> 23.4 <C> 26.6 <C> 30.0 <C> 30.5 <C> 34.2 <C> 30.2 <R> <C> NoisedBT <C> standard <C> 33.06 <C> [BOLD] 26.7 <C> 24.0 <C> [BOLD] 25.2 <C> 28.6 <C> 32.6 <C> 33.9 <C> 38.0 <C> 32.2 <R> <C> [EMPTY] <C> as BT (noised) <C> 10.66 <C> 8.1 <C> 6.5 <C> 7.5 <C> 8.2 <C> 11.1 <C> 10.0 <C> 12.7 <C> 11.3 <CAP> Table 7: Comparing standard decoding with decoding as if the input were back-translated data, meaning that it is tagged (for the TaggedBT model) or noised (for the NoisedBT model) . <COT> Looking at the "Decode type" column, we can see that there are two types of decoding: standard and as BT (back-translated).
<R> <C> Data <C> src-tgt unigram overlap <R> <C> TaggedBT (standard decode) <C> 8.9% <R> <C> TaggedBT (tagged decode) <C> 10.7% <R> <C> Bitext <C> 5.9% <R> <C> BT Data <C> 11.4 % <CAP> Table 9: Source-target overlap for both back-translated data with decoding newstest as if it were bitext or BT data. Model decodes are averaged over newstest2010-newstest2017. <COT> looking at the "TaggedBT (standard decode)" and "TaggedBT (tagged decode)" cells, we can compare the values of the "src-tgt unigram overlap" for these two types of data.
<R> <C> Class <C> Number of samples <R> <C> acq <C> 2292 <R> <C> crude <C> 374 <R> <C> earn <C> 3923 <R> <C> grain <C> 51 <R> <C> interest <C> 271 <R> <C> money-fx <C> 293 <R> <C> ship <C> 144 <R> <C> trade <C> 326 <CAP> TABLE I: Document distribution over the classes of the Reuters-8 <COT> Looking at the "Class" column, we can see that there are 8 different classes in the Reuters-8 dataset.
<R> <C> Class <C> Number of samples <R> <C> acq <C> 2292 <R> <C> crude <C> 374 <R> <C> earn <C> 3923 <R> <C> grain <C> 51 <R> <C> interest <C> 271 <R> <C> money-fx <C> 293 <R> <C> ship <C> 144 <R> <C> trade <C> 326 <CAP> TABLE I: Document distribution over the classes of the Reuters-8 <COT> Looking at the "Number of samples" column, we can see that the "earn" class has the highest number of samples with 3923.
<R> <C> Method <C> Feature <C> Accuracy (%) <C> Std. Deviation <R> <C> SA <C> w2v <C> 78.73 <C> 1.56 <R> <C> MSM <C> w2v <C> [BOLD] 90.62 <C> 0.42 <R> <C> TF-MSM <C> w2v <C> [BOLD] 92.01 <C> 0.30 <R> <C> MVB <C> binBOW <C> 62.70 <C> 0.69 <R> <C> MNB <C> tfBOW <C> [BOLD] 91.47 <C> 0.37 <R> <C> LSA <C> w2v <C> 34.58 <C> 0.40 <R> <C> LSA <C> binBOW <C> 86.92 <C> 0.74 <R> <C> LSA <C> tfBOW <C> 86.23 <C> 0.96 <R> <C> LSA <C> tfidfBOW <C> 86.35 <C> 1.03 <R> <C> SVM <C> w2v <C> 26.61 <C> 0.30 <R> <C> SVM <C> binBOW <C> 89.23 <C> 0.24 <R> <C> SVM <C> tfBOW <C> 89.10 <C> 0.29 <R> <C> SVM <C> tfidfBOW <C> 88.78 <C> 0.40 <CAP> TABLE II: Results for the text classification experiment on Reuters-8 database <COT> Looking at the "Method" column, we can see that there are multiple methods used for the text classification experiment on the Reuters-8 database.
<R> <C> Method <C> Feature <C> Accuracy (%) <C> Std. Deviation <R> <C> SA <C> w2v <C> 78.73 <C> 1.56 <R> <C> MSM <C> w2v <C> [BOLD] 90.62 <C> 0.42 <R> <C> TF-MSM <C> w2v <C> [BOLD] 92.01 <C> 0.30 <R> <C> MVB <C> binBOW <C> 62.70 <C> 0.69 <R> <C> MNB <C> tfBOW <C> [BOLD] 91.47 <C> 0.37 <R> <C> LSA <C> w2v <C> 34.58 <C> 0.40 <R> <C> LSA <C> binBOW <C> 86.92 <C> 0.74 <R> <C> LSA <C> tfBOW <C> 86.23 <C> 0.96 <R> <C> LSA <C> tfidfBOW <C> 86.35 <C> 1.03 <R> <C> SVM <C> w2v <C> 26.61 <C> 0.30 <R> <C> SVM <C> binBOW <C> 89.23 <C> 0.24 <R> <C> SVM <C> tfBOW <C> 89.10 <C> 0.29 <R> <C> SVM <C> tfidfBOW <C> 88.78 <C> 0.40 <CAP> TABLE II: Results for the text classification experiment on Reuters-8 database <COT> Looking at the "Accuracy (%)" column, we can see that the highest accuracy is achieved by the TF-MSM method with w2v feature.
<R> <C> [BOLD] Dataset <C> [BOLD] System <C> [BOLD] ERRANT  [BOLD] P <C> [BOLD] ERRANT  [BOLD] R <C> [BOLD] ERRANT  [BOLD] F0.5 <C> [BOLD] M2  [BOLD] P <C> [BOLD] M2  [BOLD] R <C> [BOLD] M2  [BOLD] F0.5 <R> <C> [BOLD] CoNLL-2014 <C> Felice et al. ( 2014 ) † <C> - <C> - <C> - <C> 39.71 <C> 30.10 <C> 37.33 <R> <C> [BOLD] CoNLL-2014 <C> Yannakoudakis et al. ( 2017 ) <C> - <C> - <C> - <C> 58.79 <C> 30.63 <C> 49.66 <R> <C> [BOLD] CoNLL-2014 <C> Chollampatt and Ng ( 2017 ) <C> - <C> - <C> - <C> 62.74 <C> 32.96 <C> 53.14 <R> <C> [BOLD] CoNLL-2014 <C> Chollampatt and Ng ( 2018 ) <C> - <C> - <C> - <C> 65.49 <C> 33.14 <C> 54.79 <R> <C> [BOLD] CoNLL-2014 <C> Ge et al. ( 2018 ) <C> - <C> - <C> - <C> [BOLD] 74.12 <C> [BOLD] 36.30 <C> [BOLD] 61.34 <R> <C> [BOLD] CoNLL-2014 <C> Bryant and Briscoe ( 2018 ) <C> 36.62 <C> 19.93 <C> 31.37 <C> 40.56 <C> 20.81 <C> 34.09 <R> <C> [BOLD] CoNLL-2014 <C> BERT <C> 33.27 <C> [BOLD] 27.14 <C> 31.83 <C> 35.69 <C> [BOLD] 27.99 <C> 33.83 <R> <C> [BOLD] CoNLL-2014 <C> GPT-1 <C> 49.58 <C> 27.06 <C> 42.5 <C> 51.08 <C> 27.45 <C> 43.57 <R> <C> [BOLD] CoNLL-2014 <C> GPT-2 <C> [BOLD] 57.73 <C> 24.75 <C> [BOLD] 45.58 <C> [BOLD] 58.51 <C> 24.9 <C> [BOLD] 46.08 <R> <C> [BOLD] FCE <C> Yannakoudakis et al. ( 2017 ) <C> - <C> - <C> - <C> [BOLD] 65.03 <C> 32.45 <C> [BOLD] 54.15 <R> <C> [BOLD] FCE <C> Bryant and Briscoe ( 2018 ) <C> 41.92 <C> 13.62 <C> 29.61 <C> 44.78 <C> 14.12 <C> 31.22 <R> <C> [BOLD] FCE <C> BERT <C> 29.56 <C> [BOLD] 34.67 <C> 30.46 <C> 31.97 <C> [BOLD] 35.01 <C> 32.53 <R> <C> [BOLD] FCE <C> GPT-1 <C> [BOLD] 62.75 <C> 32.19 <C> 52.74 <C> [BOLD] 64.01 <C> 32.33 <C> [BOLD] 53.52 <R> <C> [BOLD] FCE <C> GPT-2 <C> 61.91 <C> 33.47 <C> [BOLD] 52.92 <C> 62.64 <C> 33.74 <C> 53.48 <CAP> Table 2: Results of our Transformer-Language Model approach against similar approaches (Bryant and Briscoe, 2018) and state-of-the-art on Grammatical Error Correction. For each of the datasets, we use the corresponding test set, and we do not train our models on the corpora. As BERT, we report the best performing BERT model (12 layers, retaining uppercase characters). In the top part of each dataset, we report the scores of supervised methods and in the bottom the unsupervised ones. † denotes this system won the shared task competition. <COT> Looking at the "CoNLL-2014" row and the "GPT-2" column, we can see that the ERRANT F0.5 score for GPT-2 on the CoNLL-2014 dataset is 45.58.
<R> <C> [BOLD] Dataset <C> [BOLD] System <C> [BOLD] ERRANT  [BOLD] P <C> [BOLD] ERRANT  [BOLD] R <C> [BOLD] ERRANT  [BOLD] F0.5 <C> [BOLD] M2  [BOLD] P <C> [BOLD] M2  [BOLD] R <C> [BOLD] M2  [BOLD] F0.5 <R> <C> [BOLD] CoNLL-2014 <C> Felice et al. ( 2014 ) † <C> - <C> - <C> - <C> 39.71 <C> 30.10 <C> 37.33 <R> <C> [BOLD] CoNLL-2014 <C> Yannakoudakis et al. ( 2017 ) <C> - <C> - <C> - <C> 58.79 <C> 30.63 <C> 49.66 <R> <C> [BOLD] CoNLL-2014 <C> Chollampatt and Ng ( 2017 ) <C> - <C> - <C> - <C> 62.74 <C> 32.96 <C> 53.14 <R> <C> [BOLD] CoNLL-2014 <C> Chollampatt and Ng ( 2018 ) <C> - <C> - <C> - <C> 65.49 <C> 33.14 <C> 54.79 <R> <C> [BOLD] CoNLL-2014 <C> Ge et al. ( 2018 ) <C> - <C> - <C> - <C> [BOLD] 74.12 <C> [BOLD] 36.30 <C> [BOLD] 61.34 <R> <C> [BOLD] CoNLL-2014 <C> Bryant and Briscoe ( 2018 ) <C> 36.62 <C> 19.93 <C> 31.37 <C> 40.56 <C> 20.81 <C> 34.09 <R> <C> [BOLD] CoNLL-2014 <C> BERT <C> 33.27 <C> [BOLD] 27.14 <C> 31.83 <C> 35.69 <C> [BOLD] 27.99 <C> 33.83 <R> <C> [BOLD] CoNLL-2014 <C> GPT-1 <C> 49.58 <C> 27.06 <C> 42.5 <C> 51.08 <C> 27.45 <C> 43.57 <R> <C> [BOLD] CoNLL-2014 <C> GPT-2 <C> [BOLD] 57.73 <C> 24.75 <C> [BOLD] 45.58 <C> [BOLD] 58.51 <C> 24.9 <C> [BOLD] 46.08 <R> <C> [BOLD] FCE <C> Yannakoudakis et al. ( 2017 ) <C> - <C> - <C> - <C> [BOLD] 65.03 <C> 32.45 <C> [BOLD] 54.15 <R> <C> [BOLD] FCE <C> Bryant and Briscoe ( 2018 ) <C> 41.92 <C> 13.62 <C> 29.61 <C> 44.78 <C> 14.12 <C> 31.22 <R> <C> [BOLD] FCE <C> BERT <C> 29.56 <C> [BOLD] 34.67 <C> 30.46 <C> 31.97 <C> [BOLD] 35.01 <C> 32.53 <R> <C> [BOLD] FCE <C> GPT-1 <C> [BOLD] 62.75 <C> 32.19 <C> 52.74 <C> [BOLD] 64.01 <C> 32.33 <C> [BOLD] 53.52 <R> <C> [BOLD] FCE <C> GPT-2 <C> 61.91 <C> 33.47 <C> [BOLD] 52.92 <C> 62.64 <C> 33.74 <C> 53.48 <CAP> Table 2: Results of our Transformer-Language Model approach against similar approaches (Bryant and Briscoe, 2018) and state-of-the-art on Grammatical Error Correction. For each of the datasets, we use the corresponding test set, and we do not train our models on the corpora. As BERT, we report the best performing BERT model (12 layers, retaining uppercase characters). In the top part of each dataset, we report the scores of supervised methods and in the bottom the unsupervised ones. † denotes this system won the shared task competition. <COT> Looking at the "FCE" row and the "BERT" column, we can see that the ERRANT R score for BERT on the FCE dataset is 34.67.
<R> <C> [BOLD] Reply Type <C> [BOLD] Description <C> [BOLD] Antichat <C> [BOLD] Hackforums <R> <C> Buy <C> Someone wants to buy or bought a product. <C> 8% <C> 12% <R> <C> Sell <C> Someone making a sale offer to the original poster of a thread. <C> 8% <C> 2% <R> <C> Other <C> Anything that didn’t fall into the previous categories. <C> 84% <C> 86% <R> <C> TOTAL <C> [EMPTY] <C> 9,992 <C> 5,898 <CAP> Table III: Reply classification labels and distribution per source <COT> Looking at the "Reply Type" column and the "Antichat" and "Hackforums" rows, we can see the distribution of reply types for each source.
<R> <C> [BOLD] Reply Type <C> [BOLD] Description <C> [BOLD] Antichat <C> [BOLD] Hackforums <R> <C> Buy <C> Someone wants to buy or bought a product. <C> 8% <C> 12% <R> <C> Sell <C> Someone making a sale offer to the original poster of a thread. <C> 8% <C> 2% <R> <C> Other <C> Anything that didn’t fall into the previous categories. <C> 84% <C> 86% <R> <C> TOTAL <C> [EMPTY] <C> 9,992 <C> 5,898 <CAP> Table III: Reply classification labels and distribution per source <COT> Looking at the "Description" column and the "Antichat" row, we can see that the majority of replies in Antichat fall into the "Other" category.
<R> <C> [EMPTY] <C> [BOLD] Antichat  [ITALIC] Product <C> [BOLD] Antichat  [ITALIC] Product <C> [BOLD] Antichat  [ITALIC] Product <C> [BOLD] Antichat  [ITALIC] Reply <C> [BOLD] Antichat  [ITALIC] Reply <C> [BOLD] Antichat  [ITALIC] Reply <C> [BOLD] Hack Forums  [ITALIC] Product <C> [BOLD] Hack Forums  [ITALIC] Product <C> [BOLD] Hack Forums  [ITALIC] Product <C> [BOLD] Hack Forums  [ITALIC] Reply <C> [BOLD] Hack Forums  [ITALIC] Reply <C> [BOLD] Hack Forums  [ITALIC] Reply <R> <C> [BOLD] Model <C> Prec <C> Recall <C> F1 <C> Prec <C> Recall <C> F1 <C> Prec <C> Recall <C> F1 <C> Prec <C> Recall <C> F1 <R> <C> [BOLD] FastText <C> 0.824 <C> 0.734 <C> 0.764 <C> 0.823 <C> 0.318 <C> 0.450 <C> 0.722 <C> 0.582 <C> 0.627 <C> 0.800 <C> 0.427 <C> 0.539 <R> <C> [BOLD] Logistic Regression <C> 0.831 <C> 0.718 <C> 0.753 <C> [BOLD] 0.874 <C> 0.245 <C> 0.381 <C> 0.617 <C> 0.564 <C> 0.617 <C> [BOLD] 0.852 <C> 0.363 <C> 0.492 <R> <C> [BOLD] SVM <C> 0.817 <C> 0.748 <C> 0.767 <C> 0.654 <C> 0.213 <C> 0.301 <C> 0.716 <C> 0.578 <C> 0.614 <C> 0.812 <C> 0.332 <C> 0.440 <R> <C> [BOLD] XGBoost <C> [BOLD] 0.824 <C> 0.677 <C> 0.729 <C> 0.713 <C> 0.227 <C> 0.328 <C> [BOLD] 0.734 <C> 0.577 <C> 0.627 <C> 0.819 <C> 0.352 <C> 0.465 <CAP> Table IV: Weighted precision, recall and F1 scores of classifiers across datasets and tasks, with stratified k-fold cross-validation <COT> Looking at the "Model" column, "Logistic Regression" has the highest F1 score in the "Antichat Product" task.
<R> <C> [EMPTY] <C> [BOLD] Antichat  [ITALIC] Product <C> [BOLD] Antichat  [ITALIC] Product <C> [BOLD] Antichat  [ITALIC] Product <C> [BOLD] Antichat  [ITALIC] Reply <C> [BOLD] Antichat  [ITALIC] Reply <C> [BOLD] Antichat  [ITALIC] Reply <C> [BOLD] Hack Forums  [ITALIC] Product <C> [BOLD] Hack Forums  [ITALIC] Product <C> [BOLD] Hack Forums  [ITALIC] Product <C> [BOLD] Hack Forums  [ITALIC] Reply <C> [BOLD] Hack Forums  [ITALIC] Reply <C> [BOLD] Hack Forums  [ITALIC] Reply <R> <C> [BOLD] Model <C> Prec <C> Recall <C> F1 <C> Prec <C> Recall <C> F1 <C> Prec <C> Recall <C> F1 <C> Prec <C> Recall <C> F1 <R> <C> [BOLD] FastText <C> 0.824 <C> 0.734 <C> 0.764 <C> 0.823 <C> 0.318 <C> 0.450 <C> 0.722 <C> 0.582 <C> 0.627 <C> 0.800 <C> 0.427 <C> 0.539 <R> <C> [BOLD] Logistic Regression <C> 0.831 <C> 0.718 <C> 0.753 <C> [BOLD] 0.874 <C> 0.245 <C> 0.381 <C> 0.617 <C> 0.564 <C> 0.617 <C> [BOLD] 0.852 <C> 0.363 <C> 0.492 <R> <C> [BOLD] SVM <C> 0.817 <C> 0.748 <C> 0.767 <C> 0.654 <C> 0.213 <C> 0.301 <C> 0.716 <C> 0.578 <C> 0.614 <C> 0.812 <C> 0.332 <C> 0.440 <R> <C> [BOLD] XGBoost <C> [BOLD] 0.824 <C> 0.677 <C> 0.729 <C> 0.713 <C> 0.227 <C> 0.328 <C> [BOLD] 0.734 <C> 0.577 <C> 0.627 <C> 0.819 <C> 0.352 <C> 0.465 <CAP> Table IV: Weighted precision, recall and F1 scores of classifiers across datasets and tasks, with stratified k-fold cross-validation <COT> Looking at the "Antichat Reply" row, "SVM" has a higher F1 score than "FastText".
<R> <C> [BOLD] Task <C> [BOLD] [CLS] <C> [BOLD] Mean <C> [BOLD] Max <C> [BOLD] [SEP] <R> <C> Semantic Similarity <C> 34.1 <C> [BOLD] 84.5 <C> 80.7 <C> 13.0 <R> <C> Text Classification <C> 90.7 <C> [BOLD] 95.4 <C> 89.7 <C> 88.9 <R> <C> Entailment <C> 72.4 <C> [BOLD] 89.3 <C> 87.1 <C> 66.1 <R> <C> Surface Information <C> 45.6 <C> [BOLD] 78.9 <C> 47.3 <C> 42.8 <R> <C> Syntactic Information <C> 78.2 <C> [BOLD] 86.0 <C> 75.7 <C> 72.2 <R> <C> Semantic Information <C> 90.3 <C> [BOLD] 93.7 <C> 89.5 <C> 86.7 <R> <C> Average Score <C> 68.6 <C> [BOLD] 88.0 <C> 78.3 <C> 61.6 <CAP> Table 1: Comparison of pooling methods <COT> Looking at the "Task" column, we can see that the tasks listed are Semantic Similarity, Text Classification, Entailment, Surface Information, Syntactic Information, and Semantic Information.
<R> <C> [BOLD] Task <C> [BOLD] [CLS] <C> [BOLD] Mean <C> [BOLD] Max <C> [BOLD] [SEP] <R> <C> Semantic Similarity <C> 34.1 <C> [BOLD] 84.5 <C> 80.7 <C> 13.0 <R> <C> Text Classification <C> 90.7 <C> [BOLD] 95.4 <C> 89.7 <C> 88.9 <R> <C> Entailment <C> 72.4 <C> [BOLD] 89.3 <C> 87.1 <C> 66.1 <R> <C> Surface Information <C> 45.6 <C> [BOLD] 78.9 <C> 47.3 <C> 42.8 <R> <C> Syntactic Information <C> 78.2 <C> [BOLD] 86.0 <C> 75.7 <C> 72.2 <R> <C> Semantic Information <C> 90.3 <C> [BOLD] 93.7 <C> 89.5 <C> 86.7 <R> <C> Average Score <C> 68.6 <C> [BOLD] 88.0 <C> 78.3 <C> 61.6 <CAP> Table 1: Comparison of pooling methods <COT> Looking at the "Mean" column, we can see that the values are bolded for each task.
<R> <C> [BOLD] Dataset / Model  [BOLD] WikiPassageQA <C> [BOLD] Metrics  [BOLD] MAP <C> [BOLD] Metrics  [BOLD] P@5 <C> [BOLD] Metrics  [BOLD] P@10 <R> <C> BM25 <C> 53.7 <C> 19.5 <C> 11.5 <R> <C> Memory-CNN-LSTM Cohen2018 <C> 56.1 <C> 20.8 <C> 12.3 <R> <C> Pre-trained BERT Embedding <C> 55.0 <C> 21.6 <C> 13.7 <R> <C> SNLI Fine-tuned BERT Embedding <C> 52.5 <C> 20.6 <C> 12.8 <R> <C> In-domain Fine-tuned BERT <C> [BOLD] 74.9 <C> [BOLD] 27.2 <C> [BOLD] 15.2 <R> <C> [BOLD] InsuranceQA <C> [BOLD] P@1 <C> [BOLD] P@5 <C> [BOLD] P@10 <R> <C> BM25 <C> 60.2 <C> 19.5 <C> 10.9 <R> <C> SUBMULT+NN Wang2016g <C> 75.6 <C> - <C> - <R> <C> DSSM Huang2013a <C> 30.3 <C> - <C> - <R> <C> Pre-trained BERT Embedding <C> 44.9 <C> 17.6 <C> 10.6 <R> <C> SNLI Fine-tuned BERT Embedding <C> 48.0 <C> 18.5 <C> 11.0 <R> <C> In-domain Fine-tuned BERT <C> [BOLD] 78.3 <C> [BOLD] 25.4 <C> [BOLD] 13.7 <R> <C> [BOLD] Quasar-t <C> [BOLD] R@1 <C> [BOLD] R@5 <C> [BOLD] R@10 <R> <C> BM25 <C> 38.7 <C> 59.2 <C> 66.0 <R> <C> Pre-trained BERT Embedding <C> 48.6 <C> 66.6 <C> 71.7 <R> <C> SNLI Fine-tuned BERT Embedding <C> 49.3 <C> 66.1 <C> 71.0 <R> <C> In-domain Fine-tuned BERT <C> [BOLD] 59.5 <C> [BOLD] 70.9 <C> [BOLD] 74.6 <R> <C> [BOLD] SearchQA <C> [BOLD] R@1 <C> [BOLD] R@5 <C> [BOLD] R@10 <R> <C> BM25 <C> 50.5 <C> 83.3 <C> 90.9 <R> <C> Pre-trained BERT Embedding <C> 66.2 <C> 89.7 <C> 95.0 <R> <C> SNLI Fine-tuned BERT Embedding <C> 66.8 <C> 90.0 <C> 95.1 <R> <C> In-domain Fine-tuned BERT <C> [BOLD] 76.3 <C> [BOLD] 93.0 <C> [BOLD] 96.7 <CAP> Table 3: Results of BERT passage-level embeddings on question-answering datasets <COT> Looking at the "Quasar-t" row, we can see that the "In-domain Fine-tuned BERT" model achieves higher performance in terms of R@1, R@5, and R@10 compared to the other models.
<R> <C> [BOLD] Dataset / Model  [BOLD] WikiPassageQA <C> [BOLD] Metrics  [BOLD] MAP <C> [BOLD] Metrics  [BOLD] P@5 <C> [BOLD] Metrics  [BOLD] P@10 <R> <C> BM25 <C> 53.7 <C> 19.5 <C> 11.5 <R> <C> Memory-CNN-LSTM Cohen2018 <C> 56.1 <C> 20.8 <C> 12.3 <R> <C> Pre-trained BERT Embedding <C> 55.0 <C> 21.6 <C> 13.7 <R> <C> SNLI Fine-tuned BERT Embedding <C> 52.5 <C> 20.6 <C> 12.8 <R> <C> In-domain Fine-tuned BERT <C> [BOLD] 74.9 <C> [BOLD] 27.2 <C> [BOLD] 15.2 <R> <C> [BOLD] InsuranceQA <C> [BOLD] P@1 <C> [BOLD] P@5 <C> [BOLD] P@10 <R> <C> BM25 <C> 60.2 <C> 19.5 <C> 10.9 <R> <C> SUBMULT+NN Wang2016g <C> 75.6 <C> - <C> - <R> <C> DSSM Huang2013a <C> 30.3 <C> - <C> - <R> <C> Pre-trained BERT Embedding <C> 44.9 <C> 17.6 <C> 10.6 <R> <C> SNLI Fine-tuned BERT Embedding <C> 48.0 <C> 18.5 <C> 11.0 <R> <C> In-domain Fine-tuned BERT <C> [BOLD] 78.3 <C> [BOLD] 25.4 <C> [BOLD] 13.7 <R> <C> [BOLD] Quasar-t <C> [BOLD] R@1 <C> [BOLD] R@5 <C> [BOLD] R@10 <R> <C> BM25 <C> 38.7 <C> 59.2 <C> 66.0 <R> <C> Pre-trained BERT Embedding <C> 48.6 <C> 66.6 <C> 71.7 <R> <C> SNLI Fine-tuned BERT Embedding <C> 49.3 <C> 66.1 <C> 71.0 <R> <C> In-domain Fine-tuned BERT <C> [BOLD] 59.5 <C> [BOLD] 70.9 <C> [BOLD] 74.6 <R> <C> [BOLD] SearchQA <C> [BOLD] R@1 <C> [BOLD] R@5 <C> [BOLD] R@10 <R> <C> BM25 <C> 50.5 <C> 83.3 <C> 90.9 <R> <C> Pre-trained BERT Embedding <C> 66.2 <C> 89.7 <C> 95.0 <R> <C> SNLI Fine-tuned BERT Embedding <C> 66.8 <C> 90.0 <C> 95.1 <R> <C> In-domain Fine-tuned BERT <C> [BOLD] 76.3 <C> [BOLD] 93.0 <C> [BOLD] 96.7 <CAP> Table 3: Results of BERT passage-level embeddings on question-answering datasets <COT> Looking at the "InsuranceQA" row, we can see that the "In-domain Fine-tuned BERT" model achieves the highest P@1, P@5, and P@10 scores compared to the other models.
<R> <C> [EMPTY] <C> Arman word <C> Arman phrase <C> Peyma word <C> Peyma phrase <R> <C> Bokaei and Mahmoudi Bokaei and Mahmoudi ( 2018 ) <C> 81.50 <C> 76.79 <C> - <C> - <R> <C> Shahshahani et al.Shahshahani et al. ( 2018 ) <C> - <C> - <C> 80.0 <C> - <R> <C> Beheshti-NER (Our Model) <C> [BOLD] 84.03 <C> [BOLD] 79.93 <C> [BOLD] 90.59 <C> [BOLD] 87.62 <CAP> Table 3: comparing results of our trained model with others <COT> Looking at the "Beheshti-NER (Our Model)" row, we can see that the performance of our model is higher than the other models in all four categories (Arman word, Arman phrase, Peyma word, Peyma phrase).
<R> <C> [EMPTY] <C> Arman word <C> Arman phrase <C> Peyma word <C> Peyma phrase <R> <C> Bokaei and Mahmoudi Bokaei and Mahmoudi ( 2018 ) <C> 81.50 <C> 76.79 <C> - <C> - <R> <C> Shahshahani et al.Shahshahani et al. ( 2018 ) <C> - <C> - <C> 80.0 <C> - <R> <C> Beheshti-NER (Our Model) <C> [BOLD] 84.03 <C> [BOLD] 79.93 <C> [BOLD] 90.59 <C> [BOLD] 87.62 <CAP> Table 3: comparing results of our trained model with others <COT> Looking at the "Beheshti-NER (Our Model)" row and the "[BOLD] 84.03" cell, we can see that the performance of our model is 84.03% for the Arman word category.
<R> <C> Team <C> Team <C> Test Data 1 In Domain <C> Test Data 1 In Domain <C> Test Data 1 In Domain <C> Test Data 1 Out Domain <C> Test Data 1 Out Domain <C> Test Data 1 Out Domain <C> Test Data 1 Total <C> Test Data 1 Total <C> Test Data 1 Total <R> <C> [EMPTY] <C> [EMPTY] <C> P <C> R <C> F1 <C> P <C> R <C> F1 <C> P <C> R <C> F1 <R> <C> 1 <C> MorphoBERT <C> 88.7 <C> 85.5 <C> 87.1 <C> 86.3 <C> 83.8 <C> 85 <C> 87.3 <C> 84.5 <C> 85.9 <R> <C> 2 <C> Beheshti-NER-1 <C> 85.3 <C> 84.4 <C> 84.8 <C> 84.4 <C> 82.6 <C> 83.5 <C> 84.8 <C> 83.3 <C> 84 <R> <C> 3 <C> Team-3 <C> 87.4 <C> 77.2 <C> 82 <C> 87.4 <C> 73.4 <C> 79.8 <C> 87.4 <C> 75 <C> 80.7 <R> <C> 4 <C> ICTRC-NLPGroup <C> 87.5 <C> 76 <C> 81.3 <C> 86.2 <C> 69.6 <C> 77 <C> 86.8 <C> 72.3 <C> 78.9 <R> <C> 5 <C> UT-NLP-IR <C> 75.3 <C> 68.9 <C> 72 <C> 72.3 <C> 60.7 <C> 66 <C> 73.6 <C> 64.1 <C> 68.5 <R> <C> 6 <C> SpeechTrans <C> 41.5 <C> 39.5 <C> 40.5 <C> 43.1 <C> 38.7 <C> 40.8 <C> 42.4 <C> 39 <C> 40.6 <R> <C> 7 <C> Baseline <C> 32.2 <C> 45.8 <C> 37.8 <C> 32.8 <C> 39.1 <C> 35.7 <C> 32.5 <C> 41.9 <C> 36.6 <CAP> Table 4: Phrase-level evaluation for subtask A: 3-classes <COT> Looking at the "Test Data 1 Out Domain" column, we can see that Team 6 (SpeechTrans) has the highest precision, recall, and F1 scores.
<R> <C> Medical Device Term <C> Medical Device Score <C> Medical Robot Term <C> Medical Robot Score <C> Sports Rehab Machine Term <C> Sports Rehab Machine Score <R> <C> root <C> 0.8802 <C> stroke <C> 0.919 <C> kingdom <C> 0.907 <R> <C> mouse <C> 0.8633 <C> kingdom <C> 0.893 <C> stroke <C> 0.8495 <R> <C> kingdom <C> 0.8383 <C> vessel <C> 0.8651 <C> progressive <C> 0.8414 <R> <C> iron <C> 0.8381 <C> thread <C> 0.8385 <C> net <C> 0.8334 <R> <C> internal <C> 0.8043 <C> floating <C> 0.8045 <C> suspension <C> 0.8322 <R> <C> progressive <C> 0.7957 <C> strain <C> 0.8018 <C> induction <C> 0.8244 <R> <C> agent <C> 0.7875 <C> mouse <C> 0.7997 <C> thread <C> 0.8236 <R> <C> express <C> 0.7733 <C> progressive <C> 0.7983 <C> root <C> 0.8093 <R> <C> plasma <C> 0.7685 <C> die <C> 0.787 <C> transmission <C> 0.7871 <R> <C> net <C> 0.7678 <C> secondary <C> 0.786 <C> die <C> 0.7821 <R> <C> ⋮ <C> ⋮ <C> ⋮ <C> ⋮ <C> ⋮ <C> ⋮ <R> <C> argued <C> 0.1631 <C> corresponding <C> 0.1695 <C> corresponding <C> 0.196 <R> <C> richard <C> 0.1608 <C> corresponds <C> 0.1693 <C> told <C> 0.1958 <R> <C> authority <C> 0.1606 <C> feel <C> 0.167 <C> joseph <C> 0.1956 <R> <C> michael <C> 0.1569 <C> coating <C> 0.1666 <C> understanding <C> 0.1953 <R> <C> required <C> 0.154 <C> wife <C> 0.1603 <C> love <C> 0.1902 <R> <C> peter <C> 0.1388 <C> michael <C> 0.159 <C> economic <C> 0.1902 <R> <C> robert <C> 0.1381 <C> authority <C> 0.156 <C> coating <C> 0.1892 <R> <C> david <C> 0.1201 <C> peter <C> 0.1437 <C> pay <C> 0.1856 <R> <C> james <C> 0.1188 <C> david <C> 0.1412 <C> authority <C> 0.1838 <R> <C> charles <C> 0.1157 <C> required <C> 0.1382 <C> causing <C> 0.1822 <CAP> TABLE II: Ranked List of Target Words based on their Ambiguity Scores <COT> Looking at the "Medical Device Term" column, we can see that the term "root" has the highest score of 0.8802.
<R> <C> Target → System ↓ <italic>neural network models</italic> <C> <bold>MT</bold> <italic>neural network models</italic> <C> <bold>MT</bold> <italic>neural network models</italic> <C> <bold>OC</bold> <italic>neural network models</italic> <C> <bold>OC</bold> <italic>neural network models</italic> <C> <bold>PE</bold> <italic>neural network models</italic> <C> <bold>PE</bold> <italic>neural network models</italic> <C> <bold>VG</bold> <italic>neural network models</italic> <C> <bold>VG</bold> <italic>neural network models</italic> <C> <bold>WD</bold> <italic>neural network models</italic> <C> <bold>WD</bold> <italic>neural network models</italic> <C> <bold>WTP</bold> <italic>neural network models</italic> <C> <bold>WTP</bold> <italic>neural network models</italic> <C> <bold>Average</bold> <italic>neural network models</italic> <C> <bold>Average</bold> <italic>neural network models</italic> <R> <C> BiLSTM <C> 68.8 <C> 41.8 <C> 58.0 <C> 22.4 <C> 73.0 <C> 62.0 <C> 60.9 <C> 37.7 <C> 60.0 <C> 24.5 <C> 57.9 <C> 28.5 <C> 63.1 <C> 36.1 <R> <C> CNN:rand <C> 78.6 <C> 67.3 <C> <bold>60.5</bold> <C> <bold>25.6</bold> <C> <bold>73.6</bold> <C> 61.1 <C> <bold>65.9</bold> <C> <bold>45.0</bold> <C> 61.1 <C> 25.8 <C> 58.6 <C> 28.9 <C> 66.4 <C> 42.3 <R> <C> CNN:w2vec <C> 73.7 <C> 60.9 <C> 58.2 <C> 23.7 <C> 74.0 <C> 61.7 <C> 63.8 <C> 33.5 <C> 62.6 <C> <bold>28.9</bold> <C> 57.3 <C> 24.3 <C> 64.9 <C> 38.8 <R> <C> LSTM <C> 65.2 <C> 48.3 <C> 58.5 <C> 22.3 <C> 71.8 <C> 60.7 <C> 61.3 <C> 40.1 <C> 61.6 <C> 25.9 <C> 58.0 <C> 28.4 <C> 62.7 <C> 37.6 <R> <C> LR <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> [EMPTY] <C> [EMPTY] <R> <C> -Discourse <C> 73.0 <C> 60.8 <C> 59.9 <C> 22.9 <C> 70.6 <C> 60.6 <C> 62.5 <C> 42.6 <C> 63.7 <C> 23.2 <C> 59.7 <C> 30.2 <C> 64.9 <C> 40.0 <R> <C> -Embeddings <C> 74.6 <C> 62.9 <C> 59.6 <C> 22.6 <C> 70.4 <C> 60.4 <C> 62.9 <C> 43.1 <C> 63.9 <C> 23.5 <C> 59.4 <C> 29.9 <C> 65.1 <C> 40.4 <R> <C> -Lexical <C> 72.1 <C> 59.5 <C> 59.6 <C> 22.5 <C> 65.9 <C> 55.1 <C> 60.8 <C> 40.5 <C> 60.1 <C> 18.5 <C> 57.7 <C> 27.8 <C> 62.7 <C> 37.3 <R> <C> -Structure <C> 74.4 <C> 62.6 <C> 60.0 <C> 23.0 <C> 70.4 <C> 60.4 <C> 62.0 <C> 41.8 <C> 64.2 <C> 23.4 <C> 59.5 <C> 30.0 <C> 65.1 <C> 40.2 <R> <C> -Syntax <C> <bold>79.8</bold> <C> <bold>70.3</bold> <C> 59.8 <C> 22.9 <C> 72.1 <C> <bold>62.5</bold> <C> 63.4 <C> 43.8 <C> <bold>65.1</bold> <C> 25.5 <C> <bold>60.1</bold> <C> <bold>30.5</bold> <C> <bold>66.7</bold> <C> <bold>42.6</bold> <R> <C> All Features <C> 74.4 <C> 62.7 <C> 59.9 <C> 22.9 <C> 70.6 <C> 60.6 <C> 62.5 <C> 42.6 <C> 63.8 <C> 23.3 <C> 59.7 <C> 30.2 <C> 65.1 <C> 40.4 <R> <C> LR <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> [EMPTY] <C> [EMPTY] <R> <C> +Discourse <C> 70.0 <C> 56.7 <C> 49.4 <C> 13.8 <C> 50.1 <C> 41.7 <C> 49.6 <C> 30.6 <C> 57.6 <C> 14.9 <C> 49.5 <C> 18.4 <C> 54.4 <C> 29.3 <R> <C> +Embeddings <C> 72.4 <C> 59.8 <C> 58.8 <C> 20.8 <C> 68.2 <C> 57.7 <C> 59.7 <C> 39.3 <C> 64.2 <C> 23.8 <C> 59.0 <C> 28.9 <C> 63.7 <C> 38.4 <R> <C> +Lexical <C> 75.9 <C> 64.7 <C> 59.5 <C> 21.4 <C> 71.8 <C> 62.1 <C> 61.1 <C> 40.5 <C> 64.0 <C> 22.2 <C> 59.0 <C> 27.7 <C> 65.2 <C> 39.8 <R> <C> +Structure <C> 57.1 <C> 42.0 <C> 56.5 <C> 20.0 <C> 54.2 <C> 39.5 <C> 55.4 <C> 33.3 <C> 48.4 <C> 9.0 <C> 55.4 <C> 25.2 <C> 54.5 <C> 28.2 <R> <C> +Syntax <C> 66.7 <C> 52.5 <C> 58.1 <C> 21.0 <C> 64.1 <C> 52.9 <C> 60.7 <C> 40.4 <C> 57.6 <C> 15.5 <C> 57.0 <C> 27.0 <C> 60.7 <C> 34.9 <R> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <R> <C> Majority bsl <C> 42.9 <C> 0.0 <C> 48.0 <C> 0.0 <C> 41.3 <C> 0.0 <C> 44.5 <C> 0.0 <C> 48.6 <C> 0.0 <C> 46.7 <C> 0.0 <C> 45.3 <C> 0.0 <R> <C> Random bsl <C> 50.7 <C> 33.2 <C> 49.9 <C> 13.5 <C> 50.8 <C> 38.0 <C> 50.4 <C> 28.8 <C> 51.6 <C> 10.8 <C> 48.9 <C> 18.8 <C> 50.4 <C> 23.9 <CAP> Table 2: In-domain experiments, best values per column are highlighted. For each dataset (column head) we show two scores: Macro-F1 score (left-hand column) and F1 score for claims (right-hand column). <COT> Looking at the "Target → System" column, we can see that the "LR" system performs better than the "BiLSTM" system in terms of Macro-F1 score for all datasets except for "MT" and "OC".
<R> <C> Target → System ↓ <italic>neural network models</italic> <C> <bold>MT</bold> <italic>neural network models</italic> <C> <bold>MT</bold> <italic>neural network models</italic> <C> <bold>OC</bold> <italic>neural network models</italic> <C> <bold>OC</bold> <italic>neural network models</italic> <C> <bold>PE</bold> <italic>neural network models</italic> <C> <bold>PE</bold> <italic>neural network models</italic> <C> <bold>VG</bold> <italic>neural network models</italic> <C> <bold>VG</bold> <italic>neural network models</italic> <C> <bold>WD</bold> <italic>neural network models</italic> <C> <bold>WD</bold> <italic>neural network models</italic> <C> <bold>WTP</bold> <italic>neural network models</italic> <C> <bold>WTP</bold> <italic>neural network models</italic> <C> <bold>Average</bold> <italic>neural network models</italic> <C> <bold>Average</bold> <italic>neural network models</italic> <R> <C> BiLSTM <C> 68.8 <C> 41.8 <C> 58.0 <C> 22.4 <C> 73.0 <C> 62.0 <C> 60.9 <C> 37.7 <C> 60.0 <C> 24.5 <C> 57.9 <C> 28.5 <C> 63.1 <C> 36.1 <R> <C> CNN:rand <C> 78.6 <C> 67.3 <C> <bold>60.5</bold> <C> <bold>25.6</bold> <C> <bold>73.6</bold> <C> 61.1 <C> <bold>65.9</bold> <C> <bold>45.0</bold> <C> 61.1 <C> 25.8 <C> 58.6 <C> 28.9 <C> 66.4 <C> 42.3 <R> <C> CNN:w2vec <C> 73.7 <C> 60.9 <C> 58.2 <C> 23.7 <C> 74.0 <C> 61.7 <C> 63.8 <C> 33.5 <C> 62.6 <C> <bold>28.9</bold> <C> 57.3 <C> 24.3 <C> 64.9 <C> 38.8 <R> <C> LSTM <C> 65.2 <C> 48.3 <C> 58.5 <C> 22.3 <C> 71.8 <C> 60.7 <C> 61.3 <C> 40.1 <C> 61.6 <C> 25.9 <C> 58.0 <C> 28.4 <C> 62.7 <C> 37.6 <R> <C> LR <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> <italic>feature ablation and combination</italic> <C> [EMPTY] <C> [EMPTY] <R> <C> -Discourse <C> 73.0 <C> 60.8 <C> 59.9 <C> 22.9 <C> 70.6 <C> 60.6 <C> 62.5 <C> 42.6 <C> 63.7 <C> 23.2 <C> 59.7 <C> 30.2 <C> 64.9 <C> 40.0 <R> <C> -Embeddings <C> 74.6 <C> 62.9 <C> 59.6 <C> 22.6 <C> 70.4 <C> 60.4 <C> 62.9 <C> 43.1 <C> 63.9 <C> 23.5 <C> 59.4 <C> 29.9 <C> 65.1 <C> 40.4 <R> <C> -Lexical <C> 72.1 <C> 59.5 <C> 59.6 <C> 22.5 <C> 65.9 <C> 55.1 <C> 60.8 <C> 40.5 <C> 60.1 <C> 18.5 <C> 57.7 <C> 27.8 <C> 62.7 <C> 37.3 <R> <C> -Structure <C> 74.4 <C> 62.6 <C> 60.0 <C> 23.0 <C> 70.4 <C> 60.4 <C> 62.0 <C> 41.8 <C> 64.2 <C> 23.4 <C> 59.5 <C> 30.0 <C> 65.1 <C> 40.2 <R> <C> -Syntax <C> <bold>79.8</bold> <C> <bold>70.3</bold> <C> 59.8 <C> 22.9 <C> 72.1 <C> <bold>62.5</bold> <C> 63.4 <C> 43.8 <C> <bold>65.1</bold> <C> 25.5 <C> <bold>60.1</bold> <C> <bold>30.5</bold> <C> <bold>66.7</bold> <C> <bold>42.6</bold> <R> <C> All Features <C> 74.4 <C> 62.7 <C> 59.9 <C> 22.9 <C> 70.6 <C> 60.6 <C> 62.5 <C> 42.6 <C> 63.8 <C> 23.3 <C> 59.7 <C> 30.2 <C> 65.1 <C> 40.4 <R> <C> LR <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> <italic>single feature groups</italic> <C> [EMPTY] <C> [EMPTY] <R> <C> +Discourse <C> 70.0 <C> 56.7 <C> 49.4 <C> 13.8 <C> 50.1 <C> 41.7 <C> 49.6 <C> 30.6 <C> 57.6 <C> 14.9 <C> 49.5 <C> 18.4 <C> 54.4 <C> 29.3 <R> <C> +Embeddings <C> 72.4 <C> 59.8 <C> 58.8 <C> 20.8 <C> 68.2 <C> 57.7 <C> 59.7 <C> 39.3 <C> 64.2 <C> 23.8 <C> 59.0 <C> 28.9 <C> 63.7 <C> 38.4 <R> <C> +Lexical <C> 75.9 <C> 64.7 <C> 59.5 <C> 21.4 <C> 71.8 <C> 62.1 <C> 61.1 <C> 40.5 <C> 64.0 <C> 22.2 <C> 59.0 <C> 27.7 <C> 65.2 <C> 39.8 <R> <C> +Structure <C> 57.1 <C> 42.0 <C> 56.5 <C> 20.0 <C> 54.2 <C> 39.5 <C> 55.4 <C> 33.3 <C> 48.4 <C> 9.0 <C> 55.4 <C> 25.2 <C> 54.5 <C> 28.2 <R> <C> +Syntax <C> 66.7 <C> 52.5 <C> 58.1 <C> 21.0 <C> 64.1 <C> 52.9 <C> 60.7 <C> 40.4 <C> 57.6 <C> 15.5 <C> 57.0 <C> 27.0 <C> 60.7 <C> 34.9 <R> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <C> <italic>baselines</italic> <R> <C> Majority bsl <C> 42.9 <C> 0.0 <C> 48.0 <C> 0.0 <C> 41.3 <C> 0.0 <C> 44.5 <C> 0.0 <C> 48.6 <C> 0.0 <C> 46.7 <C> 0.0 <C> 45.3 <C> 0.0 <R> <C> Random bsl <C> 50.7 <C> 33.2 <C> 49.9 <C> 13.5 <C> 50.8 <C> 38.0 <C> 50.4 <C> 28.8 <C> 51.6 <C> 10.8 <C> 48.9 <C> 18.8 <C> 50.4 <C> 23.9 <CAP> Table 2: In-domain experiments, best values per column are highlighted. For each dataset (column head) we show two scores: Macro-F1 score (left-hand column) and F1 score for claims (right-hand column). <COT> Looking at the "Target → System" column, we can see that the "+Syntax" system performs better than the "-Syntax" system in terms of F1 score for claims for all datasets.
<R> <C> Target → System ↓ CNN:rand <C> <bold>MT</bold> 62.8 <C> <bold>MT</bold> 41.4 <C> <bold>OC</bold> <bold>57.8</bold> <C> <bold>OC</bold> <bold>22.4</bold> <C> <bold>PE</bold> <bold>59.7</bold> <C> <bold>PE</bold> 36.2 <C> <bold>VG</bold> <bold>58.6</bold> <C> <bold>VG</bold> 28.1 <C> <bold>WD</bold> <bold>54.2</bold> <C> <bold>WD</bold> <bold>14.1</bold> <C> <bold>WTP</bold> <bold>56.8</bold> <C> <bold>WTP</bold> 25.6 <C> <bold>Avg</bold> <bold>58.3</bold> <C> <bold>Avg</bold> 28.0 <R> <C> All features <C> <bold>64.7</bold> <C> <bold>49.5</bold> <C> 56.4 <C> 20.6 <C> 57.8 <C> <bold>45.8</bold> <C> 58.2 <C> <bold>36.4</bold> <C> 52.3 <C> 11.3 <C> 56.0 <C> <bold>26.0</bold> <C> 57.6 <C> <bold>31.6</bold> <R> <C> Majority bsl <C> 42.9 <C> 0.0 <C> 48.0 <C> 0.0 <C> 41.3 <C> 0.0 <C> 44.5 <C> 0.0 <C> 48.6 <C> 0.0 <C> 46.7 <C> 0.0 <C> 45.3 <C> 0.0 <R> <C> Random bsl <C> 47.5 <C> 30.6 <C> 50.5 <C> 14.0 <C> 51.0 <C> 38.4 <C> 51.0 <C> 29.3 <C> 49.3 <C> 9.3 <C> 50.3 <C> 20.2 <C> 49.9 <C> 23.6 <CAP> Table 4: Leave-one-domain-out experiments, best values per column are highlighted. For each test dataset (column head) we show two scores: Macro-F1 score (left-hand column) and F1 score for claims (right-hand column). <COT> Looking at the "Avg" row, the "MT" cell, and the "OC" cell, we can see that the F1 score for claims is higher for the "MT" system compared to the "OC" system.
<R> <C> Target → System ↓ CNN:rand <C> <bold>MT</bold> 62.8 <C> <bold>MT</bold> 41.4 <C> <bold>OC</bold> <bold>57.8</bold> <C> <bold>OC</bold> <bold>22.4</bold> <C> <bold>PE</bold> <bold>59.7</bold> <C> <bold>PE</bold> 36.2 <C> <bold>VG</bold> <bold>58.6</bold> <C> <bold>VG</bold> 28.1 <C> <bold>WD</bold> <bold>54.2</bold> <C> <bold>WD</bold> <bold>14.1</bold> <C> <bold>WTP</bold> <bold>56.8</bold> <C> <bold>WTP</bold> 25.6 <C> <bold>Avg</bold> <bold>58.3</bold> <C> <bold>Avg</bold> 28.0 <R> <C> All features <C> <bold>64.7</bold> <C> <bold>49.5</bold> <C> 56.4 <C> 20.6 <C> 57.8 <C> <bold>45.8</bold> <C> 58.2 <C> <bold>36.4</bold> <C> 52.3 <C> 11.3 <C> 56.0 <C> <bold>26.0</bold> <C> 57.6 <C> <bold>31.6</bold> <R> <C> Majority bsl <C> 42.9 <C> 0.0 <C> 48.0 <C> 0.0 <C> 41.3 <C> 0.0 <C> 44.5 <C> 0.0 <C> 48.6 <C> 0.0 <C> 46.7 <C> 0.0 <C> 45.3 <C> 0.0 <R> <C> Random bsl <C> 47.5 <C> 30.6 <C> 50.5 <C> 14.0 <C> 51.0 <C> 38.4 <C> 51.0 <C> 29.3 <C> 49.3 <C> 9.3 <C> 50.3 <C> 20.2 <C> 49.9 <C> 23.6 <CAP> Table 4: Leave-one-domain-out experiments, best values per column are highlighted. For each test dataset (column head) we show two scores: Macro-F1 score (left-hand column) and F1 score for claims (right-hand column). <COT> Looking at the "Majority bsl" row, the "CNN:rand" cell, and the "OC" cell, we can see that the F1 score for claims is higher for the "CNN:rand" system compared to the "OC" system.
<R> <C> [EMPTY] <C> 1 (strongly agree) <C> 2 (agree) <C> 3 (disagree) <C> 4 (strongly disagree) <R> <C> was able to “understand” my questions <C> 16.7% <C> <bold>50.0%</bold> <C> 33.3% <C> 00.0% <R> <C> was able to provide answers to my questions <C> 00.0% <C> <bold>50.0%</bold> <C> <bold>50.0%</bold> <C> 00.0% <R> <C> I was satisfied with the informativeness of the answers provided by <C> <bold>33.3%</bold> <C> <bold>33.3%</bold> <C> 00.0% <C> <bold>33.3%</bold> <R> <C> I was satisfied with the fluency of the answers provided by <C> 16.7% <C> <bold>50.0%</bold> <C> 16.7% <C> 16.7% <R> <C> could respond in a reasonable time <C> <bold>50.0%</bold> <C> 33.3% <C> 16.7% <C> 00.0% <R> <C> The GUI of was suitable for reading the provided answers <C> <bold>50.0%</bold> <C> 00.0% <C> 33.3% <C> 16.7% <R> <C> reduces my need to google a specific information <C> 16.7% <C> <bold>66.7%</bold> <C> 00.0% <C> 16.7% <R> <C> would help me save some time in my work <C> <bold>33.3%</bold> <C> <bold>33.3%</bold> <C> <bold>33.3%</bold> <C> 00.0% <R> <C> I would like to use in the future on a daily basis <C> 00.0% <C> <bold>66.7%</bold> <C> 00.0% <C> 33.3% <R> <C> I will use to plan for my next conference <C> 16.7% <C> 16.7% <C> <bold>33.3%</bold> <C> <bold>33.3%</bold> <CAP> Table 4: The output of the general survey. <COT> Looking at the "I was satisfied with the fluency of the answers provided by" row, the "50.0%" cell is bolded, indicating that 50.0% of the respondents strongly agree that the system was able to provide fluent answers.
<R> <C> [EMPTY] <C> 1 (strongly agree) <C> 2 (agree) <C> 3 (disagree) <C> 4 (strongly disagree) <R> <C> was able to “understand” my questions <C> 16.7% <C> <bold>50.0%</bold> <C> 33.3% <C> 00.0% <R> <C> was able to provide answers to my questions <C> 00.0% <C> <bold>50.0%</bold> <C> <bold>50.0%</bold> <C> 00.0% <R> <C> I was satisfied with the informativeness of the answers provided by <C> <bold>33.3%</bold> <C> <bold>33.3%</bold> <C> 00.0% <C> <bold>33.3%</bold> <R> <C> I was satisfied with the fluency of the answers provided by <C> 16.7% <C> <bold>50.0%</bold> <C> 16.7% <C> 16.7% <R> <C> could respond in a reasonable time <C> <bold>50.0%</bold> <C> 33.3% <C> 16.7% <C> 00.0% <R> <C> The GUI of was suitable for reading the provided answers <C> <bold>50.0%</bold> <C> 00.0% <C> 33.3% <C> 16.7% <R> <C> reduces my need to google a specific information <C> 16.7% <C> <bold>66.7%</bold> <C> 00.0% <C> 16.7% <R> <C> would help me save some time in my work <C> <bold>33.3%</bold> <C> <bold>33.3%</bold> <C> <bold>33.3%</bold> <C> 00.0% <R> <C> I would like to use in the future on a daily basis <C> 00.0% <C> <bold>66.7%</bold> <C> 00.0% <C> 33.3% <R> <C> I will use to plan for my next conference <C> 16.7% <C> 16.7% <C> <bold>33.3%</bold> <C> <bold>33.3%</bold> <CAP> Table 4: The output of the general survey. <COT> Looking at the "reduces my need to google a specific information" row, the "66.7%" cell is bolded, indicating that 66.7% of the respondents agree that the system reduces their need to google specific information.
<R> <C> <bold>Model</bold> <C> ∑ <C> <bold>InsuranceQA</bold> <C> <bold>Travel</bold> <C> <bold>Cooking</bold> <C> <bold>Academia</bold> <C> <bold>Apple</bold> <C> <bold>Aviation</bold> <C> <bold>WikiPassageQA</bold> <R> <C> <bold>Unsupervised IR Baselines</bold> <C> <bold>Unsupervised IR Baselines</bold> <C> <bold>Unsupervised IR Baselines</bold> <C> <bold>Unsupervised IR Baselines</bold> <C> <bold>Unsupervised IR Baselines</bold> <C> <bold>Unsupervised IR Baselines</bold> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BM25 <C> 30.3 <C> 24.9 <C> 38.1 <C> 30.9 <C> 29.2 <C> 21.8 <C> 37.0 <C> 53.00 / 61.71 <R> <C> TF*IDF <C> 32.4 <C> 18.7 <C> 39.9 <C> 35.1 <C> 32.2 <C> 26.7 <C> 41.9 <C> 39.92 / 46.38 <R> <C> <bold>Semantic Similarity Methods</bold> <C> <bold>Semantic Similarity Methods</bold> <C> <bold>Semantic Similarity Methods</bold> <C> <bold>Semantic Similarity Methods</bold> <C> <bold>Semantic Similarity Methods</bold> <C> <bold>Semantic Similarity Methods</bold> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> InferSent <C> 23.0 <C> 14.8 <C> 27.0 <C> 21.3 <C> 22.5 <C> 22.8 <C> 29.3 <C> 43.62 / 50.53 <R> <C> p-mean Embeddings <C> 25.7 <C> 17.0 <C> 32.1 <C> 29.3 <C> 24.3 <C> 19.6 <C> 31.7 <C> 42.82 / 50.44 <R> <C> CNN <C> 25.9 <C> 24.4 <C> 36.9 <C> 25.9 <C> 22.5 <C> 20.2 <C> 25.3 <C> 27.33 / 31.48 <R> <C> BiLSTM <C> 34.8 <C> 32.4 <C> 45.3 <C> 35.2 <C> 31.5 <C> 27.2 <C> 37.3 <C> 46.16 / 52.89 <R> <C> Att.-BiLSTM <C> 34.5 <C> 37.9 <C> 43.0 <C> 36.2 <C> 31.2 <C> 24.7 <C> 33.9 <C> 47.04 / 54.36 <R> <C> AP-BiLSTM <C> 31.3 <C> 31.9 <C> 38.8 <C> 32.2 <C> 27.3 <C> 22.9 <C> 34.5 <C> 46.98 / 55.20 <R> <C> LW-BiLSTM <C> 34.1 <C> 36.9 <C> 43.2 <C> 32.3 <C> 30.2 <C> 23.4 <C> 38.5 <C> 47.56 / 54.33 <R> <C> <bold>Relevance Matching Methods</bold> <C> <bold>Relevance Matching Methods</bold> <C> <bold>Relevance Matching Methods</bold> <C> <bold>Relevance Matching Methods</bold> <C> <bold>Relevance Matching Methods</bold> <C> <bold>Relevance Matching Methods</bold> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Bigrams <C> 18.3 <C> 19.4 <C> 19.3 <C> 16.7 <C> 19.8 <C> 13.0 <C> 21.5 <C> 39.84 / 47.55 <R> <C> CA-Wang <C> 39.1 <C> 37.0 <C> 46.5 <C> 39.4 <C> 36.1 <C> 29.2 <C> 46.5 <C> 48.71 / 56.11 <R> <C> COALA <C> 43.6 <C> 38.0 <C> 53.8 <C> 47.3 <C> 42.2 <C> 32.0 <C> 48.4 <C> <bold>60.58</bold> / <bold>69.40</bold> <R> <C> COALA p-means <C> <bold>45.2</bold> <C> <bold>39.9</bold> <C> 53.4 <C> 46.5 <C> <bold>44.2</bold> <C> <bold>34.5</bold> <C> <bold>52.9</bold> <C> 59.29 / 68.48 <R> <C> COALA syntax-aware <C> 44.3 <C> 39.5 <C> <bold>54.1</bold> <C> <bold>47.8</bold> <C> 43.5 <C> 32.7 <C> 48.3 <C> 60.48 / 68.75 <CAP> Table 2: Accuracies of the different models on the cQA datasets and MAP/MRR on WikiPassageQA. Σ denotes the average accuracy over all cQA datasets. <COT> Looking at the "Model" column, we can see that there are three categories of models: Unsupervised IR Baselines, Semantic Similarity Methods, and Relevance Matching Methods.
<R> <C> <bold>Model</bold> <C> ∑ <C> <bold>InsuranceQA</bold> <C> <bold>Travel</bold> <C> <bold>Cooking</bold> <C> <bold>Academia</bold> <C> <bold>Apple</bold> <C> <bold>Aviation</bold> <C> <bold>WikiPassageQA</bold> <R> <C> <bold>Unsupervised IR Baselines</bold> <C> <bold>Unsupervised IR Baselines</bold> <C> <bold>Unsupervised IR Baselines</bold> <C> <bold>Unsupervised IR Baselines</bold> <C> <bold>Unsupervised IR Baselines</bold> <C> <bold>Unsupervised IR Baselines</bold> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BM25 <C> 30.3 <C> 24.9 <C> 38.1 <C> 30.9 <C> 29.2 <C> 21.8 <C> 37.0 <C> 53.00 / 61.71 <R> <C> TF*IDF <C> 32.4 <C> 18.7 <C> 39.9 <C> 35.1 <C> 32.2 <C> 26.7 <C> 41.9 <C> 39.92 / 46.38 <R> <C> <bold>Semantic Similarity Methods</bold> <C> <bold>Semantic Similarity Methods</bold> <C> <bold>Semantic Similarity Methods</bold> <C> <bold>Semantic Similarity Methods</bold> <C> <bold>Semantic Similarity Methods</bold> <C> <bold>Semantic Similarity Methods</bold> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> InferSent <C> 23.0 <C> 14.8 <C> 27.0 <C> 21.3 <C> 22.5 <C> 22.8 <C> 29.3 <C> 43.62 / 50.53 <R> <C> p-mean Embeddings <C> 25.7 <C> 17.0 <C> 32.1 <C> 29.3 <C> 24.3 <C> 19.6 <C> 31.7 <C> 42.82 / 50.44 <R> <C> CNN <C> 25.9 <C> 24.4 <C> 36.9 <C> 25.9 <C> 22.5 <C> 20.2 <C> 25.3 <C> 27.33 / 31.48 <R> <C> BiLSTM <C> 34.8 <C> 32.4 <C> 45.3 <C> 35.2 <C> 31.5 <C> 27.2 <C> 37.3 <C> 46.16 / 52.89 <R> <C> Att.-BiLSTM <C> 34.5 <C> 37.9 <C> 43.0 <C> 36.2 <C> 31.2 <C> 24.7 <C> 33.9 <C> 47.04 / 54.36 <R> <C> AP-BiLSTM <C> 31.3 <C> 31.9 <C> 38.8 <C> 32.2 <C> 27.3 <C> 22.9 <C> 34.5 <C> 46.98 / 55.20 <R> <C> LW-BiLSTM <C> 34.1 <C> 36.9 <C> 43.2 <C> 32.3 <C> 30.2 <C> 23.4 <C> 38.5 <C> 47.56 / 54.33 <R> <C> <bold>Relevance Matching Methods</bold> <C> <bold>Relevance Matching Methods</bold> <C> <bold>Relevance Matching Methods</bold> <C> <bold>Relevance Matching Methods</bold> <C> <bold>Relevance Matching Methods</bold> <C> <bold>Relevance Matching Methods</bold> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Bigrams <C> 18.3 <C> 19.4 <C> 19.3 <C> 16.7 <C> 19.8 <C> 13.0 <C> 21.5 <C> 39.84 / 47.55 <R> <C> CA-Wang <C> 39.1 <C> 37.0 <C> 46.5 <C> 39.4 <C> 36.1 <C> 29.2 <C> 46.5 <C> 48.71 / 56.11 <R> <C> COALA <C> 43.6 <C> 38.0 <C> 53.8 <C> 47.3 <C> 42.2 <C> 32.0 <C> 48.4 <C> <bold>60.58</bold> / <bold>69.40</bold> <R> <C> COALA p-means <C> <bold>45.2</bold> <C> <bold>39.9</bold> <C> 53.4 <C> 46.5 <C> <bold>44.2</bold> <C> <bold>34.5</bold> <C> <bold>52.9</bold> <C> 59.29 / 68.48 <R> <C> COALA syntax-aware <C> 44.3 <C> 39.5 <C> <bold>54.1</bold> <C> <bold>47.8</bold> <C> 43.5 <C> 32.7 <C> 48.3 <C> 60.48 / 68.75 <CAP> Table 2: Accuracies of the different models on the cQA datasets and MAP/MRR on WikiPassageQA. Σ denotes the average accuracy over all cQA datasets. <COT> Looking at the "InsuranceQA" column, we can compare the accuracies of different models on the InsuranceQA dataset.
<R> <C> EN <C> Agent ≺ Cause/Instrument/Experiencer ≺ Pivot ≺ Theme ≺ Patient ≺ Material/Source/Asset ≺ Product ≺ Recipient/Beneficiary/Destination/Location ≺ Value/Stimulus/Topic/Result/Predicate/Goal/InitialLocation/Attribute/Extent <R> <C> DE <C> Agent ≺ Experiencer ≺ Stimulus/Pivot ≺ Cause ≺ Theme ≺ Patient ≺ Topic ≺ Instrument ≺ Beneficiary/InitialLocation ≺ Result ≺ Product/Goal ≺ Destination/Attribute ≺ Recipient ≺ Value/Time/CoAgent/Locus/Manner/Source/Trajectory/Location/Duration/Path/Extent <CAP> Table 4: Induced hierarchies <COT> Looking at the column headers in the table, we can see that the table represents the induced hierarchies for the English (EN) and German (DE) languages.
<R> <C> EN <C> Agent ≺ Cause/Instrument/Experiencer ≺ Pivot ≺ Theme ≺ Patient ≺ Material/Source/Asset ≺ Product ≺ Recipient/Beneficiary/Destination/Location ≺ Value/Stimulus/Topic/Result/Predicate/Goal/InitialLocation/Attribute/Extent <R> <C> DE <C> Agent ≺ Experiencer ≺ Stimulus/Pivot ≺ Cause ≺ Theme ≺ Patient ≺ Topic ≺ Instrument ≺ Beneficiary/InitialLocation ≺ Result ≺ Product/Goal ≺ Destination/Attribute ≺ Recipient ≺ Value/Time/CoAgent/Locus/Manner/Source/Trajectory/Location/Duration/Path/Extent <CAP> Table 4: Induced hierarchies <COT> Looking at the "EN" column, we can see that the hierarchy starts with the "Agent" and ends with "Value/Stimulus/Topic/Result/Predicate/Goal/InitialLocation/Attribute/Extent". Similarly, in the "DE" column, the hierarchy starts with "Agent" and ends with "Value/Time/CoAgent/Locus/Manner/Source/Trajectory/Location/Duration/Path/Extent".
<R> <C> [EMPTY] <C> EN-test <C> DE-test <R> <C> UB <C> .932 <C> .920 <R> <C> EN-train <C> .930 <C> .787 <R> <C> DE-train <C> .852 <C> .790 <R> <C> RND <C> .500 <C> .456 <CAP> Table 5: Cross-lingual evaluation, global ranker <COT> Looking at the "EN-train" row, ".930" cell, and "DE-train" row, ".852" cell, we can see that the performance of the global ranker is higher on the English training data compared to the German training data.
<R> <C> [EMPTY] <C> EN-test <C> DE-test <R> <C> UB <C> .932 <C> .920 <R> <C> EN-train <C> .930 <C> .787 <R> <C> DE-train <C> .852 <C> .790 <R> <C> RND <C> .500 <C> .456 <CAP> Table 5: Cross-lingual evaluation, global ranker <COT> Looking at the "UB" column, ".932" cell, and the "RND" column, ".500" cell, we can observe that the performance of the global ranker is higher on the English test data compared to the random baseline.
<R> <C> <bold>Test</bold> <C> <bold>Metric</bold> <C> <bold>tf-idf</bold> <C> <bold>bm25</bold> <C> <bold>emb</bold> <C> <bold>feat</bold> <C> <bold>auto/ cosine</bold> <C> <bold>auto- rank</bold> <C> <bold>auto- rank + bm25</bold> <C> <bold>auto- rank + feat</bold> <R> <C> EGFR <C> MAP <C> 0.289 <C> 0.632 <C> 0.310 <C> 0.575 <C> 0.054 <C> 0.545 <C> 0.588 <C> <bold>0.699</bold> <R> <C> EGFR <C> nDCG <C> 0.424 <C> 0.728 <C> 0.460 <C> 0.695 <C> 0.129 <C> 0.653 <C> 0.716 <C> <bold>0.810</bold> <R> <C> KRAS <C> MAP <C> 0.327 <C> 0.610 <C> 0.466 <C> 0.609 <C> 0.058 <C> 0.575 <C> 0.774 <C> <bold>0.820</bold> <R> <C> KRAS <C> nDCG <C> 0.456 <C> 0.723 <C> 0.592 <C> 0.712 <C> 0.145 <C> 0.688 <C> 0.867 <C> <bold>0.914</bold> <R> <C> BRAF <C> MAP <C> 0.342 <C> 0.656 <C> 0.427 <C> 0.704 <C> 0.063 <C> 0.563 <C> 0.702 <C> <bold>0.812</bold> <R> <C> BRAF <C> nDCG <C> 0.480 <C> 0.751 <C> 0.572 <C> 0.802 <C> 0.163. <C> 0.671 <C> 0.820 <C> <bold>0.901</bold> <R> <C> PIK3CA <C> MAP <C> 0.341 <C> 0.633 <C> 0.486 <C> 0.625 <C> 0.079 <C> 0.541 <C> 0.779 <C> <bold>0.810</bold> <R> <C> PIK3CA <C> nDCG <C> 0.473 <C> 0.729 <C> 0.617 <C> 0.718 <C> 0.171 <C> 0.656 <C> 0.859 <C> <bold>0.895</bold> <CAP> Table 3: Test Scores Document Ranking <COT> Looking at the "tf-idf" column, we can see that the scores for each gene are lower compared to the other metrics.
<R> <C> <bold>Med</bold> <C> BiLSTM <C> EG 71.60 <C> EE 80.20 <C> HG 69.28 <C> DC 65.32 <R> <C> <bold>Med</bold> <C> UB <C> 85.61 <C> 90.25 <C> 86.37 <C> 85.58 <R> <C> <bold>TEd</bold> <C> BiLSTM <C> 78.53 <C> 78.87 <C> 57.16 <C> 61.77 <R> <C> <bold>TEd</bold> <C> UB <C> 93.29 <C> 90.71 <C> 81.77 <C> 82.11 <CAP> Table 1: Individual macro-F1 scores following Schulz et al. (2019a) for each of the epistemic activities. The BiLSTM uses FastText embeddings Bojanowski et al. (2017). This architecture is equal to Flair when only using FastText embeddings. UB reports the human upper bound (inter-annotator agreement) indicating room for improvement. <COT> Looking at the "Med" column, the "BiLSTM" row, and the "EE 80.20" cell, we can see that the BiLSTM model achieves a macro-F1 score of 80.20 on the "Med" epistemic activity.
<R> <C> <bold>Med</bold> <C> BiLSTM <C> EG 71.60 <C> EE 80.20 <C> HG 69.28 <C> DC 65.32 <R> <C> <bold>Med</bold> <C> UB <C> 85.61 <C> 90.25 <C> 86.37 <C> 85.58 <R> <C> <bold>TEd</bold> <C> BiLSTM <C> 78.53 <C> 78.87 <C> 57.16 <C> 61.77 <R> <C> <bold>TEd</bold> <C> UB <C> 93.29 <C> 90.71 <C> 81.77 <C> 82.11 <CAP> Table 1: Individual macro-F1 scores following Schulz et al. (2019a) for each of the epistemic activities. The BiLSTM uses FastText embeddings Bojanowski et al. (2017). This architecture is equal to Flair when only using FastText embeddings. UB reports the human upper bound (inter-annotator agreement) indicating room for improvement. <COT> Looking at the "TEd" column, the "UB" row, and the "93.29" cell, we can see that the UB (human upper bound) macro-F1 score for the "TEd" epistemic activity is 93.29.
<R> <C> <bold>Features</bold> <C> <bold>Model</bold> <C> <bold>Development</bold> P <C> <bold>Development</bold> R <C> <bold>Development</bold> F <R> <C> text <C> BERT <C> 0.69 <C> 0.55 <C> 0.61 <R> <C> <bold>text</bold> <C> <bold>BERT*</bold> <C> <bold>0.57</bold> <C> <bold>0.79</bold> <C> <bold>0.66</bold> <R> <C> context <C> BERT <C> 0.70 <C> 0.53 <C> 0.60 <R> <C> context <C> BERT* <C> 0.63 <C> 0.67 <C> 0.65 <R> <C> BERT logits + handcrafted** <C> LR <C> 0.70 <C> 0.56 <C> 0.61 <R> <C> BERT logits + handcrafted** <C> LR* <C> 0.60 <C> 0.71 <C> 0.65 <R> <C> BERT logits + tagged spans <C> LR <C> 0.70 <C> 0.53 <C> 0.60 <R> <C> BERT logits + tagged spans <C> LR* <C> 0.61 <C> 0.71 <C> 0.66 <R> <C> BERT logits + all <C> LR <C> 0.71 <C> 0.52 <C> 0.60 <R> <C> BERT logits + all <C> LR* <C> 0.61 <C> 0.71 <C> 0.66 <CAP> Table 2: SLC experiments on different feature sets <COT> Looking at the table, we can see that there are two different models used: BERT and LR. 
<R> <C> <bold>Features</bold> <C> <bold>Model</bold> <C> <bold>Development</bold> P <C> <bold>Development</bold> R <C> <bold>Development</bold> F <R> <C> text <C> BERT <C> 0.69 <C> 0.55 <C> 0.61 <R> <C> <bold>text</bold> <C> <bold>BERT*</bold> <C> <bold>0.57</bold> <C> <bold>0.79</bold> <C> <bold>0.66</bold> <R> <C> context <C> BERT <C> 0.70 <C> 0.53 <C> 0.60 <R> <C> context <C> BERT* <C> 0.63 <C> 0.67 <C> 0.65 <R> <C> BERT logits + handcrafted** <C> LR <C> 0.70 <C> 0.56 <C> 0.61 <R> <C> BERT logits + handcrafted** <C> LR* <C> 0.60 <C> 0.71 <C> 0.65 <R> <C> BERT logits + tagged spans <C> LR <C> 0.70 <C> 0.53 <C> 0.60 <R> <C> BERT logits + tagged spans <C> LR* <C> 0.61 <C> 0.71 <C> 0.66 <R> <C> BERT logits + all <C> LR <C> 0.71 <C> 0.52 <C> 0.60 <R> <C> BERT logits + all <C> LR* <C> 0.61 <C> 0.71 <C> 0.66 <CAP> Table 2: SLC experiments on different feature sets <COT> Looking at the "text" feature column, we can see that the F-score for BERT is higher than the F-score for BERT*.
<R> <C> <bold>Propaganda</bold> <bold>Technique</bold> <C> <bold>Development</bold> <bold>P</bold> <C> <bold>Development</bold> <bold>R</bold> <C> <bold>Development</bold> <bold>F</bold> <C> <bold>Test</bold> <bold>F</bold> <R> <C> Appeal to Authority <C> 0 <C> 0 <C> 0 <C> 0.212 <R> <C> Appeal to Fear/Prejudice <C> 0.285 <C> 0.006 <C> 0.011 <C> 0 <R> <C> Bandwagon <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Black-and-White Fallacy <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Causal Oversimplification <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Doubt <C> 0.007 <C> 0.001 <C> 0.002 <C> 0 <R> <C> Exaggeration,Minimisation <C> 0.833 <C> 0.085 <C> 0.154 <C> 0 <R> <C> Flag-Waving <C> 0.534 <C> 0.102 <C> 0.171 <C> 0.195 <R> <C> Loaded Language <C> 0.471 <C> 0.160 <C> 0.237 <C> 0.130 <R> <C> Name Calling,Labeling <C> 0.270 <C> 0.112 <C> 0.158 <C> 0.150 <R> <C> O,IV,C <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Red Herring <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Reductio ad hitlerum <C> 0.318 <C> 0.069 <C> 0.113 <C> 0 <R> <C> Repetition <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Slogans <C> 0.221 <C> 0.034 <C> 0.059 <C> 0.003 <R> <C> Straw Men <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Thought-terminating Cliches <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Whataboutism <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> <bold>Overall</bold> <C> 0.365 <C> 0.073 <C> 0.122 <C> 0.131∗ <CAP> Table 5: Precision, recall and F1 scores of the FLC task on the development and test sets of the shared task. <COT> Looking at the "Overall" row in Table 5, the F1 score for the Propaganda Technique task on the test set is 0.131∗.
<R> <C> <bold>Propaganda</bold> <bold>Technique</bold> <C> <bold>Development</bold> <bold>P</bold> <C> <bold>Development</bold> <bold>R</bold> <C> <bold>Development</bold> <bold>F</bold> <C> <bold>Test</bold> <bold>F</bold> <R> <C> Appeal to Authority <C> 0 <C> 0 <C> 0 <C> 0.212 <R> <C> Appeal to Fear/Prejudice <C> 0.285 <C> 0.006 <C> 0.011 <C> 0 <R> <C> Bandwagon <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Black-and-White Fallacy <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Causal Oversimplification <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Doubt <C> 0.007 <C> 0.001 <C> 0.002 <C> 0 <R> <C> Exaggeration,Minimisation <C> 0.833 <C> 0.085 <C> 0.154 <C> 0 <R> <C> Flag-Waving <C> 0.534 <C> 0.102 <C> 0.171 <C> 0.195 <R> <C> Loaded Language <C> 0.471 <C> 0.160 <C> 0.237 <C> 0.130 <R> <C> Name Calling,Labeling <C> 0.270 <C> 0.112 <C> 0.158 <C> 0.150 <R> <C> O,IV,C <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Red Herring <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Reductio ad hitlerum <C> 0.318 <C> 0.069 <C> 0.113 <C> 0 <R> <C> Repetition <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Slogans <C> 0.221 <C> 0.034 <C> 0.059 <C> 0.003 <R> <C> Straw Men <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Thought-terminating Cliches <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Whataboutism <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> <bold>Overall</bold> <C> 0.365 <C> 0.073 <C> 0.122 <C> 0.131∗ <CAP> Table 5: Precision, recall and F1 scores of the FLC task on the development and test sets of the shared task. <COT> Looking at the "Exaggeration, Minimisation" row in Table 5, the precision, recall, and F1 score for this technique on the development set are 0.833, 0.085, and 0.154 respectively.
<R> <C> <bold>Metrics</bold> <C> <bold>rocchio</bold> <C> <bold>relevance model</bold> <C> <bold>auto-ref</bold> <C> <bold>rocchio + relevance</bold> <C> <bold>auto-ref + rocchio + relevance</bold> <R> <C> nDCG@10 <C> 0.232 <C> 0.274 <C> 0.195 <C> 0.341 <C> <bold>0.464</bold> <R> <C> nDCG@100 <C> 0.360 <C> 0.397 <C> 0.329 <C> 0.439 <C> <bold>0.536</bold> <R> <C> MAP <C> 0.182 <C> 0.223 <C> 0.156 <C> 0.270 <C> <bold>0.386</bold> <CAP> Table 4: Ranked Entity Scores for KRAS Validation and PIK3CA Testing <COT> Looking at the "Metrics" column, we can see that the highest value for each metric is marked in bold.
<R> <C> <bold>Metrics</bold> <C> <bold>rocchio</bold> <C> <bold>relevance model</bold> <C> <bold>auto-ref</bold> <C> <bold>rocchio + relevance</bold> <C> <bold>auto-ref + rocchio + relevance</bold> <R> <C> nDCG@10 <C> 0.232 <C> 0.274 <C> 0.195 <C> 0.341 <C> <bold>0.464</bold> <R> <C> nDCG@100 <C> 0.360 <C> 0.397 <C> 0.329 <C> 0.439 <C> <bold>0.536</bold> <R> <C> MAP <C> 0.182 <C> 0.223 <C> 0.156 <C> 0.270 <C> <bold>0.386</bold> <CAP> Table 4: Ranked Entity Scores for KRAS Validation and PIK3CA Testing <COT> Comparing the values in the "Metrics" column, we can observe that the "rocchio + relevance" method consistently outperforms the "auto-ref" method.
<R> <C> [EMPTY] <C> #Lang <C> #Task-Type <C> Web <C> Offline <C> Static <C> Models <C> Layers <C> Epochs <R> <C> Faruqui and Dyer (<ref id='bib-bib7'>2014a</ref>) <C> 4 <C> 10-WST <C> × <C> × <C> × <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Nayak et al. (<ref id='bib-bib14'>2016</ref>) <C> 1 <C> 7-DT <C> × <C> × <C> × <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Köhn (<ref id='bib-bib10'>2015</ref>) <C> 7 <C> 7-PT <C> [EMPTY] <C> × <C> × <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Ours <C> 28 <C> 16-PT <C> × <C> × <C> × <C> × <C> × <C> × <CAP> Table 1: Features of previous evaluation applications compared to Ours (Linspector Web). #Lang: Number of supported languages, #Task-Type: Number and type of the tasks, where WST: Word similarity tasks, DT: Downstream Tasks, PT: Probing Tasks. Static: Static word embeddings and Models: Pretrained downstream models. <COT> Looking at the "Table 1" caption, we can see that the table compares the features of previous evaluation applications to "Ours (Linspector Web)".
<R> <C> [EMPTY] <C> #Lang <C> #Task-Type <C> Web <C> Offline <C> Static <C> Models <C> Layers <C> Epochs <R> <C> Faruqui and Dyer (<ref id='bib-bib7'>2014a</ref>) <C> 4 <C> 10-WST <C> × <C> × <C> × <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Nayak et al. (<ref id='bib-bib14'>2016</ref>) <C> 1 <C> 7-DT <C> × <C> × <C> × <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Köhn (<ref id='bib-bib10'>2015</ref>) <C> 7 <C> 7-PT <C> [EMPTY] <C> × <C> × <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Ours <C> 28 <C> 16-PT <C> × <C> × <C> × <C> × <C> × <C> × <CAP> Table 1: Features of previous evaluation applications compared to Ours (Linspector Web). #Lang: Number of supported languages, #Task-Type: Number and type of the tasks, where WST: Word similarity tasks, DT: Downstream Tasks, PT: Probing Tasks. Static: Static word embeddings and Models: Pretrained downstream models. <COT> Looking at the "Ours" row, we can see that the number of supported languages is 28 and the number of layers is 16-PT.
<R> <C> Model <C> <bold>Original data</bold> <italic>ρ</italic> <C> <bold>Original data</bold> rmse <C> <bold>Original data</bold> qw<italic>κ</italic> <C> <bold>New data</bold> <italic>ρ</italic> <C> <bold>New data</bold> rmse <C> <bold>New data</bold> qw<italic>κ</italic> <R> <C> SVM (original) <C> .50 <C> .23 <C> .44 <C> – <C> – <C> – <R> <C> SVM (reproduced) <C> .49 <C> .24 <C> .47 <C> .50 <C> .21 <C> .39 <R> <C> MLP <C> .42 <C> .25 <C> .31 <C> .41 <C> .22 <C> .25 <R> <C> BiLSTM <C> .49 <C> .24 <C> .35 <C> .39 <C> .24 <C> .27 <CAP> Table 1: Results of the difficulty prediction approaches. SVM (original) has been taken from Beinborn (2016) <COT> Looking at the "Model" column, we can see that there are three different models: SVM (original), SVM (reproduced), MLP, and BiLSTM.
<R> <C> Model <C> <bold>Original data</bold> <italic>ρ</italic> <C> <bold>Original data</bold> rmse <C> <bold>Original data</bold> qw<italic>κ</italic> <C> <bold>New data</bold> <italic>ρ</italic> <C> <bold>New data</bold> rmse <C> <bold>New data</bold> qw<italic>κ</italic> <R> <C> SVM (original) <C> .50 <C> .23 <C> .44 <C> – <C> – <C> – <R> <C> SVM (reproduced) <C> .49 <C> .24 <C> .47 <C> .50 <C> .21 <C> .39 <R> <C> MLP <C> .42 <C> .25 <C> .31 <C> .41 <C> .22 <C> .25 <R> <C> BiLSTM <C> .49 <C> .24 <C> .35 <C> .39 <C> .24 <C> .27 <CAP> Table 1: Results of the difficulty prediction approaches. SVM (original) has been taken from Beinborn (2016) <COT> Looking at the "New data" column, we can see that there are values for the ρ, rmse, and qwκ metrics.
<R> <C> Strategy <C> Brown <C> Reuters <C> Gutenberg <R> <C> SEL <C> .11 <C> .12 <C> .10 <R> <C> SIZE <C> .13 <C> .15 <C> .12 <CAP> Table 2: rmse for both strategies on each corpora with randomly sampled target difficulties τ <COT> Looking at the "Strategy" column, we can see that there are two different strategies: SEL and SIZE.
<R> <C> Strategy <C> Brown <C> Reuters <C> Gutenberg <R> <C> SEL <C> .11 <C> .12 <C> .10 <R> <C> SIZE <C> .13 <C> .15 <C> .12 <CAP> Table 2: rmse for both strategies on each corpora with randomly sampled target difficulties τ <COT> Looking at the "Brown" column, we can see that the SEL strategy has an rmse of .11 on the Brown corpus.
<R> <C> [EMPTY] <C> easy (dec) SEL <C> easy (dec) SIZE <C> default DEF <C> hard (inc) SEL <C> hard (inc) SIZE <R> <C> <italic>T</italic>1 <C> – <C> – <C> .30 <C> – <C> – <R> <C> <italic>T</italic>2 <C> .17∗ <C> .11∗ <C> .34 <C> .66∗ <C> .44∗ <R> <C> <italic>T</italic>3 <C> .16∗ <C> .10∗ <C> .27 <C> .52∗ <C> .43∗ <R> <C> <italic>T</italic>4 <C> .28 <C> .09∗ <C> .30 <C> .43∗ <C> .45∗ <R> <C> Average <C> .20∗ <C> .10∗ <C> .30 <C> .53∗ <C> .44∗ <CAP> Table 3: Mean error rates e(T) per text and strategy. Results marked with ∗ deviate significantly from DEF <COT> Looking at the table, we can see that the mean error rates for the "easy (dec) SEL" and "easy (dec) SIZE" strategies are lower than the mean error rate for the "default DEF" strategy.
<R> <C> [EMPTY] <C> easy (dec) SEL <C> easy (dec) SIZE <C> default DEF <C> hard (inc) SEL <C> hard (inc) SIZE <R> <C> <italic>T</italic>1 <C> – <C> – <C> .30 <C> – <C> – <R> <C> <italic>T</italic>2 <C> .17∗ <C> .11∗ <C> .34 <C> .66∗ <C> .44∗ <R> <C> <italic>T</italic>3 <C> .16∗ <C> .10∗ <C> .27 <C> .52∗ <C> .43∗ <R> <C> <italic>T</italic>4 <C> .28 <C> .09∗ <C> .30 <C> .43∗ <C> .45∗ <R> <C> Average <C> .20∗ <C> .10∗ <C> .30 <C> .53∗ <C> .44∗ <CAP> Table 3: Mean error rates e(T) per text and strategy. Results marked with ∗ deviate significantly from DEF <COT> Looking at the table, we can see that the mean error rates for the "hard (inc) SEL" and "hard (inc) SIZE" strategies are higher than the mean error rate for the "default DEF" strategy.
<R> <C> [EMPTY] <C> <bold>model</bold> <C> <bold>with lexicon</bold> <bold>acc</bold> <C> <bold>with lexicon</bold> <bold>acc_amb</bold> <C> <bold>with lexicon</bold> <bold>F1-m</bold> <C> <bold>with lexicon</bold> <bold>F1-m_amb</bold> 28.452756pt <C> <bold>without lexicon</bold> <bold>acc</bold> <C> <bold>without lexicon</bold> <bold>acc_amb</bold> <C> <bold>without lexicon</bold> <bold>F1-m</bold> <C> <bold>without lexicon</bold> <bold>F1-m_amb</bold> <R> <C> FrameNet <C> Data Baseline <C> 79.06 <C> 69.73 <C> 33.00 <C> 37.42 <C> 79.06 <C> 69.73 <C> 33.00 <C> 37.42 <R> <C> FrameNet <C> Lexicon Baseline <C> 79.89 <C> 55.52 <C> 65.61 <C> 30.95 <C> – <C> – <C> – <C> – <R> <C> FrameNet <C> <bold>Data-Lexicon Baseline</bold> <C> 86.32 <C> 69.73 <C> 64.54 <C> 37.42 <C> – <C> – <C> – <C> – <R> <C> FrameNet <C> hermann2014semantic <C> 88.41 <C> 73.10 <C> – <C> – 28.452756pt <C> – <C> – <C> – <C> – <R> <C> FrameNet <C> Hartmann2017OOD <C> 87.63 <C> 73.80 <C> – <C> – 28.452756pt <C> 77.49 <C> – <C> – <C> – <R> <C> FrameNet <C> <bold>our</bold>_uni <C> 88.66 <C> 74.92 <C> 76.65 <C> 53.86 <C> 79.96 <C> 71.70 <C> 57.07 <C> 47.40 <R> <C> FrameNet <C> <bold>our</bold>_mm (im, synsV) <C> 88.82 <C> 75.28 <C> 76.77 <C> 54.80 <C> 81.21 <C> 72.51 <C> 57.81 <C> 49.38 <R> <C> SALSA <C> Data Baseline <C> 77.00 <C> 70.51 <C> 37.40 <C> 28.87 <C> 77.00 <C> 70.51 <C> 37.40 <C> 28.87 <R> <C> SALSA <C> Lexicon Baseline <C> 61.57 <C> 52.5 <C> 19.36 <C> 15.68 <C> – <C> – <C> – <C> – <R> <C> SALSA <C> <bold>Data-Lexicon Baseline</bold> <C> 77.16 <C> 70.51 <C> 38.48 <C> 28.87 <C> – <C> – <C> – <C> – <R> <C> SALSA <C> <bold>our</bold>_uni <C> 80.76 <C> 75.59 <C> 48.42 <C> 41.38 <C> 80.59 <C> 75.52 <C> 47.64 <C> 41.17 <R> <C> SALSA <C> <bold>our</bold>_mm (im) <C> 80.71 <C> 75.58 <C> 48.29 <C> 41.19 <C> 80.51 <C> 75.51 <C> 47.36 <C> 40.93 <CAP> Table 3: FrameId results (in %) on English (upper) and German (lower) with and without using the lexicon. Reported are accuracy and F1-macro, both also for ambiguous predicates (mean scores over ten runs). Models: (a) Data, Lexicon, and Data-Lexicon Baselines. (b) Previous models for English. (c) Ours: unimodal our-uni, multimodal on top of our-uni – our-mm – with Imagined embeddings (and synset visual embeddings for English). Best results highlighted in bold. The best run’s results for English were: our_uni: acc: 89.35 ; acc_amb: 76.45 ; F1-m: 76.95 ; F1-m_amb: 54.02 (with lexicon) our_mm (im, synsV): acc: 89.09 ; acc_amb: 75.86 ; F1-m: 78.17 ; F1-m_amb: 57.48 (with lexicon) <COT> Looking at the "FrameNet" row and the "Data-Lexicon Baseline" cell, we can see that the accuracy with lexicon for FrameNet is 86.32.
<R> <C> [EMPTY] <C> <bold>model</bold> <C> <bold>with lexicon</bold> <bold>acc</bold> <C> <bold>with lexicon</bold> <bold>acc_amb</bold> <C> <bold>with lexicon</bold> <bold>F1-m</bold> <C> <bold>with lexicon</bold> <bold>F1-m_amb</bold> 28.452756pt <C> <bold>without lexicon</bold> <bold>acc</bold> <C> <bold>without lexicon</bold> <bold>acc_amb</bold> <C> <bold>without lexicon</bold> <bold>F1-m</bold> <C> <bold>without lexicon</bold> <bold>F1-m_amb</bold> <R> <C> FrameNet <C> Data Baseline <C> 79.06 <C> 69.73 <C> 33.00 <C> 37.42 <C> 79.06 <C> 69.73 <C> 33.00 <C> 37.42 <R> <C> FrameNet <C> Lexicon Baseline <C> 79.89 <C> 55.52 <C> 65.61 <C> 30.95 <C> – <C> – <C> – <C> – <R> <C> FrameNet <C> <bold>Data-Lexicon Baseline</bold> <C> 86.32 <C> 69.73 <C> 64.54 <C> 37.42 <C> – <C> – <C> – <C> – <R> <C> FrameNet <C> hermann2014semantic <C> 88.41 <C> 73.10 <C> – <C> – 28.452756pt <C> – <C> – <C> – <C> – <R> <C> FrameNet <C> Hartmann2017OOD <C> 87.63 <C> 73.80 <C> – <C> – 28.452756pt <C> 77.49 <C> – <C> – <C> – <R> <C> FrameNet <C> <bold>our</bold>_uni <C> 88.66 <C> 74.92 <C> 76.65 <C> 53.86 <C> 79.96 <C> 71.70 <C> 57.07 <C> 47.40 <R> <C> FrameNet <C> <bold>our</bold>_mm (im, synsV) <C> 88.82 <C> 75.28 <C> 76.77 <C> 54.80 <C> 81.21 <C> 72.51 <C> 57.81 <C> 49.38 <R> <C> SALSA <C> Data Baseline <C> 77.00 <C> 70.51 <C> 37.40 <C> 28.87 <C> 77.00 <C> 70.51 <C> 37.40 <C> 28.87 <R> <C> SALSA <C> Lexicon Baseline <C> 61.57 <C> 52.5 <C> 19.36 <C> 15.68 <C> – <C> – <C> – <C> – <R> <C> SALSA <C> <bold>Data-Lexicon Baseline</bold> <C> 77.16 <C> 70.51 <C> 38.48 <C> 28.87 <C> – <C> – <C> – <C> – <R> <C> SALSA <C> <bold>our</bold>_uni <C> 80.76 <C> 75.59 <C> 48.42 <C> 41.38 <C> 80.59 <C> 75.52 <C> 47.64 <C> 41.17 <R> <C> SALSA <C> <bold>our</bold>_mm (im) <C> 80.71 <C> 75.58 <C> 48.29 <C> 41.19 <C> 80.51 <C> 75.51 <C> 47.36 <C> 40.93 <CAP> Table 3: FrameId results (in %) on English (upper) and German (lower) with and without using the lexicon. Reported are accuracy and F1-macro, both also for ambiguous predicates (mean scores over ten runs). Models: (a) Data, Lexicon, and Data-Lexicon Baselines. (b) Previous models for English. (c) Ours: unimodal our-uni, multimodal on top of our-uni – our-mm – with Imagined embeddings (and synset visual embeddings for English). Best results highlighted in bold. The best run’s results for English were: our_uni: acc: 89.35 ; acc_amb: 76.45 ; F1-m: 76.95 ; F1-m_amb: 54.02 (with lexicon) our_mm (im, synsV): acc: 89.09 ; acc_amb: 75.86 ; F1-m: 78.17 ; F1-m_amb: 57.48 (with lexicon) <COT> Looking at the "SALSA" row and the "our_uni" cell, we can see that the F1-macro without lexicon for SALSA is 47.40.
<R> <C> Method <C> <bold>BLEU-1</bold> <C> <bold>BLEU-2</bold> <C> <bold>BLEU-3</bold> <C> <bold>BLEU-4</bold> <C> <bold>METEOR</bold> <C> <bold>ROUGE</bold> <C> <bold>CIDEr</bold> <R> <C> Soft-Attention (xu2015show) <C> 70.7 <C> 49.2 <C> 34.4 <C> 24.3 <C> 23.9 <C> - <C> - <R> <C> CNN+Att (aneja2018convolutional) <C> 71.1 <C> 53.8 <C> 39.4 <C> 28.7 <C> 24.4 <C> 52.2 <C> 91.2 <R> <C> SCST (rennie2017self) <C> 77.4 <C> 60.9 <C> 46.0 <C> 34.1 <C> 26.7 <C> 55.7 <C> 114.0 <R> <C> Adaptive (lu2017knowing) <C> 74.2 <C> 58.0 <C> 43.9 <C> 33.2 <C> 26.6 <C> 54.9 <C> 108.5 <R> <C> GroupCap (chen2018groupcap) <C> 74.4 <C> 58.1 <C> 44.3 <C> 33.8 <C> 26.2 <C> - <C> - <R> <C> NBT (lu2018neural) <C> 75.5 <C> - <C> - <C> 34.7 <C> 27.1 <C> - <C> 107.2 <R> <C> StackCap (gu2018stack) <C> 78.4 <C> 62.5 <C> 47.9 <C> 36.1 <C> 27.4 <C> 56.9 <C> 120.4 <R> <C> Up-Down (anderson2018bottom) <C> 79.8 <C> 63.4 <C> 48.4 <C> 36.3 <C> 27.7 <C> 56.9 <C> 120.1 <R> <C> UGRIC <C> 81.3 <C> 65.2 <C> 50.3 <C> 38.2 <C> 28.4 <C> 58.6 <C> 123.5 <R> <C> UGRIC w/o copying <C> 80.8 <C> 64.6 <C> 49.6 <C> 37.7 <C> 28.1 <C> 58.2 <C> 122.7 <R> <C> UGRIC w/o discriminator <C> 80.5 <C> 64.2 <C> 49.1 <C> 37.3 <C> 27.9 <C> 57.9 <C> 122.3 <CAP> Table 1. The performances of various models on MSCOCO Karpathy split <COT> Looking at the "Method" column, "UGRIC" cell, and the "BLEU-4" column, we can see that UGRIC has the highest score in the BLEU-4 metric.
<R> <C> Method <C> <bold>BLEU-1</bold> <C> <bold>BLEU-2</bold> <C> <bold>BLEU-3</bold> <C> <bold>BLEU-4</bold> <C> <bold>METEOR</bold> <C> <bold>ROUGE</bold> <C> <bold>CIDEr</bold> <R> <C> Soft-Attention (xu2015show) <C> 70.7 <C> 49.2 <C> 34.4 <C> 24.3 <C> 23.9 <C> - <C> - <R> <C> CNN+Att (aneja2018convolutional) <C> 71.1 <C> 53.8 <C> 39.4 <C> 28.7 <C> 24.4 <C> 52.2 <C> 91.2 <R> <C> SCST (rennie2017self) <C> 77.4 <C> 60.9 <C> 46.0 <C> 34.1 <C> 26.7 <C> 55.7 <C> 114.0 <R> <C> Adaptive (lu2017knowing) <C> 74.2 <C> 58.0 <C> 43.9 <C> 33.2 <C> 26.6 <C> 54.9 <C> 108.5 <R> <C> GroupCap (chen2018groupcap) <C> 74.4 <C> 58.1 <C> 44.3 <C> 33.8 <C> 26.2 <C> - <C> - <R> <C> NBT (lu2018neural) <C> 75.5 <C> - <C> - <C> 34.7 <C> 27.1 <C> - <C> 107.2 <R> <C> StackCap (gu2018stack) <C> 78.4 <C> 62.5 <C> 47.9 <C> 36.1 <C> 27.4 <C> 56.9 <C> 120.4 <R> <C> Up-Down (anderson2018bottom) <C> 79.8 <C> 63.4 <C> 48.4 <C> 36.3 <C> 27.7 <C> 56.9 <C> 120.1 <R> <C> UGRIC <C> 81.3 <C> 65.2 <C> 50.3 <C> 38.2 <C> 28.4 <C> 58.6 <C> 123.5 <R> <C> UGRIC w/o copying <C> 80.8 <C> 64.6 <C> 49.6 <C> 37.7 <C> 28.1 <C> 58.2 <C> 122.7 <R> <C> UGRIC w/o discriminator <C> 80.5 <C> 64.2 <C> 49.1 <C> 37.3 <C> 27.9 <C> 57.9 <C> 122.3 <CAP> Table 1. The performances of various models on MSCOCO Karpathy split <COT> Looking at the "Method" column, "UGRIC w/o copying" cell, and the "CIDEr" column, we can see that UGRIC without copying has a lower score in the CIDEr metric compared to UGRIC.
<R> <C> <bold>Model</bold> <C> <bold>Incorrect</bold> <C> <bold>ROUGE-1</bold> <C> <bold>ROUGE-2</bold> <C> <bold>ROUGE-L</bold> <C> <bold>Length</bold> <R> <C> PGC See17 <C> 8% <C> 39.49% <C> 17.24% <C> 36.35% <C> 59.7 <R> <C> FAS Chen18 <C> 26% <C> 40.88% <C> 17.80% <C> 38.53% <C> 72.1 <R> <C> BUS Gehrmann18 <C> 25% <C> 41.52% <C> 18.76% <C> 38.60% <C> 54.4 <CAP> Table 1: Fraction of incorrect summaries produced by recent summarization systems on the CNN-DM test set, evaluated on a subset of 100 summaries. ROUGE scores (on full test set) and average summary length for reference. <COT> Looking at the "Model" column, finding the table shows the fraction of incorrect summaries produced by recent summarization systems on the CNN-DM test set.
<R> <C> <bold>Model</bold> <C> <bold>Incorrect</bold> <C> <bold>ROUGE-1</bold> <C> <bold>ROUGE-2</bold> <C> <bold>ROUGE-L</bold> <C> <bold>Length</bold> <R> <C> PGC See17 <C> 8% <C> 39.49% <C> 17.24% <C> 36.35% <C> 59.7 <R> <C> FAS Chen18 <C> 26% <C> 40.88% <C> 17.80% <C> 38.53% <C> 72.1 <R> <C> BUS Gehrmann18 <C> 25% <C> 41.52% <C> 18.76% <C> 38.60% <C> 54.4 <CAP> Table 1: Fraction of incorrect summaries produced by recent summarization systems on the CNN-DM test set, evaluated on a subset of 100 summaries. ROUGE scores (on full test set) and average summary length for reference. <COT> Looking at the "ROUGE-1" column, finding the ROUGE-1 score for PGC See17 is 39.49%.
<R> <C> <bold>Split</bold> <C> <bold>NLI Model</bold> <C> <bold>Incor.</bold> <C> Δ <C> ↑ <C> ↓ <R> <C> Val <C> <italic>Original</italic> <C> 42.1% <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Val <C> Random <C> 50.7% <C> +8.6 <C> 16 <C> 26 <R> <C> Val <C> DA <C> 51.4% <C> +9.3 <C> 13 <C> 23 <R> <C> Val <C> SSE <C> 45.8% <C> +3.7 <C> 18 <C> 22 <R> <C> Val <C> ESIM <C> 39.3% <C> -2.8 <C> 23 <C> 20 <R> <C> Val <C> InferSent <C> 38.3% <C> -3.8 <C> 24 <C> 20 <R> <C> [EMPTY] <C> BERT <C> 28.0% <C> -14.1 <C> 25 <C> 10 <R> <C> Test <C> <italic>Original</italic> <C> 26.0% <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Test <C> ESIM <C> 29.0% <C> +3.0 <C> 11 <C> 14 <CAP> Table 2: Fraction of incorrect summaries at first position after reranking with different NLI models. ↑ and ↓ show the absolute number of improved (incorrect replaced by correct) and worsened (vice versa) instances. <COT> Looking at the "Split" column, we can see that the "Test" split has a lower percentage of incorrect summaries compared to the "Val" split for all NLI models except for ESIM.
<R> <C> <bold>Split</bold> <C> <bold>NLI Model</bold> <C> <bold>Incor.</bold> <C> Δ <C> ↑ <C> ↓ <R> <C> Val <C> <italic>Original</italic> <C> 42.1% <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Val <C> Random <C> 50.7% <C> +8.6 <C> 16 <C> 26 <R> <C> Val <C> DA <C> 51.4% <C> +9.3 <C> 13 <C> 23 <R> <C> Val <C> SSE <C> 45.8% <C> +3.7 <C> 18 <C> 22 <R> <C> Val <C> ESIM <C> 39.3% <C> -2.8 <C> 23 <C> 20 <R> <C> Val <C> InferSent <C> 38.3% <C> -3.8 <C> 24 <C> 20 <R> <C> [EMPTY] <C> BERT <C> 28.0% <C> -14.1 <C> 25 <C> 10 <R> <C> Test <C> <italic>Original</italic> <C> 26.0% <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Test <C> ESIM <C> 29.0% <C> +3.0 <C> 11 <C> 14 <CAP> Table 2: Fraction of incorrect summaries at first position after reranking with different NLI models. ↑ and ↓ show the absolute number of improved (incorrect replaced by correct) and worsened (vice versa) instances. <COT> Looking at the "Δ" column, we can see that the NLI model "Random" has the highest improvement in percentage of incorrect summaries compared to the "Original" NLI model for the "Val" split.
<R> <C> à le → au <C> de lequel → duquel <R> <C> à les → aux <C> de lesquels → desquels <R> <C> à lequel → auquel <C> de lesquelles → desquelles <R> <C> à lesquels → auxquels <C> en les → ès <R> <C> à lesquelles → auxquelles <C> vois ci → voici <R> <C> de le → du <C> vois là → voilà <R> <C> de les → des <C> [EMPTY] <CAP> Table 2: French contraction rules. <COT> Looking at the table, we can see that there are different contractions in French and their corresponding expanded forms.
<R> <C> [EMPTY] <C> Setup: Full DBLESS <C> Setup: Full DBLESS <C> Setup: Full WBLESS <C> Setup: Full WBLESS <C> Setup: Full BIBLESS <C> Setup: Full BIBLESS <C> Setup: Disjoint DBLESS <C> Setup: Disjoint DBLESS <C> Setup: Disjoint WBLESS <C> Setup: Disjoint WBLESS <C> Setup: Disjoint BIBLESS <C> Setup: Disjoint BIBLESS <R> <C> [EMPTY] <C> SG <C> GL <C> SG <C> GL <C> SG <C> GL <C> SG <C> GL <C> SG <C> GL <C> SG <C> GL <R> <C> lear [Vulic:2018naaclps] <C> .957 <C> .955 <C> .905 <C> .910 <C> .872 <C> .875 <C> .528 <C> .531 <C> .555 <C> .529 <C> .381 <C> .389 <R> <C> postle dffn <C> .957 <C> .955 <C> .905 <C> .910 <C> .872 <C> .875 <C> .898 <C> .825 <C> .754 <C> .746 <C> .696 <C> .677 <R> <C> postle adv <C> .957 <C> .955 <C> .905 <C> .910 <C> .872 <C> .875 <C> <bold>.942</bold> <C> <bold>.888</bold> <C> <bold>.832</bold> <C> <bold>.766</bold> <C> <bold>.757</bold> <C> <bold>.690</bold> <CAP> Table 1: Accuracy of postle models on *BLESS datasets, for two different sets of English distributional vectors: Skip-Gram (SG) and GloVe (GL). lear reports highest scores on *BLESS datasets in the literature. <COT> Looking at the "CAP" tag, we can see that Table 1 shows the accuracy of postle models on *BLESS datasets using two different sets of English distributional vectors: Skip-Gram (SG) and GloVe (GL).
<R> <C> [EMPTY] <C> Setup: Full DBLESS <C> Setup: Full DBLESS <C> Setup: Full WBLESS <C> Setup: Full WBLESS <C> Setup: Full BIBLESS <C> Setup: Full BIBLESS <C> Setup: Disjoint DBLESS <C> Setup: Disjoint DBLESS <C> Setup: Disjoint WBLESS <C> Setup: Disjoint WBLESS <C> Setup: Disjoint BIBLESS <C> Setup: Disjoint BIBLESS <R> <C> [EMPTY] <C> SG <C> GL <C> SG <C> GL <C> SG <C> GL <C> SG <C> GL <C> SG <C> GL <C> SG <C> GL <R> <C> lear [Vulic:2018naaclps] <C> .957 <C> .955 <C> .905 <C> .910 <C> .872 <C> .875 <C> .528 <C> .531 <C> .555 <C> .529 <C> .381 <C> .389 <R> <C> postle dffn <C> .957 <C> .955 <C> .905 <C> .910 <C> .872 <C> .875 <C> .898 <C> .825 <C> .754 <C> .746 <C> .696 <C> .677 <R> <C> postle adv <C> .957 <C> .955 <C> .905 <C> .910 <C> .872 <C> .875 <C> <bold>.942</bold> <C> <bold>.888</bold> <C> <bold>.832</bold> <C> <bold>.766</bold> <C> <bold>.757</bold> <C> <bold>.690</bold> <CAP> Table 1: Accuracy of postle models on *BLESS datasets, for two different sets of English distributional vectors: Skip-Gram (SG) and GloVe (GL). lear reports highest scores on *BLESS datasets in the literature. <COT> Looking at the "postle adv" row, we can see that the accuracy for the "Disjoint BIBLESS" setup with GloVe (GL) vectors is higher than the accuracy for the same setup with Skip-Gram (SG) vectors.
<R> <C> <bold>Random</bold> <C> Target: Spanish .498 <C> Target: Spanish .498 <C> Target: Spanish .498 <C> Target: French .515 <C> Target: French .515 <C> Target: French .515 <R> <C> <bold>Distributional</bold> <C> .362 <C> .362 <C> .362 <C> .387 <C> .387 <C> .387 <R> <C> [EMPTY] <C> Ar <C> Co <C> Sm <C> Ar <C> Co <C> Sm <R> <C> postle dffn <C> <bold>.798</bold> <C> .740 <C> .728 <C> .688 <C> .735 <C> .742 <R> <C> postle adv <C> .768 <C> <bold>.790</bold> <C> <bold>.782</bold> <C> <bold>.746</bold> <C> <bold>.770</bold> <C> <bold>.786</bold> <CAP> Table 2: Average precision (AP) of postle models in cross-lingual transfer. Results are shown for both postle models (dffn and adv), two target languages (Spanish and French) and three methods for inducing bilingual vector spaces: Ar [Artetxe:2018acl], Co [Conneau:2018iclr], and Sm [Smith:2017iclr]. <COT> Looking at the "postle dffn" row, the highest average precision (AP) is shown for the target language Spanish with a value of ".798".
<R> <C> <bold>Random</bold> <C> Target: Spanish .498 <C> Target: Spanish .498 <C> Target: Spanish .498 <C> Target: French .515 <C> Target: French .515 <C> Target: French .515 <R> <C> <bold>Distributional</bold> <C> .362 <C> .362 <C> .362 <C> .387 <C> .387 <C> .387 <R> <C> [EMPTY] <C> Ar <C> Co <C> Sm <C> Ar <C> Co <C> Sm <R> <C> postle dffn <C> <bold>.798</bold> <C> .740 <C> .728 <C> .688 <C> .735 <C> .742 <R> <C> postle adv <C> .768 <C> <bold>.790</bold> <C> <bold>.782</bold> <C> <bold>.746</bold> <C> <bold>.770</bold> <C> <bold>.786</bold> <CAP> Table 2: Average precision (AP) of postle models in cross-lingual transfer. Results are shown for both postle models (dffn and adv), two target languages (Spanish and French) and three methods for inducing bilingual vector spaces: Ar [Artetxe:2018acl], Co [Conneau:2018iclr], and Sm [Smith:2017iclr]. <COT> Looking at the "Distributional" column, the average precision (AP) values are consistently lower than the corresponding values in the "Random" column.
<R> <C> [EMPTY] <C> CoNLL max <C> CoNLL MINA <C> CoNLL head <C> LEA max <C> LEA MINA <C> LEA head <R> <C> [EMPTY] <C> CoNLL-2012 test set <C> CoNLL-2012 test set <C> CoNLL-2012 test set <C> CoNLL-2012 test set <C> CoNLL-2012 test set <C> CoNLL-2012 test set <R> <C> Stanford rule-based <C> 55.60 (8) <C> 57.55 (8) <C> 57.38 (8) <C> 47.31 (8) <C> 49.65 (8) <C> 49.44 (8) <R> <C> cort <C> 63.03 (7) <C> 64.60 (6) <C> 64.51 (6) <C> 56.10 (6) <C> 58.05 (6) <C> 57.93 (6) <R> <C> Peng et al. <C> 63.05 (6) <C> 63.50 (7) <C> 63.54 (7) <C> 55.22 (7) <C> 55.76 (7) <C> 55.80 (7) <R> <C> deep-coref ranking <C> 65.59 (5) <C> 67.29 (5) <C> 67.09 (5) <C> 59.58 (5) <C> 61.70 (5) <C> 61.43 (5) <R> <C> deep-coref RL <C> 65.81 (4) <C> 67.50 (4) <C> 67.36 (4) <C> 59.76 (4) <C> 61.84 (4) <C> 61.64 (4) <R> <C> Lee et al. 2017 single <C> 67.23 (3) <C> 68.55 (3) <C> 68.53 (3) <C> 61.24 (3) <C> 62.87 (3) <C> 62.82 (3) <R> <C> Lee et al. 2017 ensemble <C> 68.87 (2) <C> 70.12 (2) <C> 70.05 (2) <C> 63.19 (2) <C> 64.76 (2) <C> 64.64 (2) <R> <C> Lee et al. 2018 <C> 72.96 (1) <C> 74.26 (1) <C> 75.29 (1) <C> 67.73 (1) <C> 69.32 (1) <C> 70.40 (1) <R> <C> [EMPTY] <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <R> <C> Stanford rule-based <C> 51.78 (4) <C> 53.79 (5) <C> 57.10 (4) <C> 43.28 (5) <C> 45.48 (6) <C> 49.28 (4) <R> <C> deep-coref ranking <C> 52.90 (3) <C> 55.16 (2) <C> 57.13 (3) <C> 44.40 (3) <C> 46.98 (3) <C> 49.05 (5) <R> <C> deep-coref RL <C> 50.73 (5) <C> 54.26 (4) <C> 57.16 (2) <C> 41.98 (6) <C> 46.02 (4) <C> 49.29 (3) <R> <C> Lee et al. 2017 single <C> 50.38 (6) <C> 52.16 (6) <C> 54.02 (6) <C> 43.86 (4) <C> 45.75 (5) <C> 47.69 (6) <R> <C> Lee et al. 2017 ensemble <C> 53.63 (2) <C> 55.03 (3) <C> 56.80 (5) <C> 47.50 (2) <C> 48.98 (2) <C> 50.87 (2) <R> <C> Lee et al. 2018 <C> 57.89 (1) <C> 59.90 (1) <C> 61.33 (1) <C> 52.42 (1) <C> 54.63 (1) <C> 56.19 (1) <CAP> Table 4: Evaluations based on maximum span, MINA, and head spans on the CoNLL-2012 test set and WikiCoref. The ranking of corresponding scores is specified in parentheses. Rankings which are different based on maximum vs. MINA spans are highlighted. <COT> Looking at the table, we can see that the scores for the Stanford rule-based system are higher on WikiCoref than on the CoNLL-2012 test set.
<R> <C> [EMPTY] <C> CoNLL max <C> CoNLL MINA <C> CoNLL head <C> LEA max <C> LEA MINA <C> LEA head <R> <C> [EMPTY] <C> CoNLL-2012 test set <C> CoNLL-2012 test set <C> CoNLL-2012 test set <C> CoNLL-2012 test set <C> CoNLL-2012 test set <C> CoNLL-2012 test set <R> <C> Stanford rule-based <C> 55.60 (8) <C> 57.55 (8) <C> 57.38 (8) <C> 47.31 (8) <C> 49.65 (8) <C> 49.44 (8) <R> <C> cort <C> 63.03 (7) <C> 64.60 (6) <C> 64.51 (6) <C> 56.10 (6) <C> 58.05 (6) <C> 57.93 (6) <R> <C> Peng et al. <C> 63.05 (6) <C> 63.50 (7) <C> 63.54 (7) <C> 55.22 (7) <C> 55.76 (7) <C> 55.80 (7) <R> <C> deep-coref ranking <C> 65.59 (5) <C> 67.29 (5) <C> 67.09 (5) <C> 59.58 (5) <C> 61.70 (5) <C> 61.43 (5) <R> <C> deep-coref RL <C> 65.81 (4) <C> 67.50 (4) <C> 67.36 (4) <C> 59.76 (4) <C> 61.84 (4) <C> 61.64 (4) <R> <C> Lee et al. 2017 single <C> 67.23 (3) <C> 68.55 (3) <C> 68.53 (3) <C> 61.24 (3) <C> 62.87 (3) <C> 62.82 (3) <R> <C> Lee et al. 2017 ensemble <C> 68.87 (2) <C> 70.12 (2) <C> 70.05 (2) <C> 63.19 (2) <C> 64.76 (2) <C> 64.64 (2) <R> <C> Lee et al. 2018 <C> 72.96 (1) <C> 74.26 (1) <C> 75.29 (1) <C> 67.73 (1) <C> 69.32 (1) <C> 70.40 (1) <R> <C> [EMPTY] <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <R> <C> Stanford rule-based <C> 51.78 (4) <C> 53.79 (5) <C> 57.10 (4) <C> 43.28 (5) <C> 45.48 (6) <C> 49.28 (4) <R> <C> deep-coref ranking <C> 52.90 (3) <C> 55.16 (2) <C> 57.13 (3) <C> 44.40 (3) <C> 46.98 (3) <C> 49.05 (5) <R> <C> deep-coref RL <C> 50.73 (5) <C> 54.26 (4) <C> 57.16 (2) <C> 41.98 (6) <C> 46.02 (4) <C> 49.29 (3) <R> <C> Lee et al. 2017 single <C> 50.38 (6) <C> 52.16 (6) <C> 54.02 (6) <C> 43.86 (4) <C> 45.75 (5) <C> 47.69 (6) <R> <C> Lee et al. 2017 ensemble <C> 53.63 (2) <C> 55.03 (3) <C> 56.80 (5) <C> 47.50 (2) <C> 48.98 (2) <C> 50.87 (2) <R> <C> Lee et al. 2018 <C> 57.89 (1) <C> 59.90 (1) <C> 61.33 (1) <C> 52.42 (1) <C> 54.63 (1) <C> 56.19 (1) <CAP> Table 4: Evaluations based on maximum span, MINA, and head spans on the CoNLL-2012 test set and WikiCoref. The ranking of corresponding scores is specified in parentheses. Rankings which are different based on maximum vs. MINA spans are highlighted. <COT> Looking at the table, we can see that the scores for the Lee et al. 2018 system are consistently higher than the scores for the Lee et al. 2017 ensemble system.
<R> <C> [BOLD] Model <C> [BOLD] CCAT10 <C> [BOLD] CCAT50 <C> [BOLD] BLOGS10 <C> [BOLD] BLOGS50 <R> <C> ST-CNN <C> 22.8 <C> 10.08 <C> 48.64 <C> 42.91 <R> <C> POS-CNN <C> 61.40 <C> 40.98 <C> 68.26 <C> 54.85 <R> <C> POS-HAN <C> [BOLD] 63.14 <C> [BOLD] 41.30 <C> [BOLD] 69.32 <C> [BOLD] 57.76 <CAP> TABLE III: The Accuracy of Different Syntactic Representations <COT> Looking at the "Model" column, we can see that the POS-HAN model has the highest accuracy for all four datasets.
<R> <C> [BOLD] Model <C> [BOLD] CCAT10 <C> [BOLD] CCAT50 <C> [BOLD] BLOGS10 <C> [BOLD] BLOGS50 <R> <C> Syntactic-HAN <C> 63.14 <C> 41.30 <C> 69.32 <C> 57.76 <R> <C> Lexical-HAN <C> 86.04 <C> 79.50 <C> 70.81 <C> 59.77 <R> <C> Style-HAN <C> [BOLD] 90.58 <C> [BOLD] 82.35 <C> [BOLD] 72.83 <C> [BOLD] 61.19 <CAP> TABLE IV: The Accuracy of Syntactic (Syntactic-HAN), Lexical (Lexical-HAN),and combined (Style-HAN) Models <COT> Looking at the "Model" column, we can see that the Style-HAN model has the highest accuracy scores for all four datasets (CCAT10, CCAT50, BLOGS10, BLOGS50).
<R> <C> [BOLD] Model <C> [BOLD] CCAT10 <C> [BOLD] CCAT50 <C> [BOLD] BLOGS10 <C> [BOLD] BLOGS50 <R> <C> Syntactic-HAN <C> 63.14 <C> 41.30 <C> 69.32 <C> 57.76 <R> <C> Lexical-HAN <C> 86.04 <C> 79.50 <C> 70.81 <C> 59.77 <R> <C> Style-HAN <C> [BOLD] 90.58 <C> [BOLD] 82.35 <C> [BOLD] 72.83 <C> [BOLD] 61.19 <CAP> TABLE IV: The Accuracy of Syntactic (Syntactic-HAN), Lexical (Lexical-HAN),and combined (Style-HAN) Models <COT> Looking at the "CCAT10" column, we can see that the Lexical-HAN model achieves an accuracy score of 86.04, which is higher than the scores of the Syntactic-HAN and Style-HAN models.
<R> <C> [BOLD] Dataset <C> [BOLD] Combined <C> [BOLD] Parallel <R> <C> CCAT10 <C> 88.36 <C> 90.58 <R> <C> CCAT50 <C> 81.21 <C> 82.35 <R> <C> BLOG10 <C> 67.38 <C> 72.83 <R> <C> BLOG50 <C> 58.81 <C> 61.19 <CAP> TABLE V: The accuracy of Different Fusion Approaches <COT> Looking at the "Combined" column, we can see that the accuracy for CCAT10 is higher than CCAT50, and the accuracy for BLOG10 is higher than BLOG50.
<R> <C> [BOLD] Dataset <C> [BOLD] Combined <C> [BOLD] Parallel <R> <C> CCAT10 <C> 88.36 <C> 90.58 <R> <C> CCAT50 <C> 81.21 <C> 82.35 <R> <C> BLOG10 <C> 67.38 <C> 72.83 <R> <C> BLOG50 <C> 58.81 <C> 61.19 <CAP> TABLE V: The accuracy of Different Fusion Approaches <COT> Looking at the "Parallel" column, we can see that the accuracy for CCAT10 is higher than CCAT50, and the accuracy for BLOG10 is higher than BLOG50.
<R> <C> [BOLD] Model <C> [BOLD] CCAT10 <C> [BOLD] CCAT50 <C> [BOLD] BLOGS10 <C> [BOLD] BLOGS50 <R> <C> SVM-affix-punctuation 3-grams <C> 78.8 <C> 69.3 <C> # <C> # <R> <C> CNN-char <C> # <C> # <C> 61.2 <C> 49.4 <R> <C> Continuous n-gram <C> 74.8 <C> 72.6 <C> 61.34 <C> 52.82 <R> <C> N-gram CNN <C> 86.8 <C> 76.5 <C> 63.74 <C> 53.09 <R> <C> Syntax-CNN <C> 88.20 <C> 81.00 <C> 64.10 <C> 56.73 <R> <C> Style-HAN <C> [BOLD] 90.58 <C> [BOLD] 82.35 <C> [BOLD] 72.83 <C> [BOLD] 61.19 <CAP> TABLE VI: Test Accuracy of models for each dataset <COT> Looking at the "Model" column, we can see that the "Style-HAN" model has the highest test accuracy for all datasets.
<R> <C> [BOLD] Model <C> [BOLD] CCAT10 <C> [BOLD] CCAT50 <C> [BOLD] BLOGS10 <C> [BOLD] BLOGS50 <R> <C> SVM-affix-punctuation 3-grams <C> 78.8 <C> 69.3 <C> # <C> # <R> <C> CNN-char <C> # <C> # <C> 61.2 <C> 49.4 <R> <C> Continuous n-gram <C> 74.8 <C> 72.6 <C> 61.34 <C> 52.82 <R> <C> N-gram CNN <C> 86.8 <C> 76.5 <C> 63.74 <C> 53.09 <R> <C> Syntax-CNN <C> 88.20 <C> 81.00 <C> 64.10 <C> 56.73 <R> <C> Style-HAN <C> [BOLD] 90.58 <C> [BOLD] 82.35 <C> [BOLD] 72.83 <C> [BOLD] 61.19 <CAP> TABLE VI: Test Accuracy of models for each dataset <COT> Looking at the "CCAT10" column, we can see that the "Style-HAN" model has a higher test accuracy compared to the "Continuous n-gram" model.
<R> <C> Experiment <C> [ITALIC] S P <C> [ITALIC] S R <C> [ITALIC] S F1 <C> [ITALIC] NS P <C> [ITALIC] NS R <C> [ITALIC] NS F1 <R> <C> SVM [ITALIC] rbl <C> 65.55 <C> 66.67 <C> 66.10 <C> 66.10 <C> 64.96 <C> 65.52 <R> <C> SVM [ITALIC] c+ [ITALIC] rbl <C> 63.32 <C> 61.97 <C> 62.63 <C> 62.77 <C> 64.10 <C> 63.5 <R> <C> LSTM [ITALIC] r <C> 67.90 <C> 66.23 <C> 67.1 <C> 67.08 <C> [BOLD] 68.80 <C> 67.93 <R> <C> LSTM [ITALIC] c+LSTM [ITALIC] r <C> 66.19 <C> 79.49 <C> 72.23 <C> 74.33 <C> 59.40 <C> 66.03 <R> <C> LSTM [ITALIC] conditional <C> [BOLD] 70.03 <C> 76.92 <C> [BOLD] 73.32 <C> 74.41 <C> 67.10 <C> [BOLD] 70.56 <R> <C> LSTM [ITALIC] ras <C> 69.45 <C> 70.94 <C> 70.19 <C> 70.30 <C> 68.80 <C> 69.45 <R> <C> LSTM [ITALIC] cas+LSTM [ITALIC] ras <C> 66.90 <C> [BOLD] 82.05 <C> [BOLD] 73.70 <C> [BOLD] 76.80 <C> 59.40 <C> 66.99 <R> <C> LSTM [ITALIC] caw+ [ITALIC] s+LSTM [ITALIC] raw+ [ITALIC] s <C> 65.90 <C> 74.35 <C> 69.88 <C> 70.59 <C> 61.53 <C> 65.75 <CAP> Table 2: Experimental results for the discussion forum dataset (bold are best scores) <COT> Looking at the "CAP" tag, we can see that Table 2 shows the experimental results for the discussion forum dataset.
<R> <C> Experiment <C> [ITALIC] S P <C> [ITALIC] S R <C> [ITALIC] S F1 <C> [ITALIC] NS P <C> [ITALIC] NS R <C> [ITALIC] NS F1 <R> <C> SVM [ITALIC] rbl <C> 65.55 <C> 66.67 <C> 66.10 <C> 66.10 <C> 64.96 <C> 65.52 <R> <C> SVM [ITALIC] c+ [ITALIC] rbl <C> 63.32 <C> 61.97 <C> 62.63 <C> 62.77 <C> 64.10 <C> 63.5 <R> <C> LSTM [ITALIC] r <C> 67.90 <C> 66.23 <C> 67.1 <C> 67.08 <C> [BOLD] 68.80 <C> 67.93 <R> <C> LSTM [ITALIC] c+LSTM [ITALIC] r <C> 66.19 <C> 79.49 <C> 72.23 <C> 74.33 <C> 59.40 <C> 66.03 <R> <C> LSTM [ITALIC] conditional <C> [BOLD] 70.03 <C> 76.92 <C> [BOLD] 73.32 <C> 74.41 <C> 67.10 <C> [BOLD] 70.56 <R> <C> LSTM [ITALIC] ras <C> 69.45 <C> 70.94 <C> 70.19 <C> 70.30 <C> 68.80 <C> 69.45 <R> <C> LSTM [ITALIC] cas+LSTM [ITALIC] ras <C> 66.90 <C> [BOLD] 82.05 <C> [BOLD] 73.70 <C> [BOLD] 76.80 <C> 59.40 <C> 66.99 <R> <C> LSTM [ITALIC] caw+ [ITALIC] s+LSTM [ITALIC] raw+ [ITALIC] s <C> 65.90 <C> 74.35 <C> 69.88 <C> 70.59 <C> 61.53 <C> 65.75 <CAP> Table 2: Experimental results for the discussion forum dataset (bold are best scores) <COT> Looking at the "LSTM conditional" row, we can see that the F1 score for the "S" category is [BOLD] 73.32, which is higher than the F1 score for the "NS" category.
<R> <C> Experiment <C> [ITALIC] S P <C> [ITALIC] S R <C> [ITALIC] S F1 <C> [ITALIC] NS P <C> [ITALIC] NS R <C> [ITALIC] NS F1 <R> <C> SVM [ITALIC] rbl <C> 64.20 <C> 64.95 <C> 64.57 <C> 69.0 <C> 68.30 <C> 68.7 <R> <C> SVM [ITALIC] c+ [ITALIC] rbl <C> 65.64 <C> 65.86 <C> 65.75 <C> 70.11 <C> 69.91 <C> 70.0 <R> <C> LSTM [ITALIC] r <C> 73.25 <C> 58.72 <C> 65.19 <C> 61.47 <C> 75.44 <C> 67.74 <R> <C> LSTM [ITALIC] c+LSTM [ITALIC] r <C> 70.89 <C> 67.95 <C> 69.39 <C> 64.94 <C> 68.03 <C> 66.45 <R> <C> LSTM [ITALIC] conditional <C> 76.08 <C> [BOLD] 76.53 <C> [BOLD] 76.30 <C> [BOLD] 72.93 <C> 72.44 <C> [BOLD] 72.68 <R> <C> LSTM [ITALIC] ras <C> 76.00 <C> 73.18 <C> 74.56 <C> 70.52 <C> 73.52 <C> 71.9 <R> <C> LSTM [ITALIC] cas+LSTM [ITALIC] ras <C> [BOLD] 77.25 <C> 75.51 <C> [BOLD] 76.36 <C> 72.65 <C> [BOLD] 74.52 <C> [BOLD] 73.57 <R> <C> LSTM [ITALIC] caw+LSTM [ITALIC] raw <C> 76.74 <C> 69.77 <C> 73.09 <C> 68.63 <C> 75.77 <C> 72.02 <R> <C> LSTM [ITALIC] caw+ [ITALIC] s+LSTM [ITALIC] raw+ [ITALIC] s <C> 76.42 <C> 71.37 <C> 73.81 <C> 69.50 <C> 74.77 <C> 72.04 <CAP> Table 3: Experimental results for Twitter dataset (bold are best scores) <COT> Looking at the "Experiment" column, we can see that there are different combinations of models and techniques being tested on the Twitter dataset.
<R> <C> Experiment <C> [ITALIC] S P <C> [ITALIC] S R <C> [ITALIC] S F1 <C> [ITALIC] NS P <C> [ITALIC] NS R <C> [ITALIC] NS F1 <R> <C> SVM [ITALIC] rbl <C> 64.20 <C> 64.95 <C> 64.57 <C> 69.0 <C> 68.30 <C> 68.7 <R> <C> SVM [ITALIC] c+ [ITALIC] rbl <C> 65.64 <C> 65.86 <C> 65.75 <C> 70.11 <C> 69.91 <C> 70.0 <R> <C> LSTM [ITALIC] r <C> 73.25 <C> 58.72 <C> 65.19 <C> 61.47 <C> 75.44 <C> 67.74 <R> <C> LSTM [ITALIC] c+LSTM [ITALIC] r <C> 70.89 <C> 67.95 <C> 69.39 <C> 64.94 <C> 68.03 <C> 66.45 <R> <C> LSTM [ITALIC] conditional <C> 76.08 <C> [BOLD] 76.53 <C> [BOLD] 76.30 <C> [BOLD] 72.93 <C> 72.44 <C> [BOLD] 72.68 <R> <C> LSTM [ITALIC] ras <C> 76.00 <C> 73.18 <C> 74.56 <C> 70.52 <C> 73.52 <C> 71.9 <R> <C> LSTM [ITALIC] cas+LSTM [ITALIC] ras <C> [BOLD] 77.25 <C> 75.51 <C> [BOLD] 76.36 <C> 72.65 <C> [BOLD] 74.52 <C> [BOLD] 73.57 <R> <C> LSTM [ITALIC] caw+LSTM [ITALIC] raw <C> 76.74 <C> 69.77 <C> 73.09 <C> 68.63 <C> 75.77 <C> 72.02 <R> <C> LSTM [ITALIC] caw+ [ITALIC] s+LSTM [ITALIC] raw+ [ITALIC] s <C> 76.42 <C> 71.37 <C> 73.81 <C> 69.50 <C> 74.77 <C> 72.04 <CAP> Table 3: Experimental results for Twitter dataset (bold are best scores) <COT> Looking at the "LSTM [ITALIC] conditional" row, we can see that the precision, recall, and F1 score for sentiment (S) are all bolded, indicating the best scores in those categories.
