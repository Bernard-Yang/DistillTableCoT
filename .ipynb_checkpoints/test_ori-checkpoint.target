Table 1 (upper part) shows the results of our basic semantic parser (with GloVe embeddings) on all six graphbanks  Our results are competitive across the board, and set a new state of the art for EDS Smatch scores (Cai and Knight, 2013) among EDS parsers which are not trained on gold syntax information.  Our EDM score (Dridan and Oepen, 2011) on EDS is lower,  The use of BERT embeddings is highly effective across the board. We set a new state of the art (without gold syntax) on all graphbanks except AMR 2017;  The improvement is particularly pronounced in the out-of-domain evaluations, illustrating BERT's ability to transfer across domains.  The results on the test dataset are shown in Table 1 (bottom). With GloVe, multi-task learning led to substantial improvements; with BERT the improvements are smaller but still noticeable.
Table 1 (upper part) shows the results of our basic semantic parser (with GloVe embeddings) on all six graphbanks  Our results are competitive across the board, and set a new state of the art for EDS Smatch scores (Cai and Knight, 2013) among EDS parsers which are not trained on gold syntax information.  Our EDM score (Dridan and Oepen, 2011) on EDS is lower,  The use of BERT embeddings is highly effective across the board. We set a new state of the art (without gold syntax) on all graphbanks except AMR 2017;  The improvement is particularly pronounced in the out-of-domain evaluations, illustrating BERT's ability to transfer across domains.  The results on the test dataset are shown in Table 1 (bottom). With GloVe, multi-task learning led to substantial improvements; with BERT the improvements are smaller but still noticeable.
We conduct transfer learning on four different combinations of MedNLI, SNLI, and MNLI as it shown in the table 4 (line 4 to 7) and also add the results of general domain tasks (MNLI, SNLI) for comparison.  BERT performs better on tasks in the general domain while BioBERT performs better on MedNLI which is in the clinical domain.  positive transfer occurs on MedNLI.  even though BioBERT is finetuned on general domain tasks before MedNLI, transfer learning shows better results than that fine-tuned on MedNLI directly.  the domain specific language representations from BioBERT are maintained while fine-tuning on general domain tasks by showing that the transfer learning results of MedNLI on BioBERT have better performance than the results on BERT (line 4 to 7).  the accuracy of MNLI and SNLI on BioBERT is lower than the accuracy on BERT.  The best combination is SNLI → MNLI → MedNLI on BioBERT.  MedNLI (expanded) shows better performance than MedNLI on BioBERT while MedNLI works better on BERT (see table 4).  the performance of MedNLI (expanded) with transfer learning is higher on BERT and lower on BioBERT than the performance of MedNLI with transfer learning.
Switching to ELMo word embeddings improves performance by 2.9 percentage points on an average, corresponding to about 53 test sentences. Of these, about 32 sentences (60% of the improvement) correspond to A-but-B and negation style sentences,  As further evidence that ELMo helps on these specific constructions, the non-ELMo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on average, only 89 (34.8%) of which are A-but-B style or negations.
Switching to ELMo word embeddings improves performance by 2.9 percentage points on an average, corresponding to about 53 test sentences. Of these, about 32 sentences (60% of the improvement) correspond to A-but-B and negation style sentences,  As further evidence that ELMo helps on these specific constructions, the non-ELMo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on average, only 89 (34.8%) of which are A-but-B style or negations.
We present statistics of our dataset10 in Table 3.  As expected, inter-annotator agreement is higher for higher thresholds (less ambiguous sentences). According to Landis and Koch (1977), κ ∈ (0.2, 0.4] corresponds to "fair agreement", whereas κ ∈ (0.4, 0.6] corresponds to "moderate agreement".  We next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences. Higher thresholds correspond to sets of less ambiguous sentences. Table 3 shows that ELMo's performance gains in Table 2 extends across all thresholds.
We present statistics of our dataset10 in Table 3.  As expected, inter-annotator agreement is higher for higher thresholds (less ambiguous sentences). According to Landis and Koch (1977), κ ∈ (0.2, 0.4] corresponds to "fair agreement", whereas κ ∈ (0.4, 0.6] corresponds to "moderate agreement".  We next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences. Higher thresholds correspond to sets of less ambiguous sentences. Table 3 shows that ELMo's performance gains in Table 2 extends across all thresholds.
The top tuning data scores for AVG COS SIM (Table 1) show that the Google embeddings with TF*IDF weighting yield the top F score for all three concept input types (.881 - .945). Somewhat expectedly, the best overall F score (.945) is produced in the setting Both, which provides the most information. Actually, this is true for all four weighting schemes for both GloVe and Google, while fastText consistently yields its top F scores (.840 - .911) in the Label setting, which provides the least information.
The top tuning data scores for AVG COS SIM (Table 1) show that the Google embeddings with TF*IDF weighting yield the top F score for all three concept input types (.881 - .945). Somewhat expectedly, the best overall F score (.945) is produced in the setting Both, which provides the most information. Actually, this is true for all four weighting schemes for both GloVe and Google, while fastText consistently yields its top F scores (.840 - .911) in the Label setting, which provides the least information.
For TOP n COS SIM AVG, the tuning data results (Table 2) are somewhat more varied: First, there is no single best performing set of embeddings: Google yields the best F score for the Label setting (.953), while GloVe (though only barely) leads in the Description setting (.912). This time, it is fastText which produces the best F score in the Both setting, which is also the best overall tuning data F score for TOP n COS SIM AVG (.954).
For TOP n COS SIM AVG, the tuning data results (Table 2) are somewhat more varied: First, there is no single best performing set of embeddings: Google yields the best F score for the Label setting (.953), while GloVe (though only barely) leads in the Description setting (.912). This time, it is fastText which produces the best F score in the Both setting, which is also the best overall tuning data F score for TOP n COS SIM AVG (.954).
The results can be found in Table 3. For comparison, the two top rows provide the best results of Gong et al. (2018). The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926). Note that our Both setting is probably the one most similar to the concept input used by Gong et al. (2018).
The results can be found in Table 3. For comparison, the two top rows provide the best results of Gong et al. (2018). The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926). Note that our Both setting is probably the one most similar to the concept input used by Gong et al. (2018).
We use two pre-trained deep models: a CNN (Jaderberg et al., 2016) and an LSTM (Ghosh et al., 2017) as baselines (BL) to extract the initial list of word hypotheses.  We experimented extracting kbest hypotheses for k = 1 . . . 10.  Table 1 presents four different accuracy metrics for this case: 1) full columns correspond to the accuracy on the whole dataset. 2) dict columns correspond to the accuracy over the cases where the target word is among the 90K words of the CNN dictionary (which correspond to 43.3% of the whole dataset. 3) list columns report the accuracy over the cases where the right word was among the k-best produced by the baseline. 4) MRR Mean Reciprocal Rank (MRR),  We compare the results of our encoder with several stateof-the-art sentence encoders, tuned or trained on the same dataset.  Table 1 are trained in the same conditions that our model with glove initialization with dual-channel overlapping non-static pre-trained embedding on the same dataset.  Our model FDCLSTM without attention achieves a better result in the case of the second baseline LSTM that full of false-positives and short words.  We also compare our result with current state-of-the-art word embeddings trained on a large general text using glove and fasttext. The word model used only object and place information, and ignored the caption. Our proposed models achieve better performance than our TWE previous model (Sabir et al., 2018), that trained a word embedding (Mikolov et al., 2013) from scratch on the same task.  As seen in Table 1, the introduction of this unigram lexicon produces the best results.
We use two pre-trained deep models: a CNN (Jaderberg et al., 2016) and an LSTM (Ghosh et al., 2017) as baselines (BL) to extract the initial list of word hypotheses.  We experimented extracting kbest hypotheses for k = 1 . . . 10.  Table 1 presents four different accuracy metrics for this case: 1) full columns correspond to the accuracy on the whole dataset. 2) dict columns correspond to the accuracy over the cases where the target word is among the 90K words of the CNN dictionary (which correspond to 43.3% of the whole dataset. 3) list columns report the accuracy over the cases where the right word was among the k-best produced by the baseline. 4) MRR Mean Reciprocal Rank (MRR),  We compare the results of our encoder with several stateof-the-art sentence encoders, tuned or trained on the same dataset.  Table 1 are trained in the same conditions that our model with glove initialization with dual-channel overlapping non-static pre-trained embedding on the same dataset.  Our model FDCLSTM without attention achieves a better result in the case of the second baseline LSTM that full of false-positives and short words.  We also compare our result with current state-of-the-art word embeddings trained on a large general text using glove and fasttext. The word model used only object and place information, and ignored the caption. Our proposed models achieve better performance than our TWE previous model (Sabir et al., 2018), that trained a word embedding (Mikolov et al., 2013) from scratch on the same task.  As seen in Table 1, the introduction of this unigram lexicon produces the best results.
Table 1 contains the results on all four datasets. SynST achieves speedups of ∼ 4 − 5× that of the vanilla Transformer, which is larger than nearly all  of the SAT configurations. Quality-wise, SynST again significantly outperforms the SAT configurations at comparable speedups on all datasets. On WMT En-De, SynST improves by 1 BLEU over LT (20.74 vs LT's 19.8 without reranking).
Table 1 contains the results on all four datasets. SynST achieves speedups of ∼ 4 − 5× that of the vanilla Transformer, which is larger than nearly all  of the SAT configurations. Quality-wise, SynST again significantly outperforms the SAT configurations at comparable speedups on all datasets. On WMT En-De, SynST improves by 1 BLEU over LT (20.74 vs LT's 19.8 without reranking).
Table 2 shows the development results of various S-LSTM settings,  Adding one additional sentence-level node  does not lead to accuracy improvements, although the number of parameters and decoding time increase accordingly.  The accuracies of S-LSTM increases as the hidden layer size for each node increases from 100 to 300, but does not further increase when the size increases beyond 300. We fix the hidden size to 300 accordingly. Without using (cid:104)s(cid:105) and (cid:104)/s(cid:105), the performance of S-LSTM drops from 82.64% to 82.36%, showing the effectiveness of having these additional nodes.
Table 2 shows the development results of various S-LSTM settings,  Adding one additional sentence-level node  does not lead to accuracy improvements, although the number of parameters and decoding time increase accordingly.  The accuracies of S-LSTM increases as the hidden layer size for each node increases from 100 to 300, but does not further increase when the size increases beyond 300. We fix the hidden size to 300 accordingly. Without using (cid:104)s(cid:105) and (cid:104)/s(cid:105), the performance of S-LSTM drops from 82.64% to 82.36%, showing the effectiveness of having these additional nodes.
As shown in Table 3, BiLSTM gives significantly better accuracies compared to uni-directional LSTM2, with the training time per epoch growing from 67 seconds to 106 seconds. Stacking 2 layers of BiLSTM gives further improvements to development results, with a larger time of 207 seconds. 3 layers of stacked BiLSTM does not further improve the results. In contrast, S-LSTM gives a development result of 82.64%, which is significantly better compared to 2-layer stacked BiLSTM, with a smaller number of model parameters and a shorter time of 65 seconds.  We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows),  CNN is the most efficient among all models compared, with the smallest model size. On the other hand, a 3-layer stacked CNN gives an accuracy of 81.46%, which is also  the lowest compared with BiLSTM, hierarchical attention and S-LSTM. The best performance of hierarchical attention is between single-layer and two-layer BiLSTMs in terms of both accuracy and efficiency. S-LSTM gives significantly better accuracies compared with both CNN and hierarchical attention.  Table 3 additionally shows the results of BiLSTM and S-LSTM when external attention is used  Attention leads to improved accuracies for both BiLSTM and S-LSTM in classification, with S-LSTM still outperforming BiLSTM significantly.
As shown in Table 3, BiLSTM gives significantly better accuracies compared to uni-directional LSTM2, with the training time per epoch growing from 67 seconds to 106 seconds. Stacking 2 layers of BiLSTM gives further improvements to development results, with a larger time of 207 seconds. 3 layers of stacked BiLSTM does not further improve the results. In contrast, S-LSTM gives a development result of 82.64%, which is significantly better compared to 2-layer stacked BiLSTM, with a smaller number of model parameters and a shorter time of 65 seconds.  We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows),  CNN is the most efficient among all models compared, with the smallest model size. On the other hand, a 3-layer stacked CNN gives an accuracy of 81.46%, which is also  the lowest compared with BiLSTM, hierarchical attention and S-LSTM. The best performance of hierarchical attention is between single-layer and two-layer BiLSTMs in terms of both accuracy and efficiency. S-LSTM gives significantly better accuracies compared with both CNN and hierarchical attention.  Table 3 additionally shows the results of BiLSTM and S-LSTM when external attention is used  Attention leads to improved accuracies for both BiLSTM and S-LSTM in classification, with S-LSTM still outperforming BiLSTM significantly.
The final results on the movie review and rich text classification datasets are shown in Tables 4 and 5, respectively.  As shown in Table 4,  S-LSTM outperforms BiLSTM significantly, with a faster speed.  S-LSTM also gives highly competitive results when compared with existing methods in the literature.
The final results on the movie review and rich text classification datasets are shown in Tables 4 and 5, respectively.  As shown in Table 4,  S-LSTM outperforms BiLSTM significantly, with a faster speed.  S-LSTM also gives highly competitive results when compared with existing methods in the literature.
The final results on the movie review and rich text classification datasets are shown in Tables 4 and 5, respectively.  As shown in Table 5, among the 16 datasets  S-LSTM gives the best results on 12, compared with BiLSTM and 2 layered BiLSTM models. The average accuracy of S-LSTM is 85.6%, significantly higher compared with 84.9% by 2-layer stacked BiLSTM. 3-layer stacked BiLSTM gives an average accuracy of 84.57%, which is lower compared to a 2-layer stacked BiLSTM, with a training time per epoch of 423.6 seconds.
The final results on the movie review and rich text classification datasets are shown in Tables 4 and 5, respectively.  As shown in Table 5, among the 16 datasets  S-LSTM gives the best results on 12, compared with BiLSTM and 2 layered BiLSTM models. The average accuracy of S-LSTM is 85.6%, significantly higher compared with 84.9% by 2-layer stacked BiLSTM. 3-layer stacked BiLSTM gives an average accuracy of 84.57%, which is lower compared to a 2-layer stacked BiLSTM, with a training time per epoch of 423.6 seconds.
As can be seen in Table 6, S-LSTM gives significantly better results compared with BiLSTM on the WSJ dataset. It also gives competitive accuracies as compared with existing methods in the literature. Stacking two layers of BiLSTMs leads to improved results compared to one-layer BiLSTM, but the accuracy does not further improve  with three layers of stacked LSTMs.
As can be seen in Table 6, S-LSTM gives significantly better results compared with BiLSTM on the WSJ dataset. It also gives competitive accuracies as compared with existing methods in the literature. Stacking two layers of BiLSTMs leads to improved results compared to one-layer BiLSTM, but the accuracy does not further improve  with three layers of stacked LSTMs.
For NER (Table 7), S-LSTM gives an F1-score of 91.57% on the CoNLL test set, which is significantly better compared with BiLSTMs. Stacking more layers of BiLSTMs leads to slightly better F1-scores compared with a single-layer BiLSTM. Our BiLSTM results are comparable to the results reported by Ma and Hovy (2016) and Lample et al. (2016),  In contrast, S-LSTM gives the best reported results under the same settings. In the second section of Table 7, Yang et al. (2017) obtain an Fscore of 91.26%
For NER (Table 7), S-LSTM gives an F1-score of 91.57% on the CoNLL test set, which is significantly better compared with BiLSTMs. Stacking more layers of BiLSTMs leads to slightly better F1-scores compared with a single-layer BiLSTM. Our BiLSTM results are comparable to the results reported by Ma and Hovy (2016) and Lample et al. (2016),  In contrast, S-LSTM gives the best reported results under the same settings. In the second section of Table 7, Yang et al. (2017) obtain an Fscore of 91.26%
Table 2 and 3 display the results on the E2E and WebNLG test sets for models of the respective challenges and our own models  On the E2E test set, our single best word- and character-based models reach comparable results to the best challenge submissions. The word-based models achieve significantly higher BLEU and ROUGE-L scores than the character-based models.
Table 2 and 3 display the results on the E2E and WebNLG test sets for models of the respective challenges and our own models  On the E2E test set, our single best word- and character-based models reach comparable results to the best challenge submissions. The word-based models achieve significantly higher BLEU and ROUGE-L scores than the character-based models.
On the WebNLG test set, the BLEU score of our best word-based model outperforms the best challenge submission by a small margin. The character-based model achieves a significantly higher ROUGE-L score than the wordbased model, whereas the BLEU score difference is not significant.
On the WebNLG test set, the BLEU score of our best word-based model outperforms the best challenge submission by a small margin. The character-based model achieves a significantly higher ROUGE-L score than the wordbased model, whereas the BLEU score difference is not significant.
Table 4 shows that increasing the number of layers from 1 to 5 results in a BLEU increase of only 0.5, while the speedup drops from 3.8× to 1.4×.  The final row of Table 4 shows that exposing the parse decoder to multiple possible chunkings of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving BLEU by 1.5 while decreasing the speedup from 3.8× to 3.1×;
Table 4 shows that increasing the number of layers from 1 to 5 results in a BLEU increase of only 0.5, while the speedup drops from 3.8× to 1.4×.  The final row of Table 4 shows that exposing the parse decoder to multiple possible chunkings of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving BLEU by 1.5 while decreasing the speedup from 3.8× to 3.1×;
Table 4 shows the BLEU and ROUGE-L development set scores when treating each human reference as prediction once and evaluating it against the remaining references, compared to the scores of the word-based and  character-based models  Strikingly, on the E2E development set, both model variants significantly outperform human texts by far with respect to both automatic evaluation measures. While the human BLEU score is significantly higher than those of both systems on the WebNLG development set, there is no statistical difference between human and system ROUGE-L scores.
Table 4 shows the BLEU and ROUGE-L development set scores when treating each human reference as prediction once and evaluating it against the remaining references, compared to the scores of the word-based and  character-based models  Strikingly, on the E2E development set, both model variants significantly outperform human texts by far with respect to both automatic evaluation measures. While the human BLEU score is significantly higher than those of both systems on the WebNLG development set, there is no statistical difference between human and system ROUGE-L scores.
One annotator (one of the authors of this paper) manually assessed the outputs of the models that obtained the best development set BLEU score as summarized in Table 56. As we can see from the bottom part of the table, all models struggle more with getting the content right than with producing linguistically correct texts; 70-80% of the texts generated by all models are completely correct linguistically.  Comparing the two datasets, we again observe that the WebNLG dataset is much more challenging than the E2E dataset, especially with respect to correctly verbalizing the content.  Moreover, spelling mistakes only appeared in WebNLG texts, mainly concerning omissions of accents or umlauts.  The most frequent content error in both datasets concerns omission of information.  Information addition and repetition only occur in the WebNLG dataset. The latter is an especially frequent problem of the character-based model, affecting more than a quarter of all texts.  In comparison, character-based models reproduce the content more faithfully on the E2E dataset while offering the same level of linguistic quality as word-based models, leading to more correct outputs overall. On the WebNLG dataset, the word-based model is more faithful to the inputs, probably because of the effective delexicalization strategy, whereas the character-based model errs less on the linguistic side. Overall, the word-based model yields more correct texts, stressing the importance of delexicalization and data normalization in low resource settings.
One annotator (one of the authors of this paper) manually assessed the outputs of the models that obtained the best development set BLEU score as summarized in Table 56. As we can see from the bottom part of the table, all models struggle more with getting the content right than with producing linguistically correct texts; 70-80% of the texts generated by all models are completely correct linguistically.  Comparing the two datasets, we again observe that the WebNLG dataset is much more challenging than the E2E dataset, especially with respect to correctly verbalizing the content.  Moreover, spelling mistakes only appeared in WebNLG texts, mainly concerning omissions of accents or umlauts.  The most frequent content error in both datasets concerns omission of information.  Information addition and repetition only occur in the WebNLG dataset. The latter is an especially frequent problem of the character-based model, affecting more than a quarter of all texts.  In comparison, character-based models reproduce the content more faithfully on the E2E dataset while offering the same level of linguistic quality as word-based models, leading to more correct outputs overall. On the WebNLG dataset, the word-based model is more faithful to the inputs, probably because of the effective delexicalization strategy, whereas the character-based model errs less on the linguistic side. Overall, the word-based model yields more correct texts, stressing the importance of delexicalization and data normalization in low resource settings.
Table 6 shows automatically computed statistics on the diversity of the generated texts of both models and human texts and on the overlap of the (generated) texts with the training set.  On both datasets, our systems produce significantly less varied outputs and reproduce more  texts and sentences from the training data than the human texts. Interestingly, however, the characterbased models generate significantly more unique sentences and copy significantly less from the training data than the word-based models, which copy about 40% of their generated sentences from the training data.
Table 6 shows automatically computed statistics on the diversity of the generated texts of both models and human texts and on the overlap of the (generated) texts with the training set.  On both datasets, our systems produce significantly less varied outputs and reproduce more  texts and sentences from the training data than the human texts. Interestingly, however, the characterbased models generate significantly more unique sentences and copy significantly less from the training data than the word-based models, which copy about 40% of their generated sentences from the training data.
Table 7 shows our manual evaluation of the top 30 hypotheses for 10 random E2E test inputs generated by models trained with data synthesized from the two templates. As is evident from the first two rows, all models learned to generalize from the training data to produce correct texts for novel inputs consisting of unseen combinations of input attributes.  Yet, the picture is a bit different for the model trained on data generated by both templates. While the top two hypotheses are equally distributed between adhering to Template 1 and Template 2, more than 5% among the lower-ranked hypotheses constitute a template combination such as the example shown in the bottom part of Figure 2.  As can be seen in the final row of Table 7, this simple reranker successfully places correct hypotheses higher up in the ranking, improving the practical usability of the generation model by now offering almost three correct variants for each input among the top five hypotheses on average.
Table 7 shows our manual evaluation of the top 30 hypotheses for 10 random E2E test inputs generated by models trained with data synthesized from the two templates. As is evident from the first two rows, all models learned to generalize from the training data to produce correct texts for novel inputs consisting of unseen combinations of input attributes.  Yet, the picture is a bit different for the model trained on data generated by both templates. While the top two hypotheses are equally distributed between adhering to Template 1 and Template 2, more than 5% among the lower-ranked hypotheses constitute a template combination such as the example shown in the bottom part of Figure 2.  As can be seen in the final row of Table 7, this simple reranker successfully places correct hypotheses higher up in the ranking, improving the practical usability of the generation model by now offering almost three correct variants for each input among the top five hypotheses on average.
The automatic evaluation scores are presented in Table 1 and Table 2.  Our method outperforms commonly used prefix baselines for this task which take the first 75 characters or 8 words of the source as a summary. Our system achieves comparable results to Wang and Lee (2018) a system based on both GANs and reinforcement training.  In Table 1, we also list scores of the stateof-the-art supervised model, an attention based  seq-to-seq model of our own implementation, as well as the oracle scores of our method obtained by choosing the best summary among all finished hypothesis from beam search.  The oracle scores are much higher, indicating that our unsupervised method does allow summaries of better quality,
The automatic evaluation scores are presented in Table 1 and Table 2.  Our method outperforms commonly used prefix baselines for this task which take the first 75 characters or 8 words of the source as a summary. Our system achieves comparable results to Wang and Lee (2018) a system based on both GANs and reinforcement training.  In Table 1, we also list scores of the stateof-the-art supervised model, an attention based  seq-to-seq model of our own implementation, as well as the oracle scores of our method obtained by choosing the best summary among all finished hypothesis from beam search.  The oracle scores are much higher, indicating that our unsupervised method does allow summaries of better quality,
For extractive sentence summarization, our method achieves good compression rate and significantly raises a previous unsupervised baseline on token level F1 score.
For extractive sentence summarization, our method achieves good compression rate and significantly raises a previous unsupervised baseline on token level F1 score.
Table 3 considers analysis of different aspects of the model.  First, we look at the  fluency model and compare the cluster smoothing (CS) approach with softmax temperature (TEMPx with x being the temperature) commonly used for generation in LM-integrated models (Chorowski and Jaitly, 2016) as well as no adjustment (NA). Second, we vary the 3-layer representation out of ELMo forward language model to do contextual matching (bot/mid/top: bottom/middle/top layer only, avg: average of 3 layers, cat: concatenation of all layers).  Results show the effectiveness of our cluster smoothing method for the vocabulary adaptive language model pfm, although temperature smoothing is an option for abstractive datasets.  When using word embeddings (bottom layer only from ELMo language model) in our contextual matching model pcm, the summarization performance drops significantly to below simple baselines as demonstrated by score decrease.
Table 3 considers analysis of different aspects of the model.  First, we look at the  fluency model and compare the cluster smoothing (CS) approach with softmax temperature (TEMPx with x being the temperature) commonly used for generation in LM-integrated models (Chorowski and Jaitly, 2016) as well as no adjustment (NA). Second, we vary the 3-layer representation out of ELMo forward language model to do contextual matching (bot/mid/top: bottom/middle/top layer only, avg: average of 3 layers, cat: concatenation of all layers).  Results show the effectiveness of our cluster smoothing method for the vocabulary adaptive language model pfm, although temperature smoothing is an option for abstractive datasets.  When using word embeddings (bottom layer only from ELMo language model) in our contextual matching model pcm, the summarization performance drops significantly to below simple baselines as demonstrated by score decrease.
We select the 1st and 2nd best performing models on the development datasets as well as the majority vote (mv) of 5 models for the final submission. The final results are shown in Table 3.
We select the 1st and 2nd best performing models on the development datasets as well as the majority vote (mv) of 5 models for the final submission. The final results are shown in Table 3.
We evaluate two configurations of the parse decoder, one in which it is trained separately from the token decoder (first column of Table 3), and the other where both decoders are trained jointly (second column of Ta  ble 3). We observe that joint training boosts the chunk F1 from 65.4 to 69.6, although, in both cases the F1 scores are relatively low, which matches our intuition as most source sentences can be translated into multiple target syntactic forms.  To measure how often the token decoder follows the predicted chunk sequence, we parse the generated translation and compute the F1 between the resulting chunk sequence and the parse decoder's prediction (fourth column of Table 3). Strong results of 89.9 F1 and 43.1% exact match indicate that the token decoder is heavily reliant on the generated chunk sequences.  The resulting F1 is indeed almost 10 points higher (third column of Table 3), indicating that the token decoder does have the ability to correct mistakes.  In Section 5.3 (see the final row of Table 3) we consider the effect of randomly sampling the max chunk size k during training. This provides a considerable boost to BLEU with a minimal impact to speedup.
The results are presented in Table 1 (top two subparts). As we can see, the three decoding settings do not differ significantly in terms of the performance on selected downstream tasks, with RNN or CNN as the decoder.  The results are also presented in Table 1 (3rd and 4th subparts). The performance of the predict-allwords RNN decoder does not significantly differ from that of any one of the autoregressive RNN de  coders, and the same situation can be also observed in CNN decoders.  In our proposed RNN-CNN model, we empirically show that the mean+max pooling provides stronger transferability than the max pooling alone does, and the results are presented in the last two sections of Table 1.
The results are presented in Table 1 (top two subparts). As we can see, the three decoding settings do not differ significantly in terms of the performance on selected downstream tasks, with RNN or CNN as the decoder.  The results are also presented in Table 1 (3rd and 4th subparts). The performance of the predict-allwords RNN decoder does not significantly differ from that of any one of the autoregressive RNN de  coders, and the same situation can be also observed in CNN decoders.  In our proposed RNN-CNN model, we empirically show that the mean+max pooling provides stronger transferability than the max pooling alone does, and the results are presented in the last two sections of Table 1.
We present the Table 4 in the supplementary material and we summarise it as follows: 1. Decoding the next sentence performed similarly to decoding the subsequent contiguous words. 2. Decoding the subsequent 30 words, which was adopted from the skip-thought training code3, gave reasonably good performance. More words for decoding didn't give us a significant performance gain, and took longer to train. 3. Adding more layers into the decoder and enlarging the dimension of the convolutional layers indeed sightly improved the performance on the three downstream tasks, but as training efficiency is one of our main concerns, it wasn't worth sacrificing training efficiency for the minor performance gain. 4. Increasing the dimensionality of the RNN encoder improved the model performance, and the additional training time required was less than needed for increasing the complexity in the CNN decoder. We report results from both smallest and largest models in Table 2.  As the transferability of the models trained in both cases perform similarly on the evaluation tasks (see rows 1 and 2 in Table 4), we focus on the simpler predictall-words CNN decoder that learns to reconstruct the next window of contiguous words.  As stated in rows 1, 3, and 4 in Table 4, decoding short target sequences results in a slightly lower Pearson score on SICK, and decoding longer target sequences lead to a longer training time.  We tweaked the CNN encoder, including different kernel size and activation function, and we report the best results of CNNCNN model at row 6 in Table 4.  The future predictor in (Gan et al., 2017) also applies a CNN as the encoder, but the decoder is still an RNN, listed at row 11 in Table 4. Compared to our designed CNN-CNN model, their CNN-LSTM model contains more parameters than our model does, but they have similar performance on the evaluation tasks, which is also worse than our RNNCNN model.  Clearly, we can tell from the comparison between rows 1, 9 and 12 in Table 4, increasing the dimensionality of the RNN encoder leads to better transferability of the model.  Compared with the model with larger-size CNN decoder, apparently, we can see that larger encoder size helps more than larger decoder size does (rows 7,8, and 9 in Table 4).
We present the Table 4 in the supplementary material and we summarise it as follows: 1. Decoding the next sentence performed similarly to decoding the subsequent contiguous words. 2. Decoding the subsequent 30 words, which was adopted from the skip-thought training code3, gave reasonably good performance. More words for decoding didn't give us a significant performance gain, and took longer to train. 3. Adding more layers into the decoder and enlarging the dimension of the convolutional layers indeed sightly improved the performance on the three downstream tasks, but as training efficiency is one of our main concerns, it wasn't worth sacrificing training efficiency for the minor performance gain. 4. Increasing the dimensionality of the RNN encoder improved the model performance, and the additional training time required was less than needed for increasing the complexity in the CNN decoder. We report results from both smallest and largest models in Table 2.  As the transferability of the models trained in both cases perform similarly on the evaluation tasks (see rows 1 and 2 in Table 4), we focus on the simpler predictall-words CNN decoder that learns to reconstruct the next window of contiguous words.  As stated in rows 1, 3, and 4 in Table 4, decoding short target sequences results in a slightly lower Pearson score on SICK, and decoding longer target sequences lead to a longer training time.  We tweaked the CNN encoder, including different kernel size and activation function, and we report the best results of CNNCNN model at row 6 in Table 4.  The future predictor in (Gan et al., 2017) also applies a CNN as the encoder, but the decoder is still an RNN, listed at row 11 in Table 4. Compared to our designed CNN-CNN model, their CNN-LSTM model contains more parameters than our model does, but they have similar performance on the evaluation tasks, which is also worse than our RNNCNN model.  Clearly, we can tell from the comparison between rows 1, 9 and 12 in Table 4, increasing the dimensionality of the RNN encoder leads to better transferability of the model.  Compared with the model with larger-size CNN decoder, apparently, we can see that larger encoder size helps more than larger decoder size does (rows 7,8, and 9 in Table 4).
As shown in Table 2, sub-word systems outperform full-word systems across the board, despite having fewer total parameters. Systems built on larger data generally benefit from larger vocabularies while smaller systems perform better with smaller vocabularies.
As shown in Table 2, sub-word systems outperform full-word systems across the board, despite having fewer total parameters. Systems built on larger data generally benefit from larger vocabularies while smaller systems perform better with smaller vocabularies.
The results of the pre-selection are reported in Table 1. All syllable-aware models comfortably outperform the Char-CNN when the budget is limited to 5M parameters. Surprisingly, a pure word-level model,6 LSTM-Word, also beats the character-aware one under such budget. The three best configurations are Syl-Concat, Syl-Sum, and Syl-CNN-
The results of the pre-selection are reported in Table 1. All syllable-aware models comfortably outperform the Char-CNN when the budget is limited to 5M parameters. Surprisingly, a pure word-level model,6 LSTM-Word, also beats the character-aware one under such budget. The three best configurations are Syl-Concat, Syl-Sum, and Syl-CNN-
The results of evaluating these three models on small (1M tokens) and medium-sized (17M– 57M tokens) data sets against Char-CNN for different languages are provided in Table 3. The models demonstrate similar performance on small data, but Char-CNN scales significantly better on medium-sized data. From the three syllable-aware models, Syl-Concat looks the most advantageous as it demonstrates stable results and has the least  number of parameters. Therefore in what follows we will make a more detailed comparison of SylConcat with Char-CNN.
To find out whether this was the case we replaced the LSTM by a Variational RHN (Zilly et al., 2017), and that resulted in a significant reduction of perplexities on PTB for both Char-CNN and Syl-Concat (Table 5). Moreover, increasing dLM from 439 to 650 did result in better performance for Syl-Concat.
To find out whether this was the case we replaced the LSTM by a Variational RHN (Zilly et al., 2017), and that resulted in a significant reduction of perplexities on PTB for both Char-CNN and Syl-Concat (Table 5). Moreover, increasing dLM from 439 to 650 did result in better performance for Syl-Concat.
On a support basis, we find a 52% increase in Kappa by adding the titles.  Cohen's Kappa is 68% higher than that for ESIM.
On a support basis, we find a 52% increase in Kappa by adding the titles.  Cohen's Kappa is 68% higher than that for ESIM.
makes the FEVER Title Five Oracle performance better than FEVER Title Five.  The Transformer model is accurate enough that oracle guessing does not help.
makes the FEVER Title Five Oracle performance better than FEVER Title Five.  The Transformer model is accurate enough that oracle guessing does not help.
The named entity retrieval strategy boosts the evidence retrieval rate to 80.8%,  The film retrievals raise evidence retrieval to 81.2%.
The named entity retrieval strategy boosts the evidence retrieval rate to 80.8%,  The film retrievals raise evidence retrieval to 81.2%.
Limiting evidence in this way when only five statements are retrieved ("narrow evidence" in Table 4) pushes FEVER score down very little, to .5550 from .5617 on the development set,  Indeed, when the system reviews the extra evidence, FEVER score goes up to .5844 on the development set.
Limiting evidence in this way when only five statements are retrieved ("narrow evidence" in Table 4) pushes FEVER score down very little, to .5550 from .5617 on the development set,  Indeed, when the system reviews the extra evidence, FEVER score goes up to .5844 on the development set.
Then, this projection is applied to the second model embeddings of the 47 locations, which are subject to armed conflicts in the year 2001 (38 after skipping pairs with outof-vocabulary elements). Table 2 demonstrates the resulting performance (reflecting how close the predicted vectors are to the actual armed groups active in this or that location). Note that out of 38 pairs from 2001, 31 were already present in the previous data set (ongoing conflicts). This explains why the evaluation on all the pairs gives high results. However, even for the new conflicts, the projection performance is encouraging.
Then, this projection is applied to the second model embeddings of the 47 locations, which are subject to armed conflicts in the year 2001 (38 after skipping pairs with outof-vocabulary elements). Table 2 demonstrates the resulting performance (reflecting how close the predicted vectors are to the actual armed groups active in this or that location). Note that out of 38 pairs from 2001, 31 were already present in the previous data set (ongoing conflicts). This explains why the evaluation on all the pairs gives high results. However, even for the new conflicts, the projection performance is encouraging.
Table 3 presents the results for these experiments, as well as baselines (averaged across 15 years). For the proposed incr. dynamic approach, the performance of the previous projections is  comparable to that of the up-to-now projections on the accuracies @5 and @10, and is even higher on the accuracy @1 (statistically significant with t-test, p < 0.01). Thus, the single-year projections are somewhat more 'focused', while taking much less time to learn, because of less training pairs. The fact that our models were incrementally updated, not trained from scratch, is crucial. The results of the separate baseline look more like random jitter. The cumulative baseline results are slightly better, probably simply because they are trained on more data. However, they still perform much worse than the models trained using incremental updates. This is because the former models are not connected to each other, and thus are initialized with a different layout of words in the vector space. This gives rise to formally different directions of semantic relations in each yearly model (the relations themselves are still there, but they are rotated and scaled differently). The results for the incr. static baseline, when tested only on the words present in the test model vocabulary (the left part of the table), seem better than those of the proposed incr. dynamic approach. This stems from the fact that incremental updating with static vocabulary means that we never add new words to the models; thus, they contain only the vocabulary learned from the 1994 texts. The result is that at test time we skip many more pairs than with the other approaches (about 62% in average). Subsequently, the projections are tested only on a minor part of the test sets. Of course, skipping large parts of the data would be a major drawback for any realistic application, so the incr. static baseline is not really plausible. For comparison, the right part of Table 3 provides the accuracies for the setup in which all the pairs are evaluated (for pairs with OOV words the accuracy is always 0). Other tested approaches are not much affected by this change, but for incr. static the performance drops drastically. As a result, for the all pairs scenario, incremental updating with vocabulary expansion outperforms all the baselines (the differences are statistically significant with t-test, p < 0.01).
Table 3 presents the results for these experiments, as well as baselines (averaged across 15 years). For the proposed incr. dynamic approach, the performance of the previous projections is  comparable to that of the up-to-now projections on the accuracies @5 and @10, and is even higher on the accuracy @1 (statistically significant with t-test, p < 0.01). Thus, the single-year projections are somewhat more 'focused', while taking much less time to learn, because of less training pairs. The fact that our models were incrementally updated, not trained from scratch, is crucial. The results of the separate baseline look more like random jitter. The cumulative baseline results are slightly better, probably simply because they are trained on more data. However, they still perform much worse than the models trained using incremental updates. This is because the former models are not connected to each other, and thus are initialized with a different layout of words in the vector space. This gives rise to formally different directions of semantic relations in each yearly model (the relations themselves are still there, but they are rotated and scaled differently). The results for the incr. static baseline, when tested only on the words present in the test model vocabulary (the left part of the table), seem better than those of the proposed incr. dynamic approach. This stems from the fact that incremental updating with static vocabulary means that we never add new words to the models; thus, they contain only the vocabulary learned from the 1994 texts. The result is that at test time we skip many more pairs than with the other approaches (about 62% in average). Subsequently, the projections are tested only on a minor part of the test sets. Of course, skipping large parts of the data would be a major drawback for any realistic application, so the incr. static baseline is not really plausible. For comparison, the right part of Table 3 provides the accuracies for the setup in which all the pairs are evaluated (for pairs with OOV words the accuracy is always 0). Other tested approaches are not much affected by this change, but for incr. static the performance drops drastically. As a result, for the all pairs scenario, incremental updating with vocabulary expansion outperforms all the baselines (the differences are statistically significant with t-test, p < 0.01).
We compared our three proposed models for the three loss functions Lτ , Lτ ce, and Lτ h, and their linear (unweighted) combination L∗, on TE3‡ and TD‡, for which the results are shown in Table 4.  A trend that can be observed is that overall performance on TD‡ is higher than that of TE3‡  If we compare loss functions Lτ , Lτ ce, and Lτh, and combination L∗, it can be noticed that, although all loss functions seem to give fairly similar performance, Lτ gives the most robust results (never lowest), especially noticeable for the smaller dataset TD‡.  The combination of losses L∗ shows mixed results, and has lower performance for S-TLM and C-TLM, but better performance for TL2RTL.  Moreover, we can clearly see that on TE3‡, CTLM performs better than the indirect models, across all loss functions.  On TD‡, the indirect models seem to perform slightly better.  the difference between C-TLM and S-TLM is small on the smaller TD‡  on TE3‡, the larger dataset, C-TLM clearly outperforms S-TLM across all loss functions,
We compared our three proposed models for the three loss functions Lτ , Lτ ce, and Lτ h, and their linear (unweighted) combination L∗, on TE3‡ and TD‡, for which the results are shown in Table 4.  A trend that can be observed is that overall performance on TD‡ is higher than that of TE3‡  If we compare loss functions Lτ , Lτ ce, and Lτh, and combination L∗, it can be noticed that, although all loss functions seem to give fairly similar performance, Lτ gives the most robust results (never lowest), especially noticeable for the smaller dataset TD‡.  The combination of losses L∗ shows mixed results, and has lower performance for S-TLM and C-TLM, but better performance for TL2RTL.  Moreover, we can clearly see that on TE3‡, CTLM performs better than the indirect models, across all loss functions.  On TD‡, the indirect models seem to perform slightly better.  the difference between C-TLM and S-TLM is small on the smaller TD‡  on TE3‡, the larger dataset, C-TLM clearly outperforms S-TLM across all loss functions,
Table 1 presents our results. The self-distance f the eBay, Legal Onion and Illegal Onion corpora lies between 0.40 to 0.45 by the JensenShannon divergence, but the distance between each pair is 0.60 to 0.65, with the three approximately forming an equilateral triangle in the space of word distributions. Similar results are obtained using Variational distance, and are omitted for brevity.
Table 1 presents our results. The self-distance f the eBay, Legal Onion and Illegal Onion corpora lies between 0.40 to 0.45 by the JensenShannon divergence, but the distance between each pair is 0.60 to 0.65, with the three approximately forming an equilateral triangle in the space of word distributions. Similar results are obtained using Variational distance, and are omitted for brevity.
each average. According to our results (Table 2), the Wikification success ratios of eBay and Illegal  Onion named entities is comparable and relatively low. However, sites selling legal drugs on Onion have a much higher Wikification percentage.
each average. According to our results (Table 2), the Wikification success ratios of eBay and Illegal  Onion named entities is comparable and relatively low. However, sites selling legal drugs on Onion have a much higher Wikification percentage.
The two top rows in Table 1 show conflicting results for UKB.  As the results show, that paper reports a suboptimal use of UKB.  the table also reports the  best performing knowledge-based systems on this dataset.  also report We (Chaplot and Sakajhutdinov, 2018), the latest work on this area, as well as the most frequent sense as given by WordNet counts  The table shows that UKB yields the best overall result.
Table 2 reports the results of supervised systems on the same dataset, taken from the two works that use the dataset  As expected, supervised systems outperform knowledge-based systems, by a small margin in some of the cases.
Table 2 reports the results of supervised systems on the same dataset, taken from the two works that use the dataset  As expected, supervised systems outperform knowledge-based systems, by a small margin in some of the cases.
Ta  The table shows that the key factor is the use of sense frequencies, and systems that do not use them (those with a nf subscript) suffer a loss between 7 and 8 percentage points in F1.  The table also shows that extending the context is mildly effective.  Regarding the algorithm, the table confirms that the best method is ppr w2w, followed by the subgraph approach (dfs) and ppr.
Ta  The table shows that the key factor is the use of sense frequencies, and systems that do not use them (those with a nf subscript) suffer a loss between 7 and 8 percentage points in F1.  The table also shows that extending the context is mildly effective.  Regarding the algorithm, the table confirms that the best method is ppr w2w, followed by the subgraph approach (dfs) and ppr.
We compare the performance of our model (Table 2) with traditional Bag of Words (BoW), TF-IDF, and n-grams features based classifiers. We also compare against averaged Skip-Gram (Mikolov et al., 2013), Doc2Vec (Le and Mikolov, 2014), CNN (Kim, 2014), Hierarchical Attention (HN-ATT) (Yang et al., 2016) and hierarchical network (HN) models.
We compare the performance of our model (Table 2) with traditional Bag of Words (BoW), TF-IDF, and n-grams features based classifiers. We also compare against averaged Skip-Gram (Mikolov et al., 2013), Doc2Vec (Le and Mikolov, 2014), CNN (Kim, 2014), Hierarchical Attention (HN-ATT) (Yang et al., 2016) and hierarchical network (HN) models.
Table 1 presents the exact matching accuracy of IRNet and various baselines on the development set and the test set. IRNet clearly outperforms all the baselines by a substantial margin. It obtains 27.0% absolute improvement over SyntaxSQLIt also obtains 19.5% absolute Net on test set. improvement over SyntaxSQLNet(augment) that performs large-scale data augmentation. When incorporating BERT, the performance of both SyntaxSQLNet and IRNet is substantially improved and the accuracy gap between them on both the development set and the test set is widened.
Table 1 presents the exact matching accuracy of IRNet and various baselines on the development set and the test set. IRNet clearly outperforms all the baselines by a substantial margin. It obtains 27.0% absolute improvement over SyntaxSQLIt also obtains 19.5% absolute Net on test set. improvement over SyntaxSQLNet(augment) that performs large-scale data augmentation. When incorporating BERT, the performance of both SyntaxSQLNet and IRNet is substantially improved and the accuracy gap between them on both the development set and the test set is widened.
As shown in Table 2, IRNet significantly outperforms SyntaxSQLNet in all four hardness levels with or without BERT. For example, compared with SyntaxSQLNet, IRNet obtains 23.3% absolute improvement in Hard level.
As shown in Table 2, IRNet significantly outperforms SyntaxSQLNet in all four hardness levels with or without BERT. For example, compared with SyntaxSQLNet, IRNet obtains 23.3% absolute improvement in Hard level.
As shown in Table 3, there are at least 6.6% and up to 14.4% absolute improvements on accuracy of exact matching on the development set. For example, when SyntaxSQLNet is learned to generate SemQL queries instead of SQL queries, it registers 8.6% absolute improvement and even outperforms SyntaxSQLNet(augment) which performs largescale data augmentation.  Table 4 presents the ablation study results. It is clear that our base model significantly outperforms SyntaxSQLNet, SyntaxSQLNet( augment) and SyntaxSQLNet(BERT). Performing schema linking ('+SL') brings about 8.5% and 6.4% absolute improvement on IRNet and IRNet(BERT).  The F1 score on the WHERE clause increases by 12.5% when IRNet performs schema linking.  The number of examples suffering from this problem decreases by 70%, when using the memory augmented pointer network.
As shown in Table 3, there are at least 6.6% and up to 14.4% absolute improvements on accuracy of exact matching on the development set. For example, when SyntaxSQLNet is learned to generate SemQL queries instead of SQL queries, it registers 8.6% absolute improvement and even outperforms SyntaxSQLNet(augment) which performs largescale data augmentation.  Table 4 presents the ablation study results. It is clear that our base model significantly outperforms SyntaxSQLNet, SyntaxSQLNet( augment) and SyntaxSQLNet(BERT). Performing schema linking ('+SL') brings about 8.5% and 6.4% absolute improvement on IRNet and IRNet(BERT).  The F1 score on the WHERE clause increases by 12.5% when IRNet performs schema linking.  The number of examples suffering from this problem decreases by 70%, when using the memory augmented pointer network.
Table 3 summarizes the performance of the three models on the SUPPORTS and REFUTES pairs from the FEVER DEV set and on the created SYMMETRIC TEST SET pairs.  All models perform relatively well on FEVER DEV but achieve less than 60% accuracy on the synthetic ones.  The re-weighting method increases the accuracy of the ESIM and BERT models by an absolute 3.4% and 3.3% respectively. One can notice that this improvement comes at a cost in the accuracy over the FEVER DEV pairs.  Applying the regularization method, using the same training data, helps to train a more robust model that performs better on our test set, where verification in context is a key requirement.
Table 3 summarizes the performance of the three models on the SUPPORTS and REFUTES pairs from the FEVER DEV set and on the created SYMMETRIC TEST SET pairs.  All models perform relatively well on FEVER DEV but achieve less than 60% accuracy on the synthetic ones.  The re-weighting method increases the accuracy of the ESIM and BERT models by an absolute 3.4% and 3.3% respectively. One can notice that this improvement comes at a cost in the accuracy over the FEVER DEV pairs.  Applying the regularization method, using the same training data, helps to train a more robust model that performs better on our test set, where verification in context is a key requirement.
Table 7 and Table 8 summarize the top 10 bigrams for SUPPORT and NOT ENOUGH INFO. The correlation between the biased phrases in the two dataset splits is not as strong as in the REFUTE label, presented in the paper.  However, one can notice that some of the biased bigrams in the training set, such as "least one" and "starred movie", translate to cues that can help in predictions over the development set.
report brief results on SQuAD (Table 3),  We also evaluate the SynNet on the NewsQAto-SQuAD direction. We directly apply the best setting from the other direction and report the result in Table 3.  The SynNet improves over the baseline by 1.6% in EM and 0.7% in F1. Limited by space, we leave out ablation studies in this direction.
report brief results on SQuAD (Table 3),  We also evaluate the SynNet on the NewsQAto-SQuAD direction. We directly apply the best setting from the other direction and report the result in Table 3.  The SynNet improves over the baseline by 1.6% in EM and 0.7% in F1. Limited by space, we leave out ablation studies in this direction.
Table 3 shows the performance of baselines against user simulator and human on the two datasets.  Equation (11)) is also proven effective in deliver  ing more achievement, which can be seen from the second and last rows of Table 3.  The results are consistent with those in Table 3.
Table 3 shows the performance of baselines against user simulator and human on the two datasets.  Equation (11)) is also proven effective in deliver  ing more achievement, which can be seen from the second and last rows of Table 3.  The results are consistent with those in Table 3.
Table 2: Correlation between Attribution word importance with POS tags, Fertility, and Syntactic Depth. Fertility can be categorized into 4 types: one-to-many ("≥ 2"), one-to-one ("1"), many-to-one ("(0, 1)"), and null-aligned ("0"). Syntactic depth shows the depth of a word in the dependency tree. A lower tree depth indicates closer to the root node in the dependency tree, which might indicate a more important word.
Table 2: Correlation between Attribution word importance with POS tags, Fertility, and Syntactic Depth. Fertility can be categorized into 4 types: one-to-many ("≥ 2"), one-to-one ("1"), many-to-one ("(0, 1)"), and null-aligned ("0"). Syntactic depth shows the depth of a word in the dependency tree. A lower tree depth indicates closer to the root node in the dependency tree, which might indicate a more important word.
Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method's result is significantly better than all baseline methods, † indicates that the method's result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S,  and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).
Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method's result is significantly better than all baseline methods, † indicates that the method's result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S,  and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).
conduct ablation studies (Table 4),  To better understand how various components in our training procedure and model impact overall performance we conduct several ablation studies, as summarized in Table 4.  Results in Table 4(A) show that using human-annotated answers to generate questions leads to a significant performance boost over using answers from an answer generation module.  This supports the hypothesis that the answers humans choose to generate questions for provide important linguistic cues for finetuning the machine comprehension model.  To see how copying impacts performance, we explore using the entire paragraph to generate the question vs. only the two sentences before and one sentence after the answer span and report results in Table 4(B).
conduct ablation studies (Table 4),  To better understand how various components in our training procedure and model impact overall performance we conduct several ablation studies, as summarized in Table 4.  Results in Table 4(A) show that using human-annotated answers to generate questions leads to a significant performance boost over using answers from an answer generation module.  This supports the hypothesis that the answers humans choose to generate questions for provide important linguistic cues for finetuning the machine comprehension model.  To see how copying impacts performance, we explore using the entire paragraph to generate the question vs. only the two sentences before and one sentence after the answer span and report results in Table 4(B).
Table 2 gives single model validation scores for es2en and en2es models with standard and iterative transfer learning. We find that the all-biomed domain gains 1-2 BLEU points from transfer learning.
Table 2 gives single model validation scores for es2en and en2es models with standard and iterative transfer learning. We find that the all-biomed domain gains 1-2 BLEU points from transfer learning.
For de2en and es2en, uniform ensembling performs similarly to the oracles, and performs similarly to BI.  We submitted three runs to the WMT19 biomedical task for each language pair: the best single all-biomed model, a uniform ensemble of models on two en-de and three es-en domains, and an ensemble with Bayesian Interpolation. Tables 3 and 4 give validation and test scores.  that a uniform multi-domain ensemble performs well, giving 0.5-1.2 BLEU improvement on the test set over strong single models.  We see small gains from using BI with ensembles on most validation sets, but only on en2es test.  we noted that, in general, we could predict BI (α = 0.5) performance by comparing the uniform ensemble with the oracle model performing best on each validation domain.  For en2es uniform ensembling underperforms the health and bio oracle models on their validation sets, and the uniform ensemble slightly underperforms BI on the test data.
For de2en and es2en, uniform ensembling performs similarly to the oracles, and performs similarly to BI.  We submitted three runs to the WMT19 biomedical task for each language pair: the best single all-biomed model, a uniform ensemble of models on two en-de and three es-en domains, and an ensemble with Bayesian Interpolation. Tables 3 and 4 give validation and test scores.  that a uniform multi-domain ensemble performs well, giving 0.5-1.2 BLEU improvement on the test set over strong single models.  We see small gains from using BI with ensembles on most validation sets, but only on en2es test.  we noted that, in general, we could predict BI (α = 0.5) performance by comparing the uniform ensemble with the oracle model performing best on each validation domain.  For en2es uniform ensembling underperforms the health and bio oracle models on their validation sets, and the uniform ensemble slightly underperforms BI on the test data.
We see small gains from using BI with ensembles on most validation sets, but only on en2es test.  For en2de, by contrast, uniform ensembling is consistently better than oracles on the dev sets, and outperforms BI on the test data.
We see small gains from using BI with ensembles on most validation sets, but only on en2es test.  For en2de, by contrast, uniform ensembling is consistently better than oracles on the dev sets, and outperforms BI on the test data.
Consequently in Table 5 we experiment with BI (α = 0.1). In this case BI matches or out-performs the uniform ensemble. Notably, for en2es, where BI (α = 0.5) performed well, taking α = 0.1 does not harm performance.
Consequently in Table 5 we experiment with BI (α = 0.1). In this case BI matches or out-performs the uniform ensemble. Notably, for en2es, where BI (α = 0.5) performed well, taking α = 0.1 does not harm performance.
We then tested the BLEU scores between machine translation results and corresponding gold standard post-editing results on the original development set, the training set and the synthetic data as shown in Table 1.  Table 1 shows that there is a significant gap between the synthetic eScape data set (Negri et al., 2018) and the real-life data sets (the development set and the original training set from posteditors),
We then tested the BLEU scores between machine translation results and corresponding gold standard post-editing results on the original development set, the training set and the synthetic data as shown in Table 1.  Table 1 shows that there is a significant gap between the synthetic eScape data set (Negri et al., 2018) and the real-life data sets (the development set and the original training set from posteditors),
Even the ensemble of 5 models did not result in significant differences especially in BLEU scores.
Even the ensemble of 5 models did not result in significant differences especially in BLEU scores.
Table 2 shows that the performance got slightly hurt (comparing "Processed MT" with "MT as PE") with pre-processing and post-processing procedures which are normally applied in training seq2seq models for reducing vocabulary size. The multi-source transformer (Base) model achieved the highest single model BLEU score without joint training with the de-noising encoder task.  Even with the ensembled model, our APE approach does not significantly improve machine translation outputs measured in BLEU (+0.46).
Table 2 shows that the performance got slightly hurt (comparing "Processed MT" with "MT as PE") with pre-processing and post-processing procedures which are normally applied in training seq2seq models for reducing vocabulary size. The multi-source transformer (Base) model achieved the highest single model BLEU score without joint training with the de-noising encoder task.  Even with the ensembled model, our APE approach does not significantly improve machine translation outputs measured in BLEU (+0.46).
We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design,  Our analysis shows that the most important factor is how workers are primed for a task, with the choice of examples and the prompt sentence affecting diversity and correctness significantly.  There was relatively little variation in grammaticality or time across the conditions.  Priming had a major impact, with the shift to lexical examples leading to a significant improvement in correctness, but much lower diversity. The surprising increase in correctness when providing no examples  changing the incentives by providing either a bonus for novelty, or no bonus at all, did not substantially impact any of the metrics.  Changing the number of paraphrases written by each worker did not significantly impact diversity (we worried that collecting more than one may lead to a decrease).  the One Paraphrase condition did have lower grammaticality,  Changing the source of the prompt sentence to create a chain of paraphrases led to a significant increase in diversity.  showing the answer to the question being para  phrased did not significantly affect correctness or diversity,
We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design,  Our analysis shows that the most important factor is how workers are primed for a task, with the choice of examples and the prompt sentence affecting diversity and correctness significantly.  There was relatively little variation in grammaticality or time across the conditions.  Priming had a major impact, with the shift to lexical examples leading to a significant improvement in correctness, but much lower diversity. The surprising increase in correctness when providing no examples  changing the incentives by providing either a bonus for novelty, or no bonus at all, did not substantially impact any of the metrics.  Changing the number of paraphrases written by each worker did not significantly impact diversity (we worried that collecting more than one may lead to a decrease).  the One Paraphrase condition did have lower grammaticality,  Changing the source of the prompt sentence to create a chain of paraphrases led to a significant increase in diversity.  showing the answer to the question being para  phrased did not significantly affect correctness or diversity,
In Table 3 and Figure 1, the results of the experiment are presented. Triframes based on WATSET clustering outperformed the other methods on both Verb F1 and overall Frame F1.  the use of the WATSET fuzzy clustering algorithm that splits the hubs by disambiguating them leads to the best results (see Table 3).
In Table 3 and Figure 1, the results of the experiment are presented. Triframes based on WATSET clustering outperformed the other methods on both Verb F1 and overall Frame F1.  the use of the WATSET fuzzy clustering algorithm that splits the hubs by disambiguating them leads to the best results (see Table 3).
Table 4 presents results on the second dataset for the best models identified on the first dataset. The LDA-Frames yielded the best results with our approach performing comparably in terms of the F1-score.
Table 4 presents results on the second dataset for the best models identified on the first dataset. The LDA-Frames yielded the best results with our approach performing comparably in terms of the F1-score.
Table 1 shows these performance measures for French-English,  using global structure greatly improves upon the state of the art baseline performance.
Table 1 shows these performance measures for French-English,  using global structure greatly improves upon the state of the art baseline performance.
Tables 4 through 6 show the summary metrics for the three language pairs for the large data experiments.  We can see that the reverse rank and forward rank methods of taking into account the global structure of interactions among predictions is still helpful, providing large improvements in performance even in this challenging large data condition over strong state of the art baselines
five source and six target NER data sets, each selected to provide a range of fields (i.e., biology, computer science, medications, local business) and tenors (i.e., encyclopedia articles, journal articles, experimental protocols, online reviews).  We use five data sets as source data, covering a range of fields (i.e., clinical, biomedical, local business and Wiki with diverse fields) and tenors (i.e., popular reporting, notes, scholarly publications, online reviews and encyclopedia).  Details of these target data are listed in Table 2.
five source and six target NER data sets, each selected to provide a range of fields (i.e., biology, computer science, medications, local business) and tenors (i.e., encyclopedia articles, journal articles, experimental protocols, online reviews).  We use five data sets as source data, covering a range of fields (i.e., clinical, biomedical, local business and Wiki with diverse fields) and tenors (i.e., popular reporting, notes, scholarly publications, online reviews and encyclopedia).  Details of these target data are listed in Table 2.
The results in Table 4 show that our proposed similarity measures are predictive of the effectiveness of the pretraining data.  VCcR is the most informative factor in predicting the effectiveness of pretrained word vectors given a target data set.
The results in Table 4 show that our proposed similarity measures are predictive of the effectiveness of the pretraining data.  VCcR is the most informative factor in predicting the effectiveness of pretrained word vectors given a target data set.
We find that word vectors and LMs pretrained on small similar sources can achieve competitive or even better performance than the ones pretrained on larger sources (Table 5).  On JNLPBA, ScienceIE and Wetlab, LMs pretrained on the small similar source perform better, while word vectors pretrained on the small similar source perform better on CRAFT, JNLPBA, and ScienceIE.
We find that word vectors and LMs pretrained on small similar sources can achieve competitive or even better performance than the ones pretrained on larger sources (Table 5).  On JNLPBA, ScienceIE and Wetlab, LMs pretrained on the small similar source perform better, while word vectors pretrained on the small similar source perform better on CRAFT, JNLPBA, and ScienceIE.
Our results suggest that this hyper-parameter setting can overall (except Wiki-ScienceIE and MIMIC-WetLab pairs) produce better performance compare to the default setting (Table 6).
Most errors in our annotated corpus are related to person deixis, specifically gender marking in the Russian translation, and the T-V distinction between informal and formal you (Latin "tu" and "vos").  From Table 3, we see that the most frequent error category related to deixis in our annotated corpus is the inconsistency of T-V forms when translating second person pronouns.
In Table 1 we can see that also for our model, EE relations are harder to recognize than the TE relations, as all models achieve higher scores for TE compared to EE relations.  What is interesting to see is that when training with the combined loss (SG or SGLR) we obtain a clear improvement on the more difficult EE relations, and perform slightly worse on TE relations compared to using pre-trained embeddings (the three upper settings).  What can be observed is that the RC+SG model performs best for low-frequency words, and RC+SGLR performs  best for the higher frequency ranges.  When evaluating on the full Dev set, both combined loss settings outperform the baselines consistently.
In Table 1 we can see that also for our model, EE relations are harder to recognize than the TE relations, as all models achieve higher scores for TE compared to EE relations.  What is interesting to see is that when training with the combined loss (SG or SGLR) we obtain a clear improvement on the more difficult EE relations, and perform slightly worse on TE relations compared to using pre-trained embeddings (the three upper settings).  What can be observed is that the RC+SG model performs best for low-frequency words, and RC+SGLR performs  best for the higher frequency ranges.  When evaluating on the full Dev set, both combined loss settings outperform the baselines consistently.
Table 2 shows that initializing the model with the pre-trained embeddings gives a significant 4 1.1 point increase in F-measure compared to random initialization, due to an increase in precision.  Fixing the embeddings gives slightly better performance than using them as initialization, an increase of 0.9 point in F-measure, mostly due to higher recall.  When extending the loss with the SGLR loss, we gain6 1.6 in F-measure compared to fixing the word embeddings, and also surpass the state of the art by 0.4 even without specialized resources.  If we train our model using the SG loss extension we obtain the best results, and gain6 1.9 points in F-measure compared to using pre-trained fixed word embeddings.  This setting also exceeds the state of the art (Lin et al., 2017) by 0.7 points in F-measure, due to a gain of 1.2 points in recall, again without using any specialized clinical NLP tools for feature engineering, in contrast to all state-of-the-art baselines.
Table 2 shows that initializing the model with the pre-trained embeddings gives a significant 4 1.1 point increase in F-measure compared to random initialization, due to an increase in precision.  Fixing the embeddings gives slightly better performance than using them as initialization, an increase of 0.9 point in F-measure, mostly due to higher recall.  When extending the loss with the SGLR loss, we gain6 1.6 in F-measure compared to fixing the word embeddings, and also surpass the state of the art by 0.4 even without specialized resources.  If we train our model using the SG loss extension we obtain the best results, and gain6 1.9 points in F-measure compared to using pre-trained fixed word embeddings.  This setting also exceeds the state of the art (Lin et al., 2017) by 0.7 points in F-measure, due to a gain of 1.2 points in recall, again without using any specialized clinical NLP tools for feature engineering, in contrast to all state-of-the-art baselines.
From Table 3 we can see that all models have difficulties with distant relations that cross sentence or clause boundaries (CCR).  Furthermore, arguments that are frequent in the supervised data (> 250) are a dominant error category.  Furthermore it can be noticed that RC+SG has less errors for infrequent arguments (< 10) in the supervised data.
From Table 3 we can see that all models have difficulties with distant relations that cross sentence or clause boundaries (CCR).  Furthermore, arguments that are frequent in the supervised data (> 250) are a dominant error category.  Furthermore it can be noticed that RC+SG has less errors for infrequent arguments (< 10) in the supervised data.
In Table 2 we report the results, that we compute as the average of ten runs with random parameter initialization.9 We use the unpaired Welch's t test to check for statistically significant difference between models.  The results show that social information helps improve the performance on Stance and Hate Speech detection, while it has no effect for Sentiment Analysis.  LING+random never improves over LING:  We find that both PV and N2V user representations lead to an improvement over LING.  where LING+N2V outperforms LING+PV,  while for Hate Speech the performance of the two models is comparable (the difference between LING+PV and LING+N2V is not statistically significative due to the high variance of the LING+PV results - see extended results table in the supplementary material).  our model outperforms any other model on both Stance and Hate Speech detection.
In Table 2 we report the results, that we compute as the average of ten runs with random parameter initialization.9 We use the unpaired Welch's t test to check for statistically significant difference between models.  The results show that social information helps improve the performance on Stance and Hate Speech detection, while it has no effect for Sentiment Analysis.  LING+random never improves over LING:  We find that both PV and N2V user representations lead to an improvement over LING.  where LING+N2V outperforms LING+PV,  while for Hate Speech the performance of the two models is comparable (the difference between LING+PV and LING+N2V is not statistically significative due to the high variance of the LING+PV results - see extended results table in the supplementary material).  our model outperforms any other model on both Stance and Hate Speech detection.
An overview of the language pairs as well as the amount of annotated parallel sentences per language pair is given in Table 1.  Overview of annotated parallel sentences per language pair
An overview of the language pairs as well as the amount of annotated parallel sentences per language pair is given in Table 1.  Overview of annotated parallel sentences per language pair
We classified ellipsis examples which lead to errors in sentence-level translations by the type of error they cause. Results are provided in Table 4.  From Table 4, we see that the two most frequent types of ambiguity caused by the presence of an elliptical structure have different nature, hence we construct individual test sets for each of them.
We classified ellipsis examples which lead to errors in sentence-level translations by the type of error they cause. Results are provided in Table 4.  From Table 4, we see that the two most frequent types of ambiguity caused by the presence of an elliptical structure have different nature, hence we construct individual test sets for each of them.
The main results of our experiments are shown in Table 1.  We notice that M.1 (Bag-of-words + Logistic Regression) and M.2 (BiLSTM) show a statistically significant difference between the two genders, with higher predicted positive class probabilities for sentences with female nouns.  On the contrary, M.3 (BERT) shows that sentences with male nouns have a statistically significant higher predicted positive class probability than sentences with female nouns.
The main results of our experiments are shown in Table 1.  We notice that M.1 (Bag-of-words + Logistic Regression) and M.2 (BiLSTM) show a statistically significant difference between the two genders, with higher predicted positive class probabilities for sentences with female nouns.  On the contrary, M.3 (BERT) shows that sentences with male nouns have a statistically significant higher predicted positive class probability than sentences with female nouns.
Because of the small size of the dataset, we report the average performance over 5 runs with different random seeds.  Table 1 shows the results. Compared with WM18, our RGB model outperforms under all conditions.  According to the cosine similarity, the HSV model is superior for most test conditions (confirming our hypothesis about simpler modifier behaviour in this space). However for Delta-E, the RGB model and ensemble perform better. Unlike cosine, Delta-E is sensitive to differences in vector length, and we would argue it is the most appropriate metric because lengths are critical to measuring the extent of lightness and darkness of colors. Accordingly the HSV model does worse under this metric, as it more directly models the direction of color modifiers, but as a consequence this leads to errors in its length predictions. Over- all the ensemble does well according to both met rics, and has the best performance for several test conditions with Delta-E.
Because of the small size of the dataset, we report the average performance over 5 runs with different random seeds.  Table 1 shows the results. Compared with WM18, our RGB model outperforms under all conditions.  According to the cosine similarity, the HSV model is superior for most test conditions (confirming our hypothesis about simpler modifier behaviour in this space). However for Delta-E, the RGB model and ensemble perform better. Unlike cosine, Delta-E is sensitive to differences in vector length, and we would argue it is the most appropriate metric because lengths are critical to measuring the extent of lightness and darkness of colors. Accordingly the HSV model does worse under this metric, as it more directly models the direction of color modifiers, but as a consequence this leads to errors in its length predictions. Over- all the ensemble does well according to both met rics, and has the best performance for several test conditions with Delta-E.
BLEU scores for our model and the baselines are given in Table 6.5 For context-aware models, all sentences in a group were translated, and then only the current sentence is evaluated. We also report BLEU for the context-agnostic baseline trained only on 1.5m dataset to show how the performance is influenced by the amount of data. We observe that our model is no worse in BLEU than the baseline despite the second-pass model  being trained only on a fraction of the data. In contrast, the concatenation baseline, trained on a mixture of data with and without context is about 1 BLEU below the context-agnostic baseline and our model when using all 3 context sentences. CADec's performance remains the same independently from the number of context sentences (1, 2 or 3) as measured with BLEU. s-hier-to-2.tied performs worst in terms of BLEU, but note that this is a shallow recurrent model, while others are Transformer-based. It also suffers from the asymmetric data setting, like the concatenation baseline.
BLEU scores for our model and the baselines are given in Table 6.5 For context-aware models, all sentences in a group were translated, and then only the current sentence is evaluated. We also report BLEU for the context-agnostic baseline trained only on 1.5m dataset to show how the performance is influenced by the amount of data. We observe that our model is no worse in BLEU than the baseline despite the second-pass model  being trained only on a fraction of the data. In contrast, the concatenation baseline, trained on a mixture of data with and without context is about 1 BLEU below the context-agnostic baseline and our model when using all 3 context sentences. CADec's performance remains the same independently from the number of context sentences (1, 2 or 3) as measured with BLEU. s-hier-to-2.tied performs worst in terms of BLEU, but note that this is a shallow recurrent model, while others are Transformer-based. It also suffers from the asymmetric data setting, like the concatenation baseline.
we reorganized the train/dev/test sets, forming new splits, which we refer to as NO-LEAK F&C. The new split sizes can be found in Table 2. We re-ran the current models on NO-LEAK F&C and, as expected, we observe a drop of 5-6% in accuracy: from the original 76% accuracy on the dev/test sets, to 70% and 71% accuracy, respectively.
we reorganized the train/dev/test sets, forming new splits, which we refer to as NO-LEAK F&C. The new split sizes can be found in Table 2. We re-ran the current models on NO-LEAK F&C and, as expected, we observe a drop of 5-6% in accuracy: from the original 76% accuracy on the dev/test sets, to 70% and 71% accuracy, respectively.
The left column of Table 4 presents results for the cleaned version of the Forbes and Choi (2017) dataset.  Results on the new objects comparison dataset we created are shown in the rightmost column of Table 4.  We get better results than previous methods on this dataset: 63% and 61% accuracy on the dev/test sets compared to 60% and 57%. These relatively low results on this new dataset indicate that it is more challenging.
The left column of Table 4 presents results for the cleaned version of the Forbes and Choi (2017) dataset.  Results on the new objects comparison dataset we created are shown in the rightmost column of Table 4.  We get better results than previous methods on this dataset: 63% and 61% accuracy on the dev/test sets compared to 60% and 57%. These relatively low results on this new dataset indicate that it is more challenging.
noun comparatives is on RELATIVE (Bagherinezhad et al., 2016), presented in Table 5.  We report the results of the original work, where the best score used a combination of visual and textual signals, achieving 83.5% accuracy. We also tested the method by Yang et al. (2018) on this dataset.  The accuracy achieved by this method is 85.8%, surpassing the previous method by more than 2 points. We evaluated our method on this dataset, achieving a new state-of-the-art result of 87.7% accuracy with k = 10 as a filter method.
noun comparatives is on RELATIVE (Bagherinezhad et al., 2016), presented in Table 5.  We report the results of the original work, where the best score used a combination of visual and textual signals, achieving 83.5% accuracy. We also tested the method by Yang et al. (2018) on this dataset.  The accuracy achieved by this method is 85.8%, surpassing the previous method by more than 2 points. We evaluated our method on this dataset, achieving a new state-of-the-art result of 87.7% accuracy with k = 10 as a filter method.
The results of the intrinsic evaluation on a sample of DOQ are shown in Table 7. The total agreement is 69%, while the specific agreements for MASS, LENGTH, SPEED and CURRENCY are 61%, 79%, 77% and 58% respectively.  We re-annotated the samples in the currency category with annotators from the U.S. and found a much higher agreement score: 76%.
The results of the intrinsic evaluation on a sample of DOQ are shown in Table 7. The total agreement is 69%, while the specific agreements for MASS, LENGTH, SPEED and CURRENCY are 61%, 79%, 77% and 58% respectively.  We re-annotated the samples in the currency category with annotators from the U.S. and found a much higher agreement score: 76%.
as our dependent variable. Significant features of the logistic regression model are shown in Table 1 with the respective significance levels. We also run a step-wise backward elimination regression.  Those components that are also significant in the step-wise model appear in bold.  Observing the significant features, in bold in Table 1, we see a combination of surface level related features, such as sentence length and average word frequency, as well as semantic features including LSA (Latent Semantic Analysis) overlaps between verbs and between adjacent sentences. Semantic features which are associated with the gist representation of content are particularly interesting to see among the predictors since based on Fuzzytrace theory (Reyna, 2012), a well-known theory of decision making under risk, gist representation of content drives individual's decision to spread misinformation online. Also among the significant features, we observe the causal connectives, that are proven to be important in text comprehension, and two indices related to the text easability and readability, both suggesting that satire articles are more sophisticated, or less easy to read, than fake news articles.
as our dependent variable. Significant features of the logistic regression model are shown in Table 1 with the respective significance levels. We also run a step-wise backward elimination regression.  Those components that are also significant in the step-wise model appear in bold.  Observing the significant features, in bold in Table 1, we see a combination of surface level related features, such as sentence length and average word frequency, as well as semantic features including LSA (Latent Semantic Analysis) overlaps between verbs and between adjacent sentences. Semantic features which are associated with the gist representation of content are particularly interesting to see among the predictors since based on Fuzzytrace theory (Reyna, 2012), a well-known theory of decision making under risk, gist representation of content drives individual's decision to spread misinformation online. Also among the significant features, we observe the causal connectives, that are proven to be important in text comprehension, and two indices related to the text easability and readability, both suggesting that satire articles are more sophisticated, or less easy to read, than fake news articles.
text of a story, and in combination. Results are shown in Table 2. The models based on the headline and text body give a similar F1 score. However, while the headline model performs poorly on precision, perhaps due to the short text, the model based on the text body performs poorly on recall. The model based on the full text of headline and body gives the best performance.
text of a story, and in combination. Results are shown in Table 2. The models based on the headline and text body give a similar F1 score. However, while the headline model performs poorly on precision, perhaps due to the short text, the model based on the text body performs poorly on recall. The model based on the full text of headline and body gives the best performance.
For all tasks, we observe a large improvement from using context. For deixis, the concatenation model (concat) and CADec improve over the baseline by 33.5 and 31.6 percentage points, respectively. On the lexical cohesion test set, CADec shows a large improvement over the context-agnostic baseline (12.2 percentage points), while concat performs similarly to the baseline.  When looking only at the scores where the latest relevant context is in the model's context window (column 2 in Table 7), s-hier-to-2.tied outperforms the concatenation baseline for lexical cohesion, but remains behind the performance of CADec.
For all tasks, we observe a large improvement from using context. For deixis, the concatenation model (concat) and CADec improve over the baseline by 33.5 and 31.6 percentage points, respectively. On the lexical cohesion test set, CADec shows a large improvement over the context-agnostic baseline (12.2 percentage points), while concat performs similarly to the baseline.  When looking only at the scores where the latest relevant context is in the model's context window (column 2 in Table 7), s-hier-to-2.tied outperforms the concatenation baseline for lexical cohesion, but remains behind the performance of CADec.
Table 3 provides a summary of the results. We compare the results of our methods of the pre-trained BERT, using both the headline and text body, and the Coh-Mertix approach, to the language-based baseline with Multinomial Naive Bayes from (Golbeck et al., 2018)2. Both the semantic cues with BERT and the linguistic cues with Coh-Metrix significantly outperform the baseline on the F1 score. The two-tailed paired t-test with a 0.05 significance level was used for testing statistical significance of performance differences. The best result is given by the BERT model. Overall, these results provide an answer to research question RQ1 regarding the existence of semantic and linguistic difference between fake news and satire.
Table 3 provides a summary of the results. We compare the results of our methods of the pre-trained BERT, using both the headline and text body, and the Coh-Mertix approach, to the language-based baseline with Multinomial Naive Bayes from (Golbeck et al., 2018)2. Both the semantic cues with BERT and the linguistic cues with Coh-Metrix significantly outperform the baseline on the F1 score. The two-tailed paired t-test with a 0.05 significance level was used for testing statistical significance of performance differences. The best result is given by the BERT model. Overall, these results provide an answer to research question RQ1 regarding the existence of semantic and linguistic difference between fake news and satire.
Table 1 shows micro F1 scores on AIDA-B of the SOTA methods and ours, which all use Wikipedia and YAGO mention-entity index. To our knowledge, ours are the only (unsupervisedly) inducing and employing more than one relations on this dataset. The others use only one relation, coreference, which is given by simple heuristics or supervised third-party resolvers. All four our models outperform any previous method, with ment-norm achieving the best results, 0.85% higher than that of Ganea and Hofmann (2017).  The experimental results show that ment-norm outperforms rel-norm, and that mention padding plays an important role.
Table 1 shows micro F1 scores on AIDA-B of the SOTA methods and ours, which all use Wikipedia and YAGO mention-entity index. To our knowledge, ours are the only (unsupervisedly) inducing and employing more than one relations on this dataset. The others use only one relation, coreference, which is given by simple heuristics or supervised third-party resolvers. All four our models outperform any previous method, with ment-norm achieving the best results, 0.85% higher than that of Ganea and Hofmann (2017).  The experimental results show that ment-norm outperforms rel-norm, and that mention padding plays an important role.
Table 2 shows micro F1 scores on 5 out-domain test sets. Besides ours, only Cheng and Roth (2013) employs several mention relations. Mentnorm achieves the highest F1 scores on MSNBC and ACE2004. On average, ment-norm's F1 score is 0.3% higher than that of Ganea and Hofmann (2017), but 0.2% lower than Guo and Barbosa (2016)'s. It is worth noting that Guo and Barbosa (2016) performs exceptionally well on WIKI, but substantially worse than ment-norm on all other datasets. Our other three models, however, have lower average F1 scores compared to the best previous model.  The experimental results show that ment-norm outperforms rel-norm, and that mention padding plays an important role.
Table 2 shows micro F1 scores on 5 out-domain test sets. Besides ours, only Cheng and Roth (2013) employs several mention relations. Mentnorm achieves the highest F1 scores on MSNBC and ACE2004. On average, ment-norm's F1 score is 0.3% higher than that of Ganea and Hofmann (2017), but 0.2% lower than Guo and Barbosa (2016)'s. It is worth noting that Guo and Barbosa (2016) performs exceptionally well on WIKI, but substantially worse than ment-norm on all other datasets. Our other three models, however, have lower average F1 scores compared to the best previous model.  The experimental results show that ment-norm outperforms rel-norm, and that mention padding plays an important role.
The results are presented in Table 2.  the results in Table 2 show very high performance, which is likely to further increase with ongoing training.
The results are presented in Table 2.  the results in Table 2 show very high performance, which is likely to further increase with ongoing training.
For ellipsis, both models improve substantially over the baseline (by 19-51 percentage points), with concat stronger for inflection tasks and CADec stronger for VPellipsis.
For ellipsis, both models improve substantially over the baseline (by 19-51 percentage points), with concat stronger for inflection tasks and CADec stronger for VPellipsis.
All results of these experiments are presented in Table 3 with the absolute improvement of the two main measures UAR and eA over the SVM-based approach
All results of these experiments are presented in Table 3 with the absolute improvement of the two main measures UAR and eA over the SVM-based approach
The results are shown in Table 1. We see that the mean NER performance increases in joint models.  As one can see from the table, it achieved the best results compared to our joint models. However, we cannot confirm the difference between EXT M FEAT and J MULTI models as the calculated p is well above .05.
The results are shown in Table 1. We see that the mean NER performance increases in joint models.  As one can see from the table, it achieved the best results compared to our joint models. However, we cannot confirm the difference between EXT M FEAT and J MULTI models as the calculated p is well above .05.
As can be seen from Table 2, we are very close to the state of the  art MD performance even if we only trained with a low number of parameters as stated in the beginning of this section. We have to also note that in contrast with the NER task, the MD task did not enjoy a performance increase from joint learning.
As can be seen from Table 2, we are very close to the state of the  art MD performance even if we only trained with a low number of parameters as stated in the beginning of this section. We have to also note that in contrast with the NER task, the MD task did not enjoy a performance increase from joint learning.
We report accuracy as the performance metric. Table 1 represents the performance comparison of our proposed models and the baselines, which shows that incorporation of knowledge graph embeddings helps to improve the model performance.  All results are summarized in Table 1.
We report accuracy as the performance metric. Table 1 represents the performance comparison of our proposed models and the baselines, which shows that incorporation of knowledge graph embeddings helps to improve the model performance.  All results are summarized in Table 1.
Results for different values of p are given in Table 9. All models have about the same BLEU, not statistically significantly different from the baseline, but they are quite different in terms of incorporating context. The denoising positively influences almost all tasks except for deixis, yielding the largest improvement on lexical cohesion.
Results for different values of p are given in Table 9. All models have about the same BLEU, not statistically significantly different from the baseline, but they are quite different in terms of incorporating context. The denoising positively influences almost all tasks except for deixis, yielding the largest improvement on lexical cohesion.
Table 1 compares the performance of these systems on the development set. Our model with no augmentation already matches the system of Choi et al. (2018) with augmentation, and incorporating ELMo gives further gains on both precision and recall. On top of this model, adding the distantly-annotated data lowers the performance; the loss function-based approach of (Choi et al., 2018) does not sufficiently mitigate the noise in this data. However, denoising makes the distantlyannotated data useful, improving recall by a substantial margin especially in the general class.  BERT performs similarly to ELMo with denoised distant data. As can be seen in the performance breakdown, BERT gains from improvements in recall in the fine class.
Table 1 compares the performance of these systems on the development set. Our model with no augmentation already matches the system of Choi et al. (2018) with augmentation, and incorporating ELMo gives further gains on both precision and recall. On top of this model, adding the distantly-annotated data lowers the performance; the loss function-based approach of (Choi et al., 2018) does not sufficiently mitigate the noise in this data. However, denoising makes the distantlyannotated data useful, improving recall by a substantial margin especially in the general class.  BERT performs similarly to ELMo with denoised distant data. As can be seen in the performance breakdown, BERT gains from improvements in recall in the fine class.
Table 2 shows the performance of all settings on the test set, with the same trend as the performance on the development set. Our approach outperforms the concurrently-published Xiong et al. (2019);
Table 2 shows the performance of all settings on the test set, with the same trend as the performance on the development set. Our approach outperforms the concurrently-published Xiong et al. (2019);
Table 3 compares the results on the development set.  On top of the baseline ORIGINAL, adding synonyms and hypernyms by consulting external knowledge does not improve the performance.  the PAIR technique results in small gains over ORIGINAL. OVERLAP is the most ef  fective heuristic technique. This simple  heuristic improves recall on EL.  FILTER,  gives similar improvements to PAIR and OVERLAP on the HEAD setting,  RELABEL and OVERLAP both improve performance on both EL and HEAD while other methods do poorly on EL.  FILTER & RELABEL outperforms all the baselines.
Table 3 compares the results on the development set.  On top of the baseline ORIGINAL, adding synonyms and hypernyms by consulting external knowledge does not improve the performance.  the PAIR technique results in small gains over ORIGINAL. OVERLAP is the most ef  fective heuristic technique. This simple  heuristic improves recall on EL.  FILTER,  gives similar improvements to PAIR and OVERLAP on the HEAD setting,  RELABEL and OVERLAP both improve performance on both EL and HEAD while other methods do poorly on EL.  FILTER & RELABEL outperforms all the baselines.
Table 4 lists the results on the OntoNotes test set following the adaptation setting of Choi et al. (2018).  denoising significantly improves over naive incorporation of distant data,  BERT still performs well but not as well as our model with augmented training data.
Table 4 lists the results on the OntoNotes test set following the adaptation setting of Choi et al. (2018).  denoising significantly improves over naive incorporation of distant data,  BERT still performs well but not as well as our model with augmented training data.
Table 5 reports the average numbers of types added/deleted by the relabeling function and the ratio of examples discarded by the filtering function.  The HEAD examples have more general types added than the EL examples since the noisy HEAD labels are typically finer. Fine-grained types are added to both EL and HEAD examples less frequently. Ultra-fine examples are frequently added to both datasets, with more added to EL;  The filtering function discards similar numbers of examples for the EL and HEAD data: 9.4% and 10% respectively.
Table 5 reports the average numbers of types added/deleted by the relabeling function and the ratio of examples discarded by the filtering function.  The HEAD examples have more general types added than the EL examples since the noisy HEAD labels are typically finer. Fine-grained types are added to both EL and HEAD examples less frequently. Ultra-fine examples are frequently added to both datasets, with more added to EL;  The filtering function discards similar numbers of examples for the EL and HEAD data: 9.4% and 10% respectively.
we found that it still showed a limitation when we consider very large sequential length data such as 162 steps average in the Ubuntu Dialogue Corpus dataset (see Table 1).  Table 1 shows properties of the Ubuntu dataset.  we generated ({question}, {answer}, flag) triples (see Table 1).  The maximum time step for calculating gradient of the RNN is determined according to the input data statistics in Table 1.
As Table 3 shows, our proposed HRDE and HRDE-LTC models achieve the best performance for the Ubuntu-v1 dataset. We also find that the RDE-LTC model shows improvements from the baseline model, RDE.
As Table 3 shows, our proposed HRDE and HRDE-LTC models achieve the best performance for the Ubuntu-v1 dataset. We also find that the RDE-LTC model shows improvements from the baseline model, RDE.
Table 4 reveals that the HRDE-LTC model is best for three cases (1 in 2 R@1, 1 in 10 R@2 and 1 in 10 R@5).  we see improvements from the RDE model to the HRDE model and additional improvements with the LTC module in all test cases (the Ubuntuv1/v2 and the Samsung QA).
Table 4 reveals that the HRDE-LTC model is best for three cases (1 in 2 R@1, 1 in 10 R@2 and 1 in 10 R@5).  we see improvements from the RDE model to the HRDE model and additional improvements with the LTC module in all test cases (the Ubuntuv1/v2 and the Samsung QA).
Table 5 indicates the proposed RDE-LTC, HRDE, and the that HRDE-LTC model show performance improvements when compared to the baseline model, TFIDF and RDE.
Table 5 indicates the proposed RDE-LTC, HRDE, and the that HRDE-LTC model show performance improvements when compared to the baseline model, TFIDF and RDE.
Our model outperforms all other models in all the metrics. This improvement is consistent, around 2%.
Our model outperforms all other models in all the metrics. This improvement is consistent, around 2%.
Table 3 and Figure 3 show a linear relationship between the accuracy of the classifier and the IWAQG. This demonstrates the effectiveness of our pipelined approach regardless of the interrogative-word classifier model.
Table 3 and Figure 3 show a linear relationship between the accuracy of the classifier and the IWAQG. This demonstrates the effectiveness of our pipelined approach regardless of the interrogative-word classifier model.
we analyze the recall of the interrogative words generated by our pipelined system. As shown in the Table 4, the total recall of using only the QG module is 68.29%, while the recall of our proposed system, IWAQG, is 74.10%, an improvement of almost 6%. Furthermore, if we assume a perfect interrogative-word classifier, the recall would be 99.72%, a dramatic improvement which proves the validity of our hypothesis.
we analyze the recall of the interrogative words generated by our pipelined system. As shown in the Table 4, the total recall of using only the QG module is 68.29%, while the recall of our proposed system, IWAQG, is 74.10%, an improvement of almost 6%. Furthermore, if we assume a perfect interrogative-word classifier, the recall would be 99.72%, a dramatic improvement which proves the validity of our hypothesis.
We tried to combine different features shown in Table 6 for the interrogative-word classifier.  The first model is only using the [CLS] BERT token embedding  The second model is the previous one with the entity type of the answer as an additional feature. The performance of this model is a bit better than the first one but it is not enough to be utilized effectively for our pipeline.  As we can see, the performance noticeably increased, which indicates that answer information is the key to predict the interrogative word needed.  the fourth model,  clearly outperforms the previous one,  The fifth model is the same as the previous one but with the addition of the entitytype embedding of the answer. The combination of the three features (answer, answer entity type, and passage) yields to the best performance.
We tried to combine different features shown in Table 6 for the interrogative-word classifier.  The first model is only using the [CLS] BERT token embedding  The second model is the previous one with the entity type of the answer as an additional feature. The performance of this model is a bit better than the first one but it is not enough to be utilized effectively for our pipeline.  As we can see, the performance noticeably increased, which indicates that answer information is the key to predict the interrogative word needed.  the fourth model,  clearly outperforms the previous one,  The fifth model is the same as the previous one but with the addition of the entitytype embedding of the answer. The combination of the three features (answer, answer entity type, and passage) yields to the best performance.
In addition, we provide the recall and precision per class for our final interrogative-word classifier (CLS + AT + NER in Table 7).  However, the recall of which is very low.  Our model has also problem with why  Lastly, the recall of 'when is also low
In addition, we provide the recall and precision per class for our final interrogative-word classifier (CLS + AT + NER in Table 7).  However, the recall of which is very low.  Our model has also problem with why  Lastly, the recall of 'when is also low
Table 1 demonstrates several characteristics of the generated forests of both the EDGEWISE and KBESTEISNER algorithms in Section 5.1, where "#Edge/#Sent" measures the forest density with the number of edges divided by the sentence length, "LAS" represents the oracle LAS score on 100 biomedical sentences with manually annotated dependency trees, and "Conn. Ratio (%)" shows the percentage of forests where both related entity mentions are connected.  Regarding the forest density, forests produced by EDGEWISE generally contain more edges than those from KBESTEISNER.  For connectivity, KBESTEISNER guarantees to generate spanning forests. On the other hand, the connectivity ratio for the forests produced by EDGEWISE drops when increasing the threshold γ. We can have more than 94% being connected with γ ≤ 0.2.
Table 1 demonstrates several characteristics of the generated forests of both the EDGEWISE and KBESTEISNER algorithms in Section 5.1, where "#Edge/#Sent" measures the forest density with the number of edges divided by the sentence length, "LAS" represents the oracle LAS score on 100 biomedical sentences with manually annotated dependency trees, and "Conn. Ratio (%)" shows the percentage of forests where both related entity mentions are connected.  Regarding the forest density, forests produced by EDGEWISE generally contain more edges than those from KBESTEISNER.  For connectivity, KBESTEISNER guarantees to generate spanning forests. On the other hand, the connectivity ratio for the forests produced by EDGEWISE drops when increasing the threshold γ. We can have more than 94% being connected with γ ≤ 0.2.
Table 2 shows the main comparison results on the BioCreative CPR testset, with comparisons to the previous state-of-the-art and our baselines.  TEXTONLY gives a performance comparable with Bran. With 1-best dependency trees, our DEPTREE baseline gives better performances than the previous state of the art.  both KBESTEISNERPS and EDGEWISEPS obtain significantly higher numbers than DEPTREE.
Table 2 shows the main comparison results on the BioCreative CPR testset, with comparisons to the previous state-of-the-art and our baselines.  TEXTONLY gives a performance comparable with Bran. With 1-best dependency trees, our DEPTREE baseline gives better performances than the previous state of the art.  both KBESTEISNERPS and EDGEWISEPS obtain significantly higher numbers than DEPTREE.
Table 3 shows the comparison with previous work on the PGR testset, where our models are significantly better than the existing models.  With 1-best trees, DEPTREE is 2.9 points better than TEXTONLY,  both KBESTEISNERPS and EDGEWISEPS significantly outperform DEPTR
Table 3 shows the comparison with previous work on the PGR testset, where our models are significantly better than the existing models.  With 1-best trees, DEPTREE is 2.9 points better than TEXTONLY,  both KBESTEISNERPS and EDGEWISEPS significantly outperform DEPTR
As shown in Table 4, we conduct a preliminary study on SemEval-2010 task 8  DEPTREE achieves similar performance as CGCN and is slightly worse than C-AGGCN,  both KBESTEISNERPS and EDGEWISEPS outperform DEPTREE  they show comparable and slightly better performances than C-AGGCN.  EDGEWISEPS is better than KBESTEISNERPS,
As shown in Table 4, we conduct a preliminary study on SemEval-2010 task 8  DEPTREE achieves similar performance as CGCN and is slightly worse than C-AGGCN,  both KBESTEISNERPS and EDGEWISEPS outperform DEPTREE  they show comparable and slightly better performances than C-AGGCN.  EDGEWISEPS is better than KBESTEISNERPS,
From the results shown in Table 1, we observe that for both CNN and PCNN models, our model outperforms the plain attention model and the HATT model.
From the results shown in Table 1, we observe that for both CNN and PCNN models, our model outperforms the plain attention model and the HATT model.
Discussion of Results The left part of Table 1 shows results with the WordNet similarity scores used as gold standard. Path2vec outperforms other graph embeddings, achieving high correlations with WordNet similarities. This shows that our model efficiently approximates different graph measures. The right part of Table 1 shows results  for the correlations with human judgments (SimLex999). We report the results for the best models for each method, all of them (except FSE) using vector size 300 for comparability.
Discussion of Results The left part of Table 1 shows results with the WordNet similarity scores used as gold standard. Path2vec outperforms other graph embeddings, achieving high correlations with WordNet similarities. This shows that our model efficiently approximates different graph measures. The right part of Table 1 shows results  for the correlations with human judgments (SimLex999). We report the results for the best models for each method, all of them (except FSE) using vector size 300 for comparability.
Discussion of Results Table 2 presents the WSD micro-F1 scores using raw WordNet similarities, 300D path2vec, DeepWalk and node2vec models, and the 128D FSE model. We evaluate on the following all-words English WSD test sets:  Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Moro and Navigli, 2015). The raw WordNet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number.
Discussion of Results Table 2 presents the WSD micro-F1 scores using raw WordNet similarities, 300D path2vec, DeepWalk and node2vec models, and the 128D FSE model. We evaluate on the following all-words English WSD test sets:  Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Moro and Navigli, 2015). The raw WordNet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number.
The results, shown in Table 3, demonstrate that the MIMICK RNN recovers about half of the loss in performance incurred by the original Polyglot training model due to out-of-vocabulary words in the "All pairs" condition. MIMICK also outperforms VarEmbed. FastText can be considered an upper bound: with a vocabulary that is 25 times larger than the other models, it was missing words from only 44 pairs on this data.
The results, shown in Table 3, demonstrate that the MIMICK RNN recovers about half of the loss in performance incurred by the original Polyglot training model due to out-of-vocabulary words in the "All pairs" condition. MIMICK also outperforms VarEmbed. FastText can be considered an upper bound: with a vocabulary that is 25 times larger than the other models, it was missing words from only 44 pairs on this data.
As can be seen from Table 3, our models produce better BLEU scores than almost all the comparisons. Especially, our models with separate decoder yield significantly higher BLEU and METEOR scores than all seq2seq-based models (approximation randomization testing, p < 0.0001) do. Better METEOR scores are achieved by the RETRIEVAL baseline, mainly due to its significantly longer arguments.  Moreover, utilizing attention over both input and the generated keyphrases further boosts our models' performance. Interestingly, utilizing system retrieved evidence yields better BLEU scores than using oracle retrieval for testing.
As can be seen from Table 3, our models produce better BLEU scores than almost all the comparisons. Especially, our models with separate decoder yield significantly higher BLEU and METEOR scores than all seq2seq-based models (approximation randomization testing, p < 0.0001) do. Better METEOR scores are achieved by the RETRIEVAL baseline, mainly due to its significantly longer arguments.  Moreover, utilizing attention over both input and the generated keyphrases further boosts our models' performance. Interestingly, utilizing system retrieved evidence yields better BLEU scores than using oracle retrieval for testing.
We report the results on the full sets and on  = 5000 tokens in Table 5 (partof-speech tagging accuracy)  For POS, the largest margins are in the Slavic languages (Russian, Czech, Bulgarian), where word order is relatively free and thus rich word representations are imperative. Chinese also exhibits impressive improvement across all settings, perhaps due to the large character inventory (> 12,000), for which a model such as MIMICK can learn well-informed embeddings using the large Polyglot vocabulary dataset, overcoming both word- and characterlevel sparsity in the UD corpus.
We report the results on the full sets and on  = 5000 tokens in Table 5 (partof-speech tagging accuracy)  For POS, the largest margins are in the Slavic languages (Russian, Czech, Bulgarian), where word order is relatively free and thus rich word representations are imperative. Chinese also exhibits impressive improvement across all settings, perhaps due to the large character inventory (> 12,000), for which a model such as MIMICK can learn well-informed embeddings using the large Polyglot vocabulary dataset, overcoming both word- and characterlevel sparsity in the UD corpus.
Table 6 (morphosyntactic attribute tagging micro-F1).  In morphosyntactic tagging, gains are apparent for Slavic languages and Chinese, but also for agglutinative languages — especially Tamil and Turkish — where the stable morpheme representation makes it easy for subword modeling to provide a type-level signal.
Table 6 (morphosyntactic attribute tagging micro-F1).  In morphosyntactic tagging, gains are apparent for Slavic languages and Chinese, but also for agglutinative languages — especially Tamil and Turkish — where the stable morpheme representation makes it easy for subword modeling to provide a type-level signal.
Table 7 presents the POS tagging improvements that MIMICK achieves over the pre-trained Polyglot models, with and without CHAR→TAG concatenation, with 10,000 tokens of training data. We obtain statistically significant improvements in most languages, even when CHAR→TAG is included. These improvements are particularly substantial for test-set tokens outside the UD training set, as shown in the right two columns. While test set OOVs are a strength of the CHAR→TAG model (Plank et al., 2016), in many languages there are still considerable improvements to be obtained from the application of MIMICK initialization.
Table 7 presents the POS tagging improvements that MIMICK achieves over the pre-trained Polyglot models, with and without CHAR→TAG concatenation, with 10,000 tokens of training data. We obtain statistically significant improvements in most languages, even when CHAR→TAG is included. These improvements are particularly substantial for test-set tokens outside the UD training set, as shown in the right two columns. While test set OOVs are a strength of the CHAR→TAG model (Plank et al., 2016), in many languages there are still considerable improvements to be obtained from the application of MIMICK initialization.
See Table 3 for the averages across 10 scenarios.  As we can see from Table 3, the perplexity scores are consistent with the accuracies: the script model again outperforms other methods, and, as expected, all the models are weaker than humans.
Ranking metrics of MRR and Precision at 1 (P@1) are utilized, with results reported in Table 4. The ranker yields significantly better scores over arguments generated from models trained with evidence, compared to arguments generated by SEQ2SEQ model.
Ranking metrics of MRR and Precision at 1 (P@1) are utilized, with results reported in Table 4. The ranker yields significantly better scores over arguments generated from models trained with evidence, compared to arguments generated by SEQ2SEQ model.
The results of all four logistic regression models are shown in Table 5.  The results for the full dataset are fully consistent with the findings shown in Table 5: there was no significant effect of surprisal on referring expression type.  In order to replicate their settings as closely as possible, we also included residualEntropy as a predictor in our model (see last predictor in Table 5); however, this did not change the results.
The results of all four logistic regression models are shown in Table 5.  The results for the full dataset are fully consistent with the findings shown in Table 5: there was no significant effect of surprisal on referring expression type.  In order to replicate their settings as closely as possible, we also included residualEntropy as a predictor in our model (see last predictor in Table 5); however, this did not change the results.
The experimental results are shown in Table 1. As Keller is created based on the PP distribution and have relatively small size while SP-10K is created based on random sampling and has a much larger size, we treat the performance on SP-10K as the major evaluation. Our embeddings significantly outperform other baselines, especially embedding based baselines. The only exception is PP on the Keller dataset due to its biased distribution. In addition, there are other interesting observations. First, compared with 'dobj' and 'nsubj', 'amod' is simpler for word2vec and GloVe. The reason behind is that conventional embeddings only capture the co-occurrence information, which is enough to predict the selectional preference of
We also compare MWE with pre-trained contextualized word embedding models in Table 4 for this task, with overall performance, embedding dimensions, and training times reported. It is observed that that MWE outperforms ELMo and achieves comparable results with BERT with smaller embedding dimension and much less training complexities.
We also compare MWE with pre-trained contextualized word embedding models in Table 4 for this task, with overall performance, embedding dimensions, and training times reported. It is observed that that MWE outperforms ELMo and achieves comparable results with BERT with smaller embedding dimension and much less training complexities.
As shown in Table 5, we compare our model with several different strategies. The first one is to put all weights to the center embedding (fix λ to 1), which never updates the local relational embeddings. As a result, it can achieve similar performance on word similarity measurement but is inferior in SP acquisition because no relationdependent information is preserved.
Our plain BPE baseline (Table 4) outperforms the current best system on WAT Ja-En, an 8-model ensemble (Morishita et al., 2017). Our syntax models achieve similar results despite producing much longer sequences.
Ensembles of two identical models trained with different seeds only slightly improve over the single model (Table 5). However, an ensemble of models producing plain BPE and linearized derivations improves by 0.5 BLEU over the plain BPE baseline.
Table 1 demonstrates the effect of computing BLEU scores with different reference tokenizations.  The changes in each column show the effect these different schemes have, as high as 1.8 for one arc, and averaging around 1.0. The biggest is the treatment of case, which is well known, yet many papers are not clear about whether they report cased or case-insensitive BLEU.  The variations in Table 1 are only some of the possible configurations, since there is no limit to the preprocessing that a group could apply.
Table 1 demonstrates the effect of computing BLEU scores with different reference tokenizations.  The changes in each column show the effect these different schemes have, as high as 1.8 for one arc, and averaging around 1.0. The biggest is the treatment of case, which is well known, yet many papers are not clear about whether they report cased or case-insensitive BLEU.  The variations in Table 1 are only some of the possible configurations, since there is no limit to the preprocessing that a group could apply.
Table 2 shows the results of several systems on both datasets.7 The column '% perf' indicates the proportion of perfectly segmented super-tokens, while the next three columns indicate precision, recall and F-score for boundary detection, not including the trivial final position characters. The first baseline strategy of not segmenting anything is given in the first row, and unsurprisingly gets many cases right, but performs badly overall. A more intelligent baseline is provided by UDPipe (Straka et al. 2016; retrained on the SPMRL data), which, for super-tokens in morphologically rich languages such as Hebrew, implements a 'most common segmentation' baseline  (i.e. each super-token is given its most common segmentation from training data, forgoing segmentation for OOV items).8 Results for yap represent pure segmentation performance from the previous state of the art (More and Tsarfaty, 2016). The best two approaches in the present paper are represented next: the Extra Trees Random Forest variant,9 called RFTokenizer, is labeled RF and the DNN-based system is labeled DNN. Surprisingly, while the DNN is a close runner up, the best performance is achieved by the RFTokenizer, de  spite not having access to word embeddings. Its high performance on the SPMRL dataset makes it difficult to converge to a better solution using the DNN, though it is conceivable that substantially more data, a better feature representation and/or more hyperparameter tuning could equal or surpass the RFTokenizer's performance. Coupled with a lower cost in system resources and external dependencies, and the ability to forgo large model files to store word embeddings, we consider the RFTokenizer solution to be better given the current training data size. Performance on the out of domain dataset is encouragingly nearly as good as on SPMRL, suggesting our features are robust. This is especially clear compared to UDPipe and yap, which degrade more substantially. A key advantage of the present approach is its comparatively high precision. While other approaches have good recall, and yap approaches RFTokenizer on recall for SPMRL, RFTokenizer's reduction in spurious segmentations boosts its F-score substantially. To see why, we examine some errors in the next section, and perform feature ablations in the following one.
Table 2 shows the results of several systems on both datasets.7 The column '% perf' indicates the proportion of perfectly segmented super-tokens, while the next three columns indicate precision, recall and F-score for boundary detection, not including the trivial final position characters. The first baseline strategy of not segmenting anything is given in the first row, and unsurprisingly gets many cases right, but performs badly overall. A more intelligent baseline is provided by UDPipe (Straka et al. 2016; retrained on the SPMRL data), which, for super-tokens in morphologically rich languages such as Hebrew, implements a 'most common segmentation' baseline  (i.e. each super-token is given its most common segmentation from training data, forgoing segmentation for OOV items).8 Results for yap represent pure segmentation performance from the previous state of the art (More and Tsarfaty, 2016). The best two approaches in the present paper are represented next: the Extra Trees Random Forest variant,9 called RFTokenizer, is labeled RF and the DNN-based system is labeled DNN. Surprisingly, while the DNN is a close runner up, the best performance is achieved by the RFTokenizer, de  spite not having access to word embeddings. Its high performance on the SPMRL dataset makes it difficult to converge to a better solution using the DNN, though it is conceivable that substantially more data, a better feature representation and/or more hyperparameter tuning could equal or surpass the RFTokenizer's performance. Coupled with a lower cost in system resources and external dependencies, and the ability to forgo large model files to store word embeddings, we consider the RFTokenizer solution to be better given the current training data size. Performance on the out of domain dataset is encouragingly nearly as good as on SPMRL, suggesting our features are robust. This is especially clear compared to UDPipe and yap, which degrade more substantially. A key advantage of the present approach is its comparatively high precision. While other approaches have good recall, and yap approaches RFTokenizer on recall for SPMRL, RFTokenizer's reduction in spurious segmentations boosts its F-score substantially. To see why, we examine some errors in the next section, and perform feature ablations in the following one.
Table 3 gives an overview of the impact on performance when specific features are removed: the entire lexicon, lexicon expansion, letter identity, 'vowel' features from Section 3.1, and both of the latter. Performance is high even in ablation scenarios, though we keep in mind that baselines for the task are high (e.g. 'most frequent lookup', the UDPipe strategy, achieves close to 90%). The results show the centrality of the lexicon: removing lexicon lookup features degrades performance by about 3.5% perfect accuracy, or 5.5 F-score points. All other ablations impact performance by less than 1% or 1.5 F-score points. Expanding the lexicon using Wikipedia data offers a contribution of 0.3–0.4 points, confirming the original lexicon's incompleteness.10 Looking more closely at the other features, it is surprising that identity of the letters is not crucial, as long as we have access to dictionary lookup using the letters. Nevertheless, removing letter identity impacts especially boundary recall, perhaps  does not break down drastically. The impact on Wiki5k is stronger, possibly because the necessary memorization of familiar contexts is less effective out of domain.  because some letters receive identical lookup values (e.g. single letter prepositions such as b 'in', l 'to') but have different segmentation likelihoods. The 'vowel' features, though ostensibly redundant with letter identity, help a little, causing 0.33 SPMRL F-score point degradation if removed. A
Table 3 gives an overview of the impact on performance when specific features are removed: the entire lexicon, lexicon expansion, letter identity, 'vowel' features from Section 3.1, and both of the latter. Performance is high even in ablation scenarios, though we keep in mind that baselines for the task are high (e.g. 'most frequent lookup', the UDPipe strategy, achieves close to 90%). The results show the centrality of the lexicon: removing lexicon lookup features degrades performance by about 3.5% perfect accuracy, or 5.5 F-score points. All other ablations impact performance by less than 1% or 1.5 F-score points. Expanding the lexicon using Wikipedia data offers a contribution of 0.3–0.4 points, confirming the original lexicon's incompleteness.10 Looking more closely at the other features, it is surprising that identity of the letters is not crucial, as long as we have access to dictionary lookup using the letters. Nevertheless, removing letter identity impacts especially boundary recall, perhaps  does not break down drastically. The impact on Wiki5k is stronger, possibly because the necessary memorization of familiar contexts is less effective out of domain.  because some letters receive identical lookup values (e.g. single letter prepositions such as b 'in', l 'to') but have different segmentation likelihoods. The 'vowel' features, though ostensibly redundant with letter identity, help a little, causing 0.33 SPMRL F-score point degradation if removed. A
The allowable input perturbation space is much larger than for word-level synonym substitutions, as shown in Table 3.  In Table 3, we show the maximum perturbation space size in the SST and AG News test set for different perturbation radii δ.  This number grows exponentially as δ increases.
The allowable input perturbation space is much larger than for word-level synonym substitutions, as shown in Table 3.  In Table 3, we show the maximum perturbation space size in the SST and AG News test set for different perturbation radii δ.  This number grows exponentially as δ increases.
Table 1 shows the results of IBP training and baseline models under δ = 3 and δ = 24 perturbations on SST and AG News, respectively.  In Table 1, comparing adversarial accuracy with exhaustive verification accuracy (oracle), we observe that although adversarial training is effective at defending against HotFlip attacks (74.9 / 76.8 / 85.5%), the oracle adversarial accuracy under exhaustive testing (25.8 / 74.6 / 81.6%) is much lower in SST-character / SST-word / AG-character level, respectively.  In Table 1, when the perturbation space is larger (SST character-level vs. SST word-level), (a) across models, there is a larger gap in adversarial accuracy and true robustness (oracle); (b) the difference in oracle robustness between IBP and adversarial training is even larger (73.1% vs. 25.8% and 76.5% vs. 74.6%).  The resulting models achieve the highest exhaustively verified accuracy at the cost of only moderate deterioration in nominal accuracy (Table 1).
Table 1 shows the results of IBP training and baseline models under δ = 3 and δ = 24 perturbations on SST and AG News, respectively.  In Table 1, comparing adversarial accuracy with exhaustive verification accuracy (oracle), we observe that although adversarial training is effective at defending against HotFlip attacks (74.9 / 76.8 / 85.5%), the oracle adversarial accuracy under exhaustive testing (25.8 / 74.6 / 81.6%) is much lower in SST-character / SST-word / AG-character level, respectively.  In Table 1, when the perturbation space is larger (SST character-level vs. SST word-level), (a) across models, there is a larger gap in adversarial accuracy and true robustness (oracle); (b) the difference in oracle robustness between IBP and adversarial training is even larger (73.1% vs. 25.8% and 76.5% vs. 74.6%).  The resulting models achieve the highest exhaustively verified accuracy at the cost of only moderate deterioration in nominal accuracy (Table 1).
Table 2 (upper part) shows the results for binary violation. We evaluate models using macroaveraged precision (P), recall (P), F1. The weak baselines (MAJORITY, COIN-TOSS) are widely outperformed by the rest of the methods. BIGRUATT outperforms in F1 (79.5 vs. 71.8) the previous best performing method (Aletras et al., 2016) in English judicial prediction.  HAN slightly improves over BIGRU-ATT (80.5 vs. 79.5), while being more robust across runs (0.2% vs. 2.7% std. dev.). the results in Table 2 indicate that performance is comparable even when this information is masked, with the exception of HIER-BERT that has quite worse results (2%) compared to using non-anonymized data, suggesting model bias.
we evaluate our four labeling strategies using the many-to-one approach, as presented in Table 1.  In all cases, clustering by type with Brown-based algorithms works better than using a sophisticated tagger such as A-HMM.
we evaluate our four labeling strategies using the many-to-one approach, as presented in Table 1.  In all cases, clustering by type with Brown-based algorithms works better than using a sophisticated tagger such as A-HMM.
Table 2 presents the intrinsic performance of the cipher grounder over all PL-CL pairs considered. The difference between the best and the worst performing PL for each CL ranges from 24.62 percentage points for Swahili to 48.34 points for French, and an average difference of 34.5 points among all languages.  The case when PL=CL is also presented in Table 2 as a reference and provides a reliable upper-bound under zero-resource conditions.  It is worth noting the difference in accuracy when comparing the best performing PL for each CL with its corresponding PL=CL upper-bound.  Among all CLs, the best cipher grounder for French (es-fr) gets the closest to its upper-bound with just 4.81 percentage points of difference, followed by the English grounder (fr-en) with 13.53 points of difference.  As shown in Table 2, this model, CIPHER-AVG, obtains accuracy scores of 56.4, 58.6, 37.4, and 37.8 % for en, fr, fa, and sw, respectively.  When compared to the best performing PL for each CL (see bold cells in Table 2), it can be noticed that the performance gap ranges from just 1.2 percentage points for Swahili to 13.3 points for French, with an average of 6.1 points among all target languages.
Table 2 presents the intrinsic performance of the cipher grounder over all PL-CL pairs considered. The difference between the best and the worst performing PL for each CL ranges from 24.62 percentage points for Swahili to 48.34 points for French, and an average difference of 34.5 points among all languages.  The case when PL=CL is also presented in Table 2 as a reference and provides a reliable upper-bound under zero-resource conditions.  It is worth noting the difference in accuracy when comparing the best performing PL for each CL with its corresponding PL=CL upper-bound.  Among all CLs, the best cipher grounder for French (es-fr) gets the closest to its upper-bound with just 4.81 percentage points of difference, followed by the English grounder (fr-en) with 13.53 points of difference.  As shown in Table 2, this model, CIPHER-AVG, obtains accuracy scores of 56.4, 58.6, 37.4, and 37.8 % for en, fr, fa, and sw, respectively.  When compared to the best performing PL for each CL (see bold cells in Table 2), it can be noticed that the performance gap ranges from just 1.2 percentage points for Swahili to 13.3 points for French, with an average of 6.1 points among all target languages.
Let us now compare the performance of CIPHER-AVG with that of a vanilla supervised neural model.11 Table 3 shows precision, recall, and F1 scores for the NOUN tag.  Even though CIPHER-AVG achieved mixed results (mid to low accuracy), the model robustly achieves mid-range performance according to F1-score for all CLs.  The results are even more optimistic in terms of recall for English and French, and in terms of precision for Farsi and Swahili.
Let us now compare the performance of CIPHER-AVG with that of a vanilla supervised neural model.11 Table 3 shows precision, recall, and F1 scores for the NOUN tag.  Even though CIPHER-AVG achieved mixed results (mid to low accuracy), the model robustly achieves mid-range performance according to F1-score for all CLs.  The results are even more optimistic in terms of recall for English and French, and in terms of precision for Farsi and Swahili.
Likewise, the utility of CIPHER-AVG tags for dependency parsing under zero-resource scenarios is summarized in Table 4 and Table 5.  We first analyze the effect of POS tag information at test time for the MALOPA setup in Table 4.  First we remove all POS signal except trivial punctuation information (NONE row), and, predictably, the scores drop significantly across all target languages. Then, we use our cipher tags (CIPHER row) and see improvements for all languages in LAS and for all but one language in UAS (de).
Likewise, the utility of CIPHER-AVG tags for dependency parsing under zero-resource scenarios is summarized in Table 4 and Table 5.  We first analyze the effect of POS tag information at test time for the MALOPA setup in Table 4.  First we remove all POS signal except trivial punctuation information (NONE row), and, predictably, the scores drop significantly across all target languages. Then, we use our cipher tags (CIPHER row) and see improvements for all languages in LAS and for all but one language in UAS (de).
Likewise, the utility of CIPHER-AVG tags for dependency parsing under zero-resource scenarios is summarized in Table 4 and Table 5.  We then take the next logical step and remove the parallel data-grounded embeddings, replacing them with fully unsupervised MUSE embeddings. Table 5 summarizes these results.  It can be observed that POS signal improves performance greatly for all languages when using MUSE embeddings.  Here we note a mixed result: whilst de, sv, and it do benefit from POS information, the other languages do not, obtaining great improvements from MUSE embed  dings instead.  Finally, consider MUSE-CIPHER (gold POS tags during training, cipher tags during testing). When compared to MUSE-NONE setup, it can be observed that, unfortunately, the heuristic POS tagger is too noisy and gets in MUSE's way.
Table 3 reports micro-averaged precision (P), recall (R), and F1 results for all methods, now including LWAN, in multi-label violation prediction. The results are also grouped by label frequency for all (OVERALL), FREQUENT, and FEW labels (articles), counting frequencies on the training subset.  We observe that predicting specific articles that have been violated is a much more difficult task than predicting if any article has been violated in a binary setup (cf. Table 2). Overall, HIER-BERT outperforms BIGRU-ATT and LWAN (60.0 vs. 57.  micro-F1), which is tailored for multi-labeling tasks, while being comparable with HAN (60.0 vs. 59.9 micro-F1). All models under-perform in labels with FEW training examples, demonstrating the difficulty of few-shot learning in ECHR legal judgment prediction.
Table 3 reports micro-averaged precision (P), recall (R), and F1 results for all methods, now including LWAN, in multi-label violation prediction. The results are also grouped by label frequency for all (OVERALL), FREQUENT, and FEW labels (articles), counting frequencies on the training subset.  We observe that predicting specific articles that have been violated is a much more difficult task than predicting if any article has been violated in a binary setup (cf. Table 2). Overall, HIER-BERT outperforms BIGRU-ATT and LWAN (60.0 vs. 57.  micro-F1), which is tailored for multi-labeling tasks, while being comparable with HAN (60.0 vs. 59.9 micro-F1). All models under-perform in labels with FEW training examples, demonstrating the difficulty of few-shot learning in ECHR legal judgment prediction.
Table 1 gives an overview of the historical datasets.  covering eight languages from different language families—English, German, Hungarian, Icelandic, Spanish, Portuguese, Slovene, and Swedish—as well as different text genres and time periods.
Table 1 gives an overview of the historical datasets.  covering eight languages from different language families—English, German, Hungarian, Icelandic, Spanish, Portuguese, Slovene, and Swedish—as well as different text genres and time periods.
Table 2 shows the results of this evaluation. The extent of spelling variation varies  greatly between datasets, with less than 15% of tokens requiring normalization (SLG) to more than 80% (HU). The maximum accuracy is above 97% for most datasets,  For the normalization systems, we observe significantly better word accuracy with SMT than NMT on four of the datasets, and non-significant differences on five others. There is only one dataset (DEA) where the NMT system by Tang et al. (2018) gets significantly better word accuracy than other systems.  Overall, the deep NMT model by Tang et al. (2018) consistently outperforms the shallow one by Bollmann (2018).  Finally, while Norma does produce competitive results on sev  eral datasets (particularly in the "combined" setting), it is generally significantly behind the SMT and NMT methods.
Table 2 shows the results of this evaluation. The extent of spelling variation varies  greatly between datasets, with less than 15% of tokens requiring normalization (SLG) to more than 80% (HU). The maximum accuracy is above 97% for most datasets,  For the normalization systems, we observe significantly better word accuracy with SMT than NMT on four of the datasets, and non-significant differences on five others. There is only one dataset (DEA) where the NMT system by Tang et al. (2018) gets significantly better word accuracy than other systems.  Overall, the deep NMT model by Tang et al. (2018) consistently outperforms the shallow one by Bollmann (2018).  Finally, while Norma does produce competitive results on sev  eral datasets (particularly in the "combined" setting), it is generally significantly behind the SMT and NMT methods.
BOW-SVR performs worse than BIGRU-ATT, while HAN is 10% and 3% better, respectively. HIER-BERT further improves the results, outperforming HAN by 17%.  HIER-BERT has the best ρ (.527), indicating a moderate positive correlation (> 0.5), which is not the case for the rest of the methods.
BOW-SVR performs worse than BIGRU-ATT, while HAN is 10% and 3% better, respectively. HIER-BERT further improves the results, outperforming HAN by 17%.  HIER-BERT has the best ρ (.527), indicating a moderate positive correlation (> 0.5), which is not the case for the rest of the methods.
Table 1 shows the Smatch scores (Cai and Knight, 2013) of our models, compared to a selection of previously published results.  The fixed-tree decoder seems to work well with either edge model, but performance of the projective decoder drops with the K&G edge scores.  As expected, the type-unaware baseline has low recall,  The fact that our models outperform the JAMR-style baseline so clearly is an indication that
Table 1 shows the Smatch scores (Cai and Knight, 2013) of our models, compared to a selection of previously published results.  The fixed-tree decoder seems to work well with either edge model, but performance of the projective decoder drops with the K&G edge scores.  As expected, the type-unaware baseline has low recall,  The fact that our models outperform the JAMR-style baseline so clearly is an indication that
Table 2 analyzes the performance of our two best systems (PD = projective, FTD = fixed-tree) in more detail,  and compares them to Wang's, Flanigan's, and Damonte's AMR parsers on the 2015 set and , and van Noord and Bos (2017b) for the 2017 dataset.  The good scores we achieve on reentrancy identification,
Table 2 analyzes the performance of our two best systems (PD = projective, FTD = fixed-tree) in more detail,  and compares them to Wang's, Flanigan's, and Damonte's AMR parsers on the 2015 set and , and van Noord and Bos (2017b) for the 2017 dataset.  The good scores we achieve on reentrancy identification,
Over the rules used on the 1-best result, more than 30% are non-terminal rules,  On the other hand, 30% are glue rules.  Finally, terminal rules take the largest percentage,
Over the rules used on the 1-best result, more than 30% are non-terminal rules,  On the other hand, 30% are glue rules.  Finally, terminal rules take the largest percentage,
Over the rules used on the 1-best result, more than 30% are non-terminal rules.  On the other hand, 30% are glue rules.  Finally, terminal rules take the largest percentage
Over the rules used on the 1-best result, more than 30% are non-terminal rules.  On the other hand, 30% are glue rules.  Finally, terminal rules take the largest percentage
According to the description in (CodaLab, 2019), the label distribution for dev and test sets are roughly 4% for each of the emotions. However, from the dev set (Table 2) we know that the proportions of each of the emotion categories are better described as %5 each, thereby we use %5 as the empirical estimation of distribution Pte(xtr i ). We did not use the exact proportion of dev set as the estimation to prevent the overfitting towards dev set. The sample distribution of the train set is used as Ptr(xtr i ).
According to the description in (CodaLab, 2019), the label distribution for dev and test sets are roughly 4% for each of the emotions. However, from the dev set (Table 2) we know that the proportions of each of the emotion categories are better described as %5 each, thereby we use %5 as the empirical estimation of distribution Pte(xtr i ). We did not use the exact proportion of dev set as the estimation to prevent the overfitting towards dev set. The sample distribution of the train set is used as Ptr(xtr i ).
The results are shown in Table 1. It shows that the proposed HRLCE model performs the best. The performance of SLD and SL are very close to each other, on the dev set, SLD performs better than SL but they have almost the same overall scores on the test set. The MacroF1 scores of each emotion category are very different from each other: the classification accuracy for emotion Sad is the highest in most of the cases, while the emotion Happy is the least accurately classified by all the models. We also noticed that the performance on the dev set is generally slightly better than that on the test set.
The results are shown in Table 1. It shows that the proposed HRLCE model performs the best. The performance of SLD and SL are very close to each other, on the dev set, SLD performs better than SL but they have almost the same overall scores on the test set. The MacroF1 scores of each emotion category are very different from each other: the classification accuracy for emotion Sad is the highest in most of the cases, while the emotion Happy is the least accurately classified by all the models. We also noticed that the performance on the dev set is generally slightly better than that on the test set.
Table 3 shows the intrinsic evaluation results, in which our alignment intrinsically outperforms JAMR aligner by achieving better alignment F1 score and leading to a higher scored oracle parser.
Table 3 shows the intrinsic evaluation results, in which our alignment intrinsically outperforms JAMR aligner by achieving better alignment F1 score and leading to a higher scored oracle parser.
Table 4 shows the results. From this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7.
Table 4 shows the results. From this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7.
Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works.  The second block in Table 6 shows the results of our ensemble parser, in which ensemble significantly improves the performance and more parsers ensembled, more improvements are achieved.
Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works.  The second block in Table 6 shows the results of our ensemble parser, in which ensemble significantly improves the performance and more parsers ensembled, more improvements are achieved.
We compare our model to Float Parser (FP) (Pasupat and Liang, 2015), Neural Programmer (NP) (Neelakantan et al., 2016b), DYNSP (Iyyer et al., 2017) and CAMP (Sun et al., 2018b) in Table 1.  We observe that our model improves the SOTA from 45.6% by CAMP to 55.1% in question accuracy (ALL), reducing the relative error rate by 18%. For the initial question (POS1), however, it is behind DYNSP by 3.7%. More interestingly, our model handles follow up questions especially well outperforming the previously best model FP by 20% on POS3, a 28% relative error reduction.  We observe that our model effectively leverages the context information by improving the average question accuracy from 45.1% to 55.1% in comparison to the use of context in DYNSP yielding 2.7% improvement.  If we provide the previous reference answers, the average question accuracy jumps to 61.7%, showing that 6.6% of the errors are due to error propagation.
We designed ESE to output thirty candidate entities (NPs) ranked based on the similarity to the seed term. Therefore, we calculated precision at k (p@k) where k is always 30.  Table 2 shows the best results when using the feature ensemble method which is more stable than the non-ensemble one (due to lower standard deviation and non-zero precision). According to the results, the best combination in terms of the mean and standard deviation is obtained when using TFIDF (Eq.5) to weigh the edges and context-dependent similarity (Eq.8) to rank NPs. This shows that the uniqueness and the significant overlap of features between noun phrases were very important.
We designed ESE to output thirty candidate entities (NPs) ranked based on the similarity to the seed term. Therefore, we calculated precision at k (p@k) where k is always 30.  Table 2 shows the best results when using the feature ensemble method which is more stable than the non-ensemble one (due to lower standard deviation and non-zero precision). According to the results, the best combination in terms of the mean and standard deviation is obtained when using TFIDF (Eq.5) to weigh the edges and context-dependent similarity (Eq.8) to rank NPs. This shows that the uniqueness and the significant overlap of features between noun phrases were very important.
Finally, for the last setting, we tested the system using the three auto-annotation modes (i.e., FA, HFA,  and UFA) as shown in Table 3. While the auto-annotation mode can allow us to reduce up to 87% of the data pool, this drastic saving also reduces the accuracy of the learned model, achieving, on average, around 81% F-Score. Overall, our framework presents a trade off between coverage and annotation cost. The HFA auto-annotation mode shows the benefit, especially in a realistic enterprise setting, when we need to annotate 33% of the data to increase F-Score by only 10% (when comparing the on average performance of HFA with ESA) is unreasonable. Table 3 appears to show FA being inferior to EAL in terms of the percentage cut for the Location class, for example. In reality FA reduced sentence annotation by 65% to reach 0.99 F-Score. But as our testing criteria demanded that we either reach 1.0 F-Score or finish all sentences from the pool, FA tried to finish the pool without any further performance improvement on the 0.99 F-Score.
Finally, for the last setting, we tested the system using the three auto-annotation modes (i.e., FA, HFA,  and UFA) as shown in Table 3. While the auto-annotation mode can allow us to reduce up to 87% of the data pool, this drastic saving also reduces the accuracy of the learned model, achieving, on average, around 81% F-Score. Overall, our framework presents a trade off between coverage and annotation cost. The HFA auto-annotation mode shows the benefit, especially in a realistic enterprise setting, when we need to annotate 33% of the data to increase F-Score by only 10% (when comparing the on average performance of HFA with ESA) is unreasonable. Table 3 appears to show FA being inferior to EAL in terms of the percentage cut for the Location class, for example. In reality FA reduced sentence annotation by 65% to reach 0.99 F-Score. But as our testing criteria demanded that we either reach 1.0 F-Score or finish all sentences from the pool, FA tried to finish the pool without any further performance improvement on the 0.99 F-Score.
The coreference annotated portion of the corpus contains 1.59 million tokens from multiple genres, presented in Table 1.  Written data constitutes the large bulk of material, primarily from newswire (Wall Street Journal data), as well as some data from the Web and the New Testament, and some translations of news and online discussions in Arabic and Chinese. The translated data has been placed in its own category: it behaves more conservatively in preferring strict agreement than non-translated language (see Section 4.2), perhaps due to translators' editorial practices. The spoken data comes primarily from television broadcasts, including dialogue data from MSNBC, Phoenix and other broadcast sources (bc.conv), or news, from CNN, ABC and others (bc.news), as well as phone conversations.
The coreference annotated portion of the corpus contains 1.59 million tokens from multiple genres, presented in Table 1.  Written data constitutes the large bulk of material, primarily from newswire (Wall Street Journal data), as well as some data from the Web and the New Testament, and some translations of news and online discussions in Arabic and Chinese. The translated data has been placed in its own category: it behaves more conservatively in preferring strict agreement than non-translated language (see Section 4.2), perhaps due to translators' editorial practices. The spoken data comes primarily from television broadcasts, including dialogue data from MSNBC, Phoenix and other broadcast sources (bc.conv), or news, from CNN, ABC and others (bc.news), as well as phone conversations.
Looking at the actual classifications obtained by the classifier produces the confusion matrix in Table 2. The matrix makes it clear that the classifier is very good at avoiding errors against the majority class: it almost never guesses 'notional' when it shouldn't. Conversely, about 1/3 of actual notional cases are misclassified, predicted to be 'strict'. Among the erroneous cases, only 6 belong to Type III (about 15% of errors) , showing that the classifier largely handles this type quite well next to the other types, since Type III covers about 20% of plural-to-singular agreement cases.
Looking at the actual classifications obtained by the classifier produces the confusion matrix in Table 2. The matrix makes it clear that the classifier is very good at avoiding errors against the majority class: it almost never guesses 'notional' when it shouldn't. Conversely, about 1/3 of actual notional cases are misclassified, predicted to be 'strict'. Among the erroneous cases, only 6 belong to Type III (about 15% of errors) , showing that the classifier largely handles this type quite well next to the other types, since Type III covers about 20% of plural-to-singular agreement cases.
Next we can consider the effect of genre, and expectations that speech promotes notional agreement. This is confirmed in Table 3. However we note that individual genres do behave differently: data from the Web is closer to spoken language. The most restrictive genre in avoiding notional agreement is translations. Both of these facts may reflect a combination of modality, genre and editorial practice effects. However the strong differences suggest that genre is likely crucial to any model attempting to predict this phenomenon.
Next we can consider the effect of genre, and expectations that speech promotes notional agreement. This is confirmed in Table 3. However we note that individual genres do behave differently: data from the Web is closer to spoken language. The most restrictive genre in avoiding notional agreement is translations. Both of these facts may reflect a combination of modality, genre and editorial practice effects. However the strong differences suggest that genre is likely crucial to any model attempting to predict this phenomenon.
We also show the number of propositions in each category in Table 3. The most frequent types are evaluation (38.3%) and fact (36.5%).
Table 4 shows that BiLSTM-CRF outperforms other methods in F1. More importantly, the perfor  mance on reviews is lower than those reached on existing datasets, e.g., an F1 of 86.7 is obtained by CRF for essays (Stab and Gurevych, 2017).
Table 4 shows that BiLSTM-CRF outperforms other methods in F1. More importantly, the perfor  mance on reviews is lower than those reached on existing datasets, e.g., an F1 of 86.7 is obtained by CRF for essays (Stab and Gurevych, 2017).
Table 4 reports the results for all of the 2-layer L-, T-, and H-biLSTMs.7 The best-performing system for each dataset and metric are highlighted in purple, and when the best-performing system for a particular dataset was a 1-layer model, that system is included in Table 4.  The highest-performing system for each is reported in Table 4.  On its own, the biLSTM with linear topology (L-biLSTM) performs  consistently better than the biLSTM with tree topology (T-biLSTM). However, the hybrid topology (H-biLSTM), consisting of both a L- and TbiLSTM is the top-performing system on UW for correlation (Table 4).  Though our methods achieve state of the art in the single-task setting, the best performing systems are mostly multi-task (Table 4 and Supplementary Materials).
Table 4 reports the results for all of the 2-layer L-, T-, and H-biLSTMs.7 The best-performing system for each dataset and metric are highlighted in purple, and when the best-performing system for a particular dataset was a 1-layer model, that system is included in Table 4.  The highest-performing system for each is reported in Table 4.  On its own, the biLSTM with linear topology (L-biLSTM) performs  consistently better than the biLSTM with tree topology (T-biLSTM). However, the hybrid topology (H-biLSTM), consisting of both a L- and TbiLSTM is the top-performing system on UW for correlation (Table 4).  Though our methods achieve state of the art in the single-task setting, the best performing systems are mostly multi-task (Table 4 and Supplementary Materials).
F1 scores for all propositions and each type are reported in Table 5.  CNN performs better for types with significantly more training samples, i.e., evaluation and fact, indicating the effect of data size on neural model's performance. Joint models (CRF-joint and BiLSTM-CRF-joint) yield the best F1 scores for all categories when gold-standard segmentation is unavailable.
F1 scores for all propositions and each type are reported in Table 5.  CNN performs better for types with significantly more training samples, i.e., evaluation and fact, indicating the effect of data size on neural model's performance. Joint models (CRF-joint and BiLSTM-CRF-joint) yield the best F1 scores for all categories when gold-standard segmentation is unavailable.
Table 1 lists all hyper-parameters which have all been chosen using only training and validation data. The two encoders have been implemented using a Bidirectional Long Short-Term Memory (B-LSTM) (Hochreiter and Schmidhuber, 1997) while the decoder uses a unidirectional LSTM. Both the encoders and the decoder use two hidden layers. For the attention network, we have used the OpenNMT's general option (Luong et al., 2015).
Table 1 lists all hyper-parameters which have all been chosen using only training and validation data. The two encoders have been implemented using a Bidirectional Long Short-Term Memory (B-LSTM) (Hochreiter and Schmidhuber, 1997) while the decoder uses a unidirectional LSTM. Both the encoders and the decoder use two hidden layers. For the attention network, we have used the OpenNMT's general option (Luong et al., 2015).
Table 2 compares the accuracy of our model on the test data with two baselines and two state-of-theart comparable systems. The MT baseline simply consists of the accuracy of the mt sentences with respect to the pe ground truth. The other baseline is given by a statistical PE (SPE) system (Simard et al., 2007) chosen by the WMT17 organizers. Table 2 shows that when our model is trained with only the 11K WMT17 official training sentences, it cannot even approach the baselines. Even when the 12K WMT16 sentences are added, its accuracy is still well below that of the baselines. However, when the 500K artificial data are added, it reports a major improvement and it outperforms them both significantly. In addition, we have compared our model with two recent systems that have used our  same training settings (500K artificial triplets + 23K manual triplets oversampled 10 times), reporting a slightly higher accuracy than both (1.43 TER and 1.93 BLEU p.p. over (Varis and Bojar, 2017) and 0.21 TER and 0.30 BLEU p.p. over (B´erard et al., 2017)). Since their models explicitly predicts edit operations rather than post-edited sentences, we speculate that these two tasks are of comparable intrinsic complexity.
Table 2 compares the accuracy of our model on the test data with two baselines and two state-of-theart comparable systems. The MT baseline simply consists of the accuracy of the mt sentences with respect to the pe ground truth. The other baseline is given by a statistical PE (SPE) system (Simard et al., 2007) chosen by the WMT17 organizers. Table 2 shows that when our model is trained with only the 11K WMT17 official training sentences, it cannot even approach the baselines. Even when the 12K WMT16 sentences are added, its accuracy is still well below that of the baselines. However, when the 500K artificial data are added, it reports a major improvement and it outperforms them both significantly. In addition, we have compared our model with two recent systems that have used our  same training settings (500K artificial triplets + 23K manual triplets oversampled 10 times), reporting a slightly higher accuracy than both (1.43 TER and 1.93 BLEU p.p. over (Varis and Bojar, 2017) and 0.21 TER and 0.30 BLEU p.p. over (B´erard et al., 2017)). Since their models explicitly predicts edit operations rather than post-edited sentences, we speculate that these two tasks are of comparable intrinsic complexity.
Table 3 displays the best performing DSMfor each property. There is a preference to word2vec and to a higher embedding dimension.
Table 3 displays the best performing DSMfor each property. There is a preference to word2vec and to a higher embedding dimension.
Evidence of this complementarity can be seen in Table 6, which contains a breakdown of system performance by governing dependency relation, for both linear and tree models, on UDS-IH2-dev. In most cases, the L-biLSTM's mean prediction is closer to the true mean. This appears to arise in part because the T-biLSTM is less confident in its predictions – i.e. its mean prediction tends to be closer to 0. This results in the L-biLSTM being too confident in certain cases – e.g. in the case of the xcomp governing relation, where the T-biLSTM mean prediction is closer to the true mean.
Evidence of this complementarity can be seen in Table 6, which contains a breakdown of system performance by governing dependency relation, for both linear and tree models, on UDS-IH2-dev. In most cases, the L-biLSTM's mean prediction is closer to the true mean. This appears to arise in part because the T-biLSTM is less confident in its predictions – i.e. its mean prediction tends to be closer to 0. This results in the L-biLSTM being too confident in certain cases – e.g. in the case of the xcomp governing relation, where the T-biLSTM mean prediction is closer to the true mean.
we augment our models with existing linguistic and affective knowledge from human experts. Specifically, we leverage lexica containing psycho-linguistic, sentiment and emotion annotations.  the word's annotations from the lexicons shown in Table 1.  As prior knowledge, we leverage the lexicons presented in Table 1. We selected widely-used lexicons that represent different facets of affective and psycho-linguistic features, namely;
we augment our models with existing linguistic and affective knowledge from human experts. Specifically, we leverage lexica containing psycho-linguistic, sentiment and emotion annotations.  the word's annotations from the lexicons shown in Table 1.  As prior knowledge, we leverage the lexicons presented in Table 1. We selected widely-used lexicons that represent different facets of affective and psycho-linguistic features, namely;
We present our results in Section 5 (Table 3)  In Table 3 we use the abbreviations "baseline" and "emb. conc." for the two baseline models respectively.  We compare the performance of the three proposed conditioning methods with the two baselines and the state-of-the-art in Table 3.  The results show that incorporating external knowledge in RNN-based architectures consistently improves performance over the baseline for all datasets. Furthermore, feature-based gating improves upon baseline concatenation in the embedding layer across benchmarks, with the exception of PsychExp dataset.  For the Sent17 dataset we achieve state-ofthe-art F1 score using the feature-based gating method; we further improve performance when combining gating with the emb. conc. method. For SST-5, we observe a significant performance boost with combined attentional gating and embedding conditioning (gate + emb. conc.). For PsychExp, we marginally outperform the state-ofthe-art also with the combined method, while for Irony18, feature-based gating yields the best results. Finally, concatenation based conditioning is the top method for SCv1, and the combination method for SCv2.
We present our results in Section 5 (Table 3)  In Table 3 we use the abbreviations "baseline" and "emb. conc." for the two baseline models respectively.  We compare the performance of the three proposed conditioning methods with the two baselines and the state-of-the-art in Table 3.  The results show that incorporating external knowledge in RNN-based architectures consistently improves performance over the baseline for all datasets. Furthermore, feature-based gating improves upon baseline concatenation in the embedding layer across benchmarks, with the exception of PsychExp dataset.  For the Sent17 dataset we achieve state-ofthe-art F1 score using the feature-based gating method; we further improve performance when combining gating with the emb. conc. method. For SST-5, we observe a significant performance boost with combined attentional gating and embedding conditioning (gate + emb. conc.). For PsychExp, we marginally outperform the state-ofthe-art also with the combined method, while for Irony18, feature-based gating yields the best results. Finally, concatenation based conditioning is the top method for SCv1, and the combination method for SCv2.
Table 5 illustrates the influence of modals and negation on the factuality of the events they have direct scope over. The context with the highest factuality on average is no direct modal and no negation (first row); all other modal contexts have varying degrees of negative mean factuality scores, with will as the most negative.
Table 5 illustrates the influence of modals and negation on the factuality of the events they have direct scope over. The context with the highest factuality on average is no direct modal and no negation (first row); all other modal contexts have varying degrees of negative mean factuality scores, with will as the most negative.
As seen in Table 2, we report the coefficient of determination (r2) for each global metric and dataset.  Note that global metrics do correlate somewhat with human judgment of local topic quality. However, the correlation is moderate to poor, especially in the case of coherence
As seen in Table 2, we report the coefficient of determination (r2) for each global metric and dataset.  Note that global metrics do correlate somewhat with human judgment of local topic quality. However, the correlation is moderate to poor, especially in the case of coherence
Humans agree more often with models trained on Amazon reviews than on New York Times.  As seen in Table 3, we report the coefficient of determination (r2) for each metric and dataset.  SWITCHP most closely approximates human judgments of local topic quality, with an r2 which indicates a strong correlation.  As evidenced by the lower r2 for SWITCHVI, even switching between related topics does not seem to line up with human judgments of local topic quality.
We summarize the main results in Table 3.  Frequency is the best performing baseline. Its precision at 1 and 5 are higher than 40%. PageRank performs worse than Frequency on all  the precision and recall metrics. Location performs the worst.  LeToR outperforms the baselines significantly on all metrics. Particularly, its P@1 value outperforms the Frequency baseline the most (4.64%), indicating a much better estimation on the most salient event. In terms of AUC, LeToR outperforms Frequency by a large margin (11.19% relative gain).  The KCE model further beats LeToR significantly on all metrics, by around 5% on AUC and precision values, and by around 10% on the recall values. Notably, the P@1 score is much higher, reaching 50%. The large relative gain on all the recall metrics and the high performance on precision show that KCE works really well on the top of the rank list.  To understand the source of performance gain of KCE, we conduct an ablation study by removing its components: -E removes of entity kernels; -EF removes the entity kernels and the features. We observe a performance drop in both cases.
We summarize the main results in Table 3.  Frequency is the best performing baseline. Its precision at 1 and 5 are higher than 40%. PageRank performs worse than Frequency on all  the precision and recall metrics. Location performs the worst.  LeToR outperforms the baselines significantly on all metrics. Particularly, its P@1 value outperforms the Frequency baseline the most (4.64%), indicating a much better estimation on the most salient event. In terms of AUC, LeToR outperforms Frequency by a large margin (11.19% relative gain).  The KCE model further beats LeToR significantly on all metrics, by around 5% on AUC and precision values, and by around 10% on the recall values. Notably, the P@1 score is much higher, reaching 50%. The large relative gain on all the recall metrics and the high performance on precision show that KCE works really well on the top of the rank list.  To understand the source of performance gain of KCE, we conduct an ablation study by removing its components: -E removes of entity kernels; -EF removes the entity kernels and the features. We observe a performance drop in both cases.
We gradually add feature groups to the Frequency baseline. The combination of Location (sentence location) and Frequency almost sets the performance for the whole model. Adding each voting feature individually produces mixed results. However, adding all voting features improves all metrics. Though the margin is small, 4 of them are statistically significant over Frequency+Location.  To understand the contribution of individual features, we conduct an ablation study of various feature settings in Table 4.
We gradually add feature groups to the Frequency baseline. The combination of Location (sentence location) and Frequency almost sets the performance for the whole model. Adding each voting feature individually produces mixed results. However, adding all voting features improves all metrics. Though the margin is small, 4 of them are statistically significant over Frequency+Location.  To understand the contribution of individual features, we conduct an ablation study of various feature settings in Table 4.
Table 7 shows results from a manual error analysis on 50 events from UDS-IH2-dev with highest absolute prediction error (using H-biLSTM(2)MultiSim w/UDS-IH2). Grammatical errors (such as run-on sentences) in the underlying text of UDS-IH2 appear to pose a particular challenge for these models;
Table 7 shows results from a manual error analysis on 50 events from UDS-IH2-dev with highest absolute prediction error (using H-biLSTM(2)MultiSim w/UDS-IH2). Grammatical errors (such as run-on sentences) in the underlying text of UDS-IH2 appear to pose a particular challenge for these models;
We inspect some pairs of events and entities in different kernels and list some examples in Table 5.  The pairs in Table 3 exhibit interesting types of relations: e.g.,"arrest-charge" and "attack-kill" form script-like chains; "911 attack" forms a quasiidentity relation (Recasens et al., 2010) with "attack"; "business" and "increase" are candidates as frame-argument structure.
We inspect some pairs of events and entities in different kernels and list some examples in Table 5.  The pairs in Table 3 exhibit interesting types of relations: e.g.,"arrest-charge" and "attack-kill" form script-like chains; "911 attack" forms a quasiidentity relation (Recasens et al., 2010) with "attack"; "business" and "increase" are candidates as frame-argument structure.
We present sample generations, quality results, and diversity results respectively in Tables 1, 2, 3.  This observation is further bolstered by the fact that the GPT generations have a higher corpus-BLEU with TBC than TBC has with itself.  The corpusBLEU between BERT models and the datasets is low, particularly with WT103.  We find that BERT generations are more diverse than GPT generations. GPT has high n-gram overlap (smaller percent of unique n-grams) with TBC, but surprisingly also with WikiText-103, despite being trained on different data. Furthermore, GPT generations have greater n-gram overlap with these datasets than these datasets have with themselves, further suggesting that GPT is relying significantly on generic sentences. BERT has lower n-gram overlap with both corpora, with similar degrees of n-gram overlap as the samples of the data.
We present sample generations, quality results, and diversity results respectively in Tables 1, 2, 3.  This observation is further bolstered by the fact that the GPT generations have a higher corpus-BLEU with TBC than TBC has with itself.  The corpusBLEU between BERT models and the datasets is low, particularly with WT103.  We find that BERT generations are more diverse than GPT generations. GPT has high n-gram overlap (smaller percent of unique n-grams) with TBC, but surprisingly also with WikiText-103, despite being trained on different data. Furthermore, GPT generations have greater n-gram overlap with these datasets than these datasets have with themselves, further suggesting that GPT is relying significantly on generic sentences. BERT has lower n-gram overlap with both corpora, with similar degrees of n-gram overlap as the samples of the data.
We present sample generations, quality results, and diversity results respectively in Tables 1, 2, 3.  We find that, compared to GPT, the BERT generations are of worse quality, but are more diverse. Surprisingly, the outside language model, which was trained on Wikipedia, is less perplexed by the GPT generations than the BERT generations, even though GPT was only trained on romance novels and BERT was trained on romance novels and Wikipedia. On actual data from TBC, the outside language model is about as perplexed as on the BERT generations, which suggests that domain shift is an issue in using a trained language  model for evaluating generations and that the GPT generations might have collapsed to fairly generic and simple sentences.  The perplexity on BERT samples is not absurdly high, and in reading the samples, we find that many are fairly coherent.
We present sample generations, quality results, and diversity results respectively in Tables 1, 2, 3.  We find that, compared to GPT, the BERT generations are of worse quality, but are more diverse. Surprisingly, the outside language model, which was trained on Wikipedia, is less perplexed by the GPT generations than the BERT generations, even though GPT was only trained on romance novels and BERT was trained on romance novels and Wikipedia. On actual data from TBC, the outside language model is about as perplexed as on the BERT generations, which suggests that domain shift is an issue in using a trained language  model for evaluating generations and that the GPT generations might have collapsed to fairly generic and simple sentences.  The perplexity on BERT samples is not absurdly high, and in reading the samples, we find that many are fairly coherent.
First, we compare to systems which relied on Wikipedia and those which used Wikipedia along with unlabeled data ('Wikipedia + unlab'), i.e. the top half of Table 1. These methods are comparable to ours, as they use the same type of information as supervision. Our model outperformed all of them on all test sets.  When evaluated on AIDA-B, their scores are still lower than ours, though significantly higher that those of the previous systems suggesting that web links are indeed valuable.  Second, we compare to fully-supervised systems, which were estimated on AIDA-CoNLL documents.  We distinguish results on a test set taken from AIDA-CoNLL (AIDA-B) and the other standard test sets not directly corresponding to the AIDA-CoNLL domain. When tested on the latter, our approach is very effective, on average outperforming fully-supervised techniques.  As expected, on the in-domain test set (AIDA-B), the majority of recent fully-supervised methods are more accurate than our model. However, even on this test set our model is not as far behind, for example, outperforming the system of Guo and Barbosa (2016).
The results are shown in Table 2 ('Wikipedia'). The resulting model is significantly less accurate than the one which used unlabeled documents. The score difference is larger  for AIDA-CoNLL test set than for the other 5 test sets.  Additionally we train our model on AIDA-CoNLL, producing its fully-supervised version ('AIDA CoNLL' row in Table 2). Though, as expected, this version is more accurate on AIDA test set, similarly to other fully-supervised methods, it overfits and does not perform that well on the 5 out-of-domain test sets.
The results are shown in Table 2 ('Wikipedia'). The resulting model is significantly less accurate than the one which used unlabeled documents. The score difference is larger  for AIDA-CoNLL test set than for the other 5 test sets.  Additionally we train our model on AIDA-CoNLL, producing its fully-supervised version ('AIDA CoNLL' row in Table 2). Though, as expected, this version is more accurate on AIDA test set, similarly to other fully-supervised methods, it overfits and does not perform that well on the 5 out-of-domain test sets.
As we do not want to test multiple systems on the final test set, we report the remaining ablations on the development set (AIDA-A), Table 3.  we constructed a baseline which only relies on link statistics in Wikipedia as well as string similarity (we refereed to its scoring function as sc). It appears surprisingly strong, however, we still outperform it by 1.6% (see Table 3).  When we use only global coherence (i.e. only second term in expression (1)) and drop any modeling of local context on the disambiguation stage, the performance drops very substantially (to 82.4% F1, see Table 3).  Without using local scores the disambiguation model appears to be even less accurate than our 'no-statisticaldisambiguation' baseline. It is also important to have an accurate global model: not using global attention results in a 1.2% drop in performance.
As we do not want to test multiple systems on the final test set, we report the remaining ablations on the development set (AIDA-A), Table 3.  we constructed a baseline which only relies on link statistics in Wikipedia as well as string similarity (we refereed to its scoring function as sc). It appears surprisingly strong, however, we still outperform it by 1.6% (see Table 3).  When we use only global coherence (i.e. only second term in expression (1)) and drop any modeling of local context on the disambiguation stage, the performance drops very substantially (to 82.4% F1, see Table 3).  Without using local scores the disambiguation model appears to be even less accurate than our 'no-statisticaldisambiguation' baseline. It is also important to have an accurate global model: not using global attention results in a 1.2% drop in performance.
Figure 4 shows the accuracy of two systems for different NER (named entity recognition) types. We consider four types: location (LOC), organization (ORG), person (PER), and miscellany (MICS). These types are given in CoNLL 2003 dataset, which was used as a basis for AIDA CoNLL. Our model is accurate for PER, achieving accuracy of about 97%, only 0.53% lower than the supervised model.
Figure 4 shows the accuracy of two systems for different NER (named entity recognition) types. We consider four types: location (LOC), organization (ORG), person (PER), and miscellany (MICS). These types are given in CoNLL 2003 dataset, which was used as a basis for AIDA CoNLL. Our model is accurate for PER, achieving accuracy of about 97%, only 0.53% lower than the supervised model.
We perform hyperparameter optimisation and make comparisons among our systems, including GCN + Bi-LSTM (GCN-based), CNN + attention + Bi-LSTM (Attbased), and their combination using a highway layer (H-combined) in Table 1.  Systems are evaluated using two types of precision, recall and F-score measures: strict MWEbased scores (every component of an MWE should be correctly tagged to be considered as true positive), and token-based scores (a partial match between a predicted and a gold MWE would be considered as true positive). We report results for all MWEs as well as discontinuous ones specifically.  GCN-based outperforms Att-based and they both outperform the strong baseline in terms of MWE-based F-score in three out of four languages. Combining GCN with attention using highway networks results in further improvements for EN, FR and FA. The Hcombined model consistently exceeds the baseline for all languages.  GCN and H-combined models each show significant improvement with regard to discontinuous MWEs, regardless of the proportion of such expressions.  The overall results confirm our assumption that a hybrid architecture can mitigate errors of individual models and bolster their strengths.
We perform hyperparameter optimisation and make comparisons among our systems, including GCN + Bi-LSTM (GCN-based), CNN + attention + Bi-LSTM (Attbased), and their combination using a highway layer (H-combined) in Table 1.  Systems are evaluated using two types of precision, recall and F-score measures: strict MWEbased scores (every component of an MWE should be correctly tagged to be considered as true positive), and token-based scores (a partial match between a predicted and a gold MWE would be considered as true positive). We report results for all MWEs as well as discontinuous ones specifically.  GCN-based outperforms Att-based and they both outperform the strong baseline in terms of MWE-based F-score in three out of four languages. Combining GCN with attention using highway networks results in further improvements for EN, FR and FA. The Hcombined model consistently exceeds the baseline for all languages.  GCN and H-combined models each show significant improvement with regard to discontinuous MWEs, regardless of the proportion of such expressions.  The overall results confirm our assumption that a hybrid architecture can mitigate errors of individual models and bolster their strengths.
Despite the underperformance of these features overall, Table 9 shows that they may still improve performance in the subset of instances where they appear.
we show the superior performance (in terms of MWE-based F-score) of our top systems on the test data compared to the baseline and stateof-the-art systems, namely, ATILF-LLF (Al Saied et al., 2017) and SHOMA (Taslimipoor and Rohanian, 2018). GCN works the best for discontinuous MWEs in EN and FA, while H-combined outperforms based on results for all MWEs except for FA.  The overall results confirm our assumption that a hybrid architecture can mitigate errors of individual models and bolster their strengths.
we show the superior performance (in terms of MWE-based F-score) of our top systems on the test data compared to the baseline and stateof-the-art systems, namely, ATILF-LLF (Al Saied et al., 2017) and SHOMA (Taslimipoor and Rohanian, 2018). GCN works the best for discontinuous MWEs in EN and FA, while H-combined outperforms based on results for all MWEs except for FA.  The overall results confirm our assumption that a hybrid architecture can mitigate errors of individual models and bolster their strengths.
Table 4 presents an alternative view of the results of the pretraining experiment (Table 2): The table shows correlations between pairs of target tasks over the space of pretrained encoders. The correlations reflect the degree to which the performance on one target task with some encoder predicts performance on another target task with the same encoder. See Appendix D for the full table and similar tables for intermediate ELMo and BERT experiments. Many correlations are low, suggesting that different tasks benefit from different forms of pretraining to a substantial degree, and bolstering the observation that no single pretraining task yields good performance on all target tasks. For reasons noted earlier, the models that tended to perform best overall also tended to overfit the WNLI training set most, leading to a negative correlation between WNLI and overall GLUE score. STS also shows a negative correlation, likely due to the observation that it does not benefit from LM pretraining. In contrast, CoLA shows a strong correlation with the overall GLUE scores, but has weak or negative correlations with many tasks: The use of LM pretraining dramatically improves CoLA performance, but most other forms of pretraining have little effect.
Table 4 presents an alternative view of the results of the pretraining experiment (Table 2): The table shows correlations between pairs of target tasks over the space of pretrained encoders. The correlations reflect the degree to which the performance on one target task with some encoder predicts performance on another target task with the same encoder. See Appendix D for the full table and similar tables for intermediate ELMo and BERT experiments. Many correlations are low, suggesting that different tasks benefit from different forms of pretraining to a substantial degree, and bolstering the observation that no single pretraining task yields good performance on all target tasks. For reasons noted earlier, the models that tended to perform best overall also tended to overfit the WNLI training set most, leading to a negative correlation between WNLI and overall GLUE score. STS also shows a negative correlation, likely due to the observation that it does not benefit from LM pretraining. In contrast, CoLA shows a strong correlation with the overall GLUE scores, but has weak or negative correlations with many tasks: The use of LM pretraining dramatically improves CoLA performance, but most other forms of pretraining have little effect.
The distribution of their named-entity (NE) tags, according to Stanford NE-tagger, is shown in Table 1. While temporal-related numbers are the most frequent, around 40% are classified only as unspecific NUMBER. By manually checking 100 random NUMBERs, we observed that 47 are relation cardinalities,1 i.e., approximately 18.86% of all numbers in Wikipedia are relation cardinalities.
The distribution of their named-entity (NE) tags, according to Stanford NE-tagger, is shown in Table 1. While temporal-related numbers are the most frequent, around 40% are classified only as unspecific NUMBER. By manually checking 100 random NUMBERs, we observed that 47 are relation cardinalities,1 i.e., approximately 18.86% of all numbers in Wikipedia are relation cardinalities.
Table 2 shows the performance of our CRF-based method in finding the correct relation cardinality, evaluated on manually annotated 20 (has part), 100 (admin. terr. entity) and 200 (child and spouse) randomly selected subjects that have at least one object.  The random-number baseline achieves a precision of 5% (has part), 3.5% (admin. territ. entity), 0% (spouse) and 11.2% (child). Compared to that, especially using only-nummod, our method gives encouraging results for has part, admin. territ. entity and child, with 30-50% precision and around 30% F1-score.  As shown by the last row of Table 2, higher quality of training data can considerably boost the performance of cardinality extraction.
Table 2 shows the performance of our CRF-based method in finding the correct relation cardinality, evaluated on manually annotated 20 (has part), 100 (admin. terr. entity) and 200 (child and spouse) randomly selected subjects that have at least one object.  The random-number baseline achieves a precision of 5% (has part), 3.5% (admin. territ. entity), 0% (spouse) and 11.2% (child). Compared to that, especially using only-nummod, our method gives encouraging results for has part, admin. territ. entity and child, with 30-50% precision and around 30% F1-score.  As shown by the last row of Table 2, higher quality of training data can considerably boost the performance of cardinality extraction.
The experimental results are shown in Table 1, with Int denoting internal comparisons (with three groups) and Ext denoting external comparisons, the highest LAS in each group is marked in bold face.  In the first group, we compare the LAS of the four single models WORD, W2V, LSTM, and CNN. In macro average of all languages, the CNN model performs 2.17% higher than the WORD model, and 1.24% higher than the W2V model. The LSTM model, however, performs only 0.9% higher than the WORD model and 1.27% lower than the CNN model.  In the second group, we observe that the additional word-lookup model does not significantly improve the CNN moodel (from 82.75% in CNN to 82.90% in CNN+WORD on average) while the LSTM model is improved by a much larger margin (from 81.48% in LSTM to 82.56% in LSTM+WORD on average). This suggests that the CNN model has already learned the most important information from the the word forms, while the LSTM model has not. Also, the combined CNN+WORD model is still better than the LSTM+WORD model, despite the large improvement in the latter.  While comparing to the best published results (Björkelund et al., 2013, 2014), we have to note that their approach uses explicit morphological features, ensemble, ranking, etc., which all can boost parsing performance. We only use a greedy parser with much fewer features, but bridge the 6 points gap between the previous best greedy parser and the best published result by more than one half.  On average, the B15-LSTM model improves their own baseline by 1.1%, similar to the 0.9% improvement of our LSTM model, which is much smaller than the 2.17% improvement of the CNN model. Furthermore, the CNN model is improved from a strong baseline: our WORD model performs already 2.22% higher than the B15-WORD model.  Comparing the individual performances on each language, we observe that the CNN model almost always outperforms the WORD model except for Hebrew. However, both LSTM and B15-LSTM perform higher than baseline only on the three agglutinative languages (Basque, Hungarian, and Korean), and lower than baseline on the other six.
The experimental results are shown in Table 1, with Int denoting internal comparisons (with three groups) and Ext denoting external comparisons, the highest LAS in each group is marked in bold face.  In the first group, we compare the LAS of the four single models WORD, W2V, LSTM, and CNN. In macro average of all languages, the CNN model performs 2.17% higher than the WORD model, and 1.24% higher than the W2V model. The LSTM model, however, performs only 0.9% higher than the WORD model and 1.27% lower than the CNN model.  In the second group, we observe that the additional word-lookup model does not significantly improve the CNN moodel (from 82.75% in CNN to 82.90% in CNN+WORD on average) while the LSTM model is improved by a much larger margin (from 81.48% in LSTM to 82.56% in LSTM+WORD on average). This suggests that the CNN model has already learned the most important information from the the word forms, while the LSTM model has not. Also, the combined CNN+WORD model is still better than the LSTM+WORD model, despite the large improvement in the latter.  While comparing to the best published results (Björkelund et al., 2013, 2014), we have to note that their approach uses explicit morphological features, ensemble, ranking, etc., which all can boost parsing performance. We only use a greedy parser with much fewer features, but bridge the 6 points gap between the previous best greedy parser and the best published result by more than one half.  On average, the B15-LSTM model improves their own baseline by 1.1%, similar to the 0.9% improvement of our LSTM model, which is much smaller than the 2.17% improvement of the CNN model. Furthermore, the CNN model is improved from a strong baseline: our WORD model performs already 2.22% higher than the B15-WORD model.  Comparing the individual performances on each language, we observe that the CNN model almost always outperforms the WORD model except for Hebrew. However, both LSTM and B15-LSTM perform higher than baseline only on the three agglutinative languages (Basque, Hungarian, and Korean), and lower than baseline on the other six.
Table 2 shows the results, where the two cases are denoted as ΔIV and ΔOOV. The general trend in the results is that the improvements of both models in the OOV case are larger than in the IV case, which means that the character composition models indeed alleviates the OOV problem. Also, CNN improves on seven languages in the IV case and eight languages in the OOV case, and it performs consistently better than LSTM in both cases.
averaged. The Table 3 presents the results of these experiments. The Table 3 shows, that: 1. in monolingual setting, we can get highquality results. The scores are significantly lower than the scores of the same model on the standard dataset, due to the smaller sizes of the training datasets. Nevertheless, one can see, that our approach to word stress detection applies not only to the Russian language data, but also to the data in the Belarusian and Ukrainian languages; 2. cross-lingual setting (1): the Belarusian training dataset, being the smallest one among the three datasets, is not a good source for training word stress detection models in other languages, while the Ukrainian dataset stands  out as a good source for training word stress detection systems both for the Russian and Belarusian languages; 3. cross-lingual setting (2): adding one or two datasets to the other languages improves the quality. For example, around 10% of accuracy is gained by adding the Russian training dataset to the Belarusian training dataset, while testing on Belarusian.
averaged. The Table 3 presents the results of these experiments. The Table 3 shows, that: 1. in monolingual setting, we can get highquality results. The scores are significantly lower than the scores of the same model on the standard dataset, due to the smaller sizes of the training datasets. Nevertheless, one can see, that our approach to word stress detection applies not only to the Russian language data, but also to the data in the Belarusian and Ukrainian languages; 2. cross-lingual setting (1): the Belarusian training dataset, being the smallest one among the three datasets, is not a good source for training word stress detection models in other languages, while the Ukrainian dataset stands  out as a good source for training word stress detection systems both for the Russian and Belarusian languages; 3. cross-lingual setting (2): adding one or two datasets to the other languages improves the quality. For example, around 10% of accuracy is gained by adding the Russian training dataset to the Belarusian training dataset, while testing on Belarusian.
We compare the performance of our neural generator when trained on either gold, silver, or gold and silver data (Table 1). Generation quality is primarily evaluated with BLEU  Semi-supervised training  leads to an 11 BLEU point improvement  compar  train  In addition to BLEU, we also report exact match accuracy on the overlapping subset. Results show that our neural models outperform the grammar-based generator by a large margin.
We compare the performance of our neural generator when trained on either gold, silver, or gold and silver data (Table 1). Generation quality is primarily evaluated with BLEU  Semi-supervised training  leads to an 11 BLEU point improvement  compar  train  In addition to BLEU, we also report exact match accuracy on the overlapping subset. Results show that our neural models outperform the grammar-based generator by a large margin.
Table 3 shows the values of hyper-parameters for our models,  In particular, the embedding sizes are set to 50 and the hidden size of LSTM models to 200. Dropout (Srivastava et al., 2014) is applied to both word and character embeddings with a rate of 0.5. Stochastic gradient descent (SGD) is used for optimization, with an initial learning rate of 0.015 and a decay rate of 0.05.
Table 3 shows the values of hyper-parameters for our models,  In particular, the embedding sizes are set to 50 and the hidden size of LSTM models to 200. Dropout (Srivastava et al., 2014) is applied to both word and character embeddings with a rate of 0.5. Stochastic gradient descent (SGD) is used for optimization, with an initial learning rate of 0.015 and a decay rate of 0.05.
As shown in Table 4, without using word segmentation, a characterbased LSTM-CRF model gives a development F1score of 62.47%. Adding character-bigram and softword representations as described in Section 3.1 increases the F1-score to 67.63% and 65.71%, respectively, demonstrating the usefulness of both sources of information. In addition, a combination of both gives a 69.64% F1-score, which is the best  among various character representations.  Table 4 shows a variety of different settings for word-based Chinese NER. With automatic segmentation, a word-based LSTM CRF baseline gives a 64.12% F1-score, which is higher compared to the character-based baseline. This demonstrates that both word information and character information are useful for Chinese NER. The two methods of  word+char LSTM and word+char LSTM(cid:48), lead to similar improvements.  A CNN representation of character sequences gives a slightly higher F1-score compared to LSTM character representations. On the other hand, further using character bigram information leads to increased F1-score over word+char LSTM, but decreased F1-score over word+char CNN.  As shown in Table 4, the lattice LSTM-CRF model gives a development F1-score of 71.62%, which is significantly7 higher compared with both the word-based and character-based methods, despite that it does not use character bigrams or word segmentation information.
As shown in Table 4, without using word segmentation, a characterbased LSTM-CRF model gives a development F1score of 62.47%. Adding character-bigram and softword representations as described in Section 3.1 increases the F1-score to 67.63% and 65.71%, respectively, demonstrating the usefulness of both sources of information. In addition, a combination of both gives a 69.64% F1-score, which is the best  among various character representations.  Table 4 shows a variety of different settings for word-based Chinese NER. With automatic segmentation, a word-based LSTM CRF baseline gives a 64.12% F1-score, which is higher compared to the character-based baseline. This demonstrates that both word information and character information are useful for Chinese NER. The two methods of  word+char LSTM and word+char LSTM(cid:48), lead to similar improvements.  A CNN representation of character sequences gives a slightly higher F1-score compared to LSTM character representations. On the other hand, further using character bigram information leads to increased F1-score over word+char LSTM, but decreased F1-score over word+char CNN.  As shown in Table 4, the lattice LSTM-CRF model gives a development F1-score of 71.62%, which is significantly7 higher compared with both the word-based and character-based methods, despite that it does not use character bigrams or word segmentation information.
The OntoNotes test results are shown in Table 5. With gold-standard segmentation, our word-based methods give competitive results to the state-of-the-art on the dataset (Che et al., 2013; Wang et al., 2013),  In addition, the results show  that our word-based models can serve as highly competitive baselines. With automatic segmentation, the F1-score of word+char+bichar LSTM decreases from 75.77% to 71.70%, showing the influence of segmentation to NER. Consistent with observations on the development set, adding lattice word information leads to an 88.81% → 93.18% increasement of F1-score over the character baseline, as compared with 88.81% → 91.87% by adding bichar+softword. The lattice model gives significantly the best F1-score on automatic segmentation.
The OntoNotes test results are shown in Table 5. With gold-standard segmentation, our word-based methods give competitive results to the state-of-the-art on the dataset (Che et al., 2013; Wang et al., 2013),  In addition, the results show  that our word-based models can serve as highly competitive baselines. With automatic segmentation, the F1-score of word+char+bichar LSTM decreases from 75.77% to 71.70%, showing the influence of segmentation to NER. Consistent with observations on the development set, adding lattice word information leads to an 88.81% → 93.18% increasement of F1-score over the character baseline, as compared with 88.81% → 91.87% by adding bichar+softword. The lattice model gives significantly the best F1-score on automatic segmentation.
Results on the MSRA dataset are shown in Table 6.  Our chosen segmentor gives 95.93% accuracy on 5-fold cross-validated training set.  Compared with the existing methods, our wordbased and character-based LSTM-CRF models give competitive accuracies. The lattice model significantly outperforms both the best characterbased and word-based models (p < 0.01), achieving the best result on this standard benchmark.
Results on the MSRA dataset are shown in Table 6.  Our chosen segmentor gives 95.93% accuracy on 5-fold cross-validated training set.  Compared with the existing methods, our wordbased and character-based LSTM-CRF models give competitive accuracies. The lattice model significantly outperforms both the best characterbased and word-based models (p < 0.01), achieving the best result on this standard benchmark.
Results on the resume NER test data are shown in Table 8.  the lattice model significantly outperforms both the word-based mode and the character-based model for Weibo and resume (p < 0.01), giving state-of-the-art results.
Results on the resume NER test data are shown in Table 8.  the lattice model significantly outperforms both the word-based mode and the character-based model for Weibo and resume (p < 0.01), giving state-of-the-art results.
Table 3 shows the development set results for Estonian. Cognate Morfessor outperforms the comparable BPE system according to both measures for Estonian. The cross-lingual segmentation is particularly beneficial for Estonian. For Estonian, we have two ensemble configurations one combining 3 monolingually finetuned independent runs, and one combining 5 monolingually finetuned savepoints from 4 independent runs.
Table 3 shows the development set results for Estonian. Cognate Morfessor outperforms the comparable BPE system according to both measures for Estonian. The cross-lingual segmentation is particularly beneficial for Estonian. For Estonian, we have two ensemble configurations one combining 3 monolingually finetuned independent runs, and one combining 5 monolingually finetuned savepoints from 4 independent runs.
Tables 1 and 2 show results on the UNCorpus datasets.  Our approach consistently outperforms Basic and Dual-0, despite the latter being trained with additional monolingual data (Sestorain et al., 2018).
Tables 1 and 2 show results on the UNCorpus datasets.  Our approach consistently outperforms Basic and Dual-0, despite the latter being trained with additional monolingual data (Sestorain et al., 2018).
edge properties are removed, which Table 3 shows has an effect of less than 1 BLEU point).  we ablate node (predicate) and edge attributes  number and tense  have the largest effect on the reported BLEU score.  has only a small impact on performance.
edge properties are removed, which Table 3 shows has an effect of less than 1 BLEU point).  we ablate node (predicate) and edge attributes  number and tense  have the largest effect on the reported BLEU score.  has only a small impact on performance.
We see that models trained with agreement perform comparably to Pivot, outperforming it in some cases, e.g., when the target is Russian, perhaps because it is quite different linguistically from the English pivot.  unlike Dual-0, Agree maintains high performance in the supervised directions (within 1 BLEU point compared to Basic),
Table 3 shows the results on the Europarl corpus.  our approach consistently outperforms Basic by 2-3 BLEU points but lags a bit behind Pivot on average (except on De where it is better). Es
Table 3 shows the results on the Europarl corpus.  our approach consistently outperforms Basic by 2-3 BLEU points but lags a bit behind Pivot on average (except on De where it is better). Es
Table 4 presents results on the original IWSLT17 task.  the vanilla training method (Johnson et al., 2016) achieves very high zero shot performance, even outperforming Pivot.
on our proposed preprocessed IWSLT17(cid:63) that eliminates the overlap and reduces the number of supervised directions (8), there is a considerable gap between the supervised and zeroshot performance of Basic.  Agree performs better than Basic and is slightly worse than Pivot.
on our proposed preprocessed IWSLT17(cid:63) that eliminates the overlap and reduces the number of supervised directions (8), there is a considerable gap between the supervised and zeroshot performance of Basic.  Agree performs better than Basic and is slightly worse than Pivot.
For the Europarl data, we see decent improvements with InitDec for En-Et (+1.11 BLEU) and En-De (+1.60 BLEU), and with InitDec+AddDec for En-Fr (+1.19 BLEU). We also observe that, for all language-pairs, both translation directions benefit from context,  On the other hand, for the Subtitles data, we see a maximum improvement of +0.30 BLEU for InitDec+AddDec.  The next set of experiments evaluates the five different approaches for computing the sourceside context. from Table 2 that for English-Estonian and English-German, our model indeed benefits from using is evident  Finally, our results with source, target and dual contexts are reported. Interestingly, just using the source context is sufficient for English-Estonian and English-German. For English-French, on the other hand, we see significant improvements for the models using the target-side conversation history over using only the source-side.  Unlike Europarl, for Subtitles, we see improvements for our Src-TgtMix dual context variant over the Src-Tgt one for En→Ru,  To summarise, for majority of the cases our Language-Specific Sentence-level Attention is a winner or a close second. Using the Target Context is useful when the base model generates reasonable-quality translations; otherwise, using the Source Context should suffice.
For the Europarl data, we see decent improvements with InitDec for En-Et (+1.11 BLEU) and En-De (+1.60 BLEU), and with InitDec+AddDec for En-Fr (+1.19 BLEU). We also observe that, for all language-pairs, both translation directions benefit from context,  On the other hand, for the Subtitles data, we see a maximum improvement of +0.30 BLEU for InitDec+AddDec.  The next set of experiments evaluates the five different approaches for computing the sourceside context. from Table 2 that for English-Estonian and English-German, our model indeed benefits from using is evident  Finally, our results with source, target and dual contexts are reported. Interestingly, just using the source context is sufficient for English-Estonian and English-German. For English-French, on the other hand, we see significant improvements for the models using the target-side conversation history over using only the source-side.  Unlike Europarl, for Subtitles, we see improvements for our Src-TgtMix dual context variant over the Src-Tgt one for En→Ru,  To summarise, for majority of the cases our Language-Specific Sentence-level Attention is a winner or a close second. Using the Target Context is useful when the base model generates reasonable-quality translations; otherwise, using the Source Context should suffice.
From Table 3, it can be seen that our model surpasses the local-context baseline for Europarl showing that the wider context is indeed beneficial  For En-Ru, it can be seen that using previous sentence is sufficient
From Table 3, it can be seen that our model surpasses the local-context baseline for Europarl showing that the wider context is indeed beneficial  For En-Ru, it can be seen that using previous sentence is sufficient
We conduct an ablation study to validate our hypothesis of using the complete context versus using only one of the three types of contexts in a bilingual multi-speaker conversation: (i) current turn, (ii) previous turns in current language, and (iii) previous turns in the other language. The results for En-De are reported in Table 4. We see decrease in BLEU for all types of contexts with significant decrease when considering only current language from previous turns.The results show that the current turn has the most influence on translating a sentence,
We conduct an ablation study to validate our hypothesis of using the complete context versus using only one of the three types of contexts in a bilingual multi-speaker conversation: (i) current turn, (ii) previous turns in current language, and (iii) previous turns in the other language. The results for En-De are reported in Table 4. We see decrease in BLEU for all types of contexts with significant decrease when considering only current language from previous turns.The results show that the current turn has the most influence on translating a sentence,
In Table 1 we compare our model (UTDSM) with our baseline (Global-DSM) and other state-ofthe-art multi-prototype approaches for the contextual semantic similarity task. It is clear that all different setups of UTDSM perform better than the baseline for both contextual semantic similarity metrics. Using a single Gaussian distribution (UTDSM + GMM (1)) at the smoothing step of our method produces similar results to the baseline model.  We also observe that random anchoring performs slightly worse than UTDSM with respect to AvgSimC.  Furthermore, we observe that GMM smoothing has a different effect on the MaxSimC and AvgSimC metrics. Specifically, for AvgSimC we consistently report lower results when GMM smoothing is applied for different number of components.  At the same time, our smoothing technique highly improves the performance of MaxSimC for all possible configurations.  Overall, the performance of our model is highly competitive to the state-of-the-art models in terms of AvgSimC, for 500-dimensional topic embeddings. We also achieve state-of-the-art performance for the MaxSimC metric, using smoothed topic embeddings of 300 or 500 dimensions with 2 or 3 Gaussian components.
In Table 1 we compare our model (UTDSM) with our baseline (Global-DSM) and other state-ofthe-art multi-prototype approaches for the contextual semantic similarity task. It is clear that all different setups of UTDSM perform better than the baseline for both contextual semantic similarity metrics. Using a single Gaussian distribution (UTDSM + GMM (1)) at the smoothing step of our method produces similar results to the baseline model.  We also observe that random anchoring performs slightly worse than UTDSM with respect to AvgSimC.  Furthermore, we observe that GMM smoothing has a different effect on the MaxSimC and AvgSimC metrics. Specifically, for AvgSimC we consistently report lower results when GMM smoothing is applied for different number of components.  At the same time, our smoothing technique highly improves the performance of MaxSimC for all possible configurations.  Overall, the performance of our model is highly competitive to the state-of-the-art models in terms of AvgSimC, for 500-dimensional topic embeddings. We also achieve state-of-the-art performance for the MaxSimC metric, using smoothed topic embeddings of 300 or 500 dimensions with 2 or 3 Gaussian components.
We evaluate the in- and out-of-domain performance of our approach by training models on either WSJ gold data only, or both WSJ gold data and Gigaword silver data, and evaluating on different domains. The results in Table 2 show that while the generator performs best on test data which matches the training domain (news), semisupervised training leads to substantial out-ofdomain improvements on the Wikipedia and the Brown corpus portions of the test set.
Evaluation results on text classification are presented in Table 2. We observe that our model performs better than the baseline across all metrics for both averaging approaches (AvgCD, AvgD), while the usage of dominant topics appears to have lower performance (MaxCD). Specifically, we get an improvement of 2 − 2.5% on topic-based average and 0.5 − 1% on simple average combination compared to using Global-DSM.
Evaluation results on text classification are presented in Table 2. We observe that our model performs better than the baseline across all metrics for both averaging approaches (AvgCD, AvgD), while the usage of dominant topics appears to have lower performance (MaxCD). Specifically, we get an improvement of 2 − 2.5% on topic-based average and 0.5 − 1% on simple average combination compared to using Global-DSM.
Results for the paraphrase identification task are presented in Table 3. AvgD yields the best results especially in F1 metric showing that cross-topic representations are semantically richer than single embeddings baseline (Global-DSM).
Results for the paraphrase identification task are presented in Table 3. AvgD yields the best results especially in F1 metric showing that cross-topic representations are semantically richer than single embeddings baseline (Global-DSM).
We show the experiment results in Table 2.  our method consistently performs better than all baselines in all settings.  our method is also better than LOF (Softmax).  we observe that on the ATIS dataset, the performance of unknown intent detection dramatically drops as the known intent increases.
Table 1 shows the time-indexed lags that are averaged to calculate AL for a wait-3 system. The lags make the problem clear: each position beyond the point where all source tokens have been read (gi = |x|) has its lag reduced by  , pulling the average lag below k.
Table 1 shows the time-indexed lags that are averaged to calculate AL for a wait-3 system. The lags make the problem clear: each position beyond the point where all source tokens have been read (gi = |x|) has its lag reduced by  , pulling the average lag below k.
We compare our approach to AMR-to-text generation by evaluating our generator on a standard AMR test set  for models trained on gold as well as gold plus silver data.9 We evaluate DMRS models both with and without predicate and edge attributes,  The results in Table 4 show that our MRS generator performs better than the AMR generator by a large margin, even when the additional MRS attributes are excluded.
We compare our approach to AMR-to-text generation by evaluating our generator on a standard AMR test set  for models trained on gold as well as gold plus silver data.9 We evaluate DMRS models both with and without predicate and edge attributes,  The results in Table 4 show that our MRS generator performs better than the AMR generator by a large margin, even when the additional MRS attributes are excluded.
The model has relatively low accuracy on the entailment and contradiction examples while close to zero accuracy on the neutral ones.  the accuracy of the model on each test set.  On our dative alternation dataset, the accuracy on our test sets is substantially lower than on the MultiNLI development set (50.78% versus 84.66% respectively),  The model has very high accuracy on the entailment examples, while close to zero on the contradiction ones.  On the numerical reasoning dataset, the model also seems to fail on this inference type with test set accuracy much lower than on the MultiNLI development set,
The model has relatively low accuracy on the entailment and contradiction examples while close to zero accuracy on the neutral ones.  the accuracy of the model on each test set.  On our dative alternation dataset, the accuracy on our test sets is substantially lower than on the MultiNLI development set (50.78% versus 84.66% respectively),  The model has very high accuracy on the entailment examples, while close to zero on the contradiction ones.  On the numerical reasoning dataset, the model also seems to fail on this inference type with test set accuracy much lower than on the MultiNLI development set,
We consistently observe +1.15/+1.13 BLEU/METEOR score improvements across the three language pairs upon comparing our best model to S-NMT (see Table 2). Overall, our document NMT model with both memories has been the most effective variant for all of the three language pairs.  From Table 2, we consistently see +.95/+1.00 BLEU/METEOR improvements between the best variants of our model and the sentence-level baseline across the three lan  For German→English, guage pairs. For French→English, all variants of document NMT model show comparable performance when using BLEU; however, when evaluated using METEOR, the dual memory model is the best. the target memory variants give comparable results, whereas for Estonian→English, the dual memory variant proves to be the best. Overall, the Memory-toContext model variants perform better than their Memory-to-Output counterparts.
We consistently observe +1.15/+1.13 BLEU/METEOR score improvements across the three language pairs upon comparing our best model to S-NMT (see Table 2). Overall, our document NMT model with both memories has been the most effective variant for all of the three language pairs.  From Table 2, we consistently see +.95/+1.00 BLEU/METEOR improvements between the best variants of our model and the sentence-level baseline across the three lan  For German→English, guage pairs. For French→English, all variants of document NMT model show comparable performance when using BLEU; however, when evaluated using METEOR, the dual memory model is the best. the target memory variants give comparable results, whereas for Estonian→English, the dual memory variant proves to be the best. Overall, the Memory-toContext model variants perform better than their Memory-to-Output counterparts.
We report the performance in probing tasks in Table 2.  In general, DCT yields better performance compared to averaging on all tasks, and larger K often yields improved performance in syntactic and semantic tasks. For the surface information tasks, SentLen and Word content (WC), c significantly outperforms AVG.  The performance decreased with increasing K in c[0 : K],  While increasing K has no positive effect on surface information tasks, syntactic and semantic tasks demonstrate performance gains with larger K. This trend is clearly observed in all syntactic tasks and three of the semantic tasks, where DCT performs well above AVG and the performance improves with increasing K. The only exception is SOMO, where increasing K actually results in lower performance, although all DCT results are still higher than AVG.
We report the performance in probing tasks in Table 2.  In general, DCT yields better performance compared to averaging on all tasks, and larger K often yields improved performance in syntactic and semantic tasks. For the surface information tasks, SentLen and Word content (WC), c significantly outperforms AVG.  The performance decreased with increasing K in c[0 : K],  While increasing K has no positive effect on surface information tasks, syntactic and semantic tasks demonstrate performance gains with larger K. This trend is clearly observed in all syntactic tasks and three of the semantic tasks, where DCT performs well above AVG and the performance improves with increasing K. The only exception is SOMO, where increasing K actually results in lower performance, although all DCT results are still higher than AVG.
Our results in Table 3 are consistent with these observations, where we see improvements in most tasks, but the difference is not as significant as the probing tasks, except in TREC question classification where increasing K leads to much better performance.
Our results in Table 3 are consistent with these observations, where we see improvements in most tasks, but the difference is not as significant as the probing tasks, except in TREC question classification where increasing K leads to much better performance.
Table 4 shows the best results for the various models as reported in Kayal and Tsat  saronis (2019), in addition to the best performance of our model denoted as c[k].  Note that the DCT-based model, DCT*, described in Kayal and Tsatsaronis (2019) performed relatively poorly in all tasks, while our model achieved close to state-of-the-art performance in both the 20-NG and R-8 tasks. Our model outperformed EignSent on all tasks and generally performed better than or on par with p-means, ELMo, BERT, and EigenSent⊕Avg on both the 20-NG and R-8. On the other hand, both EigenSent⊕Avg and ELMo performed better than all other models on SST-5.
Table 4 shows the best results for the various models as reported in Kayal and Tsat  saronis (2019), in addition to the best performance of our model denoted as c[k].  Note that the DCT-based model, DCT*, described in Kayal and Tsatsaronis (2019) performed relatively poorly in all tasks, while our model achieved close to state-of-the-art performance in both the 20-NG and R-8 tasks. Our model outperformed EignSent on all tasks and generally performed better than or on par with p-means, ELMo, BERT, and EigenSent⊕Avg on both the 20-NG and R-8. On the other hand, both EigenSent⊕Avg and ELMo performed better than all other models on SST-5.
As shown in Table 1, this does not discard too many correct hypotheses but markedly reduces the size of the cnet to an average of seven timesteps with two hypotheses.
As shown in Table 1, this does not discard too many correct hypotheses but markedly reduces the size of the cnet to an average of seven timesteps with two hypotheses.
Table 3 displays the results for our model evaluated on cnets for increasingly aggressive pruning levels (discarding only interjections, additionally discarding hypotheses with scores below 0.001 and 0.01, respectively). As can be seen, using the full cnet except for interjections does not improve over the baseline.  However, when pruning low-probability hypotheses both pooling strategies improve over the baseline. Yet, average pooling performs worse for the lower pruning threshold, which shows that the model is still affected by noise among the hypotheses.  Weighted pooling performs better for the lower pruning threshold of 0.001 with which we obtain the highest result overall, improving the joint goals accuracy by 1.6 percentage points compared to the baseline.  Moreover, we see that an ensemble model that averages the predictions of ten cnet models trained with different random seeds also outperforms an ensemble of ten baseline models.  Our ensemble models outperform Mrksic et al. (2017) for the joint requests but are a bit worse for the joint goals.
Table 3 displays the results for our model evaluated on cnets for increasingly aggressive pruning levels (discarding only interjections, additionally discarding hypotheses with scores below 0.001 and 0.01, respectively). As can be seen, using the full cnet except for interjections does not improve over the baseline.  However, when pruning low-probability hypotheses both pooling strategies improve over the baseline. Yet, average pooling performs worse for the lower pruning threshold, which shows that the model is still affected by noise among the hypotheses.  Weighted pooling performs better for the lower pruning threshold of 0.001 with which we obtain the highest result overall, improving the joint goals accuracy by 1.6 percentage points compared to the baseline.  Moreover, we see that an ensemble model that averages the predictions of ten cnet models trained with different random seeds also outperforms an ensemble of ten baseline models.  Our ensemble models outperform Mrksic et al. (2017) for the joint requests but are a bit worse for the joint goals.
As can be seen from Table 2, the DST accuracy slightly increases for the higher-quality live ASR outputs. More importantly, the DST performance drastically increases, when we evaluate on the manual transcripts that reflect the true user utterances nearly perfectly.
As can be seen from Table 2, the DST accuracy slightly increases for the higher-quality live ASR outputs. More importantly, the DST performance drastically increases, when we evaluate on the manual transcripts that reflect the true user utterances nearly perfectly.
Fourth, we introduce finer classes (Tables 1–2), which fit better the target task, where nested clauses are frequent.  The splitter produced 31,545 training, 8,036 development, and 5,563 test sentences/clauses.3 Table 2 shows their distribution in the six gold (correct) classes.  cf. Table 2),
Fourth, we introduce finer classes (Tables 1–2), which fit better the target task, where nested clauses are frequent.  The splitter produced 31,545 training, 8,036 development, and 5,563 test sentences/clauses.3 Table 2 shows their distribution in the six gold (correct) classes.  cf. Table 2),
As a result, we obtained 1,654 lines of data comprising trilingual, bilingual, and monolingual segments (mainly sentences) as summarized in Table 1. Finally, for the sake of comparability, we randomly chose 600 trilingual sentences to create a test set, and concatenated the rest of them and bilingual sentences to form development sets.
As a result, we obtained 1,654 lines of data comprising trilingual, bilingual, and monolingual segments (mainly sentences) as summarized in Table 1. Finally, for the sake of comparability, we randomly chose 600 trilingual sentences to create a test set, and concatenated the rest of them and bilingual sentences to form development sets.
Table 3 reports the precision, recall, F1 score, area under the precision-recall curve (AUC) per class, as well as micro- and macro-averages.  The self-attention mechanism (BILSTM-ATT) leads to clear overall improvements (in macro and micro F1 and AUC, Table 3) comparing to the plain BILSTM, supporting the hypothesis that selfattention allows the classifier to focus on indicative tokens. Allowing the BILSTM to consider tokens of neighboring sentences (X-BILSTM-ATT) does not lead to any clear overall improvements.  The hierarchical H-BILSTM-ATT clearly outperforms the other three methods, supporting the hypothesis that considering entire sections and allowing the sentence embeddings to interact in the upper BILSTM (Fig. 3) is beneficial.
Table 3 reports the precision, recall, F1 score, area under the precision-recall curve (AUC) per class, as well as micro- and macro-averages.  The self-attention mechanism (BILSTM-ATT) leads to clear overall improvements (in macro and micro F1 and AUC, Table 3) comparing to the plain BILSTM, supporting the hypothesis that selfattention allows the classifier to focus on indicative tokens. Allowing the BILSTM to consider tokens of neighboring sentences (X-BILSTM-ATT) does not lead to any clear overall improvements.  The hierarchical H-BILSTM-ATT clearly outperforms the other three methods, supporting the hypothesis that considering entire sections and allowing the sentence embeddings to interact in the upper BILSTM (Fig. 3) is beneficial.
Table 8 shows the BLEU scores achieved by several reasonable combinations of six-way pseudo-parallel data. We observed that the use of all six-way pseudo-parallel data (#10) significantly improved the base model for all the translation directions, except En→Ru. A translation direction often benefited when the pseudo-parallel data for that specific direction was used.  However, the resulting BLEU scores for Ja→Ru and Ru→Ja tasks do not exceed 10 BLEU points, implying the inherent limitation of the in-domain data as well as the difficulty of these translation directions.  models of our multistage fine-tuning, i.e., V and VII, achieved significantly higher BLEU scores than (b3) in Table 5, a weak baseline without using any monolingual data, and #10 in Table 8, a strong baseline established with monolingual data.
Table 8 shows the BLEU scores achieved by several reasonable combinations of six-way pseudo-parallel data. We observed that the use of all six-way pseudo-parallel data (#10) significantly improved the base model for all the translation directions, except En→Ru. A translation direction often benefited when the pseudo-parallel data for that specific direction was used.  However, the resulting BLEU scores for Ja→Ru and Ru→Ja tasks do not exceed 10 BLEU points, implying the inherent limitation of the in-domain data as well as the difficulty of these translation directions.  models of our multistage fine-tuning, i.e., V and VII, achieved significantly higher BLEU scores than (b3) in Table 5, a weak baseline without using any monolingual data, and #10 in Table 8, a strong baseline established with monolingual data.
Table 2 demonstrates the evaluation results of few-shot DA on the existing FewRel test set and the new test set.  (1) All few-shot models suffer dramatic perfor mance falls when tested on a different domain. (2) Adversarial training does improve the results on the new test domain, yet still has large space for growth. (3) BERT-PAIR outperforms all other few-shot models on both 1.0 and 2.0 test set.
All models are trained given 50% NOTA queries and tested under four different NOTA rates: 0%, 15%, 30%, 50%.  For detailed numbers of results on fewshot NOTA, please refer to Table 3.  (1) Treating NOTA as the  + 1 relation is beneficial for handling Few-Shot NOTA, though the results still fall fast when the NOTA rate increases. (2) BERT-PAIR works better under the NOTA setting for its binary-classification style model, and stays stable with rising NOTA rate. (3) Though BERT-PAIR achieves promising results, huge gaps still exist between the conventional (0% NOTA rate) and NOTA settings (gaps of 8 points for 5-way 1-shot and 7 points for 5way 5-shot with 50% NOTA rate), which calls for further research to address the challenge.
All models are trained given 50% NOTA queries and tested under four different NOTA rates: 0%, 15%, 30%, 50%.  For detailed numbers of results on fewshot NOTA, please refer to Table 3.  (1) Treating NOTA as the  + 1 relation is beneficial for handling Few-Shot NOTA, though the results still fall fast when the NOTA rate increases. (2) BERT-PAIR works better under the NOTA setting for its binary-classification style model, and stays stable with rising NOTA rate. (3) Though BERT-PAIR achieves promising results, huge gaps still exist between the conventional (0% NOTA rate) and NOTA settings (gaps of 8 points for 5-way 1-shot and 7 points for 5way 5-shot with 50% NOTA rate), which calls for further research to address the challenge.
These results are shown in table 2 as feature level transfer-learning.  These results are shown in table 2 as model level transfer learning.  Table 2 shows the results of the proposed method, its variants and the baseline methods.
These results are shown in table 2 as feature level transfer-learning.  These results are shown in table 2 as model level transfer learning.  Table 2 shows the results of the proposed method, its variants and the baseline methods.
Results are shown in Table 4.  Compared to AFET, the proposed model performs better in all types except other in the top-10 frequent types.
Results are shown in Table 4.  Compared to AFET, the proposed model performs better in all types except other in the top-10 frequent types.
Table 2 shows a selection of the f1-scores achieved on properties in the CSLB dataset in relation to the average cosine similarity of the associated words. A high average cosine similarity means that the concepts overall have similar vector representations and can thus be seen as having a low diversity. The results of the Spearman Rank correlation clearly indicate that scores achieved by nearest neighbors correlate more strongly with the average cosine than the two supervised classification approaches.
Table 2 shows a selection of the f1-scores achieved on properties in the CSLB dataset in relation to the average cosine similarity of the associated words. A high average cosine similarity means that the concepts overall have similar vector representations and can thus be seen as having a low diversity. The results of the Spearman Rank correlation clearly indicate that scores achieved by nearest neighbors correlate more strongly with the average cosine than the two supervised classification approaches.
H-BILSTM-ATT is also much faster to train than BILSTM and BILSTM-ATT (Table 4), even though it has more parameters, because it converges faster (5-7 epochs vs. 12-15).
H-BILSTM-ATT is also much faster to train than BILSTM and BILSTM-ATT (Table 4), even though it has more parameters, because it converges faster (5-7 epochs vs. 12-15).
Table 4 shows the f1-scores on the full clean datasets.  For polysemy between food and animals (Table 4), we observe that when trained on pure animal and food words and tested on polysemous animal and food words, the classifiers perform highly with a large difference to nearest neighbors.
Table 1 shows the quality of word representations in terms of the correlation between word similarity  scores obtained by the proposed models and word similarity scores defined by humans.  for that can see First, we each task, character only models had significantly worse performance than every other model trained on the same dataset.  Further, bold results show the overall trend that vector gates outperformed the other methods regardless of training dataset.  Additionally, results from the MNLI row in general, and underlined results in particular, show that training on MultiNLI produces word representations better at capturing word similarity.  Exceptions to the previous rule are models evaluated in MEN and RW.  More notably, in the RareWords dataset (Luthe word only, concat, ong et al., 2013), and scalar gate methods performed equally, despite having been trained in different datasets (p > 0.1), and the char only method performed significantly worse when trained in MultiNLI.  The vector gate, however, performed significantly better than its counterpart trained in SNLI.
Table 1 shows the quality of word representations in terms of the correlation between word similarity  scores obtained by the proposed models and word similarity scores defined by humans.  for that can see First, we each task, character only models had significantly worse performance than every other model trained on the same dataset.  Further, bold results show the overall trend that vector gates outperformed the other methods regardless of training dataset.  Additionally, results from the MNLI row in general, and underlined results in particular, show that training on MultiNLI produces word representations better at capturing word similarity.  Exceptions to the previous rule are models evaluated in MEN and RW.  More notably, in the RareWords dataset (Luthe word only, concat, ong et al., 2013), and scalar gate methods performed equally, despite having been trained in different datasets (p > 0.1), and the char only method performed significantly worse when trained in MultiNLI.  The vector gate, however, performed significantly better than its counterpart trained in SNLI.
Table 2 shows the impact that different methods for combining character and word-level word representations have in the quality of the sentence representations produced by our models.  We can observe the same trend mentioned in section 4.1, and highlighted by the difference between bold values, that models trained in MultiNLI performed better than those trained in  SNLI at a statistically significant level,  The two exceptions to the previous trend, SICKE and SICKR, benefited more from models trained on SNLI.  Additionally, there was no method that significantly outperformed the word only baseline in classification tasks.  On the other hand, the vector gate significantly outperformed every other method in the STSB task when trained in both datasets, and in the STS16 task when trained in SNLI.
Table 2 shows the impact that different methods for combining character and word-level word representations have in the quality of the sentence representations produced by our models.  We can observe the same trend mentioned in section 4.1, and highlighted by the difference between bold values, that models trained in MultiNLI performed better than those trained in  SNLI at a statistically significant level,  The two exceptions to the previous trend, SICKE and SICKR, benefited more from models trained on SNLI.  Additionally, there was no method that significantly outperformed the word only baseline in classification tasks.  On the other hand, the vector gate significantly outperformed every other method in the STSB task when trained in both datasets, and in the STS16 task when trained in SNLI.
We also identify so-called "gotcha" sentences in which pronoun gender does not match the occupation's majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these "gotchas."8 (See Table 2.)
We also identify so-called "gotcha" sentences in which pronoun gender does not match the occupation's majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these "gotchas."8 (See Table 2.)
The results of the experiments are reported in Table 1.  A facts-to-seq model exploiting our positional fact encoding performs adequately. With an additional attention mechanism (Facts-to-seq w. Attention), the results are even better.  The analysis of the Static Memory approach amounts to an ablation study,  the DMN+ is even outperformed by our Facts-to-seq baseline.
The results of the experiments are reported in Table 1.  A facts-to-seq model exploiting our positional fact encoding performs adequately. With an additional attention mechanism (Facts-to-seq w. Attention), the results are even better.  The analysis of the Static Memory approach amounts to an ablation study,  the DMN+ is even outperformed by our Facts-to-seq baseline.
Table 2 reports the results on SNLI, with the configurations that performed best on the validation set for each of the adversarial methods.  both training methods perform worse than our unmodified, non-adversarial InferSent baseline on SNLI's test set, since they remove biases that may be useful for performing this task. The difference for AdvCls is minimal, and it even slightly outperforms InferSent on the validation set. While AdvDat's results are noticeably lower than the non-adversarial InferSent, the drops are still less than 6% points.
Table 2 reports the results on SNLI, with the configurations that performed best on the validation set for each of the adversarial methods.  both training methods perform worse than our unmodified, non-adversarial InferSent baseline on SNLI's test set, since they remove biases that may be useful for performing this task. The difference for AdvCls is minimal, and it even slightly outperforms InferSent on the validation set. While AdvDat's results are noticeably lower than the non-adversarial InferSent, the drops are still less than 6% points.
For each of the most biased words in SNLI associated with the CONTRADICTION label, we computed the probability that a model predicts an example as a contradiction, given that the hypothesis contains the word. Table 3 shows the top 10 examples in the training set. For each word w, we give its frequency in SNLI, its empirical correlation with the label and with InferSent's prediction, and the percentage decrease in correlations with CONTRADICTION predictions by three configurations of our methods. Generally, the baseline correlations are more uniform than the empirical ones (ˆp(l|w)),  However, we still observed small skews towards CONTRADICTION. Thus, we investigate whether our methods reduce the probability of predicting CONTRADICTION when a hypothesis contains an indicator word. The model trained with AdvDat (where λRand = 0.4, λEnc = 1) predicts contradiction much less frequently than InferSent on examples with these words.
For each of the most biased words in SNLI associated with the CONTRADICTION label, we computed the probability that a model predicts an example as a contradiction, given that the hypothesis contains the word. Table 3 shows the top 10 examples in the training set. For each word w, we give its frequency in SNLI, its empirical correlation with the label and with InferSent's prediction, and the percentage decrease in correlations with CONTRADICTION predictions by three configurations of our methods. Generally, the baseline correlations are more uniform than the empirical ones (ˆp(l|w)),  However, we still observed small skews towards CONTRADICTION. Thus, we investigate whether our methods reduce the probability of predicting CONTRADICTION when a hypothesis contains an indicator word. The model trained with AdvDat (where λRand = 0.4, λEnc = 1) predicts contradiction much less frequently than InferSent on examples with these words.
Our datasets vary in size of entity and non-entity tokens, as shown in Table 1. The smallest, Farsi, has 4.5K entity and 50K non-entity tokens; the largest, English, has 29K entity and 170K nonentity tokens.
Our datasets vary in size of entity and non-entity tokens, as shown in Table 1. The smallest, Farsi, has 4.5K entity and 50K non-entity tokens; the largest, English, has 29K entity and 170K nonentity tokens.
a comparison with existing event extraction and event schema induction datasets, including ASTRE (Nguyen et al., 2016a), MUC 4, ACE 20052 and ERE3, is shown in Table 2. Compared with the other datasets, GNBusiness has a much larger number of documents (i.e., news clusters in GNBusiness), and a comparable number of labeled documents.
a comparison with existing event extraction and event schema induction datasets, including ASTRE (Nguyen et al., 2016a), MUC 4, ACE 20052 and ERE3, is shown in Table 2. Compared with the other datasets, GNBusiness has a much larger number of documents (i.e., news clusters in GNBusiness), and a comparable number of labeled documents.
Before preservation, MILk with a latency weight λ = 0 still showed a substantial reduction in latency from the maximum value of 27.9, indicating an intrinsic latency incentive. Furthermore, training quickly destabilized, resulting in very poor trade-offs for λs as low as 0.2.
Before preservation, MILk with a latency weight λ = 0 still showed a substantial reduction in latency from the maximum value of 27.9, indicating an intrinsic latency incentive. Furthermore, training quickly destabilized, resulting in very poor trade-offs for λs as low as 0.2.
Table 4 shows the overall performance of schema matching on GNBusinessTest. From the table, we can see that ODEE-FER achieves the best F1 scores among all the methods. By comparing Nguyen et al. (2015) and ODEEF (p = 0.01), we can see that using continuous contextual features gives better performance than discrete features.  Among ODEE models, ODEE-FE gives a 2% gain in F1 score against ODEE-F,  there is a 1% gain in F
Table 4 shows the overall performance of schema matching on GNBusinessTest. From the table, we can see that ODEE-FER achieves the best F1 scores among all the methods. By comparing Nguyen et al. (2015) and ODEEF (p = 0.01), we can see that using continuous contextual features gives better performance than discrete features.  Among ODEE models, ODEE-FE gives a 2% gain in F1 score against ODEE-F,  there is a 1% gain in F
Table 5 shows the comparison of averaged slot coherence results over all the slots in the schemas.  The averaged slot coherence of ODEE-FER is the highest,  The averaged slot coherence  of ODEE-F is comparable to that of Nguyen et al. (2015) (p = 0.3415),  The scores of ODEE-FE (p = 0.06) and ODEE-FER (p = 10−5) are both higher than that of ODEE-F,
Table 5 shows the comparison of averaged slot coherence results over all the slots in the schemas.  The averaged slot coherence of ODEE-FER is the highest,  The averaged slot coherence  of ODEE-F is comparable to that of Nguyen et al. (2015) (p = 0.3415),  The scores of ODEE-FE (p = 0.06) and ODEE-FER (p = 10−5) are both higher than that of ODEE-F,
Table 2 presents METEOR (Denkowski and Lavie, 2014) and ROUGE-L (Lin, 2004) scores for each method, where we can see score gains on  both metrics from the Editing Mechanism.  Table 2 shows that the human judges strongly favor the abstracts from our ED(2) method.
Table 2 presents METEOR (Denkowski and Lavie, 2014) and ROUGE-L (Lin, 2004) scores for each method, where we can see score gains on  both metrics from the Editing Mechanism.  Table 2 shows that the human judges strongly favor the abstracts from our ED(2) method.
We also conduct a plagiarism check in Table 3, which shows that 93.4% of 6-grams generated by ED(2) did not appear in the training data, indicating that our model is not simply copying.
We also conduct a plagiarism check in Table 3, which shows that 93.4% of 6-grams generated by ED(2) did not appear in the training data, indicating that our model is not simply copying.
We trained and evaluated our editing approach with 1-6 iterations and the experimental results (Table 5) showed that the second iteration produced the best results.
As expected, Table 4 shows that people with less domain knowledge are more easily deceived. Specifically, non-CS human judges fail at more than half of the 1-to-1 sets for the same titles, which suggests that most of our system generated abstracts follow correct grammar and consistent writing style. Domain experts fail on 1 or 2 sets, mostly because the human written abstracts in those sets don't seem very topically relevant. Additionally, the more abstracts that we provided to human judges, the easier it is to conceal the system generated abstract amongst human generated ones.
As expected, Table 4 shows that people with less domain knowledge are more easily deceived. Specifically, non-CS human judges fail at more than half of the 1-to-1 sets for the same titles, which suggests that most of our system generated abstracts follow correct grammar and consistent writing style. Domain experts fail on 1 or 2 sets, mostly because the human written abstracts in those sets don't seem very topically relevant. Additionally, the more abstracts that we provided to human judges, the easier it is to conceal the system generated abstract amongst human generated ones.
The results of unimodal sentiment prediction experiments are shown in Table 2.  The verbal models have the best performance here,  On each modality, the best performance is achieved by a multi-task learning model.  All unimodal models have significantly different performance. p = 0.009 for S+P and S+P+I Visual models, p << 0.001 for Visual and Vocal S+I models.  In multi-task learning, the main task gains additional information from the auxillary tasks. Compared to the S model, the S+P model has increased focus on the polarity of sentiment, while the S+I model has increased focus on the intensity of sentiment. On the verbal modality, the S+P model achieved the best performance, while on the visual modality the S+I model achieved the best performance.  For the vocal modality, the S+P+I model achieved the best performance, and the S+P model yielded improved performance over that of the S model.
The results of unimodal sentiment prediction experiments are shown in Table 2.  The verbal models have the best performance here,  On each modality, the best performance is achieved by a multi-task learning model.  All unimodal models have significantly different performance. p = 0.009 for S+P and S+P+I Visual models, p << 0.001 for Visual and Vocal S+I models.  In multi-task learning, the main task gains additional information from the auxillary tasks. Compared to the S model, the S+P model has increased focus on the polarity of sentiment, while the S+I model has increased focus on the intensity of sentiment. On the verbal modality, the S+P model achieved the best performance, while on the visual modality the S+I model achieved the best performance.  For the vocal modality, the S+P+I model achieved the best performance, and the S+P model yielded improved performance over that of the S model.
of results the multimodal The experiments are shown in Table 3.  We find that EF>HF>TFN>LF.  4  Unlike Zadeh et al. (2017), here the EF model outperforms the TFN model. However, the TFN model achieved the best performance on the training and validation sets.  Compared to the feature concatenation used in EF, the Cartesian product used in TFN results in higher dimensionality of the multimodal input vector,5 which in turn increases the complexity of the model. Similarly, the HF model has worse performance than the EF model here, unlike in Tian et al. (2016).  In general, the multimodal models have better performance than the unimodal models.  In fact, the HF and LF models have better performance using single-task learning. For the TFN models, only the S+P model outperforms the S model, although the improvement is not significant.7 For the EF models, multi-task learning results in better performance.  Dimension of the EF input is 420, for TFN is 65,536. 6Except that the LF models often have worse performance than the verbal S+P model. p << 0.001 for TFN S+P and verbal S+P, p = 0.017 for verbal S+P and LF S. 7p = 0.105 for S TFN and S+P TFN. 8p = 0.888 for S EF and S+P EF, p = 0.029 for S EF and S+I EF, p = 0.009 for S EF and S+P+I EF.
of results the multimodal The experiments are shown in Table 3.  We find that EF>HF>TFN>LF.  4  Unlike Zadeh et al. (2017), here the EF model outperforms the TFN model. However, the TFN model achieved the best performance on the training and validation sets.  Compared to the feature concatenation used in EF, the Cartesian product used in TFN results in higher dimensionality of the multimodal input vector,5 which in turn increases the complexity of the model. Similarly, the HF model has worse performance than the EF model here, unlike in Tian et al. (2016).  In general, the multimodal models have better performance than the unimodal models.  In fact, the HF and LF models have better performance using single-task learning. For the TFN models, only the S+P model outperforms the S model, although the improvement is not significant.7 For the EF models, multi-task learning results in better performance.  Dimension of the EF input is 420, for TFN is 65,536. 6Except that the LF models often have worse performance than the verbal S+P model. p << 0.001 for TFN S+P and verbal S+P, p = 0.017 for verbal S+P and LF S. 7p = 0.105 for S TFN and S+P TFN. 8p = 0.888 for S EF and S+P EF, p = 0.029 for S EF and S+I EF, p = 0.009 for S EF and S+P+I EF.
As shown in table 2, BioBERT trained on PubMed+PMC performs the best.
As shown in table 2, BioBERT trained on PubMed+PMC performs the best.
Table 2 shows the results of our method and all the baselines on tasks with different topics.  We can first observe that the proposed RCN consistently outperformed all the baselines across all topics. Despite being modest, all the improvements of RCN over the baselines are statistically significant at p < 0.05 with a two-tailed t-test.  BiLSTM performed the worst, showing that only using the RNN encoder for sequence encoding is not sufficient for obtaining optimal results.  DeAT and BiMPM performed similarly well;  RCN performed the best, with relative improvements from 2.1% to 10.4% over the second best.
Different approaches have been used to solve this task. The best result belongs to classifying order of paragraphs using pre-trained BERT model. It achieves around 84% accuracy on test set which outperforms other models significantly.  First, each paragraph is encoded with LSTM. The hidden state at the end of each sentence is extracted, and the resulting matrix is going through gated CNN (Dauphin et al., 2017) for extraction of single encoding for each paragraph. The accuracy is barely above 50%, which depicts that this method is not very promising.  We have used a pre-trained BERT in two different ways. First, as a feature extractor without fine-tuning, and second, by fine-tuning the weights during training.  In the case of fine-tuning, we have used different numbers for maximum sequence length to test the capability of BERT in this task.  we increased the number of tokens and accuracy respectively increases.  We found this method very promising and the accuracy significantly increases with respect to previous methods (Table 3). This result reveals fine-tuning pre-trained BERT can approximately learn the order of the paragraphs and arrow of the time in the stories.
Different approaches have been used to solve this task. The best result belongs to classifying order of paragraphs using pre-trained BERT model. It achieves around 84% accuracy on test set which outperforms other models significantly.  First, each paragraph is encoded with LSTM. The hidden state at the end of each sentence is extracted, and the resulting matrix is going through gated CNN (Dauphin et al., 2017) for extraction of single encoding for each paragraph. The accuracy is barely above 50%, which depicts that this method is not very promising.  We have used a pre-trained BERT in two different ways. First, as a feature extractor without fine-tuning, and second, by fine-tuning the weights during training.  In the case of fine-tuning, we have used different numbers for maximum sequence length to test the capability of BERT in this task.  we increased the number of tokens and accuracy respectively increases.  We found this method very promising and the accuracy significantly increases with respect to previous methods (Table 3). This result reveals fine-tuning pre-trained BERT can approximately learn the order of the paragraphs and arrow of the time in the stories.
Table 2 shows how the position of the context window influences the average model performance. Note that symmetric windows of, for instance, 10 are in fact 2 times larger than the 'left' or 'right'  windows of the same size, as they consider 10 words both to the left and to the right of the focus word. This is most likely why symmetric windows consistently outperform 'single-sided' ones on the analogy task, as they are able to include twice as much contextual input. However, the average performance on the semantic similarity task (as indicated by the Spearman correlation with the SimLex999 test set) does not exhibit the same trend. 'Left' windows are indeed worse than symmetric ones, but 'right' windows are on par with the symmetric windows for OpenSubtitles and only one percent point behind them for Gigaword. It means that in many cases (at least with English texts) taking into account only n context words to the right of the focus word is sufficient to achieve the same performance with SimLex999 as by using a model which additionally considers n words to the left, and thus requires significantly more training time.
Table 2 shows how the position of the context window influences the average model performance. Note that symmetric windows of, for instance, 10 are in fact 2 times larger than the 'left' or 'right'  windows of the same size, as they consider 10 words both to the left and to the right of the focus word. This is most likely why symmetric windows consistently outperform 'single-sided' ones on the analogy task, as they are able to include twice as much contextual input. However, the average performance on the semantic similarity task (as indicated by the Spearman correlation with the SimLex999 test set) does not exhibit the same trend. 'Left' windows are indeed worse than symmetric ones, but 'right' windows are on par with the symmetric windows for OpenSubtitles and only one percent point behind them for Gigaword. It means that in many cases (at least with English texts) taking into account only n context words to the right of the focus word is sufficient to achieve the same performance with SimLex999 as by using a model which additionally considers n words to the left, and thus requires significantly more training time.
For similarity tasks, cross-sentential contexts do not seem useful, and can even be detrimental for large window sizes. However, for analogy tasks, crosssentential contexts lead to improved results thanks to the increased window it provides. This is especially pronounced for corpora with short sentences such as OpenSubtitles (see Table 3).
As shown in Table 4, the removal of stop words does not really influence the average model performance for the semantic similarity task. The analogy task, however, benefits substantially from this filtering, for both corpora.
In Table 3 we report the accuracies obtained by our best model in both matched (first 5 genres) and mismatched (last 5 genres) development sets.  We can observe that our implementation performed like ESIM overall,
Table 1 shows that finetuned BERT outperforms pre-trained BERT by a significant margin on all the tasks (with an average of 35.9 points of absolute difference).  BERT with weights initialized from normal distribution and further fine-tuned for a given task consistently produces lower scores than the ones achieved with pre-trained BERT. In fact, for some tasks (STS-B and QNLI), initialization with random weights yields worse performance than pre-trained BERT without fine-tuning.
Table 1 shows that finetuned BERT outperforms pre-trained BERT by a significant margin on all the tasks (with an average of 35.9 points of absolute difference).  BERT with weights initialized from normal distribution and further fine-tuned for a given task consistently produces lower scores than the ones achieved with pre-trained BERT. In fact, for some tasks (STS-B and QNLI), initialization with random weights yields worse performance than pre-trained BERT without fine-tuning.
First, from the EnFr results in Table 3, we are in line with GNMT (Wu et al., 2016), and within 2 BLEU points of the RNN and Transformer models investigated by Chen et al. (2018).  we compare quite favorably with Lee et al. (2017), exceeding their reported scores by 3-6 points,
First, from the EnFr results in Table 3, we are in line with GNMT (Wu et al., 2016), and within 2 BLEU points of the RNN and Transformer models investigated by Chen et al. (2018).  we compare quite favorably with Lee et al. (2017), exceeding their reported scores by 3-6 points,
Table 2 clearly shows the characterlevel systems outperforming BPE for all language pairs.
BPE and character sys  tems differ most in the number of lexical choice errors, and in the extent to which they drop content.  Regarding lexical choice, the two systems differ not only in the number of errors, but in the nature of those errors. In particular, the BPE model had more trouble handling German compound nouns.  We also found that both systems occasionally mistranslate proper names.
BPE and character sys  tems differ most in the number of lexical choice errors, and in the extent to which they drop content.  Regarding lexical choice, the two systems differ not only in the number of errors, but in the nature of those errors. In particular, the BPE model had more trouble handling German compound nouns.  We also found that both systems occasionally mistranslate proper names.
Unfortunately, even at just 1k vocabulary items, BPE has already lost a BLEU point with respect to the character model.  Comparing the performance of our Pooled BiLSTM model against BPE, we notice that for a comparable level of compression (BPE size of 1k), BPE out-performs the pooled model by around 0.5 BLEU points. At a similar level of performance (BPE size of 4k), BPE has significantly shorter sequences.  As shown in table 6, the 3-HM configuration achieves much better compression even when this is accounted for, and also gives slightly better performance than 2-HM. In general, HM gating results in less compression but better performance than the fixed-stride techniques.
Unfortunately, even at just 1k vocabulary items, BPE has already lost a BLEU point with respect to the character model.  Comparing the performance of our Pooled BiLSTM model against BPE, we notice that for a comparable level of compression (BPE size of 1k), BPE out-performs the pooled model by around 0.5 BLEU points. At a similar level of performance (BPE size of 4k), BPE has significantly shorter sequences.  As shown in table 6, the 3-HM configuration achieves much better compression even when this is accounted for, and also gives slightly better performance than 2-HM. In general, HM gating results in less compression but better performance than the fixed-stride techniques.
The results for the tested systems are given in Table 2. Again we saw large gains from the BERT  based models over the baseline from (Zubiaga et al., 2017) and the 2-layer BiLSTM. Compared to training solely on PHEME, fine tuning from basic citation needed detection saw very little improvement (0.1 F1 points). However, fine tuning with a model trained using PU learning led to an increase of 1 F1 point over the non-PU learning model, indicating that PU learning enables the Wikipedia data to be useful for transferring to rumour detection. For PUC, we saw an improvement of 0.7 F1 points over the baseline and lower overall variance than vanilla PU learning, meaning that the results with PUC are more consistent across runs. When models are ensembled, pretraining with vanilla PU learning improved over no pretraining by almost 2 F1 points.
The results for the tested systems are given in Table 2. Again we saw large gains from the BERT  based models over the baseline from (Zubiaga et al., 2017) and the 2-layer BiLSTM. Compared to training solely on PHEME, fine tuning from basic citation needed detection saw very little improvement (0.1 F1 points). However, fine tuning with a model trained using PU learning led to an increase of 1 F1 point over the non-PU learning model, indicating that PU learning enables the Wikipedia data to be useful for transferring to rumour detection. For PUC, we saw an improvement of 0.7 F1 points over the baseline and lower overall variance than vanilla PU learning, meaning that the results with PUC are more consistent across runs. When models are ensembled, pretraining with vanilla PU learning improved over no pretraining by almost 2 F1 points.
The results for political speech check-worthiness detection are given in Table 3. We found that the vanilla BERT model performed the best of all models. As we added transfer learning and PU learning, the performance steadily dropped, with the worst performing model being the one using PUC.
The results for political speech check-worthiness detection are given in Table 3. We found that the vanilla BERT model performed the best of all models. As we added transfer learning and PU learning, the performance steadily dropped, with the worst performing model being the one using PUC.
Our results are given in Table 4.  We found that the Wikipedia and Twitter datasets contained labels which were more general, evidenced by similar high F1 scores from both annotators (> 0.8). For political speeches, we observed that the human annotators both found many more examples to be check-worthy than were labelled in the dataset.
Our results are given in Table 4.  We found that the Wikipedia and Twitter datasets contained labels which were more general, evidenced by similar high F1 scores from both annotators (> 0.8). For political speeches, we observed that the human annotators both found many more examples to be check-worthy than were labelled in the dataset.
Table  shows the number of correct predictions, precision, recall, and F1 values with all tools including our method (n-gram auto-sklearn).  We can see that the number of correct predictions are higher with our method in all three datasets, and our method achieved the highest F1 values for all three positive, all three negative, and one neutral.  In summary, our method using n-gram IDF and automated machine learning (auto-sklearn) largely outperformed existing sentiment analysis tools.
Table  shows the number of correct predictions, precision, recall, and F1 values with all tools including our method (n-gram auto-sklearn).  We can see that the number of correct predictions are higher with our method in all three datasets, and our method achieved the highest F1 values for all three positive, all three negative, and one neutral.  In summary, our method using n-gram IDF and automated machine learning (auto-sklearn) largely outperformed existing sentiment analysis tools.
Table 3 shows that the 10rvw variant consistently outperforms the word-level baseline.
Table 3 shows that the 10rvw variant consistently outperforms the word-level baseline.
Table 1: Results on the NIST Chinese-English translation task. "Params" denotes the number of model parameters. "Emb." represents the number of parameters used for word representation. "Red." represents the reduction rate of the standard size. The results of SMT* and RNNsearch* are reported by Kuang et al. (2018) with the same datasets and vocabulary settings. "↑" indicates the result is significantly better than that of the vanilla Transformer (p < 0.01), while "⇑" indicates the result is significantly better than that of all other Transformer models (p < 0.01). All significance tests are measured by paired bootstrap resampling (Koehn, 2004).  Table 1 reports the results on the NIST ChineseEnglish test sets. It is observed that the Transformer models significantly outperform SMT and RNNsearch models. Therefore, we decide to implement all of our experiments based on Transformer architecture. The direct bridging model can further improve the translation quality of the Transformer baseline. The decoder WT model improves the translation quality while reducing the number of parameters for the word representation. This improved performance happens because there are fewer model parameters, which prevents over-fitting (Press and Wolf, 2017). Finally, the performance is further improved by the proposed method while using even fewer parameters than other models.
Table 1: Results on the NIST Chinese-English translation task. "Params" denotes the number of model parameters. "Emb." represents the number of parameters used for word representation. "Red." represents the reduction rate of the standard size. The results of SMT* and RNNsearch* are reported by Kuang et al. (2018) with the same datasets and vocabulary settings. "↑" indicates the result is significantly better than that of the vanilla Transformer (p < 0.01), while "⇑" indicates the result is significantly better than that of all other Transformer models (p < 0.01). All significance tests are measured by paired bootstrap resampling (Koehn, 2004).  Table 1 reports the results on the NIST ChineseEnglish test sets. It is observed that the Transformer models significantly outperform SMT and RNNsearch models. Therefore, we decide to implement all of our experiments based on Transformer architecture. The direct bridging model can further improve the translation quality of the Transformer baseline. The decoder WT model improves the translation quality while reducing the number of parameters for the word representation. This improved performance happens because there are fewer model parameters, which prevents over-fitting (Press and Wolf, 2017). Finally, the performance is further improved by the proposed method while using even fewer parameters than other models.
Table 2: Results on the WMT English-German translation task. "‡" indicates the result is significantly better than the vanilla Transformer model (p < 0.05).  Similar observations are obtained on the English-German translation task, as shown in Table 2.
Table 2: Results on the WMT English-German translation task. "‡" indicates the result is significantly better than the vanilla Transformer model (p < 0.05).  Similar observations are obtained on the English-German translation task, as shown in Table 2.
Table 3: Results on the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks. These distant language pairs belonging to 5 different language families and written in 5 different alphabets."↑" indicates the result is significantly better than that of the vanilla Transformer (p < 0.01).  Table 3 shows the results on the small-scale IWSLT translation tasks. We observe that the proposed method stays consistently better than the vanilla model on these distant language pairs.
Table 3: Results on the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks. These distant language pairs belonging to 5 different language families and written in 5 different alphabets."↑" indicates the result is significantly better than that of the vanilla Transformer (p < 0.01).  Table 3 shows the results on the small-scale IWSLT translation tasks. We observe that the proposed method stays consistently better than the vanilla model on these distant language pairs.
Table 4: Performance of models using different sharing coefficients on the validation set of the NIST ChineseEnglish translation task.  As shown in Table 4, the decoder WT model can be seen as a kind of shared-private method where zero features are shared between the source and target word embeddings. For the proposed method, λ = (0.5, 0.5, 0.5) and λ = (1, 1, 1) are, respectively, used for sharing half and all features between the embeddings of all categories of words. This allows the model to significantly reduce the number of parameters and also improve the translation quality. For comparison purpose, we also consider sharing a large part of the features among the unrelated words by setting s3 to 0.9, i.e. λ = (0.5, 0.7, 0.9). We argue that it is hard for
Table 4: Performance of models using different sharing coefficients on the validation set of the NIST ChineseEnglish translation task.  As shown in Table 4, the decoder WT model can be seen as a kind of shared-private method where zero features are shared between the source and target word embeddings. For the proposed method, λ = (0.5, 0.5, 0.5) and λ = (1, 1, 1) are, respectively, used for sharing half and all features between the embeddings of all categories of words. This allows the model to significantly reduce the number of parameters and also improve the translation quality. For comparison purpose, we also consider sharing a large part of the features among the unrelated words by setting s3 to 0.9, i.e. λ = (0.5, 0.7, 0.9). We argue that it is hard for
Table 2 presents the time required to install each tool and complete the first annotation task.  SLATE and YEDDA are comparable in effort, which fits with their common design as simple tools with minimal dependencies. Participants had great difficulty with brat, with only two managing to install it, one just as their time finished.
Table 2 presents the time required to install each tool and complete the first annotation task.  SLATE and YEDDA are comparable in effort, which fits with their common design as simple tools with minimal dependencies. Participants had great difficulty with brat, with only two managing to install it, one just as their time finished.
Since the loss, perplexity of syntree2vec is lower than word2vec, node2vec over most of the data sizes given below we say that the syntree2vec performs slightly better than both of them.  There is a clear margin of difference between the perplexity scores of node2vec and syntree2vec.
Since the loss, perplexity of syntree2vec is lower than word2vec, node2vec over most of the data sizes given below we say that the syntree2vec performs slightly better than both of them.  There is a clear margin of difference between the perplexity scores of node2vec and syntree2vec.
For each question in ASNQ, the positive candidate answers are those sentences that occur in the long answer paragraphs in NQ and contain annotated short answers. The remaining sentences from the document are labeled as negative for the target question.  while the ASNQ statistics are reported in Table 1.  ASNQ contains 57,242 distinct questions in the training set and 2,672 distinct questions in the dev. set,
For each question in ASNQ, the positive candidate answers are those sentences that occur in the long answer paragraphs in NQ and contain annotated short answers. The remaining sentences from the document are labeled as negative for the target question.  while the ASNQ statistics are reported in Table 1.  ASNQ contains 57,242 distinct questions in the training set and 2,672 distinct questions in the dev. set,
Table 3: Performance of different models on WikiQA dataset. Here Comp-Agg + LM + LC refers to a CompareAggregate model with Language Modeling and Latent Clustering as proposed by Yoon et al. (2019). TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively.  Table 3 reports the MAP and MRR of different pre-trained transformers models for two methods: standard fine-tuning (FT) and TANDA. The latter takes two arguments that we indicate as transfer dataset → adapt dataset.
Table 3: Performance of different models on WikiQA dataset. Here Comp-Agg + LM + LC refers to a CompareAggregate model with Language Modeling and Latent Clustering as proposed by Yoon et al. (2019). TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively.  Table 3 reports the MAP and MRR of different pre-trained transformers models for two methods: standard fine-tuning (FT) and TANDA. The latter takes two arguments that we indicate as transfer dataset → adapt dataset.
Table 4: Performance of different models on TREC-QA dataset. Here Comp-Agg + LM + LC refers to a CompareAggregate model with Language Modeling and Latent Clustering as proposed in (Yoon et al. 2019). TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively.  Table 4 reports the results of our experiments with TREC-QA.
Table 4: Performance of different models on TREC-QA dataset. Here Comp-Agg + LM + LC refers to a CompareAggregate model with Language Modeling and Latent Clustering as proposed in (Yoon et al. 2019). TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively.  Table 4 reports the results of our experiments with TREC-QA.
Table 5: Model accuracy when noise is injected into WikiQA and TREC-QA datasets. ∗ indicates the target dataset for the second step of fine-tuning (adapt step).  Table 5 shows the MAP and MRR of BERTBase using FT and TANDA, also indicating the drop percentage (% ) in accuracy due to the injection of noise.
Table 5: Model accuracy when noise is injected into WikiQA and TREC-QA datasets. ∗ indicates the target dataset for the second step of fine-tuning (adapt step).  Table 5 shows the MAP and MRR of BERTBase using FT and TANDA, also indicating the drop percentage (% ) in accuracy due to the injection of noise.
Table 6: Impact of different labels of ASNQ on fine-tuning BERT for answer sentence selection. Neg and Pos refers to question-answer (QA) pairs of that particular label being chosen for fine-tuning.  We carried out experiments by fine-tuning BERTBase on ASNQ with specific label categories assigned to the negative class. Table 6 shows the results: Label 3 is the most effective negative type of the three,
Table 6: Impact of different labels of ASNQ on fine-tuning BERT for answer sentence selection. Neg and Pos refers to question-answer (QA) pairs of that particular label being chosen for fine-tuning.  We carried out experiments by fine-tuning BERTBase on ASNQ with specific label categories assigned to the negative class. Table 6 shows the results: Label 3 is the most effective negative type of the three,
Table 7 shows that both FT and TANDA using ASNQ provide significantly better performance than QNLI on the WikiQA dataset.
Table 7 shows that both FT and TANDA using ASNQ provide significantly better performance than QNLI on the WikiQA dataset.
Table 8: Comparison between FT and TANDA on real-world datasets derived from Alexa Virtual Assistant traffic  In these experiments, we used, as usual, ASNQ for the transfer step, and NAD as our target dataset for the adapt step. Table 8 reports the comparative results using simple FT on NAD (denoted simply by NAD) and tested on samples 1, 2 and 3.
Table 8: Comparison between FT and TANDA on real-world datasets derived from Alexa Virtual Assistant traffic  In these experiments, we used, as usual, ASNQ for the transfer step, and NAD as our target dataset for the adapt step. Table 8 reports the comparative results using simple FT on NAD (denoted simply by NAD) and tested on samples 1, 2 and 3.
The Vocabulary model correctly classified test items in three IC bands only (Table 5).
The Vocabulary model correctly classified test items in three IC bands only (Table 5).
Seconds a decoder takes to decode an entire dataset, given a set of scores. Score matrix entries are  We compare Cython implementations in Table 1 and Figure 1 over randomised score input. Note that our implementations are significantly faster.
Our experiments on 23 languages show our approach to be promising, surpassing the baseline on 23 of the 28 evaluation datasets.  We evaluate our setup on 28 datasets for 23 languages, finding that it outperforms the baseline on 23 of the datasets,  This results in a set of 28 treebanks for 23 languages  listed in Table 2.  The results are listed in Table 2.  the oracle does not reach 100%;  We also express the performance of our method as error reduction on the scale from baseline (0%) to upper bound (100%).  For 23 of the 28 datasets, our method achieves a positive error reduction; the median error reduction is 23%. Because of the extreme result for Korean, the average does not make much sense here. The results are worst for Korean and Japanese,  the "form" baseline very close to the upper bound  our results are very low here. We also observe deteriorations for a treebank of Italian Tweets and for treebanks of historical Latin,  On all other datasets, we observe an improvement over the baseline, with an error reduction typically between 10% and 35%. The performance is especially good for Slavic languages (cs, hr, pl, sk, uk), where the error reduction is often around 50%.  The evaluation showed the approach to be promising, surpassing the baseline on most of the evaluation datasets.
Our experiments on 23 languages show our approach to be promising, surpassing the baseline on 23 of the 28 evaluation datasets.  We evaluate our setup on 28 datasets for 23 languages, finding that it outperforms the baseline on 23 of the datasets,  This results in a set of 28 treebanks for 23 languages  listed in Table 2.  The results are listed in Table 2.  the oracle does not reach 100%;  We also express the performance of our method as error reduction on the scale from baseline (0%) to upper bound (100%).  For 23 of the 28 datasets, our method achieves a positive error reduction; the median error reduction is 23%. Because of the extreme result for Korean, the average does not make much sense here. The results are worst for Korean and Japanese,  the "form" baseline very close to the upper bound  our results are very low here. We also observe deteriorations for a treebank of Italian Tweets and for treebanks of historical Latin,  On all other datasets, we observe an improvement over the baseline, with an error reduction typically between 10% and 35%. The performance is especially good for Slavic languages (cs, hr, pl, sk, uk), where the error reduction is often around 50%.  The evaluation showed the approach to be promising, surpassing the baseline on most of the evaluation datasets.
In Table 3, we compare the combined distance measure with each of the two components used alone. The results show that combining the edit distance with the embedding similarity is stronger than using any of the measures alone.  The embedding similarity alone performs much better than the edit distance alone.
In Table 3, we compare the combined distance measure with each of the two components used alone. The results show that combining the edit distance with the embedding similarity is stronger than using any of the measures alone.  The embedding similarity alone performs much better than the edit distance alone.
Various subsets of authors were chosen and the dataset was truncated to each author having the same number of samples.  The classification was carried out with 2 author attribution datasets, one with 6 authors  and our dataset with maximum 14 authors. The larger dataset was trained with 6,8,10,12 and 14 authors to analyze the effects of increasing classes on the proposed model.  We evaluate the performance of the proposed architecture in terms of accuracy,  We also try to infer how the character-level model compares with the word level models. All models are compared for the increasing number of authors(classes) on the corpus mentioned to assess the quality of the models.  both word and character levels are summarized in Table II.  From the accuracy comparisons shown in Table II we see that Skip-gram implemented by fastText performs well in the given datasets.  Character level model performs reasonably well in competition with subword level as long as the dataset is big enough.  With larger datasets, this model will be able to perform significantly better compare character embeddings with word embeddings showing that character embeddings perform almost as good as the best word embedding model.
Although our unsupervised model doesn't support all the labels, to show the effectiveness of the approach, we compare the label accuracy of "SUPPORTS" label against a supervised approach – HexaF.  Results from Table 3 suggests that our approach is comparable to HexaF4 for φ = 0.76.
Although our unsupervised model doesn't support all the labels, to show the effectiveness of the approach, we compare the label accuracy of "SUPPORTS" label against a supervised approach – HexaF.  Results from Table 3 suggests that our approach is comparable to HexaF4 for φ = 0.76.
For the subtask of question generation, the results in Table 1 show that the system is able to generate questions given a claim with considerably good accuracy. The conversion accuracy is defined as the ratio of the number of claims in which the named entities are extracted to the number of claims. The results also support our assumption that the claims generally feature information about one or more entities.  Ta
For the subtask of question generation, the results in Table 1 show that the system is able to generate questions given a claim with considerably good accuracy. The conversion accuracy is defined as the ratio of the number of claims in which the named entities are extracted to the number of claims. The results also support our assumption that the claims generally feature information about one or more entities.  Ta
Table 2 shows the performance of our Fact Checking system on the "SUPPORTS" label, the output of our system. We compare the results against two different classification thresholds.  Here, φ = 0.76 suggests that at least 3 out of the 4 questions have to be answered correctly while φ = 0.67 suggests that at least 2 out of the 3 questions has to be answered correctly for the claim to be classified as "SUPPORTS".  In contrast to the results reported in Table 2, here we consider φ = 0.76 to be a better classification threshold as it improvises over False Positives considerably over the entire dataset.  From the results, we conclude that it is possible to verify the facts with the right kind of factoid questions.
Table 2 shows the performance of our Fact Checking system on the "SUPPORTS" label, the output of our system. We compare the results against two different classification thresholds.  Here, φ = 0.76 suggests that at least 3 out of the 4 questions have to be answered correctly while φ = 0.67 suggests that at least 2 out of the 3 questions has to be answered correctly for the claim to be classified as "SUPPORTS".  In contrast to the results reported in Table 2, here we consider φ = 0.76 to be a better classification threshold as it improvises over False Positives considerably over the entire dataset.  From the results, we conclude that it is possible to verify the facts with the right kind of factoid questions.
The results of the evaluation of the models on the test sets are shown in Table 1. We notice that additional training on WIKICREM consistently improves the performance of the models in all scenarios and on most tests. Due to the small size of some test sets, some of the results are subject to deviation. This especially applies to PDP (60 test samples) and WNLI (145 test samples). We observe that BERT WIKIRAND generally performs worse than BERT, with GAP and PDP being notable exceptions. This shows that BERT is a strong baseline and that improved performance of BERT WIKICREM is not a consequence of training on shorter sentences or with different loss function. BERT WIKICREM consistently outperforms both baselines on all tests, showing that WIKICREM can be used as a standalone dataset. We observe that training on the data from the target distribution improves the performance the most. Models trained on GAP-train usually show more than a 20% increase in their F1-score on GAP-test. Still, BERT WIKICREM GAP shows  a consistent improvement over BERT GAP on all subsets of the GAP test set. This confirms that WIKICREM works not just as a standalone dataset, but also as an additional pre-training in the transductive scenario. Similarly, BERT WIKICREM DPR outpertasks, forms BERT DPR on the majority of showing the applicability of WIKICREM to the scenario where additional training data is available. However, good results of BERT GAP DPR show that additional training on a manually constructed dataset, such as GAP, can yield similar results as additional training on WIKICREM. The reason behind this difference is the impact of the data distribution. GAP, DPR, and WIKICREM contain data that follows different distributions which strongly impacts the trained models. This  can be seen when we fine-tune BERT GAP on DPR to obtain BERT GAP DPR, as the model's performance on GAP-test drops by 8.2%. WIKICREM's data distribution strongly differs from the test sets' as described in Section 3. the achieves However, the best results are achieved when all available data is combined, as shown by the models BERT ALL and BERT WIKICREM ALL. BERT WIKICREM ALL highest performance on GAP, DPR, WNLI, and WINOBIAS among the models, and sets the new state-of-the-art result on GAP, DPR, and WINOBIAS. result on the WINOGENDER dataset is achieved by the BERT WIKICREM DPR model, while BERT WIKICREM ALL and BERT GAP DPR set the new state-of-the-art on the PDP dataset. The new state-of-the-art
The results of the evaluation of the models on the test sets are shown in Table 1. We notice that additional training on WIKICREM consistently improves the performance of the models in all scenarios and on most tests. Due to the small size of some test sets, some of the results are subject to deviation. This especially applies to PDP (60 test samples) and WNLI (145 test samples). We observe that BERT WIKIRAND generally performs worse than BERT, with GAP and PDP being notable exceptions. This shows that BERT is a strong baseline and that improved performance of BERT WIKICREM is not a consequence of training on shorter sentences or with different loss function. BERT WIKICREM consistently outperforms both baselines on all tests, showing that WIKICREM can be used as a standalone dataset. We observe that training on the data from the target distribution improves the performance the most. Models trained on GAP-train usually show more than a 20% increase in their F1-score on GAP-test. Still, BERT WIKICREM GAP shows  a consistent improvement over BERT GAP on all subsets of the GAP test set. This confirms that WIKICREM works not just as a standalone dataset, but also as an additional pre-training in the transductive scenario. Similarly, BERT WIKICREM DPR outpertasks, forms BERT DPR on the majority of showing the applicability of WIKICREM to the scenario where additional training data is available. However, good results of BERT GAP DPR show that additional training on a manually constructed dataset, such as GAP, can yield similar results as additional training on WIKICREM. The reason behind this difference is the impact of the data distribution. GAP, DPR, and WIKICREM contain data that follows different distributions which strongly impacts the trained models. This  can be seen when we fine-tune BERT GAP on DPR to obtain BERT GAP DPR, as the model's performance on GAP-test drops by 8.2%. WIKICREM's data distribution strongly differs from the test sets' as described in Section 3. the achieves However, the best results are achieved when all available data is combined, as shown by the models BERT ALL and BERT WIKICREM ALL. BERT WIKICREM ALL highest performance on GAP, DPR, WNLI, and WINOBIAS among the models, and sets the new state-of-the-art result on GAP, DPR, and WINOBIAS. result on the WINOGENDER dataset is achieved by the BERT WIKICREM DPR model, while BERT WIKICREM ALL and BERT GAP DPR set the new state-of-the-art on the PDP dataset. The new state-of-the-art
Various subsets of authors were chosen and the dataset was truncated to each author having the same number of samples.  with and without pre-training character level embedding and comparing the proposed architectures on the held-out dataset.  To illustrate the need of pre-trained character embeddings, we see from III that using a pre-trained embedding increases the accuracy across datasets and the different number of authors, regardless of the amount of data for each author.  increase the performance a few degrees.  we analyzed the importance of pretrained character embedding for author attribution and showed that pre-training can result in better performances.
The entity linking accuracy on the Wikipedia test set are summarized in Table 1. On average, our proposed PBEL method performs significantly better than the baselines, with significant accuracy gains in all nine test languages.  The EXACT baseline, which is most often used for monolingual EL (Sil et al. 2017), performs reasonably only when the test language is in the same script as English (i.e., Javanese).  Similarly TRANS, the current state-of-the-art retrieval method in cross-lingual EL (Pan et al. 2017), fails when zero data is available in the test language, unless the HRL is very closely-related to the LRL (as with jv, mr and am). On the other hand, ENCODE presents relatively strong zero-shot transfer results.  PBEL offers stronger performance than ENCODE  As seen in the BEST-53 results, the HRL that performs best is closely-related to the respective test LRL  We also observe that using multiple pivot HRLs leads to better average accuracy, with considerable improvement for some languages.  In most cases, the MANUAL HRL is also the best performing in BEST-53. However, we see that the Dravidian Telugu (te) seems to obtain higher accuracy with Indo-Aryan HRLs – Punjabi (pa) or Hindi (hi).  We also see that the Ukrainian (uk) test set has better performance with another Cyrillic script language, Kazakh (kk), rather than Russian (ru).
The entity linking accuracy on the Wikipedia test set are summarized in Table 1. On average, our proposed PBEL method performs significantly better than the baselines, with significant accuracy gains in all nine test languages.  The EXACT baseline, which is most often used for monolingual EL (Sil et al. 2017), performs reasonably only when the test language is in the same script as English (i.e., Javanese).  Similarly TRANS, the current state-of-the-art retrieval method in cross-lingual EL (Pan et al. 2017), fails when zero data is available in the test language, unless the HRL is very closely-related to the LRL (as with jv, mr and am). On the other hand, ENCODE presents relatively strong zero-shot transfer results.  PBEL offers stronger performance than ENCODE  As seen in the BEST-53 results, the HRL that performs best is closely-related to the respective test LRL  We also observe that using multiple pivot HRLs leads to better average accuracy, with considerable improvement for some languages.  In most cases, the MANUAL HRL is also the best performing in BEST-53. However, we see that the Dravidian Telugu (te) seems to obtain higher accuracy with Indo-Aryan HRLs – Punjabi (pa) or Hindi (hi).  We also see that the Ukrainian (uk) test set has better performance with another Cyrillic script language, Kazakh (kk), rather than Russian (ru).
Entity linking accuracies on the LORELEI dataset are shown in Table 2. PBEL has considerably higher accuracy than the other methods. However, we see relatively lower improvement in accuracy with Somali-Oromo than Amharic-Tigrinya.  the supervised TRANS model, which uses Wikipedia parallel data in the LRL itself as a lexicon, does not perform better than the zero-shot PBEL.  The PBEL model results for our test set with each input representation are presented in Table 3.
Entity linking accuracies on the LORELEI dataset are shown in Table 2. PBEL has considerably higher accuracy than the other methods. However, we see relatively lower improvement in accuracy with Somali-Oromo than Amharic-Tigrinya.  the supervised TRANS model, which uses Wikipedia parallel data in the LRL itself as a lexicon, does not perform better than the zero-shot PBEL.  The PBEL model results for our test set with each input representation are presented in Table 3.
see We that using phonological representations (phonemes and articulatory features) offers the ability to map between languages that use different orthographies, explaining the convincing improvement over graphemes for HRL-LRL pairs that are written in different scripts (Table 3). With graphemes, the experiments on these languages achieve ≈ 0 accuracy because the character vocabulary of the HRL encoder simply does not contain the low-resource test language characters. This is the case with Lao-Thai (lo-th), Telugu-Tamil (te-ta) and Bengali-Hindi (bn-hi). In contrast, we observe that the grapheme representation offers strong transfer performance when the LRL and HRL share orthography, notably Javanese-Indonesian (jv-id), Marathi-Hindi (mr-hi) and Ukrainian-Russian (uk-ru).
see We that using phonological representations (phonemes and articulatory features) offers the ability to map between languages that use different orthographies, explaining the convincing improvement over graphemes for HRL-LRL pairs that are written in different scripts (Table 3). With graphemes, the experiments on these languages achieve ≈ 0 accuracy because the character vocabulary of the HRL encoder simply does not contain the low-resource test language characters. This is the case with Lao-Thai (lo-th), Telugu-Tamil (te-ta) and Bengali-Hindi (bn-hi). In contrast, we observe that the grapheme representation offers strong transfer performance when the LRL and HRL share orthography, notably Javanese-Indonesian (jv-id), Marathi-Hindi (mr-hi) and Ukrainian-Russian (uk-ru).
The results are summarized in Table  Almost all the methods performed similar to random guess, showing that SQA on our dataset has its unique challenges.
The results are summarized in Table  Almost all the methods performed similar to random guess, showing that SQA on our dataset has its unique challenges.
The table 1 shows the caption retrieval recall on COCO dataset. The first two lines show the state-of-the-art results. The second pair of lines present the results of our model, with W2V and FastText embeddings used as the baseline.  We can see that our model is close to the Deep SemanticVisual Embedding (DSVE) method  while the W2V method is slightly worst, as the representation power of the word embedding is reduced.  The BIVEC English-French method is used in English and on both languages simultaneously. If trained only on English, i.e. only on the COCO dataset like the two previous methods, it shows performance similar to the one of the state-of-the-art. This means training using BIVEC does not weaken the English representation. When trained on English and French together, the recall is increased by 3.35 %, going from 65.58 % to 67.78 %. We can also see an improvement for recall@5 and recall@10, with respectively 1.17 % and 0.85 % of increase. This implies that the similarity learning with French captions increases English recognition when using BIVEC.  First of all, when training with MUSE for English only, we can see a sharp decrease in performance, with a recall going from 66.08 % to 63.10 %. By comparing the model trained with W2V, we obtain similar results. This could come from the fact that both MUSE and W2V embeddings do not have representation for out of vocabulary words like the FastText ones. Moreover, rare words have much more chance to be wrongly projected because of space transformation. When we train the model with additional languages, we can see a slight decrease in performance in English. The maximum decrease is 1.01 % for recall@10, but it is counterbalanced by an increase of 0.29 % for the recall@1.  We evaluated our method on the COCO dataset for English-only results and shown that using BIVEC embeddings enables the use of another language in order to improve the performance. The obtained improvement is a 3.35 % increase in performance on the COCO dataset, and a 15.15 % increase on the Multi30K dataset.
The table 1 shows the caption retrieval recall on COCO dataset. The first two lines show the state-of-the-art results. The second pair of lines present the results of our model, with W2V and FastText embeddings used as the baseline.  We can see that our model is close to the Deep SemanticVisual Embedding (DSVE) method  while the W2V method is slightly worst, as the representation power of the word embedding is reduced.  The BIVEC English-French method is used in English and on both languages simultaneously. If trained only on English, i.e. only on the COCO dataset like the two previous methods, it shows performance similar to the one of the state-of-the-art. This means training using BIVEC does not weaken the English representation. When trained on English and French together, the recall is increased by 3.35 %, going from 65.58 % to 67.78 %. We can also see an improvement for recall@5 and recall@10, with respectively 1.17 % and 0.85 % of increase. This implies that the similarity learning with French captions increases English recognition when using BIVEC.  First of all, when training with MUSE for English only, we can see a sharp decrease in performance, with a recall going from 66.08 % to 63.10 %. By comparing the model trained with W2V, we obtain similar results. This could come from the fact that both MUSE and W2V embeddings do not have representation for out of vocabulary words like the FastText ones. Moreover, rare words have much more chance to be wrongly projected because of space transformation. When we train the model with additional languages, we can see a slight decrease in performance in English. The maximum decrease is 1.01 % for recall@10, but it is counterbalanced by an increase of 0.29 % for the recall@1.  We evaluated our method on the COCO dataset for English-only results and shown that using BIVEC embeddings enables the use of another language in order to improve the performance. The obtained improvement is a 3.35 % increase in performance on the COCO dataset, and a 15.15 % increase on the Multi30K dataset.
Given a sentence, in any language, we evaluate the rank of the corresponding image. The evaluation is again made by batches of 1000. The results are presented in table 2.  The first two lines of the table present the state-of-the-art results, with W2V and FastText embeddings. We can see similar results as in the previous experiment. With BIVEC, we have results close to the FastText embeddings when training only in English. This time, the recall is better with an increase of 2.68 % for recall@1. When trained with English and French, the recall@1 is increased by 3.65 %. This implies, again, that we can improve performance by learning on an additional language.  Our model is able to use the multi-language representing the effectiveness of MUSE embeddings. We train the model with English, and different combinations of French, German and Czech. On English only, we have similar results to the W2V approach. When adding new languages, we can see a decrease in performance for English. We obtain a maximum decrease of 2.62 % for recall@1 when the models saw English, French, German and Czech.  For image retrieval from a caption in 4 languages, we obtain a 49.38 % recall@10 on the Multi30K dataset.
The model is trained with English only, then with English and French (en+fr), with English, French, and German (en+fr+de) and with English, French, German and Czech (all). We can see a decrease in performance when adding French that is not present with other languages. Otherwise, every time we add a new language the recall for this language logically increase. The best performance is achieved with English+French+German+Czech, with an increase of 6.42 % for multilingual retrieval.  By using MUSE embeddings, we are able to embed more languages in the same model. We showed that adding other languages decrease performance for English, but increase the recall in a multilingual environment.
With BIVEC embeddings, we learn two languages at the same time, and test retrieval on one or two of these languages. Results are shown in table 4. Trained in English alone, the model gives worse performance than  MUSE for languages not seen previously. For example, with English-German BIVEC and a model trained only in English, and test on German, we obtain only 22.96 % recall@10, where MUSE embeddings obtain 44.18 %. But when train on English and French, we obtain 55.22 % recall, an increase of 26.39 % compared to MUSE. With German and English training, we have an increase of 15.16 % on English only recall, with a recall of 61.44 %. Meaning that, once again, learning a new language with BIVEC enables better results in English, as the same kind of results is visible with French as well.
With BIVEC embeddings, we learn two languages at the same time, and test retrieval on one or two of these languages. Results are shown in table 4. Trained in English alone, the model gives worse performance than  MUSE for languages not seen previously. For example, with English-German BIVEC and a model trained only in English, and test on German, we obtain only 22.96 % recall@10, where MUSE embeddings obtain 44.18 %. But when train on English and French, we obtain 55.22 % recall, an increase of 26.39 % compared to MUSE. With German and English training, we have an increase of 15.16 % on English only recall, with a recall of 61.44 %. Meaning that, once again, learning a new language with BIVEC enables better results in English, as the same kind of results is visible with French as well.
We select a randomly chosen validator's answer to each question and compute Exact Match (EM) and word overlap F1 scores with the original to calculate non-expert human performance; Table 1 shows the result. We observe a clear trend: the stronger the model in the loop used to construct the dataset, the harder the resulting questions become for humans.
We assess unigram (R-1), bigram (R-2), and longest-commonsubsequence (R-L) overlap, and present F1, recall and precision scores in Table 1.  For the first baseline (PG), we see that incorporating discourse features consistently improves recall and F1.  We see similar observations for the second baseline (PG+Cov): recall is generally improved at the expense of precision.  Observing that our model generally has better recall (Table 1)
We assess unigram (R-1), bigram (R-2), and longest-commonsubsequence (R-L) overlap, and present F1, recall and precision scores in Table 1.  For the first baseline (PG), we see that incorporating discourse features consistently improves recall and F1.  We see similar observations for the second baseline (PG+Cov): recall is generally improved at the expense of precision.  Observing that our model generally has better recall (Table 1)
We present the test results in Table 2.  We are able to reproduce the performance of the baseline model ("CNN w/ GloVe"), and find that once again, adding the shallow discourse features improves results.  Interestingly, we found that replacing the CNN with an LSTM results in improved MAE, but worse MAPE. Adding discourse features to this model generally has marginal improvement in all cases.  When we replace the word sequence with EDUs ("Bi-LSTM w/ latent" and "Bi-LSTM w/ shallow"), we see that the latent features outperform the shallow features.
We present the test results in Table 2.  We are able to reproduce the performance of the baseline model ("CNN w/ GloVe"), and find that once again, adding the shallow discourse features improves results.  Interestingly, we found that replacing the CNN with an LSTM results in improved MAE, but worse MAPE. Adding discourse features to this model generally has marginal improvement in all cases.  When we replace the word sequence with EDUs ("Bi-LSTM w/ latent" and "Bi-LSTM w/ shallow"), we see that the latent features outperform the shallow features.
We choose different teacher size K and evaluate our models in three datasets. Table 2 shows that teacher size is sensitive to datasets. Therefore, we select the best teacher size for each dataset in the following experiment.
We choose different teacher size K and evaluate our models in three datasets. Table 2 shows that teacher size is sensitive to datasets. Therefore, we select the best teacher size for each dataset in the following experiment.
Table 3 shows the results of fine-tuning the BERT-base model on five text classification datasets and two NLI datasets. For ensemble BERT, both the voted BERT (BERTVOTE) and averaged BERT (BERTAVG) outperform the single BERT (BERTBASE). The average improvement of BERTVOTE is 5.44% (for text clas  sification) and 5.50% (for NLI), while BERTAVG follows closely with 4.07% and 3.24%. BERTVOTE outperforms BERTAVG on all tasks, which adheres to our intuition since BERTVOTE is more complicated. The self-ensemble BERT (BERTSE) has a slight improvement in classification tasks of 2.50%, but it does not work on NLI tasks. This is also a reason why we need self-distillation to improve the base models. Overall, self-distillation model has significant improvement on both classification and NLI tasks. Table 3 shows that BERTSDA and BERTSDV outperform BERTBASE on all datasets. Generally speaking, BERTSDA performs better than BERTSDV on text classification tasks with the improvement of 6.26% vs. 5.65%, but the latter performs better on NLI tasks (BERTSDA vs. BERTSDV is 4.65% vs. 5.30%). Our proposed fine-tuning strategies also outperform the previous method in [Sun et al., 2019] on text classification tasks, which makes extensive efforts to find sophisticated hyperparameters.
Table 3 shows the results of fine-tuning the BERT-base model on five text classification datasets and two NLI datasets. For ensemble BERT, both the voted BERT (BERTVOTE) and averaged BERT (BERTAVG) outperform the single BERT (BERTBASE). The average improvement of BERTVOTE is 5.44% (for text clas  sification) and 5.50% (for NLI), while BERTAVG follows closely with 4.07% and 3.24%. BERTVOTE outperforms BERTAVG on all tasks, which adheres to our intuition since BERTVOTE is more complicated. The self-ensemble BERT (BERTSE) has a slight improvement in classification tasks of 2.50%, but it does not work on NLI tasks. This is also a reason why we need self-distillation to improve the base models. Overall, self-distillation model has significant improvement on both classification and NLI tasks. Table 3 shows that BERTSDA and BERTSDV outperform BERTBASE on all datasets. Generally speaking, BERTSDA performs better than BERTSDV on text classification tasks with the improvement of 6.26% vs. 5.65%, but the latter performs better on NLI tasks (BERTSDA vs. BERTSDV is 4.65% vs. 5.30%). Our proposed fine-tuning strategies also outperform the previous method in [Sun et al., 2019] on text classification tasks, which makes extensive efforts to find sophisticated hyperparameters.
We also investigate whether self-distillation has similar findings for the BERTlarge model (BERT-L), which contains 24 Transformer layers. Due to the limitation of our devices, we only conduct an experiment on two text classification datasets and one NLI datasets and evaluate strategy BERTSDA, namely self-distillation with averaged BERT as a teacher. We set two different teacher sizes for comparison. As shown in Table 4, self-distillation also gets a significant gain while fine-tuning the BERT-large model. On two text classification tasks, BERT-LSDA(K =  − 1) gives better results and the average improvement is 7.02%. For NLI task, BERT-LSDA(K = 1) gives better result and the improvement is 6.59%.
As shown in Table 2, TransDG achieves the lowest perplexity on all the datasets, indicating that the generated responses are more grammatical.
As shown in Table 2, TransDG achieves the lowest perplexity on all the datasets, indicating that the generated responses are more grammatical.
Table 3 demonstrates that the models leveraging external knowledge achieve better performance than the standard Seq2Seq model in generating meaningful entity words and diverse responses. In particular, our model outperforms all the baselines significantly with highest entity score.
Table 3 demonstrates that the models leveraging external knowledge achieve better performance than the standard Seq2Seq model in generating meaningful entity words and diverse responses. In particular, our model outperforms all the baselines significantly with highest entity score.
The BLEU values shown in Table 4 demonstrates the comparison results from word-level overlaps. TransDG tends to generate responses that are more similar to the gold responses than baselines in most cases.
The human evaluation results are reported in Table 5. As shown in Table 5, TransDG tends to generate more appropriate and informative responses in terms of human annotation. Specifically, the responses generated by TransDG have higher knowledge relevance than other models, indicating that TransDG is effective to incorporate appropriate commonsense knowledge.
The human evaluation results are reported in Table 5. As shown in Table 5, TransDG tends to generate more appropriate and informative responses in terms of human annotation. Specifically, the responses generated by TransDG have higher knowledge relevance than other models, indicating that TransDG is effective to incorporate appropriate commonsense knowledge.
The ablation test results are reported in Table 7. From the results, we can observe that the performance of TransDG drops sharply when we discard the question representation module and the knowledge selection module transferred from KBQA.  Response guiding attention also has noticeable impact on the performance of TransDG, especially on BLEU scores.  In addition, the second-step decoder can improve the ability of TransDG to generate relevant entities per response.
The ablation test results are reported in Table 7. From the results, we can observe that the performance of TransDG drops sharply when we discard the question representation module and the knowledge selection module transferred from KBQA.  Response guiding attention also has noticeable impact on the performance of TransDG, especially on BLEU scores.  In addition, the second-step decoder can improve the ability of TransDG to generate relevant entities per response.
We compare our model with the state-of-the-art models in Table 3.  As Table 3 shows, this model alone (without any reranking) improves the state-of-the-art performance from 79.6% to 82.15% accuracy  Increasing the beam size improves the performance as expected.  Using a pre-trained model improves the performance as well.  Reranking with a threshold rule may be helpful for the overall architecture. We observe that reranking by the critic at all times may not be the best approach. We note that choosing not to rerank when all scores are below 0.5 increases the performance further. On the other hand, reranking if the difference between the best score and second best score is above the threshold we set does not help in this case.  The overall architecture improves the performance of the generator (82.1% accuracy) to 83.7% accuracy.
We compare our model with the state-of-the-art models in Table 3.  As Table 3 shows, this model alone (without any reranking) improves the state-of-the-art performance from 79.6% to 82.15% accuracy  Increasing the beam size improves the performance as expected.  Using a pre-trained model improves the performance as well.  Reranking with a threshold rule may be helpful for the overall architecture. We observe that reranking by the critic at all times may not be the best approach. We note that choosing not to rerank when all scores are below 0.5 increases the performance further. On the other hand, reranking if the difference between the best score and second best score is above the threshold we set does not help in this case.  The overall architecture improves the performance of the generator (82.1% accuracy) to 83.7% accuracy.
Table 1: The performances of the IM2LATEX-100K Bi-LSTM model. We discover that the look-ahead improves the model from the greedy search method — noted that LA is more directly comparable to the greedy search because of their same beam size. We also show the scores of the beam search for the reference
Table 1: The performances of the IM2LATEX-100K Bi-LSTM model. We discover that the look-ahead improves the model from the greedy search method — noted that LA is more directly comparable to the greedy search because of their same beam size. We also show the scores of the beam search for the reference
The average number of words in questions and answers, as well as the average longest n-gram overlap between passage and question are furthermore given in Table 3. We can again observe two clear trends: from weaker towards stronger models used in the annotation loop, the average length of answers increases, and the largest n-gram overlap drops from 3 to 2 tokens.
The average number of words in questions and answers, as well as the average longest n-gram overlap between passage and question are furthermore given in Table 3. We can again observe two clear trends: from weaker towards stronger models used in the annotation loop, the average length of answers increases, and the largest n-gram overlap drops from 3 to 2 tokens.
Table 2: The performances of the LSTM model trained on the WMT16 multimodal translation dataset with different LA steps. We show the look-ahead module is able to improve the model on the entire testing set. However, either the LA module or the beam search method harm the models when the length of the target sentences is longer than 25 words.
Table 3: We show the results of applying LA module to the transformer model trained on the WMT14 dataset. We find that the LA module slightly improves the original model but harms the performance when the LA time step is 5. We suggest one of the reasons of these results are caused by the EOS problem.
Table 3: We show the results of applying LA module to the transformer model trained on the WMT14 dataset. We find that the LA module slightly improves the original model but harms the performance when the LA time step is 5. We suggest one of the reasons of these results are caused by the EOS problem.
Table 4: We show the results of integrating auxiliary EOS loss into the training state. γ is the weight of the auxiliary EOS loss. We find the EOS loss not only boosts the performance of the model when using the greedy search, the model is more robust to the larger Look-ahead steps with reasonable weights of auxiliary EOS loss.
Table 4: We show the results of integrating auxiliary EOS loss into the training state. γ is the weight of the auxiliary EOS loss. We find the EOS loss not only boosts the performance of the model when using the greedy search, the model is more robust to the larger Look-ahead steps with reasonable weights of auxiliary EOS loss.
The entry Joint Self-attention corresponds to the results of our implementation of (He et al., 2018), that significantly improves the original results by 0.7 BLEU point on the WMT14 de-en benchmark, and 0.2 on IWSLT. The same architecture with the proposed locality constraints (Local Joint Self-attention) establishes a new state of the art in IWSLT'14 de-en with 35.7 BLEU, surpassing all previous published results by at least in 0.5 BLEU, and our results with the unconstrained version by 0.4. The  Table 1 presents a comparison of the translation quality measured via BLEU score between the currently dominant Transformer (Vaswani et al., 2017) and Dynamic Convolutions (Wu et al., 2019) models, as well as the work by He et al. (2018), which also proposes a joint encoderdecoder structure, and also other refinements over the transformer architecture like (Ahmed et al., 2017), (Chen et al., 2018), (Shaw et al., 2018) and (Ott et al., 2018) .  The Joint Self-attention model obtains the same SoTA BLEU score of (Wu et al., 2019) on WMT'14 en-de, and the same SoTA score of (Ott et al., 2018) and (Wu et al., 2019) on WMT'14 enfr. The local attention constraints do not provide a significant gain on these bigger models, but it improves the BLEU score on WMT'14 en-fr to a new SoTA of 43.3.
The entry Joint Self-attention corresponds to the results of our implementation of (He et al., 2018), that significantly improves the original results by 0.7 BLEU point on the WMT14 de-en benchmark, and 0.2 on IWSLT. The same architecture with the proposed locality constraints (Local Joint Self-attention) establishes a new state of the art in IWSLT'14 de-en with 35.7 BLEU, surpassing all previous published results by at least in 0.5 BLEU, and our results with the unconstrained version by 0.4. The  Table 1 presents a comparison of the translation quality measured via BLEU score between the currently dominant Transformer (Vaswani et al., 2017) and Dynamic Convolutions (Wu et al., 2019) models, as well as the work by He et al. (2018), which also proposes a joint encoderdecoder structure, and also other refinements over the transformer architecture like (Ahmed et al., 2017), (Chen et al., 2018), (Shaw et al., 2018) and (Ott et al., 2018) .  The Joint Self-attention model obtains the same SoTA BLEU score of (Wu et al., 2019) on WMT'14 en-de, and the same SoTA score of (Ott et al., 2018) and (Wu et al., 2019) on WMT'14 enfr. The local attention constraints do not provide a significant gain on these bigger models, but it improves the BLEU score on WMT'14 en-fr to a new SoTA of 43.3.
In Table  there are the results on the DIVA-HisDB dataset (see Section III-A). Our method achieves nearly perfect results (99.42%) and outperforms state-of-the-art (97.86%) resulting in a error reduction of 80.7%.  Note that the lower end of the heatmap scale compares favourably with state-ofthe-art (see Table I) meaning that regardless of the choice of parameters, our method produces excellent results.
In Table  there are the results on the DIVA-HisDB dataset (see Section III-A). Our method achieves nearly perfect results (99.42%) and outperforms state-of-the-art (97.86%) resulting in a error reduction of 80.7%.  Note that the lower end of the heatmap scale compares favourably with state-ofthe-art (see Table I) meaning that regardless of the choice of parameters, our method produces excellent results.
The answer is in Table II where we performed the same task, but this time we swapped our semantic segmentation network  with the pixel-level ground-truth provided along with the data. This represents the upper-bound performances, as no tool will produce a better segmentation than the ground-truth. In this scenario our method performed at 100% line IU, reinforcing our previous observation that our text-line extraction method has made the mistakes only in the presence of wrong results from the semantic segmentation step.
The answer is in Table II where we performed the same task, but this time we swapped our semantic segmentation network  with the pixel-level ground-truth provided along with the data. This represents the upper-bound performances, as no tool will produce a better segmentation than the ground-truth. In this scenario our method performed at 100% line IU, reinforcing our previous observation that our text-line extraction method has made the mistakes only in the presence of wrong results from the semantic segmentation step.
The datasets and vision-language task models are described in the appendix, but are referenced in Table 1.  Unsurprisingly, when comparing the first lines of Table 1(a,b), we find that using Word2Vec rather than an embedding trained from scratch tends to improve performance. This is more important when considering a larger vocabulary as seen comparing phrase grounding experiments on DiDeMo and ReferIt, whose embeddings trained from scratch using their smaller vocabulary compare favorably to Word2Vec.  Word2Vec only falls behind within a point or two across all tasks, and even outperforms or performs equally as well as FastText for certain tasks (e.g. text-to-clip, image captioning).  Table 1 also contains a comparison of language model variants across the five vision-language tasks we evaluate on. We see that fine-tuning a word embedding on a visionlanguage task can have dramatic effects on the performance of the language model (e.g. 5-10% increase to mean recall on image-sentence retrieval).  When comparing the architecture choices from Figure 3 we see that for retrieval-based tasks (i.e. where the output is not free-form text) the Average Embedding and SelfAttention models perform better than a simple LSTM-based approach, with Self-Attention being best on average.  The only apparent exception to this is the text-to-clip task.  InferSent and BERT reach comparable values to the best Word2Vec models for image-sentence retrieval on Flickr30K, performing more poorly for the MSCOCO dataset. For the remaining retrieval tasks, metrics are below the best performing model and embedding combination within 1-3 points, again noting the unusual exception of InferSent on phrase grounding of Flickr30K Entities, which significantly drops below scratch performance.  While all language models perform closely on ReferIt phrase grounding, this still suggests that there is no need to use the more complex LSTM language model without additional modification.  Lastly, sentence level embeddings InferSent and BERT are compared in Table 1(d); results are without fine-tuning.  The two are comparable to each other with the exception of phrase grounding accuracy on Flickr30K Entities; BERT surprisingly outperforms InferSent by 11.55%. Both InferSent and BERT do not provide the best results across any task, and thus are not a leading option for vision-language tasks.
The datasets and vision-language task models are described in the appendix, but are referenced in Table 1.  Unsurprisingly, when comparing the first lines of Table 1(a,b), we find that using Word2Vec rather than an embedding trained from scratch tends to improve performance. This is more important when considering a larger vocabulary as seen comparing phrase grounding experiments on DiDeMo and ReferIt, whose embeddings trained from scratch using their smaller vocabulary compare favorably to Word2Vec.  Word2Vec only falls behind within a point or two across all tasks, and even outperforms or performs equally as well as FastText for certain tasks (e.g. text-to-clip, image captioning).  Table 1 also contains a comparison of language model variants across the five vision-language tasks we evaluate on. We see that fine-tuning a word embedding on a visionlanguage task can have dramatic effects on the performance of the language model (e.g. 5-10% increase to mean recall on image-sentence retrieval).  When comparing the architecture choices from Figure 3 we see that for retrieval-based tasks (i.e. where the output is not free-form text) the Average Embedding and SelfAttention models perform better than a simple LSTM-based approach, with Self-Attention being best on average.  The only apparent exception to this is the text-to-clip task.  InferSent and BERT reach comparable values to the best Word2Vec models for image-sentence retrieval on Flickr30K, performing more poorly for the MSCOCO dataset. For the remaining retrieval tasks, metrics are below the best performing model and embedding combination within 1-3 points, again noting the unusual exception of InferSent on phrase grounding of Flickr30K Entities, which significantly drops below scratch performance.  While all language models perform closely on ReferIt phrase grounding, this still suggests that there is no need to use the more complex LSTM language model without additional modification.  Lastly, sentence level embeddings InferSent and BERT are compared in Table 1(d); results are without fine-tuning.  The two are comparable to each other with the exception of phrase grounding accuracy on Flickr30K Entities; BERT surprisingly outperforms InferSent by 11.55%. Both InferSent and BERT do not provide the best results across any task, and thus are not a leading option for vision-language tasks.
We see a small, but consistent improvement across most of the vision-language tasks using GrOVLE as seen in Table 2(b). These changes result in an embedding with comparable performance to the HGLMM 6K-D features, which are reported in Table 2(e). However, our word embedding tends to perform better when embeddings are the same size (i.e. 300-D). For the generation-based tasks (i.e. captioning and VQA), the benefits of using adapted embeddings are less clear. This may simply be an artifact of the challenges in evaluating these tasks (i.e., the captions are improving in a way the metrics don't capture).  Visual Word2Vec performs comparably amongst results for generation tasks (i.e. image captioning and VQA), but these tasks have little variance in results, with less than a point of difference across the adapted embeddings.  The small gain provided in generation tasks by Visual Word2Vec does not out-weight the drops in performance across other tasks such as the significant mean recall drop of 6.3 compared to HGLMM's 6K-D Self-Attention result in line two of Table 2(c) and Table 2(e) for image-sentence retrieval of Flickr30K. For comparison, GrOVLE's Self-Attention result in Table 2(b) is only 3 points lower.  Finally, we report results using HGLMM of different dimension. HGLMM 300-D features are used for a more fair comparison to other embeddings. While the HGLMM 6K-D representation primarily results in the highest performance, it performs more poorly on generation tasks and also results in high variance. For example, column one in Table 2(e) shows a range of 7.1 in mean recall, unlike GrOVLE which has a range of 2.6.
To address this, we fine-tune GrOVLE across the five VL tasks. We provide results for a four and five multi-task trained embedding. The four task experiments are performed with the final task embedding fixed to demonstrate how well the embeddings would generalize to new tasks. We also provide results for pretraining on five tasks with and without finetuning during the last task.  This multi-task variant is the best performing across all tasks, thus we release this embedding for public use.  To verify that the multi-task GrOVLE performance improvements generalize across task model architecture, we provide results using additional task models in Table 4.  Table 4 provides more models per task and demonstrates consistent results: embeddings can significantly affect performance and GrOVLE variants are still the best embedding overall. As we move down the table we find even larger performance improvements made by using the five-task pretrained GrOVLE with fine-tuning than in Table 3.
To address this, we fine-tune GrOVLE across the five VL tasks. We provide results for a four and five multi-task trained embedding. The four task experiments are performed with the final task embedding fixed to demonstrate how well the embeddings would generalize to new tasks. We also provide results for pretraining on five tasks with and without finetuning during the last task.  This multi-task variant is the best performing across all tasks, thus we release this embedding for public use.  To verify that the multi-task GrOVLE performance improvements generalize across task model architecture, we provide results using additional task models in Table 4.  Table 4 provides more models per task and demonstrates consistent results: embeddings can significantly affect performance and GrOVLE variants are still the best embedding overall. As we move down the table we find even larger performance improvements made by using the five-task pretrained GrOVLE with fine-tuning than in Table 3.
Table 3 reports results of the multi-task training procedure described above. We use the best performing language model in our comparisons for each task, i.e. Self-Attention for image-sentence retrieval and phrase grounding, and the LSTM language model for text-to-clip, image captioning, and VQA. The first lines of Table 3 report the results of the original fixed GrOVLE embedding, which should be considered the baseline. The second line of Table 3 reports performance when the four-task pretrained GrOVLE is fixed when used in the target task, i.e. the task currently being run. The third and fourth line of Table 3 report the results of our embedding when they were trained on all five tasks, and kept fixed or fine-tuned for the target task, respectively.  The results of line three and four demonstrate that our improved embedding tends to transfer better when applied with fine-tuning during the target task. We find similar trends in performance improvements across tasks: larger gains occur for image-sentence retrieval with +7.9 mean recall for the Flickr30K dataset and +6.3 for MSCOCO. All other tasks have performance improvements under one point, showing that while the vision-language tasks appear to transfer well without harming performance, they are leveraged most in image-sentence retrieval, with an exception of phrase grounding accuracy on ReferIt (+2.36%).
Table 3 reports results of the multi-task training procedure described above. We use the best performing language model in our comparisons for each task, i.e. Self-Attention for image-sentence retrieval and phrase grounding, and the LSTM language model for text-to-clip, image captioning, and VQA. The first lines of Table 3 report the results of the original fixed GrOVLE embedding, which should be considered the baseline. The second line of Table 3 reports performance when the four-task pretrained GrOVLE is fixed when used in the target task, i.e. the task currently being run. The third and fourth line of Table 3 report the results of our embedding when they were trained on all five tasks, and kept fixed or fine-tuned for the target task, respectively.  The results of line three and four demonstrate that our improved embedding tends to transfer better when applied with fine-tuning during the target task. We find similar trends in performance improvements across tasks: larger gains occur for image-sentence retrieval with +7.9 mean recall for the Flickr30K dataset and +6.3 for MSCOCO. All other tasks have performance improvements under one point, showing that while the vision-language tasks appear to transfer well without harming performance, they are leveraged most in image-sentence retrieval, with an exception of phrase grounding accuracy on ReferIt (+2.36%).
Our annotation pipeline is designed to reject any samples where the model correctly predicts the answer. How reproducible is this when retraining the same model with the same data? To measure this, we evaluate the performance of two models of identical setup for each respective architecture, which differ only in their random initialisation and data order during SGD sampling. We can thus isolate how strongly the resulting dataset depends on the particular random initialisation and order of data points used to train the model. The results of this experiment are shown in Table 4.
We present our results in Table 1. As shown in Table 1, our model CMAML achieves good performance on quality, personality, and diversity.
We present our results in Table 1. As shown in Table 1, our model CMAML achieves good performance on quality, personality, and diversity.
We next conduct a series of experiments in which we train on DBiDAF, DBERT, and DRoBERTa, and observe how well models can then learn to generalise on the respective test portions of these datasets. Table 5 shows the results, and there is a multitude of observations.  First, one clear trend we observe across all training data setups is a clear negative performance progression when evaluated against datasets constructed with a stronger model in the loop. This trend holds true for all but the BiDAF model, in each of the training configurations, and for each of the evaluation datasets. For example, RoBERTa trained on DRoBERTa achieves 71.4, 53.5, 48.6 and 38.9 F1 when evaluated on DSQuAD, DBiDAF, DBERT and DRoBERTa, respectively. Second, we observe that the BiDAF model is not able to generalise well to datasets constructed with a model in the loop, independent of its training setup. In particular it is unable to learn from DBiDAF, thus failing to overcome some of its own blind spots through adversarial training. Both when training only on DBiDAF, as well as when adding DSQuAD to DBiDAF during training (cf. Table 6), BiDAF performs poorly across all the adversarial datasets.  results in Table 5, where training on DBiDAF in several cases led to better generalisation than training on DRoBERTa.  we further train each of our three models on either DBiDAF, DBERT, or DRoBERTa and test on DSQuAD, with results in the DSQuAD columns of Table 5.  First, we observe clear generalisation improvements towards DDROP across all models compared to training on DSQuAD(10K) when using any of the DBiDAF, DBERT, or DRoBERTa datasets for training. That is, including a model in the loop for the training dataset leads to improved transfer towards DDROP. Note that the DROP dataset also makes use of a BiDAF model in the loop during annotation; these results are in line with our prior observations when testing the same setups on DBiDAF, DBERT and DRoBERTa, compared to training on DSQuAD(10K). Second, we observe overall strong transfer results towards DNQ: up to 71.0F1 for a BERT model trained on DBiDAF. Note that this result is similar and even slightly improves over model training with SQuAD data of the same size. That is, relative to training on SQuAD data, training on adversarially collected data DBiDAF does not impede generalisation to the DNQ dataset, which was created without a model in the annotation loop. We then however see a similar negative performance progression as observed before when testing on DSQuAD: the stronger the model in the annotation loop of the training dataset, the lower the test accuracy on test data from a data distribution composed without using a model in the loop.
We next conduct a series of experiments in which we train on DBiDAF, DBERT, and DRoBERTa, and observe how well models can then learn to generalise on the respective test portions of these datasets. Table 5 shows the results, and there is a multitude of observations.  First, one clear trend we observe across all training data setups is a clear negative performance progression when evaluated against datasets constructed with a stronger model in the loop. This trend holds true for all but the BiDAF model, in each of the training configurations, and for each of the evaluation datasets. For example, RoBERTa trained on DRoBERTa achieves 71.4, 53.5, 48.6 and 38.9 F1 when evaluated on DSQuAD, DBiDAF, DBERT and DRoBERTa, respectively. Second, we observe that the BiDAF model is not able to generalise well to datasets constructed with a model in the loop, independent of its training setup. In particular it is unable to learn from DBiDAF, thus failing to overcome some of its own blind spots through adversarial training. Both when training only on DBiDAF, as well as when adding DSQuAD to DBiDAF during training (cf. Table 6), BiDAF performs poorly across all the adversarial datasets.  results in Table 5, where training on DBiDAF in several cases led to better generalisation than training on DRoBERTa.  we further train each of our three models on either DBiDAF, DBERT, or DRoBERTa and test on DSQuAD, with results in the DSQuAD columns of Table 5.  First, we observe clear generalisation improvements towards DDROP across all models compared to training on DSQuAD(10K) when using any of the DBiDAF, DBERT, or DRoBERTa datasets for training. That is, including a model in the loop for the training dataset leads to improved transfer towards DDROP. Note that the DROP dataset also makes use of a BiDAF model in the loop during annotation; these results are in line with our prior observations when testing the same setups on DBiDAF, DBERT and DRoBERTa, compared to training on DSQuAD(10K). Second, we observe overall strong transfer results towards DNQ: up to 71.0F1 for a BERT model trained on DBiDAF. Note that this result is similar and even slightly improves over model training with SQuAD data of the same size. That is, relative to training on SQuAD data, training on adversarially collected data DBiDAF does not impede generalisation to the DNQ dataset, which was created without a model in the annotation loop. We then however see a similar negative performance progression as observed before when testing on DSQuAD: the stronger the model in the annotation loop of the training dataset, the lower the test accuracy on test data from a data distribution composed without using a model in the loop.
We compare three training configurations: WEBSPLIT only, WIKISPLIT only, and BOTH, which is simply their concatenation.  The WEBSPLIT model scores 35.3 BLEU on the WebSplit validation set but fails to generalize beyond its narrow domain, as evidenced by reaching only 4.2 BLEU on the WikiSplit validation set.  In contrast, the WIKISPLIT model achieves 59.4 BLEU on the WebSplit validation set,  Reintroducing the downsampled, in-domain training data (BOTH) further improves performance on the WebSplit evaluation.
We compare three training configurations: WEBSPLIT only, WIKISPLIT only, and BOTH, which is simply their concatenation.  The WEBSPLIT model scores 35.3 BLEU on the WebSplit validation set but fails to generalize beyond its narrow domain, as evidenced by reaching only 4.2 BLEU on the WikiSplit validation set.  In contrast, the WIKISPLIT model achieves 59.4 BLEU on the WebSplit validation set,  Reintroducing the downsampled, in-domain training data (BOTH) further improves performance on the WebSplit evaluation.
As shown in Table 6, the BOTH model produced the most accurate output (95% correct simple sentences), with the lowest incidence of missed or unsupported statements.  outputs from Aharoni and Goldberg(2018) (AG18), which were 22% accurate.
As shown in Table 6, the BOTH model produced the most accurate output (95% correct simple sentences), with the lowest incidence of missed or unsupported statements.  outputs from Aharoni and Goldberg(2018) (AG18), which were 22% accurate.
We relate our approach to prior work on Web-Split v1.0 by reporting scores on its test set in Table 5. Our best performance in BLEU is again obtained by combining the proposed WikiSplit dataset with the downsampled WebSplit, yielding a 32 point improvement over the prior best result.
We relate our approach to prior work on Web-Split v1.0 by reporting scores on its test set in Table 5. Our best performance in BLEU is again obtained by combining the proposed WikiSplit dataset with the downsampled WebSplit, yielding a 32 point improvement over the prior best result.
We report the quality results in Table 2, where each number represents the fraction of tests passed. While on average the local embeddings for EmbDI are largely superior to the baseline, our solution is beaten once for MA. By increasing the percentage of row permutations in Basic, results for MR improve but decrease for MA, without significant benefit for MC.
We report the quality results in Table 2, where each number represents the fraction of tests passed. While on average the local embeddings for EmbDI are largely superior to the baseline, our solution is beaten once for MA. By increasing the percentage of row permutations in Basic, results for MR improve but decrease for MA, without significant benefit for MC.
We test an unsupervised setting using (1) the algorithm proposed in Section 4.4 with EmbDI local embeddings, and (2) an existing matching system  with both pre-trained embeddings (SeepP ) and our local embeddings (SeepL). Pre-trained embeddings for tokens and tuples have been obtained from Glove .  Table 3 reports the results w.r.t. manually defined attribute matches. The simple unsupervised method with EmbDI local embeddings outperforms the baseline in terms of Fmeasure in all scenarios. Results of RefS are the best because of the high overlap between its datasets. The baseline improves when it is executed with EmbDI local embeddings, showing their superior quality w.r.t. pre-trained ones. The Basic local embeddings lead to 0 attribute matches.  We also observe that results for SeepPreTrain depend on the quality of the original attribute labels. If we replace the original (expressive and correct) labels with synthetic ones, Seep-PreTrain obtains F-measure values between .30 and .38. Local embeddings from EmbDI do not depend on the presence of the attribute labels.  Similarly, decreasing the size of the walks to 5 for the SM task raises the F-measure for RefL to .92 (from .77).
We test an unsupervised setting using (1) the algorithm proposed in Section 4.4 with EmbDI local embeddings, and (2) an existing matching system  with both pre-trained embeddings (SeepP ) and our local embeddings (SeepL). Pre-trained embeddings for tokens and tuples have been obtained from Glove .  Table 3 reports the results w.r.t. manually defined attribute matches. The simple unsupervised method with EmbDI local embeddings outperforms the baseline in terms of Fmeasure in all scenarios. Results of RefS are the best because of the high overlap between its datasets. The baseline improves when it is executed with EmbDI local embeddings, showing their superior quality w.r.t. pre-trained ones. The Basic local embeddings lead to 0 attribute matches.  We also observe that results for SeepPreTrain depend on the quality of the original attribute labels. If we replace the original (expressive and correct) labels with synthetic ones, Seep-PreTrain obtains F-measure values between .30 and .38. Local embeddings from EmbDI do not depend on the presence of the attribute labels.  Similarly, decreasing the size of the walks to 5 for the SM task raises the F-measure for RefL to .92 (from .77).
As baseline, we use our unsupervised algorithm with EmbDI embeddings and pre-trained embeddings. We also test our local embeddings in the supervised setting with a state of the art ER system (DeepERL), comparing its results to the ones obtained with pre-trained embeddings (DeepERP ).  Results in Table 4 show that EmbDI embeddings obtain better quality results in all scenarios in both settings. As observed in the SM experiments, using local embeddings instead of pre-trained ones increases significantly the quality of an existing system. In this case, supervised DeepER shows an average 6% absolute improvement in F-measure in the tested setting with 5% of the ground truth passed as training data. The improvements decreases to 4% with more training data (10%). Also for ER, the local embeddings obtained with the basic method lead to 0 row matched.  Execut
As baseline, we use our unsupervised algorithm with EmbDI embeddings and pre-trained embeddings. We also test our local embeddings in the supervised setting with a state of the art ER system (DeepERL), comparing its results to the ones obtained with pre-trained embeddings (DeepERP ).  Results in Table 4 show that EmbDI embeddings obtain better quality results in all scenarios in both settings. As observed in the SM experiments, using local embeddings instead of pre-trained ones increases significantly the quality of an existing system. In this case, supervised DeepER shows an average 6% absolute improvement in F-measure in the tested setting with 5% of the ground truth passed as training data. The improvements decreases to 4% with more training data (10%). Also for ER, the local embeddings obtained with the basic method lead to 0 row matched.  Execut
For example, RoBERTa trained on DRoBERTa reaches 38.9F1 on DRoBERTa, and this number further increases to 47.2F1 when including SQuAD during training (cf. Table 6).  In Table 6 we show experimental results for the same models and training datasets, but now including SQuAD as additional training data. In this training setup we generally see improved generalisation to DBiDAF, DBERT, and DRoBERTa. Interestingly, the relative differences between DBiDAF, DBERT, and DRoBERTa as training set used in conjunction with SQuAD are now much diminished, and especially DRoBERTa as (part of the) training set now generalises substantially better. RoBERTa achieves the strongest results on any of the DBiDAF, DBERT, and DRoBERTa evaluation sets, in particular when trained on DSQuAD+DRoBERTa.  we identify a risk of datasets constructed with weaker models in the loop becoming outdated. For example, RoBERTa achieves 58.2EM/73.2F1 on DBiDAF, in contrast to 0.0EM/5.5F1 for BiDAF – which is not far from non-expert human performance of 62.6EM/78.5F1.  We furthermore observe a gradual decrease in generalisation to SQuAD when training on DBiDAF towards training on DRoBERTa. This suggests that the stronger the model used in the annotation loop, the more dissimilar the data distribution becomes from the original SQuAD distribution. We will later find further support for this explanation in a qualitative analysis (Section 5). It may however also be due to a limitation of BERT and RoBERTa – similar to BiDAF – in learning from a data distribution designed to beat these models; an even stronger model might learn more e.g. from DRoBERTa.
For example, RoBERTa trained on DRoBERTa reaches 38.9F1 on DRoBERTa, and this number further increases to 47.2F1 when including SQuAD during training (cf. Table 6).  In Table 6 we show experimental results for the same models and training datasets, but now including SQuAD as additional training data. In this training setup we generally see improved generalisation to DBiDAF, DBERT, and DRoBERTa. Interestingly, the relative differences between DBiDAF, DBERT, and DRoBERTa as training set used in conjunction with SQuAD are now much diminished, and especially DRoBERTa as (part of the) training set now generalises substantially better. RoBERTa achieves the strongest results on any of the DBiDAF, DBERT, and DRoBERTa evaluation sets, in particular when trained on DSQuAD+DRoBERTa.  we identify a risk of datasets constructed with weaker models in the loop becoming outdated. For example, RoBERTa achieves 58.2EM/73.2F1 on DBiDAF, in contrast to 0.0EM/5.5F1 for BiDAF – which is not far from non-expert human performance of 62.6EM/78.5F1.  We furthermore observe a gradual decrease in generalisation to SQuAD when training on DBiDAF towards training on DRoBERTa. This suggests that the stronger the model used in the annotation loop, the more dissimilar the data distribution becomes from the original SQuAD distribution. We will later find further support for this explanation in a qualitative analysis (Section 5). It may however also be due to a limitation of BERT and RoBERTa – similar to BiDAF – in learning from a data distribution designed to beat these models; an even stronger model might learn more e.g. from DRoBERTa.
The results are provided in Table 4.  The scores demonstrate that on average our proposed sentence splitting corpus contains four simplified target sentences per complex source sentence, with every target proposition consisting of 12 tokens. Moreover, no input is simply copied to the output, but split into smaller components.
The results are provided in Table 4.  The scores demonstrate that on average our proposed sentence splitting corpus contains four simplified target sentences per complex source sentence, with every target proposition consisting of 12 tokens. Moreover, no input is simply copied to the output, but split into smaller components.
The results of the human evaluation are displayed in Table 6.  These scores show that we succeed in producing output sequences that reach a high level of grammatical soundness and almost always perfectly preserve the original meaning of the input. The third dimension under consideration, structural simplicity, which captures the degree of minimality in the simplified sentences, scores high values, too.
The results of the human evaluation are displayed in Table 6.  These scores show that we succeed in producing output sequences that reach a high level of grammatical soundness and almost always perfectly preserve the original meaning of the input. The third dimension under consideration, structural simplicity, which captures the degree of minimality in the simplified sentences, scores high values, too.
We use Amazon Mechanical Turk to label around 13,000 potentially derogatory tweets  Table 1 compares different labelsets that exist in the literature. For instance, Waseem and Hovy (2016) use racist, sexist, and normal as labels; Davidson et al. (2017) label their data as hateful, offensive (but not hateful), and neither, while ElSherief et al. (2018) present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as hate and non hate. Founta et al. (2018) label their data as offensive, abusive, hateful, aggressive, cyberbullying, spam, and normal.
We use Amazon Mechanical Turk to label around 13,000 potentially derogatory tweets  Table 1 compares different labelsets that exist in the literature. For instance, Waseem and Hovy (2016) use racist, sexist, and normal as labels; Davidson et al. (2017) label their data as hateful, offensive (but not hateful), and neither, while ElSherief et al. (2018) present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as hate and non hate. Founta et al. (2018) label their data as offensive, abusive, hateful, aggressive, cyberbullying, spam, and normal.
Table 1 compares the development set performance of different variants of the proposed sentence retrieval method with the state of the art results on the FEVER dataset. The results indicate that both pointwise and pairwise BERT sentence retrieval improve the recall. The UNC and DREAM precision scores are better than our methods without a decision threshold, however, a threshold can regulate the trade-off between the recall and precision, and achieve the best precision and F1 scores.
Table 1 compares the development set performance of different variants of the proposed sentence retrieval method with the state of the art results on the FEVER dataset. The results indicate that both pointwise and pairwise BERT sentence retrieval improve the recall. The UNC and DREAM precision scores are better than our methods without a decision threshold, however, a threshold can regulate the trade-off between the recall and precision, and achieve the best precision and F1 scores.
single task single language (STSL), single task multilingual (STML), and multitask multilingual models (MTML) on our dataset.  STSL performs the best among all models on the directness classification, and it is also consistent in both micro and macro-F1 scores.  due to the fact that the directness has only two labels and multilabeling is not allowed in this task. Tasks involving imbalanced data, multiclass and multilabel annotations harm the performance of the directness in multitask settings.  Since macro-F1 is the average of all F1 scores of individual labels, all deep learning models have high macro-F1 scores in English which indicates that they are particularly good at classifying the direct class. STSL is also comparable or better than traditional BOW feature-based classifiers when performed on other tasks in terms of microF1 and for most of the macro-F1 scores. This shows the power of the deep learning approach.  Except for the directness, MTSL usually outperforms STSL or is comparable to it.  MTML settings do not lead to a big improvement which may be due to the class imbalance, multilabel tasks, and the difference in the nature of the tasks.
single task single language (STSL), single task multilingual (STML), and multitask multilingual models (MTML) on our dataset.  STSL performs the best among all models on the directness classification, and it is also consistent in both micro and macro-F1 scores.  due to the fact that the directness has only two labels and multilabeling is not allowed in this task. Tasks involving imbalanced data, multiclass and multilabel annotations harm the performance of the directness in multitask settings.  Since macro-F1 is the average of all F1 scores of individual labels, all deep learning models have high macro-F1 scores in English which indicates that they are particularly good at classifying the direct class. STSL is also comparable or better than traditional BOW feature-based classifiers when performed on other tasks in terms of microF1 and for most of the macro-F1 scores. This shows the power of the deep learning approach.  Except for the directness, MTSL usually outperforms STSL or is comparable to it.  MTML settings do not lead to a big improvement which may be due to the class imbalance, multilabel tasks, and the difference in the nature of the tasks.
When we jointly train each task on the three languages, the performance decreases in most cases, other than the target group classification tasks.  Yet, multilingual training of the target group classification task improves in all languages. Since the target group classification task involves 16 labels, the amount of data annotated for each label is lower than in other tasks. Hence, when aggregating annotated data in different languages, the size of the training data also increases, due to the relative regularity of identification words of different groups in all three languages in comparison to other tasks.
When we jointly train each task on the three languages, the performance decreases in most cases, other than the target group classification tasks.  Yet, multilingual training of the target group classification task improves in all languages. Since the target group classification task involves 16 labels, the amount of data annotated for each label is lower than in other tasks. Hence, when aggregating annotated data in different languages, the size of the training data also increases, due to the relative regularity of identification words of different groups in all three languages in comparison to other tasks.
A sample of the SQuAD v1.1 test set (only the first paragraph of each of the 48 Wikipedia pages) has been translated by humans in French and Japanese. We here evaluate the performance of the fine-tuned multilingual BERT on them and compare the results to a baseline .  Table 1 displays the Exact Match (EM) and F1-score of the baseline and multilingual BERT on the selected datasets. We can observe that multilingual BERT is able to significantly outperform the baseline on both the Japanese and the French question answering task.  was already noted in the public benchmarks and we add here that BERT has a high ability for QA zero-shot transfer. It is even able to significantly outperform the baseline
A sample of the SQuAD v1.1 test set (only the first paragraph of each of the 48 Wikipedia pages) has been translated by humans in French and Japanese. We here evaluate the performance of the fine-tuned multilingual BERT on them and compare the results to a baseline .  Table 1 displays the Exact Match (EM) and F1-score of the baseline and multilingual BERT on the selected datasets. We can observe that multilingual BERT is able to significantly outperform the baseline on both the Japanese and the French question answering task.  was already noted in the public benchmarks and we add here that BERT has a high ability for QA zero-shot transfer. It is even able to significantly outperform the baseline
To run cross-lingual tests, we build six additional datasets from the existing ones by mixing context in one language with question in another language. The mixed datasets will be made available online  in a github repository. The performance of BERT on all datasets is displayed in Table 2.  the performance is the best for the En-En dataset. The performance on Fr-Fr and Jap-Jap is also very good as noted in the first experiment. We additionally note here that results on cross-lingual sets are close to monolingual results: either as good, or slightly worse or slightly better. For instance, the exact match on the En-Fr dataset is higher than the exact match on the Fr-Fr dataset. We also observe that, in general, the exact match and F1-score are close together when the context is in Japanese whereas there is generally a larger gap for the other two languages.  the performance on Jap-En is lower than on Jap-Jap whereas the performance on En-Fr is higher than on Fr-Fr,  Results for the Jap-Jap dataset are better than results for the Jap-En dataset
To run cross-lingual tests, we build six additional datasets from the existing ones by mixing context in one language with question in another language. The mixed datasets will be made available online  in a github repository. The performance of BERT on all datasets is displayed in Table 2.  the performance is the best for the En-En dataset. The performance on Fr-Fr and Jap-Jap is also very good as noted in the first experiment. We additionally note here that results on cross-lingual sets are close to monolingual results: either as good, or slightly worse or slightly better. For instance, the exact match on the En-Fr dataset is higher than the exact match on the Fr-Fr dataset. We also observe that, in general, the exact match and F1-score are close together when the context is in Japanese whereas there is generally a larger gap for the other two languages.  the performance on Jap-En is lower than on Jap-Jap whereas the performance on En-Fr is higher than on Fr-Fr,  Results for the Jap-Jap dataset are better than results for the Jap-En dataset
In Table 2, we compare the development set results of the state of the art methods with the BERT model trained on different retrieved evidence sets. The BERT claim verification system even if it is trained on the UKP-Athene sentence retrieval component (Hanselowski et al., 2018), the state of the art method with the highest recall, improves both label accuracy and FEVER score. Training based on the BERT sentence retrieval predictions significantly enhances the verification results because while it explicitly improves the FEVER score by providing more correct evidence sentences, it provides a better training set for the verification system. The large BERTs are only trained on the best retrieval systems, and as expected significantly improve the performance.
In Table 2, we compare the development set results of the state of the art methods with the BERT model trained on different retrieved evidence sets. The BERT claim verification system even if it is trained on the UKP-Athene sentence retrieval component (Hanselowski et al., 2018), the state of the art method with the highest recall, improves both label accuracy and FEVER score. Training based on the BERT sentence retrieval predictions significantly enhances the verification results because while it explicitly improves the FEVER score by providing more correct evidence sentences, it provides a better training set for the verification system. The large BERTs are only trained on the best retrieval systems, and as expected significantly improve the performance.
For each track, we selected the best models according to their BLEU score on DGT-valid. The scores are shown in Table 3,  We see that in the same data conditions (unconstrained mode), the MT+NLG models are not better than the pure MT models.
For each track, we selected the best models according to their BLEU score on DGT-valid. The scores are shown in Table 3,  We see that in the same data conditions (unconstrained mode), the MT+NLG models are not better than the pure MT models.
Table 6 shows a 5.7 BLEU improvement on Rotowire-test by our English NLG model compared to the previous state of the art.
Table 6 shows a 5.7 BLEU improvement on Rotowire-test by our English NLG model compared to the previous state of the art.
From Table 7, we see that sorting players helps, but only slightly. Using only team-level information, and no information about players gives worse but still decent BLEU scores.  Week day, player position or team-level aggregated scores can be removed without hurting BLEU. However, information about next games seems useful. Interestingly, relying on position only and removing most tags (e.g.,  ,  ) seems to be fine. In this case, we also print all-zero stats, for the position of each statistic to be consistent across players and games.
From Table 7, we see that sorting players helps, but only slightly. Using only team-level information, and no information about players gives worse but still decent BLEU scores.  Week day, player position or team-level aggregated scores can be removed without hurting BLEU. However, information about next games seems useful. Interestingly, relying on position only and removing most tags (e.g.,  ,  ) seems to be fine. In this case, we also print all-zero stats, for the position of each statistic to be consistent across players and games.
Cross-lingual transfer is powerful (RQ1). Zero-shot learning reaches an F1 score of 58% in the MEDIUM setup, which outperforms training the neural tagger on very limited gold data (plain).  Neural NER is better than traditional HMM-based tagging (TnT) (Brants, 2000) and greatly improves by unsupervised word embedding initialization (+Poly). It is noteworthy that zero-shot transfer benefits only to a limiting degree from more source data (F1 increases by almost 3% when training on all English CoNLL data).  To compare cross-lingual transfer to limited gold data (RQ2), we observe that training the neural system on the small amount of data together with Polyglot embeddings is close to the tiny-shot transfer setup.  Few-shot learning greatly improves over zero-shot learning. The most beneficial way is to add the target data to the source, in comparison to fine-tuning.  In both MEDIUM and LARGE setups are further gains obtained by adding TINY or SMALL amounts of Danish gold data. Interestingly, a) finetuning is less effective; b) it is better to transfer from a medium-sized setup than from the entire CoNLL source data.
Cross-lingual transfer is powerful (RQ1). Zero-shot learning reaches an F1 score of 58% in the MEDIUM setup, which outperforms training the neural tagger on very limited gold data (plain).  Neural NER is better than traditional HMM-based tagging (TnT) (Brants, 2000) and greatly improves by unsupervised word embedding initialization (+Poly). It is noteworthy that zero-shot transfer benefits only to a limiting degree from more source data (F1 increases by almost 3% when training on all English CoNLL data).  To compare cross-lingual transfer to limited gold data (RQ2), we observe that training the neural system on the small amount of data together with Polyglot embeddings is close to the tiny-shot transfer setup.  Few-shot learning greatly improves over zero-shot learning. The most beneficial way is to add the target data to the source, in comparison to fine-tuning.  In both MEDIUM and LARGE setups are further gains obtained by adding TINY or SMALL amounts of Danish gold data. Interestingly, a) finetuning is less effective; b) it is better to transfer from a medium-sized setup than from the entire CoNLL source data.
Existing systems (RQ3) are evaluated and results Polyglot (Al-Rfou et al., 2013) overall performs better than DKIE (Derczynski et al., 2014).2 The best system is our cross-lingual transfer NER from MEDIUM source data paired with SMALL amounts of gold data.  Per-entity evaluation shows that the neural Bilstm tagger outperforms Polyglot except for Location, which is consistent across evaluation sets.
Existing systems (RQ3) are evaluated and results Polyglot (Al-Rfou et al., 2013) overall performs better than DKIE (Derczynski et al., 2014).2 The best system is our cross-lingual transfer NER from MEDIUM source data paired with SMALL amounts of gold data.  Per-entity evaluation shows that the neural Bilstm tagger outperforms Polyglot except for Location, which is consistent across evaluation sets.
The evaluation results of different models on predicting present keyphrases are shown in Table 2. We observe that our reinforcement learning algorithm catSeqTG-2 [ITALIC] RF1 consistently improves the keyphrase extraction ability of all baseline generative models by a large margin.
We also report the average number of generated keyphrases per document, denoted as  "Avg. #". The results are shown in Table 4, where oracle is a model that always generates the ground-truth keyphrases. The resultant MAEs demonstrate that our deep reinforced models notably outperform the baselines on predicting the number of absent keyphrases and slightly outperform the baselines on predicting the number of present keyphrases. Moreover, our deep reinforced models generate significantly more absent keyphrases than the baselines  Besides, the baseline models and our reinforced models generate similar numbers of present keyphrases, while our reinforced models achieve notably higher F -measures, implying that our methods generate present keyphrases more accurately than the baselines.
We also report the average number of generated keyphrases per document, denoted as  "Avg. #". The results are shown in Table 4, where oracle is a model that always generates the ground-truth keyphrases. The resultant MAEs demonstrate that our deep reinforced models notably outperform the baselines on predicting the number of absent keyphrases and slightly outperform the baselines on predicting the number of present keyphrases. Moreover, our deep reinforced models generate significantly more absent keyphrases than the baselines  Besides, the baseline models and our reinforced models generate similar numbers of present keyphrases, while our reinforced models achieve notably higher F -measures, implying that our methods generate present keyphrases more accurately than the baselines.
We conduct an ablation study to further analyze our reinforcement learning algorithm. The results are reported in Table 5.  As seen in Table 5, although the performance of catSeq-RF1 is competitive to catSeq-2RF1 on predicting present keyphrases, it yields an extremely poor performance on absent keyphrase prediction.  By comparing the last two rows in Table 5, we observe that our RF1 reward function slightly outperforms the F1 reward function.
Table 6 shows that for all generative models, the evaluation scores computed by our method are higher than those computed by prior method.
Table 6 shows that for all generative models, the evaluation scores computed by our method are higher than those computed by prior method.
Results are shown in Table 1.  When comparing AR with AR+MMI, AR+MMI significantly outperforms AR across all metrics,  lookahead strategy to estimate For the variants of AR+MMI, AR+MMI+diverse generates a more diverse N-best list for reranking, and thus outperforms AR+MMI; AR+MMI+RL uses future and thus outperforms backward probability, AR+MMI as well.  It's hard to tell which model performs better, AR or non-AR: AR performs better than non-AR for BLEU and adversarial success, but worse for the other metrics. This means comparing with AR model, non-AR model tends to generate more diverse responses, but might be less coherent.  When comparing non-AR with AR+MMI+diverse, non-AR has relatively lower distinct score, but significantly higher scores BLEU and adversarial success.
Results are shown in Table 1.  When comparing AR with AR+MMI, AR+MMI significantly outperforms AR across all metrics,  lookahead strategy to estimate For the variants of AR+MMI, AR+MMI+diverse generates a more diverse N-best list for reranking, and thus outperforms AR+MMI; AR+MMI+RL uses future and thus outperforms backward probability, AR+MMI as well.  It's hard to tell which model performs better, AR or non-AR: AR performs better than non-AR for BLEU and adversarial success, but worse for the other metrics. This means comparing with AR model, non-AR model tends to generate more diverse responses, but might be less coherent.  When comparing non-AR with AR+MMI+diverse, non-AR has relatively lower distinct score, but significantly higher scores BLEU and adversarial success.
For dialogue coherence trend is 3. that NonAR+MMI than AR+MMI, followed by AR and Non-AR. AR is slightly better than Non-AR. For Content Richness, the is significantly better proposed NonAR+MMI than AR+MMI, and the gap is greater than dialogue coherence.  The output from the AR+MMI model is thus by far less diverse than nonAR+MMI, which obtains the MMI score for each generated token.
Results are shown in Table 4. As can be seen, the incorporation of MMI model significantly improves MT performances.
Results are shown in Table 4. As can be seen, the incorporation of MMI model significantly improves MT performances.
Table 3 compares the performance of the four weighting variants introduced in Section 3.2. TransWeight-feat, which sums the transformed representations and then weights each component of the summed representation, has the weakest performance, with only 50.82% of the test compounds receiving a rank that is lower than 5. A better performance – 52.90% – is obtained by applying the same weighting for each column of the transformations matrix H. The results of TransWeight-trans are interesting in two respects: first, it outperforms the feature variation, TransWeight-feat, despite training a smaller number of parameters (300 vs. 400 in our setup). Second, it performs on par with the TransWeight-mat variation, although the latter has a larger number of parameters (20,200 in our setup). This suggests that an effective combination method needs to take into account full transformations, i.e. entire rows of H and combine them in a systematic way. TransWeight builds on this insight by making each element of the final composed representation p dependent on each component of the transformed representation H. The result is a noteworthy increase in the quality of the predictions, with ∼12% more of the test representations having a rank ≤5. Although this weighting does use significantly more parameters than the previous weightings (4,000,200 parameters), the number of parameters is relative to the number of transformations t and does not grow with the size of the vocabulary. As the results in the next subsection show, a relatively small number of transformations is sufficient even for larger training vocabularies.
Table 3 compares the performance of the four weighting variants introduced in Section 3.2. TransWeight-feat, which sums the transformed representations and then weights each component of the summed representation, has the weakest performance, with only 50.82% of the test compounds receiving a rank that is lower than 5. A better performance – 52.90% – is obtained by applying the same weighting for each column of the transformations matrix H. The results of TransWeight-trans are interesting in two respects: first, it outperforms the feature variation, TransWeight-feat, despite training a smaller number of parameters (300 vs. 400 in our setup). Second, it performs on par with the TransWeight-mat variation, although the latter has a larger number of parameters (20,200 in our setup). This suggests that an effective combination method needs to take into account full transformations, i.e. entire rows of H and combine them in a systematic way. TransWeight builds on this insight by making each element of the final composed representation p dependent on each component of the transformed representation H. The result is a noteworthy increase in the quality of the predictions, with ∼12% more of the test representations having a rank ≤5. Although this weighting does use significantly more parameters than the previous weightings (4,000,200 parameters), the number of parameters is relative to the number of transformations t and does not grow with the size of the vocabulary. As the results in the next subsection show, a relatively small number of transformations is sufficient even for larger training vocabularies.
1. Comparing ENT-SENT, ENT-DYM and ENT-ONLY, we see that the pooling methods over the whole sentence (i.e., ENT-SENT and ENT-DYM) are significantly better than ENT-ONLY that only focuses on the two entity mentions of interest in the DDI-2013 dataset. This is true across different deep learning models in this work. Comparing ENT-SENT and ENT-DYM, their performance are comparable in DDI-2013 (except for CNN where ENT-DYM is better). Comparing the syntax-based pooling methods and the non-syntax pooling methods, the pooling based on dependency paths (i.e., ENT-DEP0) is worse than the non-syntax pooling methods (i.e., ENT-SENT and ENT-DYM) and perform comparably with ENT-ONLY in the DDI-2013 dataset over all the models (except for the CNN model where ENT-ONLY is much worse).
1. Comparing ENT-SENT, ENT-DYM and ENT-ONLY, we see that the pooling methods over the whole sentence (i.e., ENT-SENT and ENT-DYM) are significantly better than ENT-ONLY that only focuses on the two entity mentions of interest in the DDI-2013 dataset. This is true across different deep learning models in this work. Comparing ENT-SENT and ENT-DYM, their performance are comparable in DDI-2013 (except for CNN where ENT-DYM is better). Comparing the syntax-based pooling methods and the non-syntax pooling methods, the pooling based on dependency paths (i.e., ENT-DEP0) is worse than the non-syntax pooling methods (i.e., ENT-SENT and ENT-DYM) and perform comparably with ENT-ONLY in the DDI-2013 dataset over all the models (except for the CNN model where ENT-ONLY is much worse).
We show the main result in Table 2 and 3. Despite training on a fraction of the data available, the proposed BERT-based models surpass the previous state-of-the-art models by a large margin on all datasets
However, this comparison is reversed for the BB3 dataset where ENT-ONLY is in general better or comparable to ENT-SENT and ENT-DYM over different deep learning models. in the BB3 dataset, ENT-SENT singificantly outperforms ENT-DYM over all the models. When we switch to the BB3 dataset, it turns out thatENT-DEP0 is significantly better than all the non-syntax pooling methods (i.e., ENT-ONLY, ENT-SENT and ENT-DYM) for all the comparing models.
However, this comparison is reversed for the BB3 dataset where ENT-ONLY is in general better or comparable to ENT-SENT and ENT-DYM over different deep learning models. in the BB3 dataset, ENT-SENT singificantly outperforms ENT-DYM over all the models. When we switch to the BB3 dataset, it turns out thatENT-DEP0 is significantly better than all the non-syntax pooling methods (i.e., ENT-ONLY, ENT-SENT and ENT-DYM) for all the comparing models.
Basic statistics such as corpus sizes, frequency of various semantic annotations, and information about the length of the extracted triples of OPIEC and its subcorpora are shown in Tab. 2.  Roughly 30% of the triples (104M) in OPIEC are clean according to the above constraints. Table 2 shows that clean triples are generally shorter on average and tend to have a higher confidence score than the full set of triples in OPIEC.  About 49% of all triples in OPIEC contain some sort of semantic annotation (cf. Tab. 2); in OPIEC-Linked, the fraction increases to 58%. Most of the semantic annotations referred to quantities, space or time; these annotations provide important context for the extractions. There is a significantly smaller amount of negative polarity and possibility modality annotations. One reason for the lack of such annotations may be in the nature of the Wikipedia articles, which aim to contain encyclopedic, factual statements and are thus more rarely negated or hedged.
Basic statistics such as corpus sizes, frequency of various semantic annotations, and information about the length of the extracted triples of OPIEC and its subcorpora are shown in Tab. 2.  Roughly 30% of the triples (104M) in OPIEC are clean according to the above constraints. Table 2 shows that clean triples are generally shorter on average and tend to have a higher confidence score than the full set of triples in OPIEC.  About 49% of all triples in OPIEC contain some sort of semantic annotation (cf. Tab. 2); in OPIEC-Linked, the fraction increases to 58%. Most of the semantic annotations referred to quantities, space or time; these annotations provide important context for the extractions. There is a significantly smaller amount of negative polarity and possibility modality annotations. One reason for the lack of such annotations may be in the nature of the Wikipedia articles, which aim to contain encyclopedic, factual statements and are thus more rarely negated or hedged.
Tab. 4 shows the most frequent open relations aligned to the DBpedia relations location, associatedMusicalArtist, and spouse. The frequencies correspond to the number of OIE triples that (1) have the specified open relation (e.g., "be wife of ") and (2) have a KB hit with the specified KB relation (e.g., spouse). There is clearly no 1:1 correspondence between open relations and KB relations. On the one hand, open relations can be highly ambiguous (e.g., "be" has hits to location and associatedMusicalArtits). On the other hand, open relations can also be more specific than KB relations (e.g., "be guitarist of " is more specific than associatedMusicalArtist) or semantically different (e.g., "be widow of " and spouse) than the KB relations they align to.
Tab. 4 shows the most frequent open relations aligned to the DBpedia relations location, associatedMusicalArtist, and spouse. The frequencies correspond to the number of OIE triples that (1) have the specified open relation (e.g., "be wife of ") and (2) have a KB hit with the specified KB relation (e.g., spouse). There is clearly no 1:1 correspondence between open relations and KB relations. On the one hand, open relations can be highly ambiguous (e.g., "be" has hits to location and associatedMusicalArtits). On the other hand, open relations can also be more specific than KB relations (e.g., "be guitarist of " is more specific than associatedMusicalArtist) or semantically different (e.g., "be widow of " and spouse) than the KB relations they align to.
Table 2 and 3 compare the accuracies of our models to the results reported by several representative studies in frame-semantic parsing.  As can be seen from the tables, PAFIBERT outperformed other models on both the in-domain Das's test set and the out-of-domain YAGS test set, yielding new state-of-the-art results for frame identification. As expected, the results obtained with LU-based frame filtering were slightly better than those produced using target-based frame filtering, suggesting that a realistic setup is crucial in measuring model performance more precisely.  On Das's test set, for example, PAFIBERT achieved absolute improvements of 4-5% and 9% with frame filtering (Table 2) and without frame filtering (Table 3) respectively. As reported by , frame identification models usually suffer a drastic drop in performance when tested on out-of-domain data. The same trend was observed in our experiments for the YAGS test set. Nevertheless, PAFIBERT still outperformed existing methods by a large margin on this out-of-domain test set. Besides, in the more challenging setup that involved no frame filtering, the results obtained by PAFIBERT were on par with the prior state-of-the-art accuracies achieved with frame filtering.
Table 2 and 3 compare the accuracies of our models to the results reported by several representative studies in frame-semantic parsing.  As can be seen from the tables, PAFIBERT outperformed other models on both the in-domain Das's test set and the out-of-domain YAGS test set, yielding new state-of-the-art results for frame identification. As expected, the results obtained with LU-based frame filtering were slightly better than those produced using target-based frame filtering, suggesting that a realistic setup is crucial in measuring model performance more precisely.  On Das's test set, for example, PAFIBERT achieved absolute improvements of 4-5% and 9% with frame filtering (Table 2) and without frame filtering (Table 3) respectively. As reported by , frame identification models usually suffer a drastic drop in performance when tested on out-of-domain data. The same trend was observed in our experiments for the YAGS test set. Nevertheless, PAFIBERT still outperformed existing methods by a large margin on this out-of-domain test set. Besides, in the more challenging setup that involved no frame filtering, the results obtained by PAFIBERT were on par with the prior state-of-the-art accuracies achieved with frame filtering.
We show the main result in Table 2 and 3. Despite training on a fraction of the data available, the proposed BERT-based models surpass the previous state-of-the-art models by a large margin on all datasets
We show the main result in Table 2 and 3. Despite training on a fraction of the data available, the proposed BERT-based models surpass the previous state-of-the-art models by a large margin on all datasets
table 6, where we compare hyperparameter tuning methodologies.  Perturbations on the hyperparameter search are listed in table 6 and compare performance with different hyperparameter values found using different tuning strategies.  We tested how manual tuning, requiring less expert knowledge than Bayesian optimization, would compare to the random search strategy in table 6. For both i2b2 and ddi corpora, manual search outperformed random search.
table 6, where we compare hyperparameter tuning methodologies.  Perturbations on the hyperparameter search are listed in table 6 and compare performance with different hyperparameter values found using different tuning strategies.  We tested how manual tuning, requiring less expert knowledge than Bayesian optimization, would compare to the random search strategy in table 6. For both i2b2 and ddi corpora, manual search outperformed random search.
importance of pre-processing in performance improvements. Experiments in table 4 reveal that they can cause larger variations in performance than modeling.  Punctuation and digits hold more importance for the ddi dataset, which is a biomedical dataset, compared to the other two datasets.  We found that stop words seem to be important for relation extraction for all three datasets that we looked at, to a smaller degree for i2b2 compared to the other two datasets.  Entity blinding causes almost 9% improvement in classification performance and 1% improvement in detection performance.  While entity blinding hurts performance for semeval, possibly due to the coarse-grained nature of the replacement, NER blinding does not hurt performance.  entity blinding seems to help test set performance for ddi in table 4, but shows no statistical significance.
importance of pre-processing in performance improvements. Experiments in table 4 reveal that they can cause larger variations in performance than modeling.  Punctuation and digits hold more importance for the ddi dataset, which is a biomedical dataset, compared to the other two datasets.  We found that stop words seem to be important for relation extraction for all three datasets that we looked at, to a smaller degree for i2b2 compared to the other two datasets.  Entity blinding causes almost 9% improvement in classification performance and 1% improvement in detection performance.  While entity blinding hurts performance for semeval, possibly due to the coarse-grained nature of the replacement, NER blinding does not hurt performance.  entity blinding seems to help test set performance for ddi in table 4, but shows no statistical significance.
No statistical significance is seen even when the test set result worsens in performance for BERTCLS and Piecewise Pool in table 5 where it hurts test set performance on ddi but is not statistically significant when cross validation is performed.  In table 5, we tested the generalizability of the commonly used piecewise pooling technique  While piecewise pooling helps i2b2 by 1%, it hurts test set performance on ddi and doesn't affect performance on semeval.  We found ELMo and BER tokens to boost performance significantly for all datasets, but that BERT-CLS hurt performance for the medical datasets. While BERT-CLS boosted test set performance for semeval, this was not found to be a statistically significant difference for cross validation.
No statistical significance is seen even when the test set result worsens in performance for BERTCLS and Piecewise Pool in table 5 where it hurts test set performance on ddi but is not statistically significant when cross validation is performed.  In table 5, we tested the generalizability of the commonly used piecewise pooling technique  While piecewise pooling helps i2b2 by 1%, it hurts test set performance on ddi and doesn't affect performance on semeval.  We found ELMo and BER tokens to boost performance significantly for all datasets, but that BERT-CLS hurt performance for the medical datasets. While BERT-CLS boosted test set performance for semeval, this was not found to be a statistically significant difference for cross validation.
We report test set tokenized BLEU (Papineni et al., 2002) results in Table 1. We can see that replacing softmax by entmax does not hurt performance in any of the datasets; indeed, sparse attention Transformers tend to have slightly higher BLEU, but their sparsity leads to a better potential for analysis. In the next section, we make use of this potential by exploring the learned internal mechanics of the self-attention heads.
We report test set tokenized BLEU (Papineni et al., 2002) results in Table 1. We can see that replacing softmax by entmax does not hurt performance in any of the datasets; indeed, sparse attention Transformers tend to have slightly higher BLEU, but their sparsity leads to a better potential for analysis. In the next section, we make use of this potential by exploring the learned internal mechanics of the self-attention heads.
Table 6 shows that the C-LSTM model performs well on the scientific embeddings, but consistently worse than the SVM model using handcrafted features and achieves a macro-F1 score of 67.49 and 67.02 for subtask 1.1 and subtask 1.2 respectively.
Table 6 shows that the C-LSTM model performs well on the scientific embeddings, but consistently worse than the SVM model using handcrafted features and achieves a macro-F1 score of 67.49 and 67.02 for subtask 1.1 and subtask 1.2 respectively.
Table 1 presents comparisons of our method with existing state-of-the-art approaches.  Our proposed method consistently outperforms all other methods on all four datasets. The improvement is particularly significant on the more challenging datasets, such as UNC+ which has no location words and G-Ref which contains longer and richer query expressions.
As shown in Table 2, the proposed cross-modal self-attention outperforms all other attention methods significantly.
As shown in Table 2, the proposed cross-modal self-attention outperforms all other attention methods significantly.
As shown in Table 3 (top 4 rows), the proposed crossmodal self-attentive feature based approaches achieve significantly better performance than other baselines.  the word based method CMSA-W outperforms sentence based method CMSA-S for multimodal feature representation.  As presented in the bottom 5 rows in Table 3, the proposed gated multi-level fusion outperforms these other multi-scale feature fusion methods.
We report metric results when grouping crowd workers into 5 bins using the full range of ADR of the dataset. The ADR-based bias measures (Tab. 2) follow the expected trend (model 1 appears more biased than models 2 and 3) contrary to the protected attributes-based measures. Model 1 trained with MV labels exhibits similar performance across demographic groups  Model 2's performance is different across groups
We report metric results when grouping crowd workers into 5 bins using the full range of ADR of the dataset. The ADR-based bias measures (Tab. 2) follow the expected trend (model 1 appears more biased than models 2 and 3) contrary to the protected attributes-based measures. Model 1 trained with MV labels exhibits similar performance across demographic groups  Model 2's performance is different across groups
We compare the classification accuracy of our model against several other recent methods (Table 3).  Our model outperforms state-of-the-art methods by 1.6% on SwDA, the primary dataset for this task, and comes within 0.6% on MRDA. It also beats a TF-IDF GloVe baseline (described in Section 5.2) by 16.4% and 12.2%, respectively.  The improvements that the model is able to make over the other methods are significant, however, the gains on MRDA still fall short of the state-of-the-art by 0.6%.
We compare the classification accuracy of our model against several other recent methods (Table 3).  Our model outperforms state-of-the-art methods by 1.6% on SwDA, the primary dataset for this task, and comes within 0.6% on MRDA. It also beats a TF-IDF GloVe baseline (described in Section 5.2) by 16.4% and 12.2%, respectively.  The improvements that the model is able to make over the other methods are significant, however, the gains on MRDA still fall short of the state-of-the-art by 0.6%.
We report experimental results on four common benchmark datasets (totalling 1241 testing data points) in Table 1.  As shown in Table 1, the average performance increases from RANDOM to APPEAR-AP: a trend that becomes particularly obvious for F1@10.
We report experimental results on four common benchmark datasets (totalling 1241 testing data points) in Table 1.  As shown in Table 1, the average performance increases from RANDOM to APPEAR-AP: a trend that becomes particularly obvious for F1@10.
As shown in Table 4, BigRNN and Transformer are not able to outperform BaseRNN.
As shown in Table 4, BigRNN and Transformer are not able to outperform BaseRNN.
The power these features bring to the model, beyond what is captured in the ELMo-LSTM representations, is evident in table 2, column 1.  The results in table 2, column 2, clearly favor the combined model ELMo-LSTM-CRF-HB that uses both these features and the ELMo-LSTM.  Our test-set results, given in table 2, column 3, show the power of our combined model ELMo-LSTM-CRF-HB.  We report results for the standard test set. The power of the combined model ELMo-LSTM-CRF-HB is again evident in the results in table 2, column 4.  Our results for this experiment are given in table 2, column 5, and point to the superiority of our combined model ELMo-LSTM-CRF-HB.  Our discussion seeks to show that the combined model ELMo-LSTM-CRF-HB, which shows superior performance in all tasks (table 2), is making meaningful use of both kinds of features (hand-built and ELMo) and both of the major model components (LSTM and CRF).  We note also that, where the performance of the two base models is very similar (table 2), the potential scores in the combined model are also more similar.
The power these features bring to the model, beyond what is captured in the ELMo-LSTM representations, is evident in table 2, column 1.  The results in table 2, column 2, clearly favor the combined model ELMo-LSTM-CRF-HB that uses both these features and the ELMo-LSTM.  Our test-set results, given in table 2, column 3, show the power of our combined model ELMo-LSTM-CRF-HB.  We report results for the standard test set. The power of the combined model ELMo-LSTM-CRF-HB is again evident in the results in table 2, column 4.  Our results for this experiment are given in table 2, column 5, and point to the superiority of our combined model ELMo-LSTM-CRF-HB.  Our discussion seeks to show that the combined model ELMo-LSTM-CRF-HB, which shows superior performance in all tasks (table 2), is making meaningful use of both kinds of features (hand-built and ELMo) and both of the major model components (LSTM and CRF).  We note also that, where the performance of the two base models is very similar (table 2), the potential scores in the combined model are also more similar.
Table 3 suggests that the combined model does make progress here, in that the largest gains, across all relevant datasets, tend to be for the smallest categories.
Table 3 suggests that the combined model does make progress here, in that the largest gains, across all relevant datasets, tend to be for the smallest categories.
we describe data and label distribution of Friends and EmotionPush datasets. In terms of label distribution for both datasets, Neutral are the most common class, followed by Joy, Sadness, and Anger. Both datasets have imbalanced class distribution, and especially the ratio of Sadness and Anger is very small. For instance, they account for only 3.4% and 5.2%, respectively in the Friends dataset. In the case of EmotionPush, Anger label accounts for less than 1% of the training set.
we describe data and label distribution of Friends and EmotionPush datasets. In terms of label distribution for both datasets, Neutral are the most common class, followed by Joy, Sadness, and Anger. Both datasets have imbalanced class distribution, and especially the ratio of Sadness and Anger is very small. For instance, they account for only 3.4% and 5.2%, respectively in the Friends dataset. In the case of EmotionPush, Anger label accounts for less than 1% of the training set.
Table 1 compares the performance of ESSENTIA with the FSA baseline for paraphrase mining. Specifically, we show the number of phrase pairs extracted by ESSENTIA and FSA from both datasets ("# of extracted pairs" column), number of valid paraphrases within these pairs ("# of valid pairs" column), and precision ("Precision" column). Although FSA has higher precision due to conservative sentence alignment, ESSENTIA extracts significantly more paraphrases, improving the recall by 460% (Snips) and 247% (HotelQA) over the baseline.
The classification of BR and US datasets used the 60 and 49 most relevant features, respectively. The test accuracy for the BR and US datasets were 85% and 72%, respectively, with a baseline score of 50%. The combined dataset used a reduced set of features consisting of an intersection of the most relevant features observed in both BR and US, achieving a test score of 70% using only 18 features.  classes. We have shown that these features may be used to classify news articles in a language other than English.
The classification of BR and US datasets used the 60 and 49 most relevant features, respectively. The test accuracy for the BR and US datasets were 85% and 72%, respectively, with a baseline score of 50%. The combined dataset used a reduced set of features consisting of an intersection of the most relevant features observed in both BR and US, achieving a test score of 70% using only 18 features.  classes. We have shown that these features may be used to classify news articles in a language other than English.
Table 1 gives full archive sync size of each data type for geth and ethanos at the 8M block. Ethanos reduces the total blockchain size about 16GB compared to geth. Especially, it reduces the size of Trie nodes about 18GB, because ethanos only maintains the state trie only with active accounts during a month which are only 10% of the total accounts. However, hash field of bloom filter in block header slightly increases Headers, and restore transactions increment Bodies and Receipts about 1.6GB. As a result, ethanos reduced full archive node by about 8% compared to ethereum.
Table 2 shows storage size of geth and ethanos for each bootstrapping mode. Ethanos reduces Trie nodes size of geth from 2,220MB to 804.79MB; however, it increases the transaction size (Bodies, Receipts) with block headers (Headers). Therefore, ethanos did not reduce the size of fast sync much, but it significantly reduced compact sync from 2,592.54MB of geth to 1,206.97MB, which is less than 50%. Our result shows that the trie node size of each checkpoint is about 800MB,
Table 2 shows storage size of geth and ethanos for each bootstrapping mode. Ethanos reduces Trie nodes size of geth from 2,220MB to 804.79MB; however, it increases the transaction size (Bodies, Receipts) with block headers (Headers). Therefore, ethanos did not reduce the size of fast sync much, but it significantly reduced compact sync from 2,592.54MB of geth to 1,206.97MB, which is less than 50%. Our result shows that the trie node size of each checkpoint is about 800MB,
our model can stably generate adversarial samples without significant change in semantics with the same training setting for different models,
our model can stably generate adversarial samples without significant change in semantics with the same training setting for different models,
The left side of Table 1 shows the performance for the three baselines and for our multi-granularity network on the FLC task.  Table 1 (right) shows that using additional information from the sentence-level for the token-level classification (BERT-Granularity) yields small improvements. The multi-granularity models outperform all baselines thanks to their higher precision. This shows the effect of the model excluding sentences that it determined to be non-propagandistic from being considered for token-level classification.  The right side of Table 1 shows the results for the SLC task. We apply our multi-granularity network model to the sentence-level classification task to see its effect on low granularity when we train the model with a high granularity task. Interestingly, it yields huge performance improvements on the sentence-level classification result. Compared to the BERT baseline, it increases the recall by 8.42%, resulting in a 3.24% increase of the F1 score.
The left side of Table 1 shows the performance for the three baselines and for our multi-granularity network on the FLC task.  Table 1 (right) shows that using additional information from the sentence-level for the token-level classification (BERT-Granularity) yields small improvements. The multi-granularity models outperform all baselines thanks to their higher precision. This shows the effect of the model excluding sentences that it determined to be non-propagandistic from being considered for token-level classification.  The right side of Table 1 shows the results for the SLC task. We apply our multi-granularity network model to the sentence-level classification task to see its effect on low granularity when we train the model with a high granularity task. Interestingly, it yields huge performance improvements on the sentence-level classification result. Compared to the BERT baseline, it increases the recall by 8.42%, resulting in a 3.24% increase of the F1 score.
Table 2 shows results that compare a BERT baseline that uses only the CQA inputs and the same architecture but trained using inputs that contain explanations from CoS-E during training. The BERT baseline model reaches 64% accuracy and adding open-ended human explanations (CoS-E-open-ended) alongside the questions during training results in a 2% boost in accuracy.  In Table 2, using CAGE-reasoning at both train and validation resulted in an accuracy of 72%, but Table 4 shows that if CAGE-reasoning truly captured all information provided in CoS-E-openended, performance would be 90%.
Table 2 shows results that compare a BERT baseline that uses only the CQA inputs and the same architecture but trained using inputs that contain explanations from CoS-E during training. The BERT baseline model reaches 64% accuracy and adding open-ended human explanations (CoS-E-open-ended) alongside the questions during training results in a 2% boost in accuracy.  In Table 2, using CAGE-reasoning at both train and validation resulted in an accuracy of 72%, but Table 4 shows that if CAGE-reasoning truly captured all information provided in CoS-E-openended, performance would be 90%.
Table 3 shows the results obtained on the CQA test split. We report our two best models that represent using human explanations (CoS-E-openended) for training only and using language model explanations (CAGE-reasoning) during both train and test.
Table 3 shows the results obtained on the CQA test split. We report our two best models that represent using human explanations (CoS-E-openended) for training only and using language model explanations (CAGE-reasoning) during both train and test.
Table 4 also contains results that use only the explanation and exclude the original question from CQA denoted by 'w/o question'. These variants also use explanation during both train and validation.  We observe that even using these limited kind of explanations improves over the BERT baseline in Table 4, which suggests that the explanations are providing useful information beyond just mentioning the correct or incorrect answers.  In Table 2, using CAGE-reasoning at both train and validation resulted in an accuracy of 72%, but Table 4 shows that if CAGE-reasoning truly captured all information provided in CoS-E-openended, performance would be 90%.
Table 4 also contains results that use only the explanation and exclude the original question from CQA denoted by 'w/o question'. These variants also use explanation during both train and validation.  We observe that even using these limited kind of explanations improves over the BERT baseline in Table 4, which suggests that the explanations are providing useful information beyond just mentioning the correct or incorrect answers.  In Table 2, using CAGE-reasoning at both train and validation resulted in an accuracy of 72%, but Table 4 shows that if CAGE-reasoning truly captured all information provided in CoS-E-openended, performance would be 90%.
Table 6 shows the results obtained by the BERT baseline without explanations and using our transferred explanations from CQA to SWAG and Story Cloze. We observed that adding explanations led to a very small decrease (< 0.6%) in the performance compared to the baseline for both tasks.
Table 6 shows the results obtained by the BERT baseline without explanations and using our transferred explanations from CQA to SWAG and Story Cloze. We observed that adding explanations led to a very small decrease (< 0.6%) in the performance compared to the baseline for both tasks.
Our results for citation needed detection are given in Table 1. The vanilla BERT model already significantly outperforms the state of the art model from Redi et al. (2019) (a GRU network with global attention) by 6 F1 points. We saw further gains in performance with PU learning, as well as when using PUC. Additionally, the models using PU learning had lower variance, indicating more consistent performance across runs. The best performing model we saw was the one trained using PUC with an F1 score of 0.826.
Our results for citation needed detection are given in Table 1. The vanilla BERT model already significantly outperforms the state of the art model from Redi et al. (2019) (a GRU network with global attention) by 6 F1 points. We saw further gains in performance with PU learning, as well as when using PUC. Additionally, the models using PU learning had lower variance, indicating more consistent performance across runs. The best performing model we saw was the one trained using PUC with an F1 score of 0.826.
Table 1 lists the properties of the training and test set including the inter-rater agreement (κ). The agreement is substantial, with some variation between the domains: κ is higher for tools and fruit than for the musical domain.
Table 1 lists the properties of the training and test set including the inter-rater agreement (κ). The agreement is substantial, with some variation between the domains: κ is higher for tools and fruit than for the musical domain.
Table 2 lists the importance of each variable in the global model and in the three single domain models, where the variable with the highest weight is ranked 1. The lists are relatively stable, with some marked differences, such as the importance of the gloss length and the number of partOf relations for music and fruit, respectively.
We find that normalization of both structural and lexical features leads to a significant performance gain. Normalization of the frequency feature seems to hurt performance (Table 3)
We find that normalization of both structural and lexical features leads to a significant performance gain. Normalization of the frequency feature seems to hurt performance (Table 3)
Tables 1 and 2 report, respectively, the distribution of the events per token part-of speech (POS) and per event class.  verbs are the largest annotated category, followed by nouns, adjectives, and prepositional phrases.
Tables 1 and 2 report, respectively, the distribution of the events per token part-of speech (POS) and per event class.  verbs are the largest annotated category, followed by nouns, adjectives, and prepositional phrases.
Tables 1 and 2 report, respectively, the distribution of the events per token part-of speech (POS) and per event class.  As for the classes, OCCURRENCE and STATE represent the large majority of all events, followed by the intensional ones (I STATE and  ACTION), expressing some factual relationship between the target events and their arguments, and finally the others (REPORTING, ASPECTUAL, and PERCEPTION).
Tables 1 and 2 report, respectively, the distribution of the events per token part-of speech (POS) and per event class.  As for the classes, OCCURRENCE and STATE represent the large majority of all events, followed by the intensional ones (I STATE and  ACTION), expressing some factual relationship between the target events and their arguments, and finally the others (REPORTING, ASPECTUAL, and PERCEPTION).
Results for the experiments are illustrated in Table 3.  The network obtains the best F1 score, both for detection (F1 of 0.880 for strict evaluation and 0.903 for relaxed evaluation with Fastext-It embeddings) and for classification (F1-class of 0.756 for strict evaluation, and 0.751 for relaxed evaluation with Fastext-It embeddings).  Although FBKHLT suffers in the classification subtask, it qualifies as a highly competitive system for the detection subtask.  By observing the strict F1 scores, FBK-HLT beats three configurations (DH-FBK100, ILC-ItWack, Berardi2015 Glove) 11, almost equals one (Berardi2015 w2v) 12, and it is outperformed only by one (Fastext-It) 13. In the relaxed evaluation setting, DH-FBK-100 is the only configuration that does not beat FBK-HLT (although the difference is only 0.001 point). Nevertheless, it is remarkable to observe that FBK-HLT has a very high Precision (0.902, relaxed evaluation mode), that is overcome by only one embedding configuration, ILC-ItWack. The results also indicates that word embeddings have a major contribution on Recall,  these results are obtained using a single step approach,
Results for the experiments are illustrated in Table 3.  The network obtains the best F1 score, both for detection (F1 of 0.880 for strict evaluation and 0.903 for relaxed evaluation with Fastext-It embeddings) and for classification (F1-class of 0.756 for strict evaluation, and 0.751 for relaxed evaluation with Fastext-It embeddings).  Although FBKHLT suffers in the classification subtask, it qualifies as a highly competitive system for the detection subtask.  By observing the strict F1 scores, FBK-HLT beats three configurations (DH-FBK100, ILC-ItWack, Berardi2015 Glove) 11, almost equals one (Berardi2015 w2v) 12, and it is outperformed only by one (Fastext-It) 13. In the relaxed evaluation setting, DH-FBK-100 is the only configuration that does not beat FBK-HLT (although the difference is only 0.001 point). Nevertheless, it is remarkable to observe that FBK-HLT has a very high Precision (0.902, relaxed evaluation mode), that is overcome by only one embedding configuration, ILC-ItWack. The results also indicates that word embeddings have a major contribution on Recall,  these results are obtained using a single step approach,
Tables 1 and 2 show the performance of DialogWAE and baselines on the two datasets. DialogWAE outperforms the baselines in the majority of the experiments. In terms of BLEU scores, DialogWAE (with a Gaussian mixture prior network) generates more relevant responses, with the average recall of 42.0% and 37.2% on both of the datasets. These are significantly higher than those of the CVAE baselines (29.9% and 26.5%). We observe a similar trend to the BOW embedding metrics.
Tables 1 and 2 show the performance of DialogWAE and baselines on the two datasets. DialogWAE outperforms the baselines in the majority of the experiments. In terms of BLEU scores, DialogWAE (with a Gaussian mixture prior network) generates more relevant responses, with the average recall of 42.0% and 37.2% on both of the datasets. These are significantly higher than those of the CVAE baselines (29.9% and 26.5%). We observe a similar trend to the BOW embedding metrics.
To validate the previous results, we further conduct a human evaluation with Amazon Mechanical Turk. We randomly selected 50 dialogues from the test set of DailyDialog. For each dialogue context, we generated 10 responses from each of the four models. Responses for each context were inspected by 5 participants who were asked to choose the model which performs the best in regarding to coherence, diversity and informative while being blind to the underlying algorithms. The average percentages that each model was selected as the best to a specific criterion are shown in Table 5.
To validate the previous results, we further conduct a human evaluation with Amazon Mechanical Turk. We randomly selected 50 dialogues from the test set of DailyDialog. For each dialogue context, we generated 10 responses from each of the four models. Responses for each context were inspected by 5 participants who were asked to choose the model which performs the best in regarding to coherence, diversity and informative while being blind to the underlying algorithms. The average percentages that each model was selected as the best to a specific criterion are shown in Table 5.
From the BLEU scores in Table 2, we can see that although MultiSeq has the highest BLEU score, all models have similar BLEU scores and the difference is not that significant.  We can clearly notice from human evaluations shown in Table 2 that our model, RL Look-ahead, outperforms all of the others in all three evaluated categories.
From the BLEU scores in Table 2, we can see that although MultiSeq has the highest BLEU score, all models have similar BLEU scores and the difference is not that significant.  We can clearly notice from human evaluations shown in Table 2 that our model, RL Look-ahead, outperforms all of the others in all three evaluated categories.
Taking CONCAT as a representative, we compare the performance of our model with other models, as shown in Table 1. As illustrated, our model outperforms baselines by a large margin with or without BERT, achieving new SOTA performances on both datasets. Compared with the previous SOTA without BERT on SPARC, our model improves Ques.Match and Int.Match by 10.6 and 5.4 points, respectively.
Taking CONCAT as a representative, we compare the performance of our model with other models, as shown in Table 1. As illustrated, our model outperforms baselines by a large margin with or without BERT, achieving new SOTA performances on both datasets. Compared with the previous SOTA without BERT on SPARC, our model improves Ques.Match and Int.Match by 10.6 and 5.4 points, respectively.
Table 3 shows the results on the unsupervised tasks of hypernym detection and direction predictions, reporting average precision and average accuracy, respectively. The first row titled Count based (in Table 3) depicts the performance of a Hearst-like Pattern system baseline, that uses a frequency based threshold to classify candidate hyponym-hypernym pairs as positive (i.e. exhibiting hypernymy) or negative (i.e. not exhibiting hypernymy). The ppmi approach in Table 3 builds upon the Count based approach  by using Pointwise Mutual Information values for classification. SVD ppmi approach, the main contribution from Roller, Kiela, and Nickel (2018) builds low-rank embeddings of the PPMI matrix, which allows to make predictions for unseen pairs as well. HyperbolicCones is the SOTA (Le et al. 2019) in both these tasks. The final row reports the application of SPON (on the input provided by SVD ppmi) which is an original contribution of our work.
Table 3 shows the results on the unsupervised tasks of hypernym detection and direction predictions, reporting average precision and average accuracy, respectively. The first row titled Count based (in Table 3) depicts the performance of a Hearst-like Pattern system baseline, that uses a frequency based threshold to classify candidate hyponym-hypernym pairs as positive (i.e. exhibiting hypernymy) or negative (i.e. not exhibiting hypernymy). The ppmi approach in Table 3 builds upon the Count based approach  by using Pointwise Mutual Information values for classification. SVD ppmi approach, the main contribution from Roller, Kiela, and Nickel (2018) builds low-rank embeddings of the PPMI matrix, which allows to make predictions for unseen pairs as well. HyperbolicCones is the SOTA (Le et al. 2019) in both these tasks. The final row reports the application of SPON (on the input provided by SVD ppmi) which is an original contribution of our work.
The analysis in Section 4 which shows that our choice of function f satisfies asymmetry and transitive properties, holds true because f satisfies f ((cid:126)x) ≥ (cid:126)x component-wise.  Table 4 shows the results for each of these ablation experiments, when evaluated on the unsupervised hypernym detection task across four datasets chosen randomly. Removing the Residual layer and using RELU activation function only, violates the aforementioned component-wise inequality f ((cid:126)x) ≥ (cid:126)x, and has the worst results out of the three. On the other hand, using Residual connections with Tanh activations may not violate the aforementioned inequality, since, it depends upon the sign of the activation outputs. This argument is supported by the results in Table 4, wherein using Tanh activations instead of RELU almost provides identical results, except for the BLESS dataset. Nevertheless, the results in Table 4 show that encouraging asymmetry and transitive properties for this task, in fact improves the results as opposed to not doing the same.
The analysis in Section 4 which shows that our choice of function f satisfies asymmetry and transitive properties, holds true because f satisfies f ((cid:126)x) ≥ (cid:126)x component-wise.  Table 4 shows the results for each of these ablation experiments, when evaluated on the unsupervised hypernym detection task across four datasets chosen randomly. Removing the Residual layer and using RELU activation function only, violates the aforementioned component-wise inequality f ((cid:126)x) ≥ (cid:126)x, and has the worst results out of the three. On the other hand, using Residual connections with Tanh activations may not violate the aforementioned inequality, since, it depends upon the sign of the activation outputs. This argument is supported by the results in Table 4, wherein using Tanh activations instead of RELU almost provides identical results, except for the BLESS dataset. Nevertheless, the results in Table 4 show that encouraging asymmetry and transitive properties for this task, in fact improves the results as opposed to not doing the same.
Furthermore, Table 5 illustrates the results on the unsupervised hypernym detection task for BLESS dataset, wherein we compare our proposed SPON model to other supervised SOTA approaches for hypernym prediction task, namely Order Embeddings (OE) approach as introduced by (Vendrov et al. 2016), and Smoothed Box model as introduced by (Li et al. 2019).
Furthermore, Table 5 illustrates the results on the unsupervised hypernym detection task for BLESS dataset, wherein we compare our proposed SPON model to other supervised SOTA approaches for hypernym prediction task, namely Order Embeddings (OE) approach as introduced by (Vendrov et al. 2016), and Smoothed Box model as introduced by (Li et al. 2019).
Also we compare performance of our model with two recent models, we see 2.39 ROUGE-1 improvements compared to the ML+RL with intra-attn approach(previous SOTA) carries over to this dataset, which is a large margin. On ROUGE-2, our model also get an im- provement of 0.51. The experiment proves that our approach can outperform competitive methods on different data distributions.
Also we compare performance of our model with two recent models, we see 2.39 ROUGE-1 improvements compared to the ML+RL with intra-attn approach(previous SOTA) carries over to this dataset, which is a large margin. On ROUGE-2, our model also get an im- provement of 0.51. The experiment proves that our approach can outperform competitive methods on different data distributions.
Similarly, Table 7 shows the results on the two domain-specific tasks of music and medical domain corpora. SPON outperforms the SOTA systems in all tasks except for the medical domain in which it achieves comparable results.
Similarly, Table 7 shows the results on the two domain-specific tasks of music and medical domain corpora. SPON outperforms the SOTA systems in all tasks except for the medical domain in which it achieves comparable results.
We calculate these similarity scores for each Real targeted output for the real inputs and the inputs found by the proposed model and report the average value in Table 1. According to the table, we observe that even inputting the real inputs, the similarity scores between the outputs and the target outputs are not high. Besides, with the crafted inputs from the proposed framework, these similarity scores are significantly improved. For example, for RL BeamSearch(200), the similarity is improved by 41.5%, 34.0% and 28.3% for the target outputs with length 1-3, 4-6 and 7-10, respectively.
Table 1 shows our main experimental results. In two of the cases (SST and ROC), SoPa outperforms all models. On Amazon, SoPa performs within 0.3 points of CNN and BiLSTM, and outperforms the other two baselines. The table also shows the number of parameters used by each model for each task.  SoPa performs better or roughly the same as a BiLSTM, which has 3–6 times as many parameters.  Table 1 also shows an ablation of the differences between SoPa and CNN: max-product semiring with sigmoid vs. max-sum semiring with identity, self-loops, and (cid:15)-transitions. The last line is equivalent to a CNN with  multiple window sizes. Interestingly, the most notable difference between SoPa and CNN is the semiring and encoder function, while (cid:15) transitions and self-loops have little effect on performance.1
To evaluate the annotations, inter-annotator agreement was calculated based on a subset of the gold standard corpus.4 Table 4 illustrates the values of observed agreement (Ao) and Cohen's κ (Cohen, 1960) obtained for question, feature and answer annotation. The agreement values obtained for question types were over 0.6 (for all annotators combined).
To evaluate the annotations, inter-annotator agreement was calculated based on a subset of the gold standard corpus.4 Table 4 illustrates the values of observed agreement (Ao) and Cohen's κ (Cohen, 1960) obtained for question, feature and answer annotation. The agreement values obtained for question types were over 0.6 (for all annotators combined).
Experimental Results Table 3 shows the performance of the different models over our two datasets, in the form of the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations.  On Wikipedia, we observe that the performance of BILSTM, INCEPTION, and JOINT is much better than that of  all four baselines. INCEPTION achieves 2.9% higher accuracy than BILSTM. The performance of JOINT achieves an accuracy of 59.4%, which is 5.3% higher than using textual features alone (BILSTM) and 2.4% higher than using visual features alone (INCEPTION). Based on a one-tailed Wilcoxon signed-rank test, the performance of Jis sta  of-the-art results in combination. For arXiv, baseline methods MAJORITY, BENCHMARK, and INCEPTIONFIXED outperform BILSTM over cs.ai, in large part because of the class imbalance in this dataset (90% of papers are rejected). Surprisingly, INCEPTIONFIXED is better than MAJORITY and BENCHMARK over the arXiv cs.lg subset, which verifies the usefulness of visual features, even when only the last layer is fine-tuned. Table 3 also shows that INCEPTION and BILSTM achieve similar performance on arXiv, showing that textual and visual representations are equally discriminative: INCEPTION and BILSTM are indistinguishable over cs.cl; BILSTM achieves 1.8% higher accuracy over cs.lg, while INCEPTION achieves 1.3% higher accuracy over cs.ai. Once again, the JOINT model achieves the highest accuracy on cs.ai and cs.cl by combining textual and visual representations (at a level of statistical significance for cs.ai). This, again, confirms that textual and visual  state-of-the-art results. On arXiv cs.lg, JOINT achieves a 0.6% higher accuracy than INCEPTION by combining visual features and textual features, but BILSTM achieves the highest accuracy. One characteristic of cs.lg documents is
Experimental Results Table 3 shows the performance of the different models over our two datasets, in the form of the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations.  On Wikipedia, we observe that the performance of BILSTM, INCEPTION, and JOINT is much better than that of  all four baselines. INCEPTION achieves 2.9% higher accuracy than BILSTM. The performance of JOINT achieves an accuracy of 59.4%, which is 5.3% higher than using textual features alone (BILSTM) and 2.4% higher than using visual features alone (INCEPTION). Based on a one-tailed Wilcoxon signed-rank test, the performance of Jis sta  of-the-art results in combination. For arXiv, baseline methods MAJORITY, BENCHMARK, and INCEPTIONFIXED outperform BILSTM over cs.ai, in large part because of the class imbalance in this dataset (90% of papers are rejected). Surprisingly, INCEPTIONFIXED is better than MAJORITY and BENCHMARK over the arXiv cs.lg subset, which verifies the usefulness of visual features, even when only the last layer is fine-tuned. Table 3 also shows that INCEPTION and BILSTM achieve similar performance on arXiv, showing that textual and visual representations are equally discriminative: INCEPTION and BILSTM are indistinguishable over cs.cl; BILSTM achieves 1.8% higher accuracy over cs.lg, while INCEPTION achieves 1.3% higher accuracy over cs.ai. Once again, the JOINT model achieves the highest accuracy on cs.ai and cs.cl by combining textual and visual representations (at a level of statistical significance for cs.ai). This, again, confirms that textual and visual  state-of-the-art results. On arXiv cs.lg, JOINT achieves a 0.6% higher accuracy than INCEPTION by combining visual features and textual features, but BILSTM achieves the highest accuracy. One characteristic of cs.lg documents is
Table 4 shows the confusion matrix of JOINT on Wikipedia. We can see that more than 50% of documents for each quality class are correctly classified, except for the  class where more documents are misclassified into B.
Table 4 shows the confusion matrix of JOINT on Wikipedia. We can see that more than 50% of documents for each quality class are correctly classified, except for the  class where more documents are misclassified into B.
We present our results on two freely available large scale datasets introduced by Zhang et al. . We also shrink the scale of training samples by randomly picking up the same amount of data in each category (see Table 1).  AG News The AG News corpus consists of news articles from the AG's corpus of news articles on the web pertaining to the 4 largest classes, which are Work, Sports, Business, Sci/Tech. The data set contains 30,000 training samples for each class, 1,900 samples for each class for testing. In this data set, there are three columns which are label, title, description, we treat title as abstract input and description as contents input. Sogou News A Chinese news data set. This data set is a combination of the SogouCA and SogouCS news corpora pertaining to 5 categories, which are Sports, Finance, Entertainment, Automobile and Technology. It contains 450,000 training samples and 60,000 samples for testing in total. Sogou New also has three columns in data set files, label, title, description, similarly, we treat title as abstract input and description as contents input.
We present our results on two freely available large scale datasets introduced by Zhang et al. . We also shrink the scale of training samples by randomly picking up the same amount of data in each category (see Table 1).  AG News The AG News corpus consists of news articles from the AG's corpus of news articles on the web pertaining to the 4 largest classes, which are Work, Sports, Business, Sci/Tech. The data set contains 30,000 training samples for each class, 1,900 samples for each class for testing. In this data set, there are three columns which are label, title, description, we treat title as abstract input and description as contents input. Sogou News A Chinese news data set. This data set is a combination of the SogouCA and SogouCS news corpora pertaining to 5 categories, which are Sports, Finance, Entertainment, Automobile and Technology. It contains 450,000 training samples and 60,000 samples for testing in total. Sogou New also has three columns in data set files, label, title, description, similarly, we treat title as abstract input and description as contents input.
The experiment results are presented in Table 2. The best performances of our configurations are highlighted in red, which are 8.37, 3.67, 13.79 and 7.89 for the error rates of our proposed model on AG, Sogou, AG(5k), Sogou(10k) respectively. The bold numbers are the officially reported accuracy of VDCNN, to which our proposed model is close. The numbers in blue are the results coming from our comparison experiment by using VDCNN where we set the sequence length to 256 under word level. From these results we can see that our model outperforms VDCNN on AG News for the official error rate, and is very close to VDCNN's performance on Sogou News. If we set VCDNN with the same input sequence length (256) in word-level, the performance of our proposed model is obviously better.  Most of the contributions come from external matrix From the results of Table 2, in the case of training with a large scale dataset, no matter we use simple LSTM or more complex bi-directional LSTM with self-attention, the testing error rates of different configurations are basically similar to each other. It can be said that such near state-of-the-art performance mainly attributes to the contribution from external memory.  Using abstract to build the external memory is better than contents  From Table 2 we can see that the results of using the description to construct the semantics matrix are better than using the abstract.  SeMemNN can still work on a few-shot learning Table 2 shows that although we have greatly shrank the scale of the training set, our proposed method can still outperform VDCNN. After shrinking the scale of the data, the performance of VDCNN has been greatly decreased, especially for Sogou news, VDCNN has been unable to learn from the training samples.
Table III provides detailed results of Experiment 2. The numbers are absolute accuracies, i.e., they correspond to cases where the arithmetic expression generated is 100% correct, leading to the correct numeric answer. Results by , , ,  are sparse but indicate the scale of success compared to recent past approaches. Prefix, postfix, and infix representations in Table III show that network capabilities are changed by how teachable the target data is.  While our networks fell short of  AI2 testing accuracy, we present state-of-the-art results for the remaining three datasets.  The type 2 postfix Transformer received the highest testing average of 87.2%.  Our attempt at language pre-training fell short of our expectations in all but one tested dataset.
Table III provides detailed results of Experiment 2. The numbers are absolute accuracies, i.e., they correspond to cases where the arithmetic expression generated is 100% correct, leading to the correct numeric answer. Results by , , ,  are sparse but indicate the scale of success compared to recent past approaches. Prefix, postfix, and infix representations in Table III show that network capabilities are changed by how teachable the target data is.  While our networks fell short of  AI2 testing accuracy, we present state-of-the-art results for the remaining three datasets.  The type 2 postfix Transformer received the highest testing average of 87.2%.  Our attempt at language pre-training fell short of our expectations in all but one tested dataset.
The values in the last column of Table III are summarized in Table IV. How the models compare with respect to accuracy closely resembles the comparison of BLEU scores, presented earlier. Thus, BLEU scores seem to correlate well with accuracy values in our case.
The values in the last column of Table III are summarized in Table IV. How the models compare with respect to accuracy closely resembles the comparison of BLEU scores, presented earlier. Thus, BLEU scores seem to correlate well with accuracy values in our case.
The experimental results on WOZ 2.0 corpus are presented in Table 1.  the three baseline models, BERT+RNN, BERT+RNN+Ontology, and the slot-dependent  SUMBT, showed no significant performance differences. On the other hand, the slot-independent SUMBT which learned the shared information from all across domains and slots significantly outperformed those baselines, resulting in 91.0% joint accuracy.  The proposed model achieved state-of-theart performance in both WOZ 2.0 and MultiWOZ datasets.
The experimental results on WOZ 2.0 corpus are presented in Table 1.  the three baseline models, BERT+RNN, BERT+RNN+Ontology, and the slot-dependent  SUMBT, showed no significant performance differences. On the other hand, the slot-independent SUMBT which learned the shared information from all across domains and slots significantly outperformed those baselines, resulting in 91.0% joint accuracy.  The proposed model achieved state-of-theart performance in both WOZ 2.0 and MultiWOZ datasets.
Table 2 shows the experimental results of the slot-independent SUMBT model on MultiWOZ corpus.  The SUMBT greatly surpassed the performances of previous approaches by yielding 42.4% joint accuracy.
Table 2 shows the experimental results of the slot-independent SUMBT model on MultiWOZ corpus.  The SUMBT greatly surpassed the performances of previous approaches by yielding 42.4% joint accuracy.
Table 3 summarizes the results for these transfer learning experiments when the source task is MNLI. Gain shows the difference between Fine-tuned model's accuracy and Baseline's accuracy. For HUBERT (Transformer), we observe substantial gain across all 5 target corpora after transfer. However, for BERT we have a drop for QNLI, QQP, and SST.
Table 3 summarizes the results for these transfer learning experiments when the source task is MNLI. Gain shows the difference between Fine-tuned model's accuracy and Baseline's accuracy. For HUBERT (Transformer), we observe substantial gain across all 5 target corpora after transfer. However, for BERT we have a drop for QNLI, QQP, and SST.
Again transferring roles gives positive results except for RTE. Filler vectors learned from QQP are more transferable compared to MNLI and gives a boost to all tasks except for SNLI. Surprisingly, transferring BERT parameters is hurting the results now even when TPR is present.
Again transferring roles gives positive results except for RTE. Filler vectors learned from QQP are more transferable compared to MNLI and gives a boost to all tasks except for SNLI. Surprisingly, transferring BERT parameters is hurting the results now even when TPR is present.
We also verified that our TPR layer is not hurting the performance by comparing the test set results for HUBERT (Transformer) and BERT. The results are obtained by submitting models to the GLUE evaluation server. The results are presented in Table 5.
We also verified that our TPR layer is not hurting the performance by comparing the test set results for HUBERT (Transformer) and BERT. The results are obtained by submitting models to the GLUE evaluation server. The results are presented in Table 5.
Table 1 compares the performance of all model variants. The SHiP models significantly improved over the BOW baselines on the two diagnosis tasks (p < 0.001 under Welch's t-test): for CCS prediction, the best SHiP models improved top-1 recall by 7.9 percentage points and top-5 recall by 4.8 percentage points, respectively, over the best BOW models; for ICD-9 prediction, area under the precision-recall curve (AUPRC) increased by 2.1 percentage points and weighted area under the ROC curve (AUROC) increased by 0.8 percentage points. For mortality prediction, we saw negligible benefit from the SHiP architecture. The SHiP models also improved over the corresponding hierarchical models without pretraining. For mortality, pretraining the all-features model increased AUPRC by 0.8 percentage points (p = 0.06) and AUROC by 0.6 percentage points (p = 0.004); for primary CCS, pretraining the all-feature model increased top-1 recall by 8.0 percentage points (p < 0.001), while pretraining the notes-only model increased top-5 recall by 4.7 percentage points (p < 0.001); for all ICD-9, pretraining the notes-only model increased AUPRC by 0.7 percentage points (p = 0.03) and weighted AUROC by 0.4 percentage points (p = 0.01).
Table 1 compares the performance of all model variants. The SHiP models significantly improved over the BOW baselines on the two diagnosis tasks (p < 0.001 under Welch's t-test): for CCS prediction, the best SHiP models improved top-1 recall by 7.9 percentage points and top-5 recall by 4.8 percentage points, respectively, over the best BOW models; for ICD-9 prediction, area under the precision-recall curve (AUPRC) increased by 2.1 percentage points and weighted area under the ROC curve (AUROC) increased by 0.8 percentage points. For mortality prediction, we saw negligible benefit from the SHiP architecture. The SHiP models also improved over the corresponding hierarchical models without pretraining. For mortality, pretraining the all-features model increased AUPRC by 0.8 percentage points (p = 0.06) and AUROC by 0.6 percentage points (p = 0.004); for primary CCS, pretraining the all-feature model increased top-1 recall by 8.0 percentage points (p < 0.001), while pretraining the notes-only model increased top-5 recall by 4.7 percentage points (p < 0.001); for all ICD-9, pretraining the notes-only model increased AUPRC by 0.7 percentage points (p = 0.03) and weighted AUROC by 0.4 percentage points (p = 0.01).
The compression results on the six adopted datasets, including parameter size, inference speedup and classification accuracy, are summarized in Table 1.  Overall speaking, on all the evaluated datasets, the proposed AdaBERT method achieves significant efficiency improvement while maintaining comparable performance. Compared to the BERT12-T, the compressed models are 11.5x to 17.0x smaller in parameter size and 12.7x to 29.3x faster in inference speed with an average performance degradation of 2.79%.  Comparing structureheterogeneous method, BiLSTMSOF  , AdaBERT searches CNN-based models and achieves much another with better improvements, especially on the MNLI dataset.  Comparing with different Transformers-based compression baselines, the proposed AdaBERT method is 1.35x to 3.12x faster than the fastest baseline, TinyBERT4, and achieves comparable performance with the two baselines that have the best averaged accuracy, BERT6-PKD and TinyBERT4.
The compression results on the six adopted datasets, including parameter size, inference speedup and classification accuracy, are summarized in Table 1.  Overall speaking, on all the evaluated datasets, the proposed AdaBERT method achieves significant efficiency improvement while maintaining comparable performance. Compared to the BERT12-T, the compressed models are 11.5x to 17.0x smaller in parameter size and 12.7x to 29.3x faster in inference speed with an average performance degradation of 2.79%.  Comparing structureheterogeneous method, BiLSTMSOF  , AdaBERT searches CNN-based models and achieves much another with better improvements, especially on the MNLI dataset.  Comparing with different Transformers-based compression baselines, the proposed AdaBERT method is 1.35x to 3.12x faster than the fastest baseline, TinyBERT4, and achieves comparable performance with the two baselines that have the best averaged accuracy, BERT6-PKD and TinyBERT4.
The results of cross-task validation is summarized in Table 3. From Table 3, we can observe that: The searched structures achieve the best performance on their original target tasks compared with other tasks, in other words, the performance numbers along the diagonal line of this table are the best. Further, the performance degradation is quite significant across different kinds of tasks (for example, applying the searched structures of sentiment classification tasks to entailment recognition task, or vice verse), while the performance degradations within the same kind  of tasks (for example, MRPC and QQP for semantic equivalence classification) are relatively small, since they have the same input format (i.e., a pair of sentences) and similar targets.  From the last row of Table 3, we can see that the randomly sampled structures perform worse than the searched structures and their performances are not stable.
The results of cross-task validation is summarized in Table 3. From Table 3, we can observe that: The searched structures achieve the best performance on their original target tasks compared with other tasks, in other words, the performance numbers along the diagonal line of this table are the best. Further, the performance degradation is quite significant across different kinds of tasks (for example, applying the searched structures of sentiment classification tasks to entailment recognition task, or vice verse), while the performance degradations within the same kind  of tasks (for example, MRPC and QQP for semantic equivalence classification) are relatively small, since they have the same input format (i.e., a pair of sentences) and similar targets.  From the last row of Table 3, we can see that the randomly sampled structures perform worse than the searched structures and their performances are not stable.
The model performance and corresponding model size are reported in Table 5. On the one hand, removing the efficiency-aware loss (β = 0) leads to the increase in model parameter size, on the other hand, a more aggressive efficiency preference (β = 8) results in the small model size but degraded performance, since a large β encourages the compressed model to adopt more lightweight operations such as zero and skip which hurt the performance. A moderate efficiency constraint (β = 4) provides a regularization, guiding the AdaBERT method to achieve a trade-off between the small parameter size and the good performance.
The model performance and corresponding model size are reported in Table 5. On the one hand, removing the efficiency-aware loss (β = 0) leads to the increase in model parameter size, on the other hand, a more aggressive efficiency preference (β = 8) results in the small model size but degraded performance, since a large β encourages the compressed model to adopt more lightweight operations such as zero and skip which hurt the performance. A moderate efficiency constraint (β = 4) provides a regularization, guiding the AdaBERT method to achieve a trade-off between the small parameter size and the good performance.
The Base-KD is a naive knowledge distillation version in which only the logits of the last layer are distilled without considering hidden layer knowledge and supervised label knowledge. By incorporating the probe models, the performance (line 2 in Table 4) is consistently improved, indicating the benefits from hierarchically decomposed taskoriented knowledge. We then leverage Data Augmentation (DA) to enrich task-oriented knowledge and this technique also improves performance for all tasks, especially for tasks that have a limited scale of data (i.e., MRPC and RTE). DA is also adopted in existing KD-based compression studies (Tang et al., 2019; Jiao et al., 2019). When taking the supervised label knowledge (LCE) into consideration, the performance is further boosted, showing that this term is also important for AdaBERT by providing focused search hints.
The Base-KD is a naive knowledge distillation version in which only the logits of the last layer are distilled without considering hidden layer knowledge and supervised label knowledge. By incorporating the probe models, the performance (line 2 in Table 4) is consistently improved, indicating the benefits from hierarchically decomposed taskoriented knowledge. We then leverage Data Augmentation (DA) to enrich task-oriented knowledge and this technique also improves performance for all tasks, especially for tasks that have a limited scale of data (i.e., MRPC and RTE). DA is also adopted in existing KD-based compression studies (Tang et al., 2019; Jiao et al., 2019). When taking the supervised label knowledge (LCE) into consideration, the performance is further boosted, showing that this term is also important for AdaBERT by providing focused search hints.
Our proposed approach sets a new state of the art of 84.38% binary accuracy on CMU-MOSI dataset of multimodal sentiment analysis; a significant leap from previous state of  We compare the performance of M-BERT with the following models on the multimodal sentiment analysis task: RMFN (SOTA1)1 fuses multimodal information in multiple stages by focusing on a subset of signals in each stage (Liang et al., 2018). MFN (SOTA2) synchronizes states of three separate LSTMs with a multi-view gated memory (Zadeh et al., 2018a). MARN (SOTA3) models view-specific interactions using hybrid LSTM memories and cross-modal interactions using a Multi-Attention Block(MAB) (Zadeh et al., 2018c).  We perform two different evaluation tasks on CMU-MOSI datset: i) Binary Classification, and ii) Regression. We formulate it as a regression problem and report Mean-absolute Error (MAE) and the correlation of model predictions with true labels. Besides, we convert the regression outputs into categorical values to obtain binary classification accuracy (BA) and F1 score.  The performances of M-BERT and BERT are described in Table 1.  M-BERT model outperforms all the baseline models (described in Sec.4.4) on every evaluation metrics with large margin. It sets new state-of-the-art performance for this task and achieves 84.38% accuracy, a 5.98% increase with respect to the SOTA1 and 1.02% increase with respect to BERT (text-only).  Even BERT (text-only) model achieves 83.36% accuracy, an increase of 4.96% from the SOTA1 78.4%, using text information only. It achieves higher performance in all evaluation metrics compare to SOTA1; reinforcing the expressiveness and utility of BERT contextual representation.
Our proposed approach sets a new state of the art of 84.38% binary accuracy on CMU-MOSI dataset of multimodal sentiment analysis; a significant leap from previous state of  We compare the performance of M-BERT with the following models on the multimodal sentiment analysis task: RMFN (SOTA1)1 fuses multimodal information in multiple stages by focusing on a subset of signals in each stage (Liang et al., 2018). MFN (SOTA2) synchronizes states of three separate LSTMs with a multi-view gated memory (Zadeh et al., 2018a). MARN (SOTA3) models view-specific interactions using hybrid LSTM memories and cross-modal interactions using a Multi-Attention Block(MAB) (Zadeh et al., 2018c).  We perform two different evaluation tasks on CMU-MOSI datset: i) Binary Classification, and ii) Regression. We formulate it as a regression problem and report Mean-absolute Error (MAE) and the correlation of model predictions with true labels. Besides, we convert the regression outputs into categorical values to obtain binary classification accuracy (BA) and F1 score.  The performances of M-BERT and BERT are described in Table 1.  M-BERT model outperforms all the baseline models (described in Sec.4.4) on every evaluation metrics with large margin. It sets new state-of-the-art performance for this task and achieves 84.38% accuracy, a 5.98% increase with respect to the SOTA1 and 1.02% increase with respect to BERT (text-only).  Even BERT (text-only) model achieves 83.36% accuracy, an increase of 4.96% from the SOTA1 78.4%, using text information only. It achieves higher performance in all evaluation metrics compare to SOTA1; reinforcing the expressiveness and utility of BERT contextual representation.
We find the inference times are lower in the former case with negligible difference in BLEU scores.  The inference times are of the various models are mentioned in Table 1.
We find the inference times are lower in the former case with negligible difference in BLEU scores.  The inference times are of the various models are mentioned in Table 1.
The results are summarized in Figure 6 and Table 3. The probability that a new image belongs to an unseen class P (N |t) is higher than the ME score of the classifier through most of the learning phase. Comparing the statistics of the datasets to the inductive biases in the classifiers, the ME score for the classifiers is substantially lower than the baseline ME measure in the dataset, P (N |t) (Table 3). For instance, the ImageNet classifier drops its ME score below 0.05 after about 8,960 images, while the approximate ME measure for the dataset shows that new classes are encountered at above this rate until at least 111,000 images.
The results are summarized in Figure 6 and Table 3. The probability that a new image belongs to an unseen class P (N |t) is higher than the ME score of the classifier through most of the learning phase. Comparing the statistics of the datasets to the inductive biases in the classifiers, the ME score for the classifiers is substantially lower than the baseline ME measure in the dataset, P (N |t) (Table 3). For instance, the ImageNet classifier drops its ME score below 0.05 after about 8,960 images, while the approximate ME measure for the dataset shows that new classes are encountered at above this rate until at least 111,000 images.
1 2 , lblstm Table V shows the BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. In Table V, l1 denotes the Transformer model under attack (e.g. en-de), l2 denotes the other Transformer model (e.g. en-fr), and lblstm are the BLSTM counterparts of l1 and l2.
1 2 , lblstm Table V shows the BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. In Table V, l1 denotes the Transformer model under attack (e.g. en-de), l2 denotes the other Transformer model (e.g. en-fr), and lblstm are the BLSTM counterparts of l1 and l2.
Table VI shows the BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. In Table VI, l1 denotes the BLSTM model under attack, l2 denotes the other BLSTM model, and ltrans are the Transformer counterparts of l1 and l2. , ltrans 1 2
Table VI shows the BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. In Table VI, l1 denotes the BLSTM model under attack, l2 denotes the other BLSTM model, and ltrans are the Transformer counterparts of l1 and l2. , ltrans 1 2
We first show that noising EnDe bitext sources does not seriously impact the translation quality of the transformer-base baseline.  Table 2 shows results for various values of p. Specifically, it presents the somewhat unexpected finding that even when noising 100% of the source bitext (so the model has never seen wellformed English), BLEU on well-formed test data only drops by 2.5.
We first show that noising EnDe bitext sources does not seriously impact the translation quality of the transformer-base baseline.  Table 2 shows results for various values of p. Specifically, it presents the somewhat unexpected finding that even when noising 100% of the source bitext (so the model has never seen wellformed English), BLEU on well-formed test data only drops by 2.5.
We repeat these experiments for WMT EnRo (Table 4).  In this case, NoisedBT is actually harmful, lagging standard BT by -0.6 BLEU. TaggedBT closes this gap and passes standard BT by +0.4 BLEU, for a total gain of +1.0 BLEU over NoisedBT.  We further investigate the effects of TaggedBT by performing one round of iterative backtranslation (Cotterell and Kreutzer, 2018; Vu Cong Duy Hoang and Cohn, 2018; Niu et al., 2018), and find another difference between the different varieties of BT: NoisedBT and TaggedBT allow the model to bootstrap improvements from an improved reverse model, whereas standard BT does not.  SacreBLEU scores for all these models are displayed in Table 4.  We find that the iteration-3 BT models improve over their Iteration-1 counterparts only for NoisedBT (+1.0 BLEU, dev+test avg) and TaggedBT (+0.7 BLEU, dev+test avg), whereas the Iteration-3 BT model shows no improvement over its Iteration-1 counterpart (-0.1 BLEU, dev+test avg).  iteration-3 TaggedBT (Table 4),  Our best BLEU score of 33.4 BLEU, obtained using Iterative TaggedBT, shows a gain of +3.5 BLEU over the highest previously published result on this test-set that we are aware of.  We furthermore match or out-perform the highest published results we are aware of on WMT EnDe that use only back-translation, with higher or equal BLEU on five of seven test sets.
We repeat these experiments for WMT EnRo (Table 4).  In this case, NoisedBT is actually harmful, lagging standard BT by -0.6 BLEU. TaggedBT closes this gap and passes standard BT by +0.4 BLEU, for a total gain of +1.0 BLEU over NoisedBT.  We further investigate the effects of TaggedBT by performing one round of iterative backtranslation (Cotterell and Kreutzer, 2018; Vu Cong Duy Hoang and Cohn, 2018; Niu et al., 2018), and find another difference between the different varieties of BT: NoisedBT and TaggedBT allow the model to bootstrap improvements from an improved reverse model, whereas standard BT does not.  SacreBLEU scores for all these models are displayed in Table 4.  We find that the iteration-3 BT models improve over their Iteration-1 counterparts only for NoisedBT (+1.0 BLEU, dev+test avg) and TaggedBT (+0.7 BLEU, dev+test avg), whereas the Iteration-3 BT model shows no improvement over its Iteration-1 counterpart (-0.1 BLEU, dev+test avg).  iteration-3 TaggedBT (Table 4),  Our best BLEU score of 33.4 BLEU, obtained using Iterative TaggedBT, shows a gain of +3.5 BLEU over the highest previously published result on this test-set that we are aware of.  We furthermore match or out-perform the highest published results we are aware of on WMT EnDe that use only back-translation, with higher or equal BLEU on five of seven test sets.
We performed a minimal set of experiments on WMT EnFr, which are summarized in Table 5.  In this case, we see that BT alone hurts performance compared to the strong bitext baseline, but NoisedBT indeed surpasses the bitext model.  It is worth noting that our numbers are lower than those reported by Edunov et al. (2018) on the years they report (36.1, 43.8, and 40.9 on 2013, 2014, and 2015 respectively).  On WMT16 EnRo, TaggedBT improves on vanilla BT by 0.4 BLEU.
We performed a minimal set of experiments on WMT EnFr, which are summarized in Table 5.  In this case, we see that BT alone hurts performance compared to the strong bitext baseline, but NoisedBT indeed surpasses the bitext model.  It is worth noting that our numbers are lower than those reported by Edunov et al. (2018) on the years they report (36.1, 43.8, and 40.9 on 2013, 2014, and 2015 respectively).  On WMT16 EnRo, TaggedBT improves on vanilla BT by 0.4 BLEU.
To understand how the model treats the tag and what biases it learns from the data, we investigate the entropy of the attention probability distribution, as well as the attention captured by the tag.  For the tagged variants, there is heavy attention on the tag when it is present (Table 6), indicating that the model relies on the information signalled by the tag.  Table 6 reports the average length-normalized Shannon entropy:  The entropy of the attention probabilities from the model trained on BT data is the clear outlier.
To understand how the model treats the tag and what biases it learns from the data, we investigate the entropy of the attention probability distribution, as well as the attention captured by the tag.  For the tagged variants, there is heavy attention on the tag when it is present (Table 6), indicating that the model relies on the information signalled by the tag.  Table 6 reports the average length-normalized Shannon entropy:  The entropy of the attention probabilities from the model trained on BT data is the clear outlier.
The BLEU scores of each decoding method are presented in Table 7.  The noised decode – decoding newstest sentences with the NoisedBT model after noising the source – yields poor performance.  The tagged decode, however, yields only somewhat lower performance than the standard decode on the same model (-2.9BLEU on average).
The BLEU scores of each decoding method are presented in Table 7.  The noised decode – decoding newstest sentences with the NoisedBT model after noising the source – yields poor performance.  The tagged decode, however, yields only somewhat lower performance than the standard decode on the same model (-2.9BLEU on average).
We quantify the copy rate with the unigram overlap between source and target as a percentage of tokens in the target side, and compare those statistics to the bitext and the back-translated data (Table 9).
We used the Reuters-8 dataset without stop words from  aiming at single-label classification, which is a preprocessed format of the Reuters-215782. Words in the texts were considered as they appeared, without performing stemming or typo correction. This database has eight different classes with the number of samples varying from 51 to over 3000 documents, as can be seen in Table I.
We used the Reuters-8 dataset without stop words from  aiming at single-label classification, which is a preprocessed format of the Reuters-215782. Words in the texts were considered as they appeared, without performing stemming or typo correction. This database has eight different classes with the number of samples varying from 51 to over 3000 documents, as can be seen in Table I.
In this experiment, we performed text classification among the classes in the Reuters-8 database.  The results can be seen in Table II. The simplest baseline, SA with w2v, achieved an accuracy rate of 78.73%. This  LSA with BOW features was almost 10% more accurate than SA, where the best results with binary weights were achieved with an approximation with 130 dimensions, with TF weights were achieved with 50 dimensions, and with TFIDF weights were achieved with 30 dimensions. SVM with BOW features was about 3% more accurate than LSA, with binary weights leading to a higher accuracy rate.  Among the baselines, the best method was MNB with tfBOW features, with an accuracy of 91.47%,  MSM with w2v had an accuracy rate of 90.62%, with the best results achieved with word subspace dimensions for the training classes ranging from 150 to 181, and for the query ranging from 3 to 217. Incorporating the frequency information in the subspace modeling resulted in higher accuracy, with TFMSM achieving 92.01%, with dimensions of word subspaces
In this experiment, we performed text classification among the classes in the Reuters-8 database.  The results can be seen in Table II. The simplest baseline, SA with w2v, achieved an accuracy rate of 78.73%. This  LSA with BOW features was almost 10% more accurate than SA, where the best results with binary weights were achieved with an approximation with 130 dimensions, with TF weights were achieved with 50 dimensions, and with TFIDF weights were achieved with 30 dimensions. SVM with BOW features was about 3% more accurate than LSA, with binary weights leading to a higher accuracy rate.  Among the baselines, the best method was MNB with tfBOW features, with an accuracy of 91.47%,  MSM with w2v had an accuracy rate of 90.62%, with the best results achieved with word subspace dimensions for the training classes ranging from 150 to 181, and for the query ranging from 3 to 217. Incorporating the frequency information in the subspace modeling resulted in higher accuracy, with TFMSM achieving 92.01%, with dimensions of word subspaces
Table 2 presents the results of our method comparing them against recent state-of-the-art supervised models and the simple n-gram language model used by Bryant and Briscoe (2018).  A key result of Table 2 is that Transformer Language Models prove to be more than just a competitive baseline to legitimate Grammatical Error Correction systems on their own. Across the board, Transformer Models are able to outperform the simple n-gram model and even approach the performance of supervised GEC systems.  we see that their performance is nearly identical with GPT-2 leading by a small margin in the CoNLL14 dataset.  BERT surpasses the n-gram baseline overall, it achieves worse performance than the rest in terms of precision and F0.5 score.
Table 2 presents the results of our method comparing them against recent state-of-the-art supervised models and the simple n-gram language model used by Bryant and Briscoe (2018).  A key result of Table 2 is that Transformer Language Models prove to be more than just a competitive baseline to legitimate Grammatical Error Correction systems on their own. Across the board, Transformer Models are able to outperform the simple n-gram model and even approach the performance of supervised GEC systems.  we see that their performance is nearly identical with GPT-2 leading by a small margin in the CoNLL14 dataset.  BERT surpasses the n-gram baseline overall, it achieves worse performance than the rest in terms of precision and F0.5 score.
We label each reply into three categories: buy, sell and other. The distribution of reply types is highly dependant on the structure and rules of the forum as shown in Table III.
We label each reply into three categories: buy, sell and other. The distribution of reply types is highly dependant on the structure and rules of the forum as shown in Table III.
Similarly a single classifier outperforms the rest in reply classification, as seen in Table IV. By our weighted nonother precision metric, Logistic Regression performed the best across both datasets, providing 0.874 precision on Antichat and 0.852 precision on Hack Forums.
Similarly a single classifier outperforms the rest in reply classification, as seen in Table IV. By our weighted nonother precision metric, Logistic Regression performed the best across both datasets, providing 0.874 precision on Antichat and 0.852 precision on Hack Forums.
are summarized in Table 1, where the score for each task category is calculated by averaging the normalized values for the tasks within each category. Although the activations of [CLS] token hidden states are often used in fine-tuning BERT for classification tasks, Mean-pooling of hidden states performs the best in all task categories among all the pooling methods.
are summarized in Table 1, where the score for each task category is calculated by averaging the normalized values for the tasks within each category. Although the activations of [CLS] token hidden states are often used in fine-tuning BERT for classification tasks, Mean-pooling of hidden states performs the best in all task categories among all the pooling methods.
The comparison between BERT embeddings and other models is presented in Table 3. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA (33% improvement in MAP) and InsuranceQA (version 1.0) (3.6% improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and (u, v, u ∗ v, |u − v|) shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM (Huang et al., 2013), but still far behind the state-of-the-art interaction-based model SUBMULT+NN (Wang and Jiang, 2016) and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly.
The comparison between BERT embeddings and other models is presented in Table 3. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA (33% improvement in MAP) and InsuranceQA (version 1.0) (3.6% improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and (u, v, u ∗ v, |u − v|) shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM (Huang et al., 2013), but still far behind the state-of-the-art interaction-based model SUBMULT+NN (Wang and Jiang, 2016) and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly.
these models are evaluated on two common datasets for NER: PEYMA and ARMAN. Bokaei and Mahmoudi (Bokaei and Mahmoudi, 2018) and Shahshahani et al. (Shahshahani et al., 2018) had reported the best results which you can see in Table 3  As you see in Table 3 in both word and phrase levels, our model outperform other NER approaches for the Persian language.  dataset.Table 3 shows that our results are 10 percent better than Shahshahani and colleagues on the same platform. On the other hand Bokaei and Mahmoudi (Bokaei and Mahmoudi, 2018) reported their results on Arman dataset Which is lower than ours in both word and phrase levels according to Table 3.
these models are evaluated on two common datasets for NER: PEYMA and ARMAN. Bokaei and Mahmoudi (Bokaei and Mahmoudi, 2018) and Shahshahani et al. (Shahshahani et al., 2018) had reported the best results which you can see in Table 3  As you see in Table 3 in both word and phrase levels, our model outperform other NER approaches for the Persian language.  dataset.Table 3 shows that our results are 10 percent better than Shahshahani and colleagues on the same platform. On the other hand Bokaei and Mahmoudi (Bokaei and Mahmoudi, 2018) reported their results on Arman dataset Which is lower than ours in both word and phrase levels according to Table 3.
Tables 4, 5, 6, 7 and 8 show the results of evaluation reported by competition for all teams which participated in the challenge. Our method is mentioned as Beheshti-NER-1  Table 4 and 5 show the results for subtask A. according to the tables, we reached to 84.0% and 87.9% F1 score respectively for phrase and word level evaluations.
In order to study the cases of disagreement between the approaches proposed by this paper and Ferrari et al. (2018), the top-5 words with the largest absolute differences between the assigned ranks have been reported for each scenario by Table III. The number of target words for each project scenario have also been mentioned in parenthesis. It can be observed that most of the cases of disagreement have a higher rank, i.e. relatively lower ambiguity score assigned by the linear transformation approach proposed by this paper. Most of such cases are proper names such as robert, peter, and daniel.
The average performances of LR−syntax and CNN:rand are virtually identical, both for Macro6Described as FscoreM in Sokolova and Lapalme (2009). F1 and Claim-F1, with a slight advantage for the feature-based approach, but their difference is not statistically significant (p ≤ 0.05). Altogether, these two systems exhibit significantly better average performances than all other models surveyed here, both those relying on and those not relying on hand-crafted features (p ≤ 0.05).  The performance of the learners is quite divergent across datasets, with Macro-F1 scores6 ranging from 60% (WTP) to 80% (MT), average 67% (see Table 2).  On all datasets, our best systems clearly outperform both baselines. In isolation, lexical, embedding, and syntax features are most helpful, whereas structural features did not help in most cases. Discourse features only contribute significantly on MT. When looking at the performance of the feature-based approaches, the most striking finding is the importance of lexical (in our setup, unigram) information.
The average performances of LR−syntax and CNN:rand are virtually identical, both for Macro6Described as FscoreM in Sokolova and Lapalme (2009). F1 and Claim-F1, with a slight advantage for the feature-based approach, but their difference is not statistically significant (p ≤ 0.05). Altogether, these two systems exhibit significantly better average performances than all other models surveyed here, both those relying on and those not relying on hand-crafted features (p ≤ 0.05).  The performance of the learners is quite divergent across datasets, with Macro-F1 scores6 ranging from 60% (WTP) to 80% (MT), average 67% (see Table 2).  On all datasets, our best systems clearly outperform both baselines. In isolation, lexical, embedding, and syntax features are most helpful, whereas structural features did not help in most cases. Discourse features only contribute significantly on MT. When looking at the performance of the feature-based approaches, the most striking finding is the importance of lexical (in our setup, unigram) information.
We also performed experiments with mixed sources, the results are shown in Table 4.  In this scenario, the neural network systems seem to benefit from the increased amount of training data and thus gave the best results.
We also performed experiments with mixed sources, the results are shown in Table 4.  In this scenario, the neural network systems seem to benefit from the increased amount of training data and thus gave the best results.
The results of the survey shows a general sat, confirmisfaction feeling of interactions with ing our motivation that the existence of such an agent helps researchers (See Table 4). 83% of participants agree that reduces their needs to search through the web (e.g. using search engines) to obtain information related to their research; and in the future. However, 66% of hu66% use man judges disagreed on using for planning their schedule for a conference. This observation could be because the current version of mainly retrieves information for users but planning for a conference needs some inferences on such information as well.
The results of the survey shows a general sat, confirmisfaction feeling of interactions with ing our motivation that the existence of such an agent helps researchers (See Table 4). 83% of participants agree that reduces their needs to search through the web (e.g. using search engines) to obtain information related to their research; and in the future. However, 66% of hu66% use man judges disagreed on using for planning their schedule for a conference. This observation could be because the current version of mainly retrieves information for users but planning for a conference needs some inferences on such information as well.
We compare all approaches across the InsuranceQA and WikiPassageQA benchmarks as well as the five StackExchange datasets in Table 2. For the cQA answer selection datasets we measure the accuracy, which is the ratio of correctly selected answers, and for the passage retrieval in WikiPassageQA we report MAP/MRR. The results show that COALA substantially outperforms all other relevance matching and semantic similarity approaches on all seven datasets. For instance, on the cQA datasets COALA improves by 4.5pp over CA-Wang and by 8.8pp over the best semantic similarity method on average.  Our extended approach COALA p-means improves the performance of COALA on these datasets by an additional 1.6pp. The proposed power mean aggregation achieves a strong improvement on four datasets and results in a small performance decrease in the remaining three cases.  The results in Table 2 show that our proposed syntax-aware extension COALA syntax-aware, which incorporates syntactic roles of word sequences to learn syntax-aware aspect representations, improves the results in five out of seven cases.  It thereby achieves an an average improvement of 0.7pp over COALA in our cQA datasets.
We compare all approaches across the InsuranceQA and WikiPassageQA benchmarks as well as the five StackExchange datasets in Table 2. For the cQA answer selection datasets we measure the accuracy, which is the ratio of correctly selected answers, and for the passage retrieval in WikiPassageQA we report MAP/MRR. The results show that COALA substantially outperforms all other relevance matching and semantic similarity approaches on all seven datasets. For instance, on the cQA datasets COALA improves by 4.5pp over CA-Wang and by 8.8pp over the best semantic similarity method on average.  Our extended approach COALA p-means improves the performance of COALA on these datasets by an additional 1.6pp. The proposed power mean aggregation achieves a strong improvement on four datasets and results in a small performance decrease in the remaining three cases.  The results in Table 2 show that our proposed syntax-aware extension COALA syntax-aware, which incorporates syntactic roles of word sequences to learn syntax-aware aspect representations, improves the results in five out of seven cases.  It thereby achieves an an average improvement of 0.7pp over COALA in our cQA datasets.
Table 4 shows full rankings extracted for English and German data. While some correspondence to the hierarchies proposed for English Agent in literature is evident (e.g. ≺ Instrument ≺ Theme, similar to (Fillmore, 1968)), a direct comparison is impossible due to the differences in role definitions and underlying syntactic formalisms. Notice the high number of ties: some roles never co-occur (either by chance or by design) or occur on the same syntactic rank (e.g. oblique) so there is no evidence for preference even if we enforce transitivity.
Table 4 shows full rankings extracted for English and German data. While some correspondence to the hierarchies proposed for English Agent in literature is evident (e.g. ≺ Instrument ≺ Theme, similar to (Fillmore, 1968)), a direct comparison is impossible due to the differences in role definitions and underlying syntactic formalisms. Notice the high number of ties: some roles never co-occur (either by chance or by design) or occur on the same syntactic rank (e.g. oblique) so there is no evidence for preference even if we enforce transitivity.
Table 5 contrasts the performance of THs induced from English and German training data, and evaluated on German and English test data respectively. While the cross-lingual performance is expectedly lower than the monolingual performance, it outperforms the random baseline by a large margin, suggesting the potential for crosslingual hierarchy induction.
Table 5 contrasts the performance of THs induced from English and German training data, and evaluated on German and English test data respectively. While the cross-lingual performance is expectedly lower than the monolingual performance, it outperforms the random baseline by a large margin, suggesting the potential for crosslingual hierarchy induction.
In Table 3 we have listed the average MAP and nDCG scores of the test sets. The tf-idf model is outperformed by most of the other models. However, bm25, which additionally takes the length of a document into account, performs very tf-idf and bm25 have the major benefit of well. fast computation. The feat model slightly outperforms the auto  auto-rank + feat model is slightly better than the auto-rank + bm25 model, both of which have the overall best performance. This shows, that the auto-encoder learns something orthogonal to term frequency and document length. The best model with respect to document ranking is the auto-rank + feat model. In Figure 3 we show the correlation between the different models. Interestingly, the bm25 and the feat strongly correlate. However, the scores of bm25 do not correlate with the scores of the combination of auto-rank and bm25. This indicates, that the model does not primarily learn to use the bm25 score but also focuses on the the auto-encoded representation. This underlines the hypothesis that the auto-encoder is able to represent latent features of the relationship of the query terms in the document. rank model. The distance features are a strong indicator for the semantic dependency between entities. These relationships need to be learned in the auto-rank model. The cosine similarity of a query and a document (auto/cos) does not yield a good result. This shows that the auto-encoder has learned many features, most of which do not correlate with our task. We also find that emb does not yield an equal performance to auto-rank. The combination of the
In Table 1, we report the performance of the BiLSTM implementation for predicting epistemic activities in the Med and TEd data. As we can see, the difficulty of predicting the classes varies between different activities. Despite some room for improvement with respect to the human upper bound (UB) based on inter-rater agreement, the interactive nature of FAMULUS helps in succeeding in this attempt by continually improving the model when new data is available. We conduct similar experiments for the prediction of fine-grained diagnostic entities, but omit a comprehensive discussion due to space limitations.
In Table 1, we report the performance of the BiLSTM implementation for predicting epistemic activities in the Med and TEd data. As we can see, the difficulty of predicting the classes varies between different activities. Despite some room for improvement with respect to the human upper bound (UB) based on inter-rater agreement, the interactive nature of FAMULUS helps in succeeding in this attempt by continually improving the model when new data is available. We conduct similar experiments for the prediction of fine-grained diagnostic entities, but omit a comprehensive discussion due to space limitations.
The final model used the finetuned BERT model mentioned above with a condition to predict non-propaganda only if the prediction probability is above 0.70 for the nonpropaganda class. Otherwise the prediction of the sentence will be propaganda even if the majority of the prediction probability mass was for the non-propaganda class. This was a way to handle the unbalance in the training data without having to discard part of the data. The 0.70 threshold was chosen after elaborate experiments on both the local and the shared-task's development sets. This condition consistently provided an improvement of around 5 points in F1 score of the propaganda class on all experiments using different sets of features as shown in Table 2.  In SLC, we ran multiple experiments using BERT with and without additional features as shown in Table 2. The features include using the text passed as is to BERT without any preprocessing. Also, we experimented with adding the context which includes the two sentences that come before and after the target sentence. Context sentences were concatenated and passed as the second BERT input, while the target sentence was passed as the first BERT input. In addition, we experimented with using BERT logits (i.e., the probability predictions per class) as features in a Logistic Regression (LR) classifier concatenated with handcrafted features (e.g., LIWC, quotes, questions), and with predictions of our FLC classifier (tagged spans: whether the sentence has a propaganda fragment or not). However, none of these features added any statistically significant improvements. Therefore, we used BERT predictions for our final model with a condition to predict the majority class non-propaganda only if its prediction probability is more than 0.70 as shown in Table 3. This is a modified threshold as opposed to 0.80 in the experiments shown in Table 2 to avoid overfitting on a one dataset. The final threshold of 0.70 was chosen after experiments on both the local and shared task development sets, which also represents the ratio of the non-propaganda class in the training set.
The final model used the finetuned BERT model mentioned above with a condition to predict non-propaganda only if the prediction probability is above 0.70 for the nonpropaganda class. Otherwise the prediction of the sentence will be propaganda even if the majority of the prediction probability mass was for the non-propaganda class. This was a way to handle the unbalance in the training data without having to discard part of the data. The 0.70 threshold was chosen after elaborate experiments on both the local and the shared-task's development sets. This condition consistently provided an improvement of around 5 points in F1 score of the propaganda class on all experiments using different sets of features as shown in Table 2.  In SLC, we ran multiple experiments using BERT with and without additional features as shown in Table 2. The features include using the text passed as is to BERT without any preprocessing. Also, we experimented with adding the context which includes the two sentences that come before and after the target sentence. Context sentences were concatenated and passed as the second BERT input, while the target sentence was passed as the first BERT input. In addition, we experimented with using BERT logits (i.e., the probability predictions per class) as features in a Logistic Regression (LR) classifier concatenated with handcrafted features (e.g., LIWC, quotes, questions), and with predictions of our FLC classifier (tagged spans: whether the sentence has a propaganda fragment or not). However, none of these features added any statistically significant improvements. Therefore, we used BERT predictions for our final model with a condition to predict the majority class non-propaganda only if its prediction probability is more than 0.70 as shown in Table 3. This is a modified threshold as opposed to 0.80 in the experiments shown in Table 2 to avoid overfitting on a one dataset. The final threshold of 0.70 was chosen after experiments on both the local and shared task development sets, which also represents the ratio of the non-propaganda class in the training set.
We also experimented with stacking BERT embeddings with all or some of the embeddings mentioned above. However, this resulted on lower 2https://www.urbandictionary.com/ 3https://data.world/jaredfern/urban-dictionary embedding  In FLC, we only show the results of our best model in Table 5 to focus more on the differences between propaganda techniques. A more elaborate study of performance of different models should follow in future work. The best model is a BiLSTM-CRF with flair and urban glove embed  As we can see in Table 5, we can divide the propaganda techniques into three groups according to the model's performance on the development and test sets. The first group includes techniques with non-zero F1 scores on both datasets: Flag-Waving, Loaded Language, Name Calling,Labeling and Slogans. This group has techniques that appear frequently in the data and/or techniques with strong lexical signals (e.g. "American People" in Flag-Waving) or punctuation signals (e.g. quotes in Slogans). The second group has the techniques with a nonzero F1 score on only one of the datasets but not the other, such as: Appeal to Authority, Appeal to Fear, Doubt, Reduction, and Exaggeration,Minimisation. Two out of these five techniques (Appeal to Fear and Doubt) have very small non-zero F1 on the development set which indicates that they are generally challenging on our model and were only tagged due to minor differences between the two datasets. However, the remaining three types show significant drops from development to test sets or vice-versa. This requires further analysis to understand why the model was able to do well on one dataset but get zero on the other dataset, which we leave for future work. The third group has the remaining nine techniques were our sequence tagger fails to correctly tag any text span on either dataset. This group has the most infrequent types as well as types beyond the ability for our tagger to spot by looking at the sentence only such as Repetition.  Overall, our model has the highest precision among all teams on both datasets, which could be due to adding the UBY  one-hot encoded features that highlighted some strong signals for some propaganda types. This also could be the reason for our model to have the lowest recall among the top 7 teams on both datasets as having explicit handcrafted signals suffers from the usual sparseness that accompanies these kinds of representations which could have made the model more conservative in tagging text spans.
We also experimented with stacking BERT embeddings with all or some of the embeddings mentioned above. However, this resulted on lower 2https://www.urbandictionary.com/ 3https://data.world/jaredfern/urban-dictionary embedding  In FLC, we only show the results of our best model in Table 5 to focus more on the differences between propaganda techniques. A more elaborate study of performance of different models should follow in future work. The best model is a BiLSTM-CRF with flair and urban glove embed  As we can see in Table 5, we can divide the propaganda techniques into three groups according to the model's performance on the development and test sets. The first group includes techniques with non-zero F1 scores on both datasets: Flag-Waving, Loaded Language, Name Calling,Labeling and Slogans. This group has techniques that appear frequently in the data and/or techniques with strong lexical signals (e.g. "American People" in Flag-Waving) or punctuation signals (e.g. quotes in Slogans). The second group has the techniques with a nonzero F1 score on only one of the datasets but not the other, such as: Appeal to Authority, Appeal to Fear, Doubt, Reduction, and Exaggeration,Minimisation. Two out of these five techniques (Appeal to Fear and Doubt) have very small non-zero F1 on the development set which indicates that they are generally challenging on our model and were only tagged due to minor differences between the two datasets. However, the remaining three types show significant drops from development to test sets or vice-versa. This requires further analysis to understand why the model was able to do well on one dataset but get zero on the other dataset, which we leave for future work. The third group has the remaining nine techniques were our sequence tagger fails to correctly tag any text span on either dataset. This group has the most infrequent types as well as types beyond the ability for our tagger to spot by looking at the sentence only such as Repetition.  Overall, our model has the highest precision among all teams on both datasets, which could be due to adding the UBY  one-hot encoded features that highlighted some strong signals for some propaganda types. This also could be the reason for our model to have the lowest recall among the top 7 teams on both datasets as having explicit handcrafted signals suffers from the usual sparseness that accompanies these kinds of representations which could have made the model more conservative in tagging text spans.
To evaluate the ranking of entity terms we have computed nDCG@10, nDCG@100 and MAP, see Table 4 for the results. We also compute Recall@k of relevant documents for automatically refined queries using the 1st, 2nd and 3rd ranked entities. The scores can be found in Table 5. Tables 4 and 5 show that the Relevance Model outperforms the Rocchio algorithm in every aspect. Both models outperform the auto-encoder approach (auto-ref ). We suspect that summing over the encodings distorts the individual features too much for a correct extraction of relevant entities to be possible. The combination of all three models (auto-ref + rocchio + relevance) outperforms the other models in most cases. Especially the performance for ranking of entity terms is increased using the autoencoded features. However, it is interesting to see that the rocchio + relevance model outperforms the recall for second and third best terms. This indicates that for user-evaluated term suggestions, the inclusion of the auto-encoded features is advisable. For automatic query refinement however, in average, this is not the case.
To evaluate the ranking of entity terms we have computed nDCG@10, nDCG@100 and MAP, see Table 4 for the results. We also compute Recall@k of relevant documents for automatically refined queries using the 1st, 2nd and 3rd ranked entities. The scores can be found in Table 5. Tables 4 and 5 show that the Relevance Model outperforms the Rocchio algorithm in every aspect. Both models outperform the auto-encoder approach (auto-ref ). We suspect that summing over the encodings distorts the individual features too much for a correct extraction of relevant entities to be possible. The combination of all three models (auto-ref + rocchio + relevance) outperforms the other models in most cases. Especially the performance for ranking of entity terms is increased using the autoencoded features. However, it is interesting to see that the rocchio + relevance model outperforms the recall for second and third best terms. This indicates that for user-evaluated term suggestions, the inclusion of the auto-encoded features is advisable. For automatic query refinement however, in average, this is not the case.
A now retired evaluation suite for word embeddings was wordvectors.org (Faruqui and Dyer, 2014a).  VecEval (Nayak et al., 2016) is another web based suite for static English word embeddings that perform evaluation on a set of downstream tasks which may take several hours.  K¨ohn (2015) introduced an offline, multilingual probing suite for static embeddings limited in terms of the languages and the probing tasks.  A comparison of the system features of previous studies is given in Table 1.
A now retired evaluation suite for word embeddings was wordvectors.org (Faruqui and Dyer, 2014a).  VecEval (Nayak et al., 2016) is another web based suite for static English word embeddings that perform evaluation on a set of downstream tasks which may take several hours.  K¨ohn (2015) introduced an offline, multilingual probing suite for static embeddings limited in terms of the languages and the probing tasks.  A comparison of the system features of previous studies is given in Table 1.
The right-hand side of table 1 shows the performance of our SVM and the two neural methods. The results indicate that the SVM setup is well suited for the difficulty prediction task and that it successfully generalizes to new data.
The right-hand side of table 1 shows the performance of our SVM and the two neural methods. The results indicate that the SVM setup is well suited for the difficulty prediction task and that it successfully generalizes to new data.
Table 2 shows the results for our three corpora. Throughout all three corpora, both manipulation strategies perform well. SEL consistently outperforms SIZE, which matches our observations from the previous experiment.
Table 2 shows the results for our three corpora. Throughout all three corpora, both manipulation strategies perform well. SEL consistently outperforms SIZE, which matches our observations from the previous experiment.
Table 3 shows the error rates per C-test and strategy. Both SEL and SIZE are overall able to significantly (p < 0.025) increase and decrease the test's difficulty over DEF, and with the exception of  SEL,dec , the effect is also statistically significant 4 for all individual text and strategy pairs.
Table 3 shows the error rates per C-test and strategy. Both SEL and SIZE are overall able to significantly (p < 0.025) increase and decrease the test's difficulty over DEF, and with the exception of  SEL,dec , the effect is also statistically significant 4 for all individual text and strategy pairs.
First, we report our results on English data (see Table 3, top)  we compare against German data (see Table 3, bottom).  Our new strong Data-Lexicon Baseline reaches a considerable accuracy of 86.32 %, which is hard to beat by trained models. Even the most recent state of the art only beats it by about two points: 88.41 % (Hermann et al., 2014). However, the accuracy of the baseline drops for ambiguous predicates (69.73 %) and the F1-macro score reveals its weakness toward minority classes (drop from 64.54 % to 37.42 %).  Our unimodal system trained and evaluated on English data slightly exceeds the accuracy of the previous state of the art (88.66 % on average versus 88.41 % for Hermann et al., 2014); our best run's accuracy is 89.35 %.  Table 3: FrameId results (in %) on English (upper) and German (lower) with and without using the lexicon. Reported are accuracy and F1-macro, both also for ambiguous predicates (mean scores over ten runs). Models: (a) Data, Lexicon, and Data-Lexicon Baselines. (b) Previous models for English. (c) Ours: unimodal our-uni, multimodal on top of our-uni – our-mm – with IMAGINED embeddings (and synset visual embeddings for English). Best results highlighted in bold. The best run's results for English were: our uni: acc: 89.35 ; acc amb: 76.45 ; F1-m: 76.95 ; F1-m amb: 54.02 (with lexicon) our mm (im, synsV): acc: 89.09 ; acc amb: 75.86 ; F1-m: 78.17 ; F1-m amb: 57.48 (with lexicon)  Our system evaluated on German data sets a new state of the art on this corpus with 80.76 % accuracy, outperforming the baselines (77.16 %; no other system evaluated on this dataset).  We report results achieved without the lexicon to evaluate independently of its quality (Hartmann et al., 2017). On English data, our systems outperforms Hartmann et al. (2017) by more than two points in accuracy and we achieve a large improvement over the Data Baseline. Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data. For German data, the increase of F1-macro with lexicon versus without is small (one point).
First, we report our results on English data (see Table 3, top)  we compare against German data (see Table 3, bottom).  Our new strong Data-Lexicon Baseline reaches a considerable accuracy of 86.32 %, which is hard to beat by trained models. Even the most recent state of the art only beats it by about two points: 88.41 % (Hermann et al., 2014). However, the accuracy of the baseline drops for ambiguous predicates (69.73 %) and the F1-macro score reveals its weakness toward minority classes (drop from 64.54 % to 37.42 %).  Our unimodal system trained and evaluated on English data slightly exceeds the accuracy of the previous state of the art (88.66 % on average versus 88.41 % for Hermann et al., 2014); our best run's accuracy is 89.35 %.  Table 3: FrameId results (in %) on English (upper) and German (lower) with and without using the lexicon. Reported are accuracy and F1-macro, both also for ambiguous predicates (mean scores over ten runs). Models: (a) Data, Lexicon, and Data-Lexicon Baselines. (b) Previous models for English. (c) Ours: unimodal our-uni, multimodal on top of our-uni – our-mm – with IMAGINED embeddings (and synset visual embeddings for English). Best results highlighted in bold. The best run's results for English were: our uni: acc: 89.35 ; acc amb: 76.45 ; F1-m: 76.95 ; F1-m amb: 54.02 (with lexicon) our mm (im, synsV): acc: 89.09 ; acc amb: 75.86 ; F1-m: 78.17 ; F1-m amb: 57.48 (with lexicon)  Our system evaluated on German data sets a new state of the art on this corpus with 80.76 % accuracy, outperforming the baselines (77.16 %; no other system evaluated on this dataset).  We report results achieved without the lexicon to evaluate independently of its quality (Hartmann et al., 2017). On English data, our systems outperforms Hartmann et al. (2017) by more than two points in accuracy and we achieve a large improvement over the Data Baseline. Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data. For German data, the increase of F1-macro with lexicon versus without is small (one point).
The automatic evaluation results on the test set of MSCOCO are shown in Table 1. Our model outperforms all the compared approaches on all automatic evaluation metrics. In particular, by benefiting from the retrieved captions, our model gets the highest CIDEr score of 123.5, suggesting that the captions generated by our model are informative.
The automatic evaluation results on the test set of MSCOCO are shown in Table 1. Our model outperforms all the compared approaches on all automatic evaluation metrics. In particular, by benefiting from the retrieved captions, our model gets the highest CIDEr score of 123.5, suggesting that the captions generated by our model are informative.
Table 1 shows the evaluation results. In line with the findings for sentence summarization (Cao et al., 2018; Li et al., 2018), we observe that factual errors are also a frequent problem for document summarization. Interestingly, the fraction of incorrect summaries is substantially higher for FAS and BUS compared to PGC. The length of the generated summaries appears to be unrelated to the number of errors. Instead, the higher abstractiveness of summaries produced by FAS and BUS, as analyzed in their respective papers, seems to also increase the chance of introducing errors. In addition, we also observe that among the three systems correctness and ROUGE scores do not correlate, emphasizing one more time that a ROUGE based evaluation alone is far too limited to account for the full scope of the summarization task.
Table 1 shows the evaluation results. In line with the findings for sentence summarization (Cao et al., 2018; Li et al., 2018), we observe that factual errors are also a frequent problem for document summarization. Interestingly, the fraction of incorrect summaries is substantially higher for FAS and BUS compared to PGC. The length of the generated summaries appears to be unrelated to the number of errors. Instead, the higher abstractiveness of summaries produced by FAS and BUS, as analyzed in their respective papers, seems to also increase the chance of introducing errors. In addition, we also observe that among the three systems correctness and ROUGE scores do not correlate, emphasizing one more time that a ROUGE based evaluation alone is far too limited to account for the full scope of the summarization task.
For 107 out of the 200 documents, an incorrect and correct summary is among the 5 alternatives. Table 2 shows that in this sample from the validation data, the fraction of incorrect summaries at first position, when the 5 alternatives are ranked as during beam search, is at 42.1%.  Using entailment probabilities of ESIM and InferSent, we can slightly improve upon that and reduce incorrect summaries. However, with DA and SSE, more incorrect summaries end up in the first position. Note that these results are not in line with the model's NLI accuracies, underlining that performance on NLI does not directly transfer to our task. Only for BERT, which outperforms the other models on NLI by a large margin, we also see substantially better reranking performance. But even for this powerful model, more than half of the errors still remain in the summaries.5 Interestingly, we also find that for ESIM and InferSent, reranking hurts in many cases, leaving just a few cases of net improvement.  Given the validation results, we then applied reranking to the CNN-DM test data followed by a post-hoc correctness evaluation as in Section 4. We used the ESIM model and reranked all 10  beam hypotheses generated by FAS.6 In contrast to the validation sample, the fraction of incorrect summaries increases from 26% to 29% (Table 2), demonstrating that the slight improvement on the validation data does not transfer to the test set.
For 107 out of the 200 documents, an incorrect and correct summary is among the 5 alternatives. Table 2 shows that in this sample from the validation data, the fraction of incorrect summaries at first position, when the 5 alternatives are ranked as during beam search, is at 42.1%.  Using entailment probabilities of ESIM and InferSent, we can slightly improve upon that and reduce incorrect summaries. However, with DA and SSE, more incorrect summaries end up in the first position. Note that these results are not in line with the model's NLI accuracies, underlining that performance on NLI does not directly transfer to our task. Only for BERT, which outperforms the other models on NLI by a large margin, we also see substantially better reranking performance. But even for this powerful model, more than half of the errors still remain in the summaries.5 Interestingly, we also find that for ESIM and InferSent, reranking hurts in many cases, leaving just a few cases of net improvement.  Given the validation results, we then applied reranking to the CNN-DM test data followed by a post-hoc correctness evaluation as in Section 4. We used the ESIM model and reranked all 10  beam hypotheses generated by FAS.6 In contrast to the validation sample, the fraction of incorrect summaries increases from 26% to 29% (Table 2), demonstrating that the slight improvement on the validation data does not transfer to the test set.
The two best-performing approaches in the task of generating sentences from dependency trees have been feature-based incremental text generation (Bohnet et al., 2010; Liu et al., 2015; Puduppully et al., 2016; King and White, 2018) and 2http://universaldependencies.org/ techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018).  Each of these approaches has their advantages and limitations (Table 2).
The accuracy scores on *BLESS test sets are provided in Table 1.8 Our POSTLE models display exactly the same performance as LEAR in the FULL setting: this is simply because all words found in *BLESS datasets are covered by the lexical constraints, and POSTLE does not generalize the initial LEAR transformation to unseen test words. In the DISJOINT setting, however, LEAR is left "blind" as it has not seen a single test word in the constraints: it leaves distributional vectors of *BLESS test words identical. In this setting, LEAR performance is equivalent to the original distributional space. In contrast, learning to generalize the LE specialization function from LEAR-specializations of other words, POSTLE models are able to successfully LE-specialize vectors of test *BLESS words. As in the graded LE, the adversarial POSTLE architecture outperforms the simpler DFFN model.
The accuracy scores on *BLESS test sets are provided in Table 1.8 Our POSTLE models display exactly the same performance as LEAR in the FULL setting: this is simply because all words found in *BLESS datasets are covered by the lexical constraints, and POSTLE does not generalize the initial LEAR transformation to unseen test words. In the DISJOINT setting, however, LEAR is left "blind" as it has not seen a single test word in the constraints: it leaves distributional vectors of *BLESS test words identical. In this setting, LEAR performance is equivalent to the original distributional space. In contrast, learning to generalize the LE specialization function from LEAR-specializations of other words, POSTLE models are able to successfully LE-specialize vectors of test *BLESS words. As in the graded LE, the adversarial POSTLE architecture outperforms the simpler DFFN model.
The average precision (AP) ranking scores achieved via cross-lingual transfer of POSTLE are shown in Table 2. We report AP scores using three methods for cross-lingual word embedding induction, and compare their performance to two baselines: 1) random word pair scoring, and 2) the original (FASTTEXT) vectors. The results uncover the inability of distributional vectors to capture LE – they yield lower performance than the random baseline, which strongly emphasizes the need for the LE-specialization. The transferred POSTLE yields an immense improve
The average precision (AP) ranking scores achieved via cross-lingual transfer of POSTLE are shown in Table 2. We report AP scores using three methods for cross-lingual word embedding induction, and compare their performance to two baselines: 1) random word pair scoring, and 2) the original (FASTTEXT) vectors. The results uncover the inability of distributional vectors to capture LE – they yield lower performance than the random baseline, which strongly emphasizes the need for the LE-specialization. The transferred POSTLE yields an immense improve
The reinforcement learning model of deep-coref, i.e., deep-coref RL, has the most significant difference when it is evaluated based on maximum vs. minimum spans (about 4 points). The ensemble model of e2e-coref, on the other hand, has the least difference between maximum and minimum span scores (1.4 points), which indicates it better recognizes maximum span boundaries in out-of-domain data. The ranking of systems is very different by using maximum vs. min  Table 4 shows the maximum vs. minimum span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current stateof-the-art system by Lee et al. (2018).  The coreference resolver of Peng et al. (2015) has the smallest difference between its maximum and minimum span evaluation scores.  Based on maximum spans, Peng et al. (2015) performs on-par with cort while cort outperforms it by about one percent when they are evaluated based on minimum spans.
The reinforcement learning model of deep-coref, i.e., deep-coref RL, has the most significant difference when it is evaluated based on maximum vs. minimum spans (about 4 points). The ensemble model of e2e-coref, on the other hand, has the least difference between maximum and minimum span scores (1.4 points), which indicates it better recognizes maximum span boundaries in out-of-domain data. The ranking of systems is very different by using maximum vs. min  Table 4 shows the maximum vs. minimum span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current stateof-the-art system by Lee et al. (2018).  The coreference resolver of Peng et al. (2015) has the smallest difference between its maximum and minimum span evaluation scores.  Based on maximum spans, Peng et al. (2015) performs on-par with cort while cort outperforms it by about one percent when they are evaluated based on minimum spans.
Table III reports the accuracy of different syntactic representations for all the benchmark datasets. In ST encoding, the  the corresponding paper. The experimental results demon strate that our proposed syntactic representation (POS-CNN outperforms the previously proposed method (ST-CNN) by the benchmark datasets (38.6% in a large margin in all CCAT10, 30.80% in CCAT50, 19.62% in BLOGS10, 11.94% in BLOGS50). This improvement in performance can be due  syntactic representations are kept identical. According to Table III, POS-HAN outperforms POS-CNN model consistently across all the benchmark datasets (1.74% in CCAT10, 0.32% in CCAT50, 1.06% in BLOGS10, 2.91% in BLOGS50). This
the table, lexical model consistently outperforms the syntactic model across all the benchmark datasets. Moreover, combining the two representations further improves the performance results. Based on the observation, we realize that even if Syntactic-HAN achieves a comparable performance results combining it with Lexical-HAN, slightly improves the overall performance (Style-HAN). As shown in Table IV, the performance improvement in terms of accuracy is consistent across all the benchmark datasets (4.54% in CCAT10, 2.85% in CCAT50, 2.02% in BLOGS10, 1.42% in BLOGS50)
the table, lexical model consistently outperforms the syntactic model across all the benchmark datasets. Moreover, combining the two representations further improves the performance results. Based on the observation, we realize that even if Syntactic-HAN achieves a comparable performance results combining it with Lexical-HAN, slightly improves the overall performance (Style-HAN). As shown in Table IV, the performance improvement in terms of accuracy is consistent across all the benchmark datasets (4.54% in CCAT10, 2.85% in CCAT50, 2.02% in BLOGS10, 1.42% in BLOGS50)
Table V reports the accuracy of the combined and the parallel fusion approaches. According to these results, training two parallel networks for lexical and syntax encoding achieves higher accuracy when compared to training the same network with combined embeddings.
Table V reports the accuracy of the combined and the parallel fusion approaches. According to these results, training two parallel networks for lexical and syntax encoding achieves higher accuracy when compared to training the same network with combined embeddings.
Table VI reports the accuracy of the models on the four benchmark datasets. All the results are bold. It shows that Style-HAN outperforms the baselines by 2.38%, 1.35%, 8.73%, and 4.46% over the CCAT10, CCAT50, BLOGs10, and BLOGS50 datasets, respectively.
