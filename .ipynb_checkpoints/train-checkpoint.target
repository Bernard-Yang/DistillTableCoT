The table provides results of fine-tuned models on Balanced COPA using BERT-large-FT and RoBERTa-large-FT.
RoBERTa-large (finetuned) has the highest accuracy among all the models listed in the table.
BERT-large (finetuned) has a slightly lower accuracy compared to BERT-large and BERT-large (Li et al. 2019).
Table 2 provides information about the applicability, productivity, and coverage of different words in the alternatives of the COPA dev set.
The word "in" has an applicability score of 47, a productivity score of 55.3, and a coverage score of 9.40.
The table provides the results of human performance evaluation for the Original COPA and Balanced COPA datasets.
The Original COPA dataset shows higher accuracy compared to the Balanced COPA dataset.
BERT-large performs better on COPA than B-COPA.
RoBERTa-large performs better on B-COPA than BERT-large.
The precision for negative sentiment classification using the CNN-LSTM-Our-neg-Ant classifier is 0.87.
The Fscore for positive sentiment classification using the CNN-LSTM-Our-neg-Ant classifier is 0.78.
The table shows the performance of a negation classifier for scope detection with gold cues and scope.
The proposed method outperforms the Punctuation and BiLSTM methods for both in-scope and out-scope detection.
The F-Score for Actual cues is higher than the F-Score for False cues.
There are more instances of Actual cues than False cues in the dataset.
The table presents human evaluation results for three different models: DAMD, DAMD (+), and HDSA (+).
The diversity score is higher for DAMD (+) compared to DAMD.
The table provides evaluation results for different models and decoding schemes.
The DAMD model with beam search decoding scheme shows improved performance with data augmentation.
The table compares the performance of different models for response generation on MultiWOZ.
The models in the table are trained on the SQuAD training data.
The F1 score for in-domain SQuAD using the MQAN model is 75.37.
The MQAN model performs the worst on the out-of-domain Glockner dataset.
The MQAN model with coverage performs better on the in-domain MultiNLI dataset compared to the MQAN model without coverage.
The KL-divergence between GP-MBCM dialog policy and the human dialog is 0.238.
The ALDM dialog policy has a higher KL-divergence compared to the GDPL dialog policy.
GDPL performs better than other methods in terms of "Agenda Inform", "Agenda Match", and "Agenda Success".
GDPL has the highest values for "Agenda Match" and "Agenda Success" among all the methods.
PPO outperforms other methods in terms of ACER, VHUS Inform, and VHUS Turns on the neural user simulator.
GDPL achieves the highest performance in terms of VHUS Match and VHUS Success on the neural user simulator.
PPO is preferred over ALDM in terms of efficiency.
ALDM is preferred over PPO in terms of quality.
There are two types of sessions: "Full" and "Other".
The mean success score for the "Other" type sessions is -71.62.
The table presents precision scores for the Analogy Test.
Word2Vec achieves a precision score of 73.03 on the Analogy Test.
The table provides the results of the Word Intrusion Test for two different models, GloVe and Imparted.
The mean score for the GloVe model in the Word Intrusion Test is 85 with a standard deviation of 6.9.
The table provides precision scores for different subsets of questions in the Semantic Analogy Test.
The proposed method outperforms GloVe and Word2Vec in terms of precision scores for all subsets of questions in the Semantic Analogy Test.
The table presents the combined within- and cross-document entity coreference results on the ECB+ test set for three different models: Cluster+Lemma, Disjoint, and Joint.
The Joint model outperforms the Cluster+Lemma and Disjoint models in terms of the CEAF-[ITALIC]e F1 score.
The Joint model variant achieves a higher CEAF-e F1 score than the Disjoint model variant.
The Cluster+KCP model variant achieves a higher B3 F1 score than the Cluster+Lemma model variant.
The CoNLL F1 score for the "Joint" model is 71.2.
The F1 score for "B3" in the "Cluster+Lemma" model is 65.6.
The CoNLL F1 score for the "Joint" model variant is 79.5.
The MUC precision for the "Cluster+KCP" model is 79.3.
As the threshold increases, the recall values decrease.
As the threshold increases, the precision values decrease in our model.
As the threshold increases, the recall values decrease.
Our Model has the highest AUC value among the models.
As the value of d increases, the recall values decrease.
The time taken to process the data increases as the value of d increases.
Table 5 shows the precisions on the Wikidata dataset with different numbers of dynamic routing iterations.
The recall values decrease as the number of dynamic routing iterations increases.
All the systems listed in the table have used the ROUGE-L metric for evaluation.
Our system achieved an ROUGE-1 score of 39.6.
Our system receives a significantly higher average human rating compared to the Refresh and ExtAbsRL systems in the human evaluation on extractive summaries.
In a higher percentage of documents, our system receives the highest human rating compared to the Refresh and ExtAbsRL systems in the human evaluation on extractive summaries.
The table compares the performance of ExtAbsRL with different reward functions: R-L (original) and Learned (ours).
The Learned (ours) reward function yields significantly higher average human ratings compared to the R-L (original) reward function.
The SRU encoder with 2 layers has 3.7M parameters and takes 14.7 milliseconds for inference time.
The LSTM encoder with 4 layers takes 174.8 milliseconds for inference time.
Table 3 provides the AUC and AUC@p metrics for the model on the propriety help desk dataset.
The AUC values for both the validation and test sets decrease as the threshold value decreases.
The random whitelist performs better than the frequency whitelist for a size of 1K+.
The frequency whitelist performs better for a size of 1K compared to a size of 10K.
The coverage of the whitelist decreases as the size of the whitelist decreases.
The recall@1 increases as the size of the whitelist decreases.
The acceptance rate for responses produced by the model is higher than the acceptance rate for the whitelist.
The percentage of good responses is higher than the percentage of bad responses.
The "Parallelism+URL" baseline achieves the highest performance for females.
The "Transformer-Single" baseline achieves higher performance for males.
The table shows the performance of off-the-shelf resolvers on the GAP development set, split by Masculine and Feminine, and Overall.
The off-the-shelf resolver developed by Wiseman et al. achieved the best performance in the overall category with a score of 0.88.
The Parallelism method performs better for males compared to females.
The Transformer-Single method has lower performance compared to other methods.
Table 8 shows the coreference signal of a Transformer model on the validation dataset, by encoder attention layer and head.
The coreference signal of the Transformer model at head H5 is different for encoder attention layers L5 and L0.
The table compares the predictions of the Parallelism and Transformer-Single heuristics over the GAP development dataset.
The Parallelism heuristic has a 48.7% accuracy in making correct predictions on the GAP development dataset.
Table 3 provides a performance comparison of a model with different values of m on two datasets.
The F1 score for m=4 is [BOLD] 0.571.
Our model outperforms all other models on both NYT10 and NYT11 datasets in terms of F1 score.
PCNN zeng2015distant achieves the highest recall score on the NYT10 dataset among all the models.
The model with window size (ws)=5 has the highest F1 score among all the models.
The model with standard attention has the lowest precision among all the models.
The table shows the category-wise performance with the default split of Flickr30k Entities for four different methods.
ZSGNet - Res50 (cls) has the highest overall performance among the mentioned methods.
The table compares the performance of different methods using VGG and Res50 networks on two different datasets, Flickr30k and ReferIt.
ZSGNet with Res50 network achieves a higher accuracy on the Flickr30k dataset compared to ZSGNet with VGG network.
ZSGNet with Res50 achieves the highest accuracy across all the unseen splits.
The accuracy of ZSGNet with VGG is lower than the accuracy of ZSGNet with Res50.
The ablation study in Table 6 investigates the performance of different variations of the Base Model (BM) on RefClef.
The addition of Focal Loss (FL) and image resizing improves the accuracy of the Base Model (BM) on RefClef.
Training scheme 6, which is "1 then Bio, EWC", achieves the highest BLEU score.
Training schemes involving the "Health" domain generally achieve higher BLEU scores compared to training schemes involving the "Bio" domain.
BI + IS outperforms the oracle unadapted model on the en-de News domain.
IS outperforms the oracle unadapted model on the es-en Health domain.
The "BI + IS" configuration outperforms both the "Uniform" and "Oracle model" configurations in terms of Test BLEU.
The rule-based model, Hybrid-EL-CMP1, has the highest precision among all the models.
The rule-based model, Hybrid-EL-CMP1, has a higher recall than the probability-based model, Hybrid-EL-CMP2.
"Add Logits+Expert" achieves the highest precision, recall, and F1 score among all selection methods.
"Concat Hidden" has slightly lower precision, recall, and F1 score compared to "Max Hidden" and "Add Hidden".
The method "NLP-Capsule" achieves the highest MAP score on the TREC QA dataset.
The method "HD-LSTM" achieves the highest MRR score on the TREC QA dataset.
The table includes results for the RCV1 and EUR-Lex datasets.
The table includes percentage improvements for the different methods.
The dataset distribution for Sub Task A - Task 9: Suggestion Mining from Online Reviews consists of 2085 training instances labeled as "Suggestion" and 6415 training instances labeled as "Non Suggestion".
The training dataset for Sub Task A - Task 9: Suggestion Mining from Online Reviews consists of 2085 instances labeled as "Suggestion" and 6415 instances labeled as "Non Suggestion".
The ULMFit* model outperforms all other models in terms of F1 score on both the train and test datasets.
The Multinomial Naive Bayes model performs the worst among all the models in terms of F1 score on the test dataset.
The team "OleNet" is ranked first in the SemEval Task 9: Sub Task A competition with a performance (F1) score of 0.7812.
The team "MIDAS (our team)" is ranked 10th in the SemEval Task 9: Sub Task A competition with a performance (F1) score of 0.7011*.
The AAS method with simulated + real training data has a lower Test WER (%) than the AAS method with simulated or real training data.
The FSEGAN method has a higher Test WER (%) than the AAS method with simulated or real training data.
The AAS (wAC=1, wAD=105) method has the lowest WER percentage and a DCE value of 0.303.
The Minimizing DCE method has the lowest DCE value of 0.269.
The method "AAS (wAC=1, wAD=105)" achieves the lowest WER (%) of 26.1% and a DCE of 0.462 on the CHiME4-simulated test set.
The method "Minimizing DCE" achieves the lowest DCE value of 0.392 on the CHiME4-simulated test set.
The "Wiki-Manual" system has the highest accuracy among all the systems.
The "Local-Manual" system has the highest recall among all the systems.
The table provides details on datasets used for experiments.
The table provides the average number of tokens per message for each dataset.
The "Our Approach" system has the highest accuracy, precision, recall, and F-measure compared to other systems.
The precision of the "Manual" system is 70.84%.
The system labeled as "[ITALIC] Our Approach" has the highest accuracy, precision, and F-measure.
The system labeled as "Transform" has the highest recall.
The Upsample system achieves the highest performance in terms of accuracy, precision, recall, and F-measure.
Our approach achieves an accuracy of 62.90%.
The system "Our Approach" has the highest accuracy and F-measure compared to the other systems.
The system "Transform" has the highest recall compared to the other systems.
Table 2 provides the average recall of diachronic analogy inference for the Gigaword and NOW datasets.
NOW dataset has a higher average recall at the top-10 position compared to the Gigaword dataset.
The F1 score is higher when the algorithm used is "Threshold" compared to "Baseline" for both Giga and NOW.
The COMER model achieves a joint accuracy of 88.64% on the WoZ2.0 dataset.
The MLP model achieves a joint accuracy of 83.24% on the WoZ2.0 dataset.
The DST models "StateNet_PSI" and "COMER" have higher joint goal accuracy than the baseline model "Baselines Mrksic et al. (2017)" on the WoZ 2.0 dataset.
The DST models "HyST (ensemble)", "DSTRead (ensemble)", "TRADE", and "COMER" have higher joint goal accuracy than the baseline model "Baselines Mrksic et al. (2017)" on the MultiWoZ dataset.
The joint domain accuracy for the COMER model is 95.52%.
The joint goal accuracy for the model with the "Order" ablation is 42.84%.
The accuracy of transferring between aspects is highest for the "Beer look+palate" source and "Beer aroma" target combination.
The accuracy of transferring between aspects is lowest for the "Beer look+aroma" source and "Beer palate" target combination.
The table compares the performance of three different models in a domain transfer from beer to hotel.
All three models perform well in terms of hotel cleanliness in the domain transfer from beer to hotel.
The GAN model achieves the highest scores for all four datasets (Inspec, Krapivin, NUS, KP20k) in the α-nDCG@5 metrics.
The Catseq-RL model performs better on the Krapivin dataset compared to the Inspec and KP20k datasets in the α-nDCG@5 metrics.
The table provides the percentage of female, male, and neutral gender pronouns for each category of occupation.
The average percentage of male gender pronouns is higher than the average percentage of female gender pronouns across all occupation categories.
The average percentage of female gender pronouns across all BLS occupation categories is 11.76%.
The percentage of neutral gender pronouns in the healthcare practitioners and technical occupation category is 15.116%.
The table contains data from different studies such as "Waseem and Hovy", "Davidson et al.", "Golbeck et al.", and "Founta et al."
The values in the "piblack" and "piwhite" columns represent the probabilities of a certain class being predicted as "black" or "white" respectively.
Table 1 shows the performance of different classifiers on various datasets.
The classifier by D. et al. has a precision score of 0.32 for the Hate class.
Table 2 presents the results of Experiment 1.
The table shows the probabilities of the "piblack" and "piwhite" classes.
The table presents experiment results for different datasets including "Waseem and Hovy", "Davidson et al.", "Golbeck et al.", and "Founta et al.".
The table shows the probability or score of the classes for different datasets.
The sparsemax method consistently outperforms the softmax and TVmax methods in terms of MSCOCO spice, MSCOCO cider, MSCOCO rouge L, MSCOCO bleu4, MSCOCO meteor, and MSCOCO rep↓ scores.
The TVmax method performs better than the softmax and sparsemax methods in terms of Flickr30k rouge L and Flickr30k meteor scores.
The TVmax method has the highest score in the "attention relevance" column.
The softmax method has a lower score than the sparsemax method.
The "sparse-TVmax" model achieves the highest score in the "Test-Dev Overall" column.
The "softmax" model achieves a score of 59.11 in the "Test-Standard Other" column.
The table shows the performance of different languages on three error generation algorithms.
Tamil language achieves the highest precision at 1 among all the languages in the table.
Telugu language achieves the highest Mean Reciprocal Rank (MRR) among all the languages in the table.
Hebrew has the highest suggestion time for ED=1 among all the languages.
Tamil has the lowest ranking time among all the languages.
The table provides comparison results for a public dataset.
Aspell achieves a precision of 60.82% at position 1, 80.81% at position 3, 87.26% at position 5, and 91.35% at position 10.
Danish has the highest percentage of detected sentences among all languages.
Thai has the lowest percentage of detected sentences among all languages.
The performance of the models trained on "Cars" and tested on "Retail" is 62.9.
The performance of the models trained on "Transport" and tested on "Food & Bev." is 59.3.
There are 9 different categories in the dataset.
The total number of complaints in the dataset is 1232.
The feature "not" has a higher correlation with complaints than the feature "!".
The feature "[URL]" has the same correlation with not complaints as the feature ".".
The LIWC feature "NEGATE" is strongly associated with complaints.
The logistic regression model with all features achieves an accuracy of 80.5% and an F1 score of 78.0%.
The neural network models (MLP and LSTM) do not have any values reported for accuracy, F1 score, or AUC.
The accuracy of the "Dist. Supervision + EasyAdapt" model is 81.2%.
The F1 score of the "Most Frequent Class" model is 39.1.
Table 8 shows the performance of models in Macro F1 on tweets from each domain.
The "Pooling" and "EasyAdapt" models have higher performance than the "In-Domain" model for the "Food & Beverage" and "Cars" domains.
GPT-2 + Multitasking has the highest validation accuracy among all the models in the table.
ULMFiT (no LM Finetuning) and ULMFiT have the same pretraining time.
Table 5 presents an ablation study on the effects of pretraining for multitasking-based and standard GPT-2 finetuning.
The results of the ablation study show that pretraining greatly accounts for almost half of the performance in both multitasking-based and standard GPT-2 finetuning.
Increasing the number of heads in the attention mechanisms improves performance up to 10 attention heads, after which there is no significant improvement in accuracy.
Increasing the number of heads in the attention mechanisms reduces the validation loss.
The second extended play by the artists of the mini-album Code#01 is called Code#02 Pretty Pretty.
Mastodon has more members than Hole.
HGN (ours) achieves the highest scores in all metrics (Ans EM, Ans F1, Sup EM, Sup F1, Joint EM, Joint F1) compared to other models.
The Baseline Model (Yang et al. 2018) has the lowest scores in all metrics (Ans EM, Ans F1, Sup EM, Sup F1, Joint EM, Joint F1) compared to other models.
The "Graph Recur. Retriever" model achieves the highest exact match (Ans EM) score among all the models listed in the table.
The "HGN (ours)" model achieves the highest F1 score on the joint task (Joint F1) among all the models listed in the table.
The model "Hier. Graph" outperforms the other models in terms of Ans F1, Sup F1, and Joint F1.
The model "Hier. Graph" achieves the highest Joint F1 score among all the models.
HGN (RoBERTa) achieves the highest scores in all three metrics: Ans F1, Sup F1, and Joint F1.
EPS (BERT-wwm) outperforms DFGN (BERT-base) in all three metrics: Ans F1, Sup F1, and Joint F1.
Table 7 shows the results of HGN for different reasoning types.
The "Sup F1" value is higher than the "Ans F1" value for the "comp-span" reasoning type.
Table 4 shows the BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.
The BLEU score of Dual2seq on the little prince data with automatic AMR annotation is 16.8.
The Dual2seq system achieves the highest BLEU and Meteor scores when using the full training data.
The OpenNMT-tf system has the lowest BLEU score when trained only with the NC-v11 data.
The RoBERTa model achieves the highest performance on the GLUE benchmark with a score of [BOLD] 89.4.
The ELMo model achieves the highest performance on the SQuAD 1.1 benchmark with a score of [BOLD] 89.9.
The F1 score for ACE05 is higher than the F1 score for SciERC.
The F1 score for GENIA is higher than the F1 score for BERT Finetune with CorefProp.
Our model performs better than the SOTA on SciERC for entity recognition.
The F1 score for BERT FineTune on ACE05 is 62.1.
The F1 score for +CorefProp on SciERC is 45.3.
SciBERT performs better on GENIA Entity than BERT.
BERT performs worse on SciERC Entity than SciBERT.
The F1 score of relation extraction on ACE05 dev set is higher when using a BERT context window size of 3 compared to a size of 1.
The F1 score of argument classification is higher when using BERT Finetune compared to BERT+LSTM.
Both "Artetxe et al., 2018b" and "Noise-aware Alignment" methods achieve the same performance for the En→It translation task.
The "Noise-aware Alignment" method outperforms the "Artetxe et al., 2018b" method for the En→De translation task.
The "Hier. Right Branch." approach has the highest performance in both the RST-DTtest and Instr-DTtest datasets.
The "Two-StageOurs(max)" approach performs better than the "Two-StageOurs(avg)" approach in the Instr-DTtest dataset.
DKRN (ours) achieves the highest success rate in both TGPC and CWC tasks.
DKRN (ours) outperforms all other systems in terms of success rate in the TGPC task.
DKRN (ours) outperforms other systems in both keyword prediction and response retrieval in TGPC and CWC datasets.
The response retrieval performance, measured by MRR, is better in the CWC dataset compared to the TGPC dataset for all systems.
The system "DKRN (ours)" has the highest success rate among all the systems.
The system "DKRN (ours)" has the highest smoothness score among all the systems.
Table 6 shows the results of the Human Rating on CWC.
According to the Human Rating on CWC, the retrieval strategy with the highest percentage of "Ours Better" is PMI with 54%.
The SA (S: 1,2,3 - B: 3) configuration achieves the highest performance on the evaluation set.
The SA (S: 1,2,3 - B: 1), SA (S: 1,2,3 - B: 2), and SA (S: 1,2,3 - B: 3) configurations have the same number of parameters.
SA (S: 3 - B: 2,4,6) has the highest evaluation set percentage among all the experiments.
SA (S: 3 - B: 1,3,5) has a higher parameter count than SA (S: 3 - M: 1).
The HAN models have higher ROUGE-1 R scores compared to other systems.
The LogReg system has a lower Sentence-Level F score compared to the HAN models.
The HAN model with redundancy removal achieves higher scores in all evaluation metrics compared to the HAN model without redundancy removal.
The StateNet_PSI model has the highest joint goal accuracy on the DSTC2 test set.
The Belief Tracking: CNN model has the highest joint goal accuracy on the WOZ 2.0 test set.
Table 5 shows the textual similarity scores for the EN → DE and DE → EN translation tasks using the FME and AME models.
The AME model outperforms the FME model in terms of the textual similarity scores for the EN → DE translation task.
The alignment method used for the "AME" model is different from the other models.
The "AME" model performs the best in terms of image to text retrieval at rank 1.
The AME method has an alignment rate of 66.91%.
The Mono method has a Text to Image R@1 score of 26.5.
The alignment for the "AME" model is 82.54%.
The "AME" model has the highest Image to Text R@1 score.
The alignment accuracy for the "AME" model is 92.70%.
The Image to Text R@1 score for the "FME" model is 40.7.
The table shows the difference between the averages of the similarities of pairs with same vs. different gender in Italian and German compared to English.
The similarities of pairs with same gender are higher in the original versions of both Italian and German compared to the debiased versions.
The difference in average rankings between same-gender and different-gender pairs in Italian is smaller than in German.
Table 6 provides the results on SimLex-999 and WordSim-353 in Italian and German, before and after debiasing.
The score for SimLex in German before debiasing is 0.343 and the score for WordSim in German before debiasing is 0.547.
Table 7 shows the results of cross-lingual embedding alignment in Italian and German, before and after debiasing.
The debiased embeddings have higher accuracy than the original embeddings in all language pairs.
The BERT distance measure has the highest ARI score among all the distance measures.
The TF-IDF distance measure has the lowest ARI score among all the distance measures.
The method "OD" has the highest scores for both ARI and Silhouette coefficient in all four categories.
The ARI score for the method "OD" in the category "Pornography" is 0.41.
The quality of opinion distance is higher for the "Seanad Abolition" topic compared to the baselines.
The quality of opinion distance is highest for the "Video Games" topic when the LSA feature is added.
The Absolute Opinion Distance measure performs better on the Seanad Abolition topic compared to the Video Games topic.
The Difference Function Opinion Distance measure shows a higher value for Seanad Abolition compared to Pornography.
The accuracy on the testing set is higher than the accuracy on the development set.
The macro F1 score on the testing set is lower than the macro F1 score on the development set.
The confusion matrix shows the number of predictions for each label in the testing set.
The model made the most predictions for the "Supporting" label in the testing set.
The ABSA SemEval 2014-2016 dataset includes texts in multiple languages including English, Spanish, French, Dutch, Russian, and Turkish.
The English dataset has the largest number of tokens and opinion targets in both the training and test sets compared to the datasets in other languages.
The precision for the combination of Local and Brown Yelp features in 2015 is 71.73.
The F1 score for the combination of Local and Clark Yelp Food 100-Clark Yelp Reviews 200 features in 2016 is 73.07.
Table 6 compares the multilingual results in terms of F1 scores for ABSA SemEval 2016.
The F1 scores in Table 6 are associated with different systems for each language.
The table displays the number of false positives and false negatives for different ABSA settings in the years 2014-2016.
The ABSA setting in Russian (ru) had 390 false positives in the year 2016.
The highest percentage for "Category no oov words" in the "Category Semantic" row is 65.50%.
The highest percentage for "with oov words" in the "Overall" row is 55.54%.
There are 13650 tuples in the semantic category of relations.
The "common_capital_country" relation has 42 pairs and 1722 tuples.
The "cc.el.300" category has the highest performance among all the categories in terms of semantics.
The overall performance without oov words is higher than the overall performance with oov words.
The category "Semantic" with no OOV words has the highest accuracy of 88.50% using the "cc.el.300" model.
The overall accuracy with OOV words is 71.29%.
Table 4 provides word similarity results for different models.
The model "gr_def" has the highest Pearson correlation coefficient among the models listed in Table 4.
Table 7 provides in-domain and out-of-domain evaluations for the pt and wb genres of the CoNLL test set.
The "deep-coref +EPM" model achieves a score of 60.74 on the out-of-domain LEA dataset.
The table shows the impact of different EPM feature groups on the performance metrics (MUC, B3, CEAF, CoNLL, LEA) on the CoNLL development set.
The +EPM feature group has higher performance metrics (MUC, B3, CEAF, CoNLL, LEA) compared to the -pairwise feature group on the CoNLL development set.
The "+EPM" configuration achieves the highest F1 score for the "deep-coref" system on the WikiCoref dataset.
The "+EPM" configuration improves the F1 score for the "deep-coref" system compared to the "ranking" configuration on the WikiCoref dataset.
Table 1 shows the impact of linguistic features on deep-coref models on the CoNLL development set.
The scores for MUC, B^3, CEAF^e, CoNLL, and LEA vary in each row of Table 1.
Table 2 shows the out-of-domain evaluation of deep-coref models on the WikiCoref dataset.
The "+EPM" extension improves the MUC R score of the "deep-coref" system.
The "ensemble" configuration improves the B3 F1 score of the "e2e" system.
Table 4 shows the lexicon member coverage (%) for different types and targets.
The coverage for the "lemma" type is higher than the coverage for the "target" type.
The "lemma" context consistently outperforms the "type" context across all VSMs.
The "dep" context consistently outperforms the "x+POS" context across all VSMs.
"+BoC (Wiki-PubMed-PMC)" performs the best among all features for both LR and SVM models in terms of F1 score.
"+ASM" feature performs better than "+Sentence Embeddings(SEs)" feature in terms of F1 score for the ANN model.
The "BoC(Wiki-PubMed-PMC) ANN" model has the highest F1 score among all the models.
The intra-sentential co-occurrence with ρ=0 for the "Threat(O,CF/TR)" relation type is 0.85.
Multi-News has the highest number of pairs compared to other datasets.
CNNDM has the largest vocabulary size compared to other datasets.
Table 4 provides the percentage of n-grams in summaries that do not appear in the input documents for different datasets.
The TAC11 dataset has the highest percentage of tri-grams in summaries that do not appear in the input documents.
The method "CopyTransformer" achieves the highest R-1 score of [BOLD] 43.57.
The method "Hi-MAP (Our Model)" achieves the highest R-SU score of [BOLD] 17.41.
Table 5 shows the performance breakdown of the PRKGC+NS model.
The "Shortest Path" model has the highest "Answerability Macro P/R/F" score.
The "PRKGC" model has the highest "Derivation Prec. BL-4" score.
The PRKGC+NS model has a higher accuracy than the PRKGC model.
The EntityGCN model has the highest accuracy among all the models listed in the table.
The ensemble performance is higher than the single model performance.
The performance decreases as we move from LSTM-400 to NO-HIGHWAY to SUBMISSION.
The score for "ep_2" is the highest among the three submitted runs.
The score for "ep_2" with correction is the highest among the three submitted runs.
The "Species" mention class has the highest F1 score.
The F1 score for the training set is higher than the F1 score for the test set.
The TCSum approach performs better on DUC'02 than on DUC'01.
The SRSum approach performs better on DUC'02 than on DUC'01.
Our ^σUx has significantly higher correlation values over all other approaches in the DUC'01, DUC'02, and DUC'04 datasets.
ASRL has lower correlation values compared to the other approaches in the DUC'01, DUC'02, and DUC'04 datasets.
The "SIF (DE-EN)" method achieves the highest Micro F1 score of 0.757 in Task B.
The "W2V (d=500)" method achieves a Micro F1 score of 0.756 in Task B.
The model with "W2V (d=500)" achieves the highest Micro F1 score.
The baseline model achieves a Micro F1 score of 0.882.
SIF (DE-EN) has the highest Micro F1 score among all the models.
SIF (DE-EN) performs better than SIF (DE) in terms of Micro F1 score.
The proportion of tweets with emoji is higher than the proportion of tweets with hashtags.
The emojis in the table are represented by their aliases.
The Δ% column represents the percentage change in the number of tweets containing emojis after removing them.
The table provides performance results on the Winograd and WinoCoref datasets.
The KnowComb system achieves a precision score of [BOLD] 76.41 on the Winograd dataset.
Table 8 provides performance results on ACE and OntoNotes datasets.
The "IlliCons" system performs better on the OntoNotes dataset compared to the "KnowComb" system.
The table presents the performance results of KnowComb system with Type 1 and Type 2 schema knowledge on WinoCoref dataset.
The performance of the KnowComb system is better when tested on Cat1 data for Type 1 schema and on Cat2 data for Type 2 schema.
The accuracy of applying Yelp BB to SST-2 is lower than the accuracy of SST-2 BB on Yelp.
The non-rejection accuracy and classification quality decrease as the rejection threshold increases.
Table 6 shows the Word Error Rate (WER) for adaptation of the TED-LIUM model without i-vectors and the Somali model using best path as supervision with varying fractions of the adaptation data.
The WER for the TED-LIUM dev set is 9.1 and the WER for the TED-LIUM test set is 9.0 in the ALL-LAT 100% adaptation scenario.
Table 4 shows the WER for adaptation of the MGB model to episodes in the longitudinal eval data.
Table 1 provides percent accuracies for humans and models on different languages and models.
The percent accuracy for the triphone English model is 88.8.
AIM achieves the highest scores in the relevance metrics (BLEU, ROUGE, and Average) compared to other models.
DAIM achieves the highest scores in the diversity metrics (Dist-1, Dist-2, and Ent-4) compared to other models.
AIM achieves the highest scores in all relevance metrics on the Reddit dataset.
DAIM achieves the highest diversity score (Diversity Dist-2) on the Reddit dataset.
The accuracy of sentence selection for Dyn (T+M) with a selection method of 1.5 is 84.9% on NewsQA.
The accuracy of sentence selection for Top k (T+M+N) with a selection method of 1.9 is 99.3% on SQuAD.
The Our selector with training techniques T+M and T+M+N achieves the highest accuracy on both SQuAD and NewsQA datasets.
The TF-IDF model achieves an accuracy of 81.2% on the SQuAD dataset.
Our selector has a faster inference speed than the TF-IDF method for both TriviaQA (Wikipedia) and SQuAD-Open datasets.
Our selector achieves a higher F1 score than the TF-IDF method for both TriviaQA (Wikipedia) and SQuAD-Open datasets.
The performance of the model on the SNLI dataset improves as the number of blocks or encoding layers increases, except for a slight decrease with 5 encoding layers.
The model achieves the highest performance on the Scitail dataset with 2 blocks and 3 encoding layers.
Table 6 presents the results of an ablation study on the development sets of different datasets.
The model without encoding information performs worse than the original model and the model with residual connections on the SNLI dataset.
Table 3 compares the normalization of query and key in N-SAN.
The Bilingual@4 score is higher when both the query and key are normalized in N-SAN.
Table 2 compares the performance of different normalization methods in NSA.
Table 4 compares various variants of GSA.
The query-dependent variant of GSA has the highest performance in terms of B@4, M, R, C, and S.
The "Transformer (Ours)" model outperforms the "VATEX" model in all metrics on the VATEX dataset.
The "+NSA" model outperforms the "Transformer (Ours)" model in all metrics on the VATEX dataset.
The threshold values for different tasks in Table 1 vary.
The maximum difference in F1-score for statistical significance is different for each task in Table 1.
The task "CoNLL 2003 - NER-De" shows the highest improvement in F1-score between the dev and test sets.
The task "ACE 2005 - Entities" has a low percentage of statistically significant differences in F1-score between the dev and test sets.
The task "CoNLL 2003 - NER-De" shows the highest improvement in F1-score between the dev and test sets.
The task "ACE 2005 - Entities" has a low percentage of statistically significant differences in F1-score between the dev and test sets.
Subset1 has better predictive performance than Subset2 and FB15K.
The optimal loss function affects the predictive performance of the data sets.
The table provides evaluation results on link prediction for different datasets.
The WN11 dataset has the lowest scores in all models.
The TransA model has the highest scores in all datasets.
There is no significant correlation between BLEU-1 and the ground-truth system ranking.
The chosen metric has a significant correlation with the ground-truth system ranking.
The table compares the performance of two different methods (ORG and SHF) on two different datasets (DURel and SURel).
The SHF method outperforms the ORG method for the DURel dataset.
The SGNS representation achieves the highest "best" and "mean" ρ scores among all the similarity measures for the DURel dataset.
The PPMI representation performs better than the other similarity measures in terms of both "best" and "mean" ρ scores for the SURel dataset.
The table provides mean ρ scores for CD across the alignments for the DURel and SURel datasets.
The mean ρ score for CD across the alignments is [BOLD] 0.621 for the OP+ alignment.
The table compares the performance of different methods (LBL (Baseline), LBL + Lazy, LBL + Periodic, and LBL + Retrofitting) on various tasks.
The LBL + Periodic method shows the greatest improvement in performance on the RG-65 and SYN-REL tasks.
Table 2 shows the absolute performance changes with retrofitting on different tasks.
Glove achieves a score of 89.7 on the TOEFL task.
Table 5 shows the results of different architectures with different pre-trained knowledge on CNN/DailyMail.
Adding GloVe to the SeqLab model improves the R-1 Baseline and R-2 Baseline scores compared to adding BERT or Newsroom.
The "C-LSTM-Att-w" approach has the highest performance in terms of accuracy, precision, recall, F1, and AUC scores among all the evaluated models.
The "OURS-Att" approach has a precision score of 0.8525 and a recall score of 0.9158.
Table 2 lists the performance of two different models: LSTM-CRF and LSTM-SIG.
LSTM-SIG has a lower RMS deviation compared to LSTM-CRF.
The model "WeGen" achieves the highest scores in all metrics compared to other models in Table 1.
Removing the pre-training step in the WeGen model leads to slightly lower scores in all metrics.
The DNN+RNN+CNN model has the lowest Word Error Rate (WER) among all the models.
The CNN model has a higher Word Error Rate (WER) than the RNN model.
Table 4 compares word error rates for CE and sequence trained unfolded RNN and DNN models with score fusion and joint modeling on Hub5'00.
The WER for the score fusion of CE models is higher than the WER for the score fusion of ST models.
The system "This system" uses SWB+Fisher+CH training data.
The system "Soltau et al." uses SWB training data.
The highest precision value in the table is 75.015%.
The highest value in the "C" column is 3.0.
The precision decreases as the values of λ1, λ2, λ3, λ4, and λ5 increase.
The number of support vectors (# SVs) increases as the values of λ1, λ2, λ3, λ4, and λ5 increase.
Table 2 shows the test set performance of the NBT-CNN model using three different word vector collections on DSTC2 and WOZ 2.0 datasets.
The GloVe word vectors show statistically significant improvement over the baseline xavier word vectors in terms of DSTC2 Goals, DSTC2 Requests, and WOZ 2.0 Goals.
The F1 score for the VarNDRR model is 50.56.
The table shows the performance metrics (accuracy, precision, recall, F1 score) of different models on the Com vs Other task.
The model "(J & E TACL536)" achieves the highest F1 score among all the models.
The model "VarNDRR" achieves the highest recall score among all the models.
The F1 score for the VarNDRR model is 29.54.
The "+ neg sampling" augmentation method improves the performance of the GCBiA model in both seen and unseen PPR3 metrics.
The "+ neg sampling" augmentation method improves the performance of the GCBiA model in both seen and unseen AOR metrics.
The table compares different models based on their performance on seen and unseen benchmarks.
The model "This work + Res-conn in gated unit (GCBiA)" achieves the highest performance on both seen and unseen benchmarks in terms of PPR3 and AOR.
The SA-WER for the "SOT-ASR + Spk-Enc + Inv-Attn" model is lower than the SA-WER for the "SOT-ASR + random speaker assignment" model.
The total SER for the "SOT-ASR + d-vec speaker identification" model is lower than the total SER for the "SOT-ASR + random speaker assignment" model.
The table presents the values of the "ASG" and "CTC" metrics for the "dev-clean" and "test-clean" datasets.
The "dev-clean" dataset has a higher value for the "ASG" metric compared to the "test-clean" dataset.
As the batch size increases, the values in the "CTC CPU", "CTC GPU", and "ASG CPU" columns also increase.
The values in the "CTC CPU" and "ASG CPU" columns are the same for each corresponding batch size.
The performance of CTC GPU increases as the batch size increases.
The performance of ASG CPU increases as the batch size increases.
Table 2 provides the LER/WER of the best sets of hyper-parameters for different feature types.
The LER/WER for the "test-clean" dataset using different feature types are: MFCC LER = 6.9, MFCC WER = 7.2, PS LER = 9.1, PS WER = 9.4, Raw LER = 10.6, and Raw WER = 10.1.
The accuracy of EARL with adaptive learning is higher than EARL without adaptive learning for both LC-QuAD and QALD datasets.
The accuracy of EARL with adaptive learning is higher than EARL without adaptive learning for the QALD dataset.
The time complexity for Brute Force GTSP is O(n^2), while the time complexity for LKH - GTSP is O(nL^2).
The accuracy for Brute Force GTSP and Connection Density is the same at 0.61.
The values of Rf based on Ri are consistently lower than the values of Rf based on C,H for all values of k.
The highest value of Rf based on C,H is 0.737, which occurs at k = 50.
The accuracy of EARL with adaptive learning is higher than the accuracy of EARL without adaptive learning for both LC-QuAD and QALD datasets.
The 2nd-order FOFE-FNNLM has the lowest perplexity among all the models listed in the table.
The perplexity of the n-gram FNNLM models decreases as the n-gram order increases.
The experiments in Table 2 involve different modalities, including text and image.
The F1 scores for aggression detection vary depending on the features and fusion used.
The total number of instances for each concept/code is the sum of the counts in the "Twitter" and "Tumblr" columns.
The count for the "aggression" code in parentheses is based on per-tweet majority votes.
The "New Human Generated" system outperforms the "Original Ground-Truth" system in all quality metrics for the Topical-Chat dataset.
The "Language Model" system achieves a higher overall quality score compared to the "LSTM Seq2Seq" and "KV-MemNN" systems in the PersonaChat dataset.
Table 1 presents inter-annotator agreement for various metrics in the Topical-Chat and PersonaChat datasets.
The Pearson correlation values for the "Overall Quality" metric are 0.7096 for Topical-Chat and 0.6603 for PersonaChat.
The USR (x = c) sub-metric has a higher Spearman correlation than the USR metric.
The USR - DR (x = f) sub-metric has a higher Pearson correlation than the USR metric.
The BERTScore (base) metric has a Pearson correlation of 0.3540 with the Overall Quality ratings.
The Skip-Thought metric has a Spearman correlation of 0.1181 with the Overall Quality ratings.
Table 3 provides information about the speech recognition model performance in WER.
The WER for "Fisher dev" is 25.7 and the WER for "Fisher test" is 23.2.
TACAM-BERT Base achieves the highest performance in the "two-class" classification task with a score of [BOLD] 0.81.
TACAM-BERT Large achieves the highest performance in the "three-class" classification task with a score of [BOLD] 0.69.
TACAM-BERT Base and TACAM-BERT Large achieve the highest F1 scores across all topics in the three-classes task.
TACAM-BERT Base and TACAM-BERT Large achieve the highest F1 scores in the Cloning topic.
Table 4 shows the topic-dependent cross-topic classification results for different methods and topics.
TACAM-BERT Base achieves the highest F1 scores for all topics in the three-classes classification task compared to other methods.
Table 6 presents language modeling test results for different models trained and tested on different datasets.
The HPYP-DP model has a lower perplexity score compared to the HPYP 5-gram model in both training and testing on WSJ.
The table shows the effect of including different context elements in the model conditioning contexts.
Including the context element [ITALIC] σ2. [ITALIC] w improves the UAS and LAS scores.
The "seq2seq + CNN" method has the longest wall clock time among all the methods.
The "ESPnet (PyTorch)" method has a shorter wall clock time compared to the "ESPnet (Chainer)" method.
The table compares different methods for the ASR task.
The table provides comparisons of the ASR systems using different evaluation metrics.
The "Uniform" processing method has the highest perplexity value for the "Finnish, 5k classes" subset.
The "Weighting" processing method has the lowest WER value for the "Estonian, 212k subwords, 5k classes" subset.
CLTC3 has the highest number of examples among CLTC1, CLTC2, and CLTC3.
UCLTC has the highest LIFG percentage among CLTC1, CLTC2, CLTC3, and UCLTC.
The Baseline Results for the Source and Target combinations are different.
The F-score for the E-G combination in the DCI method is higher than the F-score for the E-F combination.
The option "resto-paris-expen-frech-8stars?" has a value of 0.96 for the Layer 1 → [ITALIC] r1. 
The Layer 2 [ITALIC] z2 has a value of 0.12 for the statement "Sure let me find another option." 
The value in the cell "Layer 1 → [ITALIC] r1" corresponding to the sentence "Daniel went to the hallway." is 0.83.
The answer to the question "Where is the apple?" is "hallway" for all instances in the table.
The statement "Gertrude is a mouse" has the highest confidence level in the layer 1 reasoning process.
The answer "wolf" has the highest confidence level in the layer 1 reasoning process for the question "What is Gertrude afraid of?".
Table 4 presents the evaluation results of VOGNet in the GT5 setting by training and testing on SVSQ, TEMP, and SPAT.
Table 3 compares the performance of VOGNet against ImgGrnd and VidGrnd on different evaluation metrics.
VOGNet outperforms ImgGrnd and VidGrnd on all evaluation metrics.
Table 7 is an ablative study comparing gains from Multi-Modal Transformer (MTx) and Object Transformer (OTx) and Relative Position Encoding (RPE).
The combination of Object Transformer (OTx), Multi-Modal Transformer (MTx), and Relative Position Encoding (RPE) achieves the highest accuracy, visual accuracy, consistency, and semantic accuracy compared to other combinations.
Table 3 shows the total number of lemmatized words in the train set of ASRL.
The total number of lemmatized words in the train set of ASRL is 338.
Table 4 compares the performance of different models trained with GT5 and P100 in a P100 setting.
The VidGrnd model trained with P100 has a higher TEMP Acc compared to the VidGrnd model trained with GT5.
The accuracy of the models generally increases as the number of layers and heads in the Transformers increase.
VOGNet has the highest accuracy among all the models.
The minimum edit distance between the words "A" and "algorithm" is 4.
The minimum edit distance between the words "A" and "distance" is 0.
The minimum edit distance between "simple" and "algorithm" is 4.
The minimum edit distance between "algorithm" and "edit" is 2.
The minimum edit distance between the word "A" and the word "algorithm" is 0.
The minimum edit distance between the word "new" and the word "distance" is 0.
The minimum edit distance between the words "new" and "algorithm" is 4.
The minimum edit distance between the words "algorithm" and "distance" is 3.
The table shows the results of three different variants of the model on hypernym extraction.
The "End2End" model outperforms all other variants of the model in terms of accuracy on hypernym extraction.
The "Top-K=5 ST" model has a higher score than the "c|=3+KLD+REP" model.
The "SupGen | c|=3" model has a higher score than the "SupGen | c|=30" model.
Table 2 shows the effect in the encoder and decoder.
The "Our NMT Systems" perform better than the "Existing NMT Systems" in both the Zh-En and En-De tasks.
The "capsule-Transformer-Base" architecture performs better than the "Transformer-Base" architecture in both the Zh-En and En-De tasks.
The ASR scores are higher than the POLQA scores for all commands and SER ranges in Table 2.
The PLAY command has the highest accuracy for all SER ranges in Table 2.
Table 1 provides phone accuracy (%) for the noisy TIMIT database.
The "mix uni." noise model achieves the highest phone accuracy (%) in the ASR system.
As the spatial order increases, the FLOPs of the finite-difference TTI wave-equation stencil also increase.
The optimizations applied to the finite-difference TTI wave-equation stencil result in a reduction in FLOPs.
Table 6 shows the accuracy scores of the German-English models on the ContraWSD and MuCoW test suites.
The German-English models perform better on the ContraWSD 6+6 task when using the 7Ftoken+1L encoder heads.
The average rating for the "baseline" generated memes is lower than the average rating for the "our model" generated memes.
The model "FGA (F-RCNNx101)" achieves the highest performance in terms of MRR, R@1, R@5, R@10, and NDCG among all the models.
The model "5×FGA (F-RCNNx101)" outperforms the model "FGA (VGG)" in terms of MRR, R@1, R@5, R@10, and NDCG scores.
The model "9×FGA (VGG)" performs the best among all the models in terms of MRR, R@1, R@5, R@10, and Mean.
The model "CoAtt-GAN-w/ R inte-TF" outperforms the model "CorefNMN (ResNet-152)" in terms of MRR, R@1, R@5, R@10, and Mean.
Table 2 shows the performance on the question generation task.
The FGA model outperforms the SF-QIH-se-2 model in terms of MRR, R@1, R@5, and R@10.
The +CSH+PSH system achieves the highest average BLEU score across all three NIST datasets.
The +CSH+PSH system achieves higher BLEU scores than the Transformer system on all three NIST datasets.
The BLEU score increases as we add more components to the system.
The RIBES score increases as we add more components to the system.
The table shows the BLEU scores for Chinese-to-English (Zh-En), English-to-Chinese (En-Zh), and English-to-German (En-De) translation tasks for four different systems.
The system "+CSH+PSH" achieves the highest BLEU score for the English-to-Chinese (En-Zh) translation task.
ERNIE performs better than BERT on the FewRel dataset.
ERNIE performs better than BERT on the TACRED dataset.
ERNIE achieves the highest accuracy, macro F1 score, and micro F1 score among all the models.
NFGEC (Attentive) and NFGEC (LSTM) have similar accuracy, macro F1 score, and micro F1 score.
ERNIE achieves the highest F1 score among all the models.
UFET has a higher precision score than BERT.
BERTBASE and ERNIE have similar performance on the MNLI-(m/mm) task.
BERTBASE outperforms ERNIE on the SST-2 task.
ERNIE achieves the highest F1 score among all the models in the ablation study.
ERNIE outperforms BERT in terms of the F1 score in the ablation study.
Table 5 presents the fluency scores of different models on native and non-native reviews.
The overall fluency score for original reviews is higher than the overall fluency score for all the models.
The rate for fine-tuned GPT-2 is higher than the rate for pretrained GPT-2 on Yelp.
The rate for sentiment modeling is lower than the rate for mLSTM on Amazon.
The table shows word error rates of monolingual and multilingual ASR systems evaluated on the development sets for different languages.
The precision scores increase when VTLN is applied to the input features for the UTD and/or cAE systems.
The precision scores increase when VTLN is applied to the input features for the Gold pairs.
The table shows the word error rates of monolingual sgmm and 10-lingual TDNN ASR system evaluated on the development sets.
The word error rates for all languages in the monolingual sgmm system are lower than the word error rates in the multi-lingual TDNN system.
The "10-lingual BNFs" feature performs better than other features in ES, HA, HR, and ZH languages.
The "cAE-BNF gold" feature performs better than the "cAE-BNF UTD" feature in all languages.
The "2-Layer Bi-LSTM" parser achieves the highest UAS score on the development set.
The "Bi-LSTM" parser and the "2-Layer Bi-LSTM" parser achieve the same LAS score on the test set.
The "Bi-LSTM Hierarchical†" parser has the highest UAS and LAS scores among all the parsers.
The inclusion of tag embeddings in the parser leads to lower UAS and LAS scores.
The table provides test F-scores for constituency parsing on Penn Treebank and CTB-5 using different parsers.
The "Chinese beam" parser has lower F-scores for constituency parsing on both Penn Treebank and CTB-5.
The dimensions for the "Word" embeddings are smaller than the dimensions for the "Tags" embeddings.
The number of LSTM units for each direction is the same.
The table presents the experimental results on dropped pronoun recovery for four different models: SVMSL, SVMSS, SVMDL, and SVMDS.
The accuracy for dropped pronoun recovery is higher for Baidu Zhidao compared to OntoNotes 4.0.
Table 7 presents experimental results on zero pronoun resolution.
The F-score for the "overall" category in the "Automatic Parsing & Automatic AZP SOTA" column is 17.3.
Mathematics has the lowest ROUGE-L score among the three categories.
The number of sentences recognized for Mathematics is higher than the number of sentences recognized for People.
The number of BLEU assessable sentences for Mathematics is higher than the number of BLEU assessable sentences for People.
The table compares the performance of different systems for full diacritization.
Our system achieves a WER% of 6.0 for full diacritization.
The most common error type in the dataset is "Wrong selection" with a frequency of 215.
The error type "Affix diacritization error" accounts for 3.0% of the errors in the dataset.
The most common causes for the error "a ⇔ u" are POS error and subject vs. object confusion.
The error "i ⇔ a" is the second most frequent error among all the errors listed in the table.
The most common error in the dataset is the confusion between "a" and "u", with a count of 2,907 and a percentage of 28.4%.
The error of incorrect attachment between "i" and "u" is the second most common error in the dataset, with a count of 1,316 and a percentage of 12.9%.
The majority of errors in the Core Word Error Types for CA are caused by invalid diacritized forms.
Problems with the reference contribute to a small percentage of errors in the Core Word Error Types for CA.
Our system has lower error rates than other state-of-the-art systems.
Our system has lower MSA values than other state-of-the-art systems.
Table 1 provides information about the number of test documents, unique entities, and the mean amount of unique entities for different entity types in the TamperedNews dataset.
In the TamperedNews dataset, there are 34,051 test documents that contain persons, and the mean amount of unique entities in these documents is 4.03.
There are 322 documents in the News400 dataset that contain verified persons.
The average number of unique entities in articles containing any type of entity in the News400 dataset is 137.35.
The Random method outperforms PsC, PsG, and PsCG on the Persons category in terms of Collection Retrieval AP-clean.
The EsP method has a lower performance than Random on the Events category in terms of Collection Retrieval AUC.
The table shows the effect of self-attention (1-head) in each encoder.
The image encoder with self-attention (1-head) achieves a retrieval accuracy of 70.1 for sentence retrieval.
The "Proposed Hybrid CNN" model has the lowest WER on both the "dev04f" and "rt04" datasets.
The WER for the "Hybrid DNN" model is higher than the WER for the "Proposed Hybrid CNN" model on the "dev04f" dataset.
Table 3 shows the WER as a function of the number of hidden units.
The WER is 21.9 when using 128/256 hidden units.
Table 6 shows the results with different pooling types.
The WER for Max Pooling is 18.9, for Stochastic Pooling is 18.8, and for [ITALIC] lp pooling is 18.9.
The "Pooling in Time, Max" method and the "Baseline" method have the same WER of 18.9.
The "Pooling in Time, Stochastic" method and the "Pooling in Time, lp" method have the same WER of 18.8.
Table 9 provides information about the Word Error Rate (WER) with improved fMLLR features.
The WER with the proposed fMLLR + VTLN-warped log-mel+d+dd features is 18.3.
Table 10 provides the WER of HF Sequence Training + Dropout for different non-linearities and dropout configurations.
The ReLU non-linearity with dropout fixed for CG iterations has the lowest WER among all the non-linearities and dropout configurations.
The table shows the Word Error Rate (WER) on Broadcast News for three different models: Hybrid DNN, Old Hybrid CNN, and Proposed Hybrid CNN.
The Proposed Hybrid CNN model achieves the lowest Word Error Rate (WER) on the dev04f dataset.
The table compares the performance of two methods, MultiDDS and MultiDDS-S, based on their mean and variance of the average BLEU score for the Diverse group.
The MultiDDS-S method performs better than the MultiDDS method in terms of the average BLEU score for the Diverse group.
The method "MultiDDS-S" achieves the highest scores for both "M2O Related" and "O2M Diverse".
The "Ours" method outperforms the "Baseline" method in terms of both "M2O Related" and "O2M Diverse".
MultiDDS-S settings outperform the baseline for both M2O and O2M translations.
MultiDDS-S [High] setting performs better than the baseline.
The combination of Projection and BRNN-POS-H2 - OOV achieves the highest accuracy for both Italian MSC-IT-1 and Italian MSC-IT-2.
The GETALP model achieves the lowest accuracy for both French MSC-FR-1 and French MSC-FR-2.
The accuracy for French All words in the "Projection + BRNN - OOV" model is [BOLD] 85.6 and for Spanish OOV is [BOLD] 81.9.
The accuracy for German All words in the "Projection + SRNN" model is 77.0 and for Greek OOV is 74.6.
The H-NVDM models have the lowest perplexities across all three datasets (20-NG, RCV1, and CADE).
The LDA model has the highest perplexity on the 20-NG dataset.
G-VHRED performs the best in terms of activities, according to the F1 scores in Table 3.
H-VHRED performs the best in terms of entities, according to the F1 scores in Table 3.
The G-NVDM model shows higher word sensitivity for the words "time" and "today" compared to other words.
The H-NVDM-5 model shows higher word sensitivity for the words "summer" and "season" compared to other words.
The table shows the bootstrapping results on MLDoc with and without pretraining, trained on 1k/10k LASER labels.
The LASER code achieves higher performance than the random initialization (1k) and random initialization (10k) in all languages.
Table 2 compares zero-shot and supervised methods on MLDoc.
The MultiFiT method achieves the highest accuracy among all methods in the "Supervised (1,000 target language examples)" scenario.
Table 5 compares the results of MultiFiT with different pretraining corpora and ULMFiT, fine-tuned with 1k labels on MLDoc.
MultiFiT achieves the highest performance in the "de", "es", and "zh" languages.
Table 7 compares the tokenization strategies for different languages on MLDoc.
The tokenization strategy performs the worst for the Russian language on MLDoc.
The count for the "NN" tag is highest in the "NN" cell, with a count of 17,196.
The count for the "RB" tag is highest in the "RB" cell, with a count of 281.
The table shows the F1 scores of the chunking phase using different POS tags.
The F1 score for the test set using the SR+SR POS tag is [BOLD] 93.12.
Table 10 provides F1 scores calculated using the conlleval.pl script for NER taggers.
The F1 score achieved by SpeeRead is 10% below the state-of-the-art achieved by SENNA.
SpeedRead is the fastest NER tagger with a speed of 153,194 tokens per second.
SpeedRead is 13.9 times faster than Stanford.
The STOI scores for PC - gaps increase as the size of intrusion increases.
The PESQ scores for Time + Freq. decrease as the size of intrusion increases.
"Submission 1 (all features)" achieves the highest score among all submissions.
"Rclass1+ class2" performs better than "Pclass1+ class2" across all feature groups.
The F1-measure for class 1 (ADR) is highest for "Our best result" with a value of 0.446.
The precision, recall, and F1-measure for class 1 (ADR) using SVM-unigrams are 0.391, 0.298, and 0.339 respectively.
Submission 1 (all features) has the highest values for Pclass1, Rclass1, and Fclass1.
The feature template "TF-IDF" has the lowest F1 score of 41.5.
The feature templates "Max-NE", "Ne+Common", "Google Rank", and "In Quest" all have a decrease in F1 score compared to the "WebQA" feature template.
The "CompQ" system performs better than the "WebQA" system in terms of F1 score.
The "WebQA-Subset" system outperforms the "WebQA" system in terms of MRR score.
CD-Seq2Seq performs better than SyntaxSQL-con on the development set in terms of question match.
SyntaxSQL-con performs better than CD-Seq2Seq on the test set in terms of interaction match.
The Seq2Seq model has a higher BLEU score on both the development and test sets compared to the Template model.
The Pointer-generator model has a higher grammar correctness rate (LCR) on the test set compared to the Template model.
The table presents the accuracy of user dialog act prediction for two models: Majority and TBCNN-pair.
TBCNN-pair model performs better than the Majority model in user dialog act prediction on the test set.
The table shows the ablation results of different variations of the HMNet model on AMI's test set.
The HMNet model without the hierarchy feature performs worse in terms of ROUGE-1, R-2, and R-SU4 scores compared to other variations.
The model with the highest AMI ROUGE-1 score is HMNet (ours).
The model with the highest AMI R-2 score is PGNet.
The highest threshold boundary values for each dimensionality are marked as [BOLD].
The dimensionality values increase in increments of 100.
The log Mel feature has a value of 66.4 when the proportion of si284 is 1/16.
The proportion of si284 is 1/4 when the CPC feature has a value of 38.8.
Table 1 provides ASR results of APC with varying n during pre-training and different transfer learning approaches.
Table 4 provides speech translation results with BLEU scores.
Table 4 presents different methods used for speech translation.
Table 4 shows the results of all proposed causal models using stratified 10-fold cross-validation.
The combined system has an F1 score of 0.46.
The LSTM-CRF-TI(g) model achieves the highest performance on the CoNLL 2003 English NER dataset.
The LSTM-CRF-T model achieves a lower performance compared to the LSTM-CRF-TI(g) model on the CoNLL 2003 English NER dataset.
The model "LSTM-CRF-TI(g)" outperforms other models in all languages and settings.
The performance of the models improves from "LSTM-CNN-CRFMa and Hovy (2016)" to "LSTM-CRF-TI(g)" in the English E+A setting.
The macro-F1 score for the aggregated results is higher when using feature set combination 4 compared to using feature set combination 01234.
The macro-F1 score for homeschooling is higher when using feature set combination 4 compared to using feature set combination 0.
The table provides the class distribution of the gold data Toulmin corpus, approximated to the sentence level boundaries.
The class "O" has the highest relative percentage of sentences in the data, with 56.8%.
Table 1 presents the evaluation results of various models on both NMT and VQA tasks.
The model with Hard Attention and Sampled expectation has the highest VQA NLL value among all models.
The "Marginal Likelihood" model has the highest BLEU score of 33.29.
The "Minimum Risk Training" model outperforms all other models on the IWSLT 2014 German-English test set.
The "Hard Attention + Enum" model has the lowest entropy values for both NMT and VQA tasks.
The "Variational Relaxed Attention" model has the highest entropy value among all models.
The F1 score for the model trained on KBP2015 data is 0.693.
The F1 score for the model trained on KBP2015 + iFLYTEK data is 0.731.
The NExT method performs better on Relation Extraction (TACRED) than on Sentiment Analysis (SemEval).
The BiLSTM+ATT method performs better on Sentiment Analysis (SemEval) than on Relation Extraction (TACRED).
The NExT method achieves the highest F1 score for Relation Extraction and Sentiment Analysis, with scores of 75.8±0.8 and 62.8±1.9 respectively.
The PCNN method achieves higher F1 scores for Relation Extraction and Sentiment Analysis compared to the ATAE-LSTM and CBOW-GloVe methods.
As the confidence threshold for "S [ITALIC] u" increases, the accuracy of NLProlog also increases.
The accuracy of NLProlog without any additional information is 74.57.
The table presents the TACRED results on 130 explanations and 100 explanations.
The NEXT model achieves higher F1 scores on TACRED 130 explanations and TACRED 100 explanations.
The model "NEXT (E+S)" achieves the highest F1 score on SemEval 150 explanations.
The table provides results for different models on the Laptop dataset with 55 explanations and 70 explanations.
The precision for LF (E) on 60 explanations is 86.0.
The F1 score for NEXT (E+S) on 75 explanations is 76.4.
Table 12 presents BERT experiments on the Restaurant dataset using 45 and 75 explanations.
The NExT model achieves higher accuracy on the Restaurant dataset when using 75 explanations compared to 45 explanations.
Table 3 is an ablation study for the TAPA model, reporting F1 scores on test sets.
The F1 score for the full TAPA model with early fusion on the PAWS dataset is 42.2.
Table 1 provides summarization results for different models with their R1, R2, and RL scores.
GPT-2 (48-layer, zero-shot) has lower R1, R2, and RL scores compared to other models.
The table shows different representations used for document classification.
The "+ bottlenecks" representation achieves the highest accuracy in document classification.
The "L+Attn-Seq2Seq" model achieves the highest BLEU-4 score among all the generation-based approaches.
The "Pre+Seq2Seq" model has the lowest test loss among all the generation-based approaches.
Table 5 shows the BLEU scores of retrieval-based approaches.
The "Feature" approach has higher BLEU scores than the "Embedding" approach.
The retrieval-based approach with feature +I-Rerank has the highest "Equivalence" percentage for the "Intention" category.
The retrieval-based approach with feature +I-E-Rerank has the highest "Equivalence" percentage for the "Emotion" category.
The Multilingual model achieves the highest performance in terms of accuracy, F1-M score, Pearson correlation, and mean squared error in the EP dataset.
The Multilingual model achieves the highest performance in terms of accuracy, F1-M score, Pearson correlation, and mean squared error in the EP+BP dataset.
AlbertPT achieves the highest accuracy and F1-W score among all the models in Table 5.
Both BertPT and AlbertPT outperform araujo2016@sac and Multilingual in terms of F1-W score in Table 5.
The "Becker:2017:MEC:3063600.3063706" model does not have any values in the "6 classes Acc", "6 classes F1-W", and "Binary Acc" columns.
The "BertPT" model has the highest value in the "Binary F1-W" column.
The accuracy of the "Ours Q+Tuple+A-IMG" model is higher on the balanced training set compared to the unbalanced training set.
The accuracy of the "Blind-Q+Tuple" model is higher than the accuracy of the "Prior ("no")" model.
The accuracy of the "Q+Tuple+H-IMG" method on the unbalanced training set is 0%.
The accuracy of the "Q+Tuple+A-IMG" method on the balanced training set is [BOLD] 34.73%.
Table 3 presents an ablation study for 4 aspects of ROTS.
The highest value in the "ROTS + ParaNMT Vectors + Dependency Tree" row for the "STSB dev" column is [BOLD] 84.6.
Table 1 shows the results of weakly supervised models on the STS-Benchmark dataset.
The ROTS+ dependency tree model achieves the highest scores in both the Dev and Test columns.
The model type "ROTS + binary tree" achieves a similarity score of 78.7 on STS14.
The average similarity score across all STS datasets for the weighted average model type is 67.8.
The "SCC-mult" model has the highest fluency score.
The "Accuracy" score for the "Ground Truth" is 3.88.
The "Evaluation / Experiments" section has a higher proportion of negative references compared to positive references.
The "Approach / Method" section has a higher proportion of positive references compared to negative references.
The total number of instances in the SentiCiteDB dataset is 2100.
The number of instances in the test set of the SentiCiteDB dataset is 1983.
The "Combination" feature performs better than the "Only POS" feature in both SC-SVM and SC-Paum.
The "Combination" feature performs better than the "Only POS" feature in SC-Paum.
SC-SVM performs better than SC-Paum in terms of the overall F1 score.
SC-SVM performs better than SC-Paum in terms of the F3 score.
There are three test sets mentioned in Table 1: DICT01, BNS-1, and BNS-2.
Each test set in Table 1 has a different style or characteristic mentioned.
The LRT accuracy for the DICT01 test set is 0.992.
The APR difference for the BNS-2 test set is +0.058 (6.4%).
Table 4 shows the performance degradation in the exaggerated voice-overs when applying the optimized thresholds of the read-speech utterances.
The LRT ACC for BNS-1 is 0.813 and the LRT Δ is -0.117.
The sections in the table are ordered from highest to lowest based on the kappa score.
There is only 1 document in the Summary section.
The RST discourse segmenters perform better on the "News" domain compared to the "Medical" domain.
The "Neural" RST discourse segmenter performs better on the "Medical" domain compared to the "News" domain.
The postag "i" has higher P@50, P@100, P@500, and P@1000 values compared to the postag "a" and "v".
The table contains information about the Positive PageRank.
The Positive PageRank for the tag "a" at position 500 is 0.370.
The F1 score for finance is higher than the F1 score for entertainment and digital.
The precision for digital is consistently higher than the precision for finance and entertainment.
The "27k puhpum" senone set has better performance than the "9k" senone set for all model architectures.
The "BLSTM+ResNet+LACE+CNN-BLSTM" combination of model architectures has the lowest test WER for all senone sets.
As more inputs are added to the model, the perplexity on the devset and test set decreases.
As more inputs are added to the model, the word error rate on the devset and test set decreases.
The Nested Attention Hybrid Model performs better on segments of the set that contain OOVs compared to the Word NMT + UNK replacement and Hybrid model.
The Hybrid model has the highest overall score compared to the Word NMT + UNK replacement and Nested Attention Hybrid Model.
The training set contains 2,608,679 sentence pairs.
The test set contains 1,312 sentence pairs.
The Nested Attention Hybrid Model has the highest performance on the development set.
The Nested Attention Hybrid Model has the highest performance on the test set.
Table 7 provides the precision, recall, and F0.5 results on the "small changes" and "large changes" portions of the OOV segment in CoNLL-13.
The Hybrid model performs better in the "small changes" portion compared to the "large changes" portion.
Table 3 presents the Word Error Rate (WER) of three different LSTMLM lattice rescoring algorithms: Push-forward (Algorithm 1), LSTM State Pooling (Algorithm 2), and Arc Beam (Algorithm 3).
The WER (%) for the "Default setting" condition is 12.8.
The "pretrain + CNN + SAP" system has the best performance in terms of EER, minDCF08, and minDCF10.
The "pretrain + CNN + SAP" system has the best performance in terms of minDCF10.
Table 6 shows the CER (%) on HKUST datasets compared to previous works.
The model "Characters-D1024-H16 (speed perturb)" achieves the lowest CER (%) on the HKUST datasets.
The baseline classifier "Same" has an accuracy of 69.26.
The "MaxEnt-2C" classifier has a precision of 73.95, recall of 90.99, F1 score of 81.59, and accuracy of 71.56.
The F1 score for "Graph-cut" is higher than the F1 score for "ILP" in the "Top-3 at SemEval-2015 Task 3" system.
The accuracy for "Graph-cut" is higher than the accuracy for "ILP" in the "Global Inference Classifiers" system.
Increasing the training fraction improves the classification accuracy for all testing fractions.
The highest classification accuracy is achieved when the training fraction is 1.0 and the testing fraction is 0.2.
The statistical approach has higher precision in the computer science (CS) area compared to the biomedical (Bio) area.
The hybrid approach has a higher yield in both the computer science (CS) and biomedical (Bio) areas compared to the other approaches.
Table 4 displays the Bag of Concepts F1 score of the baseline and neural model on two curated datasets.
The baseline model achieves a score of 54.2 on the Bio dataset.
Table 2 presents the precision, recall, and F1 scores for different methods on Chinese GALE test data.
DiscAlign (NER) method achieves the highest F1 score among all the methods listed in Table 2.
DiscAlign and DiscAlign (BPE) achieve higher F1 scores on Arabic GALE test data compared to other methods.
The average attention scores for Avg. attention. and Avg. attention (BPE) are lower compared to other methods on Arabic GALE test data.
The table provides F1 results on OntoNotes test for systems trained using FastAlign and DiscAlign methods.
The F1 scores for both FastAlign and DiscAlign methods are higher when trained on 53K data compared to when trained on 36K data.
The table presents the performance scores and sentences per minute for four different models: Human, DiscAlign, Hu. (NER), and DA (NER).
The Human model achieves the highest F1 score among the four models mentioned in the table.
The R-2 score increases as the training data size increases.
The Train Speed increases as the training data size increases.
The model "W2T-DGGNN" outperforms all other models in terms of "Dist-1", "Dist-2", "Acc", and "Macro-F1" metrics.
The model "HRED" achieves the highest BLEU-4 score among all the models.
The "W2T-DGGNN" model has the highest BLEU4 score among all the models.
The "HRED" model has the highest BERTScore among all the models.
The human passing score is the same for both the valid set and the test set.
SeaReader has a higher accuracy than both Neural Reasoner and R-NET on the MedQA task.
The "transformerxl" model has the lowest bits per byte value among all the models in Table 2.
The "shazeer2017outrageously" and "shazeer2018mesh" models use different segmentation methods.
The MLE Reimplementation has a lower BLEU score compared to MLE.
The + Risk method has a higher BLEU score compared to MLE.
Table 1 shows the test accuracy in terms of BLEU on IWSLT'14 German-English translation with various loss functions.
The test accuracy of the model proposed by Huang et al. (2017) with language modeling (+LM) is 29.16.
The table shows the validation and test BLEU for loss combination strategies.
The validation BLEU score for the weighted combination strategy is 32.85.
Table 3 shows the effect of initializing sequence-level training (Risk) with parameters from token-level likelihood (TokNLL) or label smoothing (TokLS).
Online generation performs slightly better than offline generation for both the valid and test datasets.
Offline generation performs slightly worse than online generation for both the valid and test datasets.
The highest score for Rouge-1 (RG-1) is 36.96 for "+ Risk RG-1" in Table 6.
Table 6 shows the accuracy on Gigaword abstractive summarization in terms of F-measure Rouge-1 (RG-1), Rouge-2 (RG-2), and Rouge-L (RG-L) for different approaches.
Our RCN model outperforms all other methods in terms of F{}_{1} score.
The precision of the Our RCN model is the highest when using "Word + Entity Type*" as input.
The table displays the performance of an automatic severity classification method.
The precision, recall, and F1 score for severe stenosis are 75.00%, 90.00%, and 81.82% respectively.
Table VI compares different input features with different routing iterations.
The highest precision score in the "Word Only P" column is 95.14.
The table compares the interest of Uni/Bi-LSTMs and Capsules.
The Precision, Recall, and F1 score for All Bi-LSTMs are 94.76, 97.60, and 96.16 respectively.
Table 2 describes the results of across data set experiments.
The "CNN" model performs the best in the "Youtube" dataset for the "Visual" modality.
Table 1 shows the results on three word similarity datasets.
The original RG65 dataset has a score of 75.63.
The perplexity on the original validation set for the PTB dataset with AWD-LSTM without finetuning is 60.7.
The perplexity on the test set with FRAGE for the WT2 dataset with AWD-LSTM and continuous cache pointer is 51.0.
The "Transformer Big" method performs better than the "Transformer Base" method on the WMT En→De task.
The "Transformer with FRAGE" method performs better than the "ConvS2S+Risk" method on the IWSLT De→En task.
The "Transformer Base" model and the "Transformer Base with FRAGE" model achieve higher BLEU scores on the WMT En→De task compared to the other models.
The "Transformer" model and the "Transformer with FRAGE" model achieve higher BLEU scores on the IWSLT De→En task compared to the other models.
The TreeLSTM model outperforms the MLP, MLPKernel, and MLPShared models in terms of F1 score.
Models using the Softmax loss function achieve higher F1 scores compared to models using the Margin loss function.
DIORA+PP has the highest F1 score among all the models in the table.
PRPN-UP has the lowest F1 score among all the models in the table.
PRPN-UP, PRPN-LM, and DIORA models have higher F1 scores compared to Random and Balanced models.
The DIORA+PP model achieves the highest F1 score among all the models.
The NP phrase type has the highest count among the 10 most frequent phrase types.
The highest recall value for each phrase type is indicated by bolding in the table.
The ELMo model achieves the highest P@1 score on the CoNLL 2012 dataset compared to other models in the table.
The F1 score is higher in FQuAD1.0-dev compared to FQuAD1.1-dev.
The EM score is higher in FQuAD1.1-test compared to FQuAD1.1-dev new samples.
The percentage of "Location" answer type is higher in FQuAD1.1 compared to SQuAD1.1.
The percentage of "Common noun" answer type is higher in FQuAD1.1 compared to "Other proper nouns".
The "spigot" model achieves the highest accuracy of 86.3% on sentiment classification on Stanford Sentiment Treebank.
The "pipeline" model achieves the highest accuracy of 85.7% on sentiment classification on Stanford Sentiment Treebank among the "BiLSTM", "pipeline", and "ste" models.
The accuracy of the "Capital-World" relation is highest with GloVe Word dimensionality 300 and DAWT Entity dimensionality 300.
The average accuracy is highest with GloVe Word dimensionality 200 and DAWT Entity dimensionality 1000.
The accuracy of P3 increases when trained and evaluated with labeled logical forms, food webs, or both.
The increase in accuracy of P3 is higher when trained and evaluated with both labeled logical forms and food webs compared to training and evaluating with only one of them.
The table shows the accuracy of different models under three different supervision settings: Supervision QA, Supervision QA+E, and Supervision QA+E+LF.
The tfidf+x_conc representation outperforms the hash+x_conc representation across different dimensions of the distributed representations.
The tf-idf representation performs better than the hash representation across different sizes of the training data.
The RAS-Elman model achieves a Rouge-2 score of 28.97.
The big-words-lvt2k-1sent model achieves a Rouge-2 score of 9.46.
Table 1 provides information about different Read-Again models and their performance.
The size of the decoder vocabulary for the Ours-LSTM model is not specified in Table 1.
Table 3 shows the parsing accuracies of Multilingual (Univ→Univ) on different languages.
The parsing accuracies for the language PT are higher than for the language FR.
The table shows the SMTL results for Swedish without sharing BiLSTM(chars).
The UAS for Swedish is 86.79.
The "context encoder (previous sentence)" model achieves the highest BLEU score among all the models.
The BLEU score of the "context encoder (next sentence)" model is lower than the BLEU score of the "context encoder (previous sentence)" model.
The table shows the performance of CoreNLP and our model's attention mechanism compared to human assessment for examples with at least 1 noun in the context sentence.
Our model's attention mechanism has a performance of 53% for examples with at least 1 noun in the context sentence.
The FeHED model has the highest second-person rating in terms of persuasiveness.
The HED+RNN model has the highest third-person rating in terms of naturalness.
Baseline 2 performs better than Baseline 1 on the System Test Set for all languages.
Kannada has a lower slot error rate than Navajo, Spanish, and Turkish.
The embedding dimension used in the inflection models for Task 0 and Task 2 is 256.
The inflection models for Task 0 and Task 2 use 4 encoder layers.
Table 2 provides macro-averaged results over all languages on the official development and test sets for Task 0.
The Base model is used as a comparison for the different sub-models.
The Baseline system achieves the highest score on the official development data.
The Trm-PG system outperforms the Trm system on the official development data.
The "Zero-shot +TR" model outperforms the "Zero-shot" model in terms of average F1 score for all domains.
The "Zero-shot Coach" model performs better than the "Zero-shot RZT" model in terms of F1 score for the "GetWeather" domain.
Table 2 shows the averaged F1-scores for seen and unseen slots over all target domains.
The Coach+TR model achieves the highest F1-scores for both seen and unseen slots.
Table 4 shows the PPL and relative WERRs (%) with varying floor interpolation weights for the translation component in the 180 hour setup.
As the floor interpolation weight increases, the WERR % also increases.
Table 2 presents different NMT adaptation strategies.
The combination of SIF selection, Rescoring, and SLM score - top 75% achieves the lowest PPL and highest Relative WERR %.
The Error Detection Precision for + [ITALIC] Elm is lower than the Error Detection Precision for + [ITALIC] Eaes.
The Error Detection Cost for + [ITALIC] Eged is higher than the Error Detection Cost for + [ITALIC] Eaes.
The Eaes model has a lower cost than the Elm model.
The Eged model has a higher Spearman correlation than the Elm model.
The table presents the results of a control experiment on the zero-shot performance on the Amazon German test set.
The + Freeze Encoder model achieves the highest performance score on the Amazon German test set.
The proposed model with the addition of a pre-trained encoder achieves higher accuracy on all datasets compared to the proposed model without a pre-trained encoder.
The state-of-the-art model achieves higher accuracy on the SST (En) dataset compared to the proposed model with a freeze encoder.
The proposed model with the pre-trained encoder performs better on all test sets compared to the proposed model without the pre-trained encoder.
The proposed model with the freeze encoder performs better on all test sets compared to the proposed model without the freeze encoder.
Our best zero-shot Encoder-Classifier achieves the highest score of 73.88 on the SNLI (Fr) dataset.
The INVERT model performs worse with a score of 62.60 on the SNLI (Fr) dataset compared to other baselines.
The "Symmetric data (full)" parallel data type for NMT achieves better performance on SNLI (En) compared to the "Symmetric data (half)" and "Asymmetric data (half)" parallel data types.
The "Symmetric data (full)" parallel data type for NMT achieves better performance on SNLI (Fr) compared to the "Symmetric data (half)" and "Asymmetric data (half)" parallel data types.
Table 6 shows the zero-shot analyses of classifier network model capacity for Simpler classifier and Complex classifier on SNLI (En) and SNLI (Fr).
The performance of the Complex classifier on SNLI (En) increases as the number of layers in the encoder component increases.
Table 7 shows the effect of parameter smoothing on the English SNLI test set and zero-shot performance on the French test set.
The accuracy on the English SNLI test set increases as the smoothing range increases.
The actual value for the human-written test in the empirical test is 119.
The actual value for the machine test using RevGAN is 78.
The log-likelihood value for the "RevGAN+CD+SA+PD" model is -34305.
The BLEU-4 score for the "SeqGAN" model is 15.06%.
The actual value for the human-written test in the empirical test is 119.
The actual value for the machine test in the empirical test using RevGAN+PD is 62.
Table 2 shows the results comparison of different models with average and standard deviation of five runs.
The baseline model has an average exact match (EM) score of 62.99 with a standard deviation of 0.16 in answer span prediction.
Table 1 provides a comparison of results with average and standard deviation of five runs for two models: baseline and GSN.
The GSN model performs slightly better than the baseline model in terms of Exact Match (EM) score.
Table 5 shows the results on the FEVER development and test sets.
The GSN model outperforms the baselines in terms of accuracy and FEVER scores.
There are four different models in the table: LVG-D-16, LVG-G-16, GM-LVeG-D, and GM-LVeG-S.
GM-LVeG-D has a higher token accuracy for German POS tagging compared to LVG-D-16.
GM-LVeG-S achieves the highest F1 scores for both "dev (all)" and "test (all)".
LVG-D-16 achieves the highest exact match score for sentences with a length of 40 or less.
LVG-D-1 performs better on token accuracy than sentence accuracy for German data.
GM-LVeG-D performs better on sentence accuracy than token accuracy for English data.
The majority of sentences in the dataset are labeled as NFS.
The percentage of sentences assigned to the CFS class is 23.87% and the percentage of sentences assigned to the UFS class is 10.45%.
The number of parameters in the discriminator increases as we move from "Discriminators Linear" to "Discriminators BiLSTM Big" to "Discriminators UniT" to "Discriminators BiT" in both "embed." and "others" rows.
The total number of parameters is the sum of the numbers in the "embed." and "others" rows.
Table 7 provides information about cross-corpora generalization accuracy using a TransfBig generator and UniT discriminator.
The accuracy for the CCNews + Wiki combination is 81.0.
The accuracy of the discriminator on the Med (380) top-k test set is higher for BiT cross-architecture compared to BiT in-domain.
The accuracy of the discriminator on the Big (762) temp=1 test set is higher for BiT in-domain compared to TF-IDF∗ in-domain.
Table 1 presents the performance of automatic feature selection compared to prior work.
The constrained model achieves an F1 score of 64.6.
The feature selection accuracy for the dataset trained and tested on PB is 61.7.
The feature selection accuracy for the dataset trained on FN and tested on PB is 74.8.
As the number of features used for argument identification increases, the performance of FrameNet (FN) also increases.
As the number of features used for argument identification increases, the performance of Propbank (PB) also increases.
The performance of "Ours (Full)" is better than the performance of "Ours (LocAttEL)" on all metrics except for BBN Accuracy.
The accuracy of "AFET" is lower than the accuracy of "AAA" and "NFETC".
The GELU activation function has the lowest perplexity on both the training and dev sets.
The perplexity values of the activation functions are similar on the training set.
The LSTM model achieves a WER of 2.1%.
The former model achieves a WER of 54.2%.
Table 8 presents the Word Error Rates (WERs) for attention-based models on the LibriSpeech 960hr dataset.
The attention-based model with the Transformer architecture achieves a WER of 35.9% on the LibriSpeech 960hr dataset.
Models with sinusoidal positional encoding have higher perplexity values compared to models without positional encoding.
As the number of layers (L) increases, the perplexity values decrease.
The Hierarchical-Shared Model (GloVe) achieves the highest F1 score for FG-NER.
The table shows the performance improvement of named entity types when ELMo is used.
ELMo improves the performance of named entities compared to GloVe.
The predicted POS accuracy for the Ar-He language pair is 92.48.
The BLEU score for the Fr-En language pair is 95.55.
When attention is used, the POS accuracy for the encoder is higher compared to when attention is not used.
When attention is used, the BLEU score for the Arabic-English translation is higher compared to when attention is not used.
The table shows the POS tagging accuracy using word-based and char-based encoder/decoder representations.
The char-based encoder achieves a higher POS accuracy compared to the word-based encoder.
Table 5 presents the accuracy of POS and morphology predictions using word- and char-based representations from different layers of *-to-En systems.
The accuracy of morphology predictions for German (*-to-En systems) is highest in Layer 1 compared to Layer 0 and Layer 2.
The input feature/embedding size for the Transformer encoder is 512.
The size of Dense layer 2 in the Transformer encoder is 1024.
The FullAttn T-T (Ours) model has the lowest WER with language modeling on both clean and other test sets.
The Hybrid model does not have a specified parameter size.
Table 3 provides information about the limited left context per layer for the audio encoder.
As the number of "Audio Mask left" decreases, the WER (%) on the Test-clean dataset also decreases.
As the number of right context masks decreases, the WER on both Test-clean and Test-other increases.
The WER on Test-clean is consistently lower than the WER on Test-other for all configurations.
Decreasing the value of Label Mask left leads to a decrease in WER (%) for both Test-clean and Test-other.
The WER (%) for Test-other is higher than the WER (%) for Test-clean.
The "BERT pip. (contemporaneous)" system has the highest exact match accuracy in answering questions among all the systems in Table 2.
The "Entity-centric BERT Pipeline" system has a higher exact match accuracy in supporting facts compared to the "CogQA Ding et al. (2019)" system.
The GoldEn Retriever setting outperforms the Single-hop query and HotpotQA IR settings in terms of Ans F1, Sup F1, and R@10.
The GoldEn Retriever setting achieves a higher R@10 score compared to the Single-hop query and HotpotQA IR settings.
The Athene model achieves a score of 93.55 on the OFEVER dataset.
The UCL MRG model does not have a score available for the OFEVER dataset.
As the threshold increases, the precision of GEAR decreases while the recall increases.
The F1 score of GEAR is highest at a threshold of 10^-3.
The highest label accuracy on the difficult dev set is achieved with ERNet layer 2 and evidence aggregator Mean.
The label accuracy generally increases with the increase in ERNet layers.
The table shows the results of two models: SRNN and CTC.
The table presents the results of two types of acoustic features: FBANK and fMLLR.
The table shows phone error rates of baseline SRNN and CTC models.
The table compares the performance of models with different dimensions (128 and 250).
The "Ours, beam 1" method has the lowest decoding time for all language pairs.
Table 2 shows the BLEU scores for English-to-German translation for different beam sizes and feature sets.
As the beam size increases, the BLEU score for English-to-German translation also increases.
The coverage ratio on the test set increases as more words are added to the vocabulary.
The coverage ratio on the test set is higher for summarization compared to simplification.
CoRe performs better than LEAD, Moses, and ABS in terms of informativeness in the Summarization task.
CoRe generates less copied text compared to LEAD, Moses, and ABS in the Simplification task.
Table 4 shows the PDP and PCFP results for all languages and models, averaged over 4 runs.
The PDP F_{\mathrm{grid}} scores are higher than the PDP F_{\mathrm{par}} scores for all languages and models.
Table 7 shows benchmark variations for Arabic nouns and Latin nouns.
The PDP F_{\mathrm{grid}} score is higher for the larger corpus compared to the smaller corpus for both Arabic nouns and Latin nouns.
The number of reference chains, dialogue segments, and image types increases from the train split to the test split.
The train split has a higher number of targets compared to the val and test splits.
Table 6 presents the results for target images in the validation set for three different models: "No History", "History", and "No image".
The model "LEMON" achieves the highest F1 score of 62.19% on the Weibo NER dataset.
The models [peng2016improving], [he2017unified], and [zhang2018lattice] do not have reported precision, recall, or F1 scores on the Weibo NER dataset.
The F1 score for the "Bi-RNN + Lex" model in the Transformer architecture is 78.04.
The precision for the "Auto FOFE" model in the Baseline architecture is 61.75.
The model [zhang2018lattice] achieved the highest F1 score of 93.18% on the MSRA dataset.
The model LEMON achieved the highest precision score of 95.39% on the MSRA dataset.
The model "LEMON" achieves the highest F1 score of 94.82% on the Resume NER dataset.
The model "char+bichar+softword{\dagger}" achieves a precision score of 94.53% on the Resume NER dataset.
The models [wang2013effective], [che2013named], [yang2016combining], [zhang2018lattice], and LEMON are evaluated on the OntoNotes-4 dataset.
The F1 scores for the models [wang2013effective], [che2013named], [yang2016combining], LEMON, and [zhang2018lattice] are provided.
The recall rate at 1 for the "Random (Marín et al., 2018)" method is different from the recall rate at 1 for the "JNE (Marín et al., 2018)" method.
The recall rate at 10 for the "IA" method is different from the recall rate at 10 for the "AdaMine (Carvalho et al., 2018)" method.
There are 3711 instances belonging to the "Present" class in Dataset-I Train.
There are 5 instances belonging to the "Hypothetical" class in Dataset-II Test.
Dataset-I has a higher maximum value and a lower minimum value compared to Dataset-II.
Dataset-I has a higher mean value compared to Dataset-II.
The baseline model outperforms the scope localization model for all assertion classes in Dataset-I.
The scope localization model performs better than the baseline model for the "Possibility" assertion class in Dataset-II.
The precision for the ngram Bag-of-words baseline sentence embeddings performance on the DisSent training task is 40.2.
The precision for the discourse marker "and" in the Books 5 dataset is 71.4.
The F1 score increases as we move from GloVe-bow to Ngram-bow to BiLSTM to BERT.
The accuracy increases as we move from GloVe-bow to Ngram-bow to BiLSTM to BERT.
The LSMTL model achieves the highest scores in all the tasks compared to the other models.
The InferSent model performs worse than the LSMTL model on the SICK-E task.
The model "DisSent Books ALL" has the highest IMP score of 42.9 among all the sentence encoder models.
The model "BERT + DisSent Books ALL" has the highest MVU score of 81.8 among all the fine-tuned models.
The precision for the discourse marker "and" is higher when considering 8 books compared to considering 5 books.
The overall accuracy is higher when considering 5 books compared to the average accuracy.
The LC-BLSTM model achieves an evaluation score of 12.9.
The Transformer model achieves a development score of 18.3.
The models used in the experiment are either "Linear" or "VGG".
The size of the models used in the experiment ranges from 19.4 million to 34.7 million parameters.
Table 2 shows the results of three different models: Transformer, [EMPTY], and BLSTM, with their corresponding values for "IC", "Size (M)", "N", "dk", and "dev".
The table provides results of streaming Transformer models, including Transformer, Transformer-XL, BLSTM, and LC-BLSTM.
Table 3 presents the results of ablation tests on different architecture choices using the MTMSN model.
The MTMSN model without question/passage vectors has lower EM and F1 scores compared to the other variations.
Table 2 presents the results of ablation tests of base and large models on the DROP dev set.
The F1 score for the model without negation is 70.9.
Table 4 provides a breakdown of the performance of NABERT and MTMSN based on different gold answer types.
The NABERT model has an EM score of 0% and an F1 score of 22.7% for the "Multi Span" answer type.
The table provides a performance breakdown of NABERT and MTMSN by predicted answer types.
NABERT performs better on the Span and Add/Sub question types compared to the Count and Negation question types.
The average Spearman's correlation for all approaches in the Phrase Similarity dataset is 0.5333.
The Spearman's correlation for the WS-REL dataset in the Phrase Similarity dataset is 0.6662.
The average number of pairs not found in the Wikipedia Miner dataset is 217.
The Spearman's correlation value for the ConVec (Heuristic) dataset is 0.3612.
The total number of sentences in the dataset is 383,449.
In each source, the number of unique tokens is smaller than the total number of tokens.
The table shows the splits of the Benchmark Evaluation Parallel Data into three types: Igbo-English, English-Igbo, and Total.
The Igbo-English data in the Benchmark Evaluation Parallel Data is sourced from "https://www.bbc.com/igbo" and the English-Igbo data is mostly sourced from local newspapers such as "Punch".
The IG-EN split has more data points in both the development set and the test set than the EN-IG split.
The EN-IG split has more data points in the hidden test set than the IG-EN split.
BERT + STVF has a higher accuracy than BERT on SICK.
The accuracy of the Cap2Both 2018:NAACL_sentence_visual model on SICK is 81.7%.
The table shows the performance of different models on the Dev and Test datasets.
The Integrated TriAN + ConceptNet model performs better on the Test dataset compared to the Dev dataset.
TriAN performs better than Integrated on the "what" question type.
Integrated performs better than TriAN on the "who" question type.
The model was trained on both MSCOCO and Flicker 30k datasets.
The model was tested on both MSCOCO and Flicker 30k datasets.
The BERT model with STVF achieves the highest test accuracy on SNLI among all the models listed in Table 4.
The ESIM model with ELMo achieves a test accuracy of 88.7% on SNLI.
Apt performs better on the MEN dataset without DI compared to the MEN dataset with DI.
SimLex-999 performs better with DI compared to WordSim-353 (rel) without DI.
The table displays the feature spaces for the lexemes "white" and "clothes".
The offset feature for the lexeme "white" is 1.
The first two rows of the table have empty cells in the "Composition by union Distributional Features" and "Composition by union Co-occurrence Count" columns.
The last two rows of the table have the same values in the "Composition by intersection Distributional Features" and "Composition by intersection Co-occurrence Count" columns.
Table 4 compares the performance of different neighbor retrieval functions.
The density window approach performs better than the baseline without DI, static top n, and WordNet on the WordSim-353 (rel) dataset.
The intersection method performs better than the union method for both "No Distributional Inference" and "Density Window".
The intersection method performs better than the union method for "No Distributional Inference" and "Density Window", and the intersection method performs the best for "Verb-Object" among the three methods.
Table 2 shows the accuracy scores for the data augmentation and the two dataset extension strategies in comparison to the same FF model without any augmentation or extension.
The Distributional Composition Augmentation strategy achieves the highest accuracy score among the data augmentation and dataset extension strategies.
The table shows the best-of-100 clustering errors with fewer training examples for different models.
The table includes four different models.
Table 2 lists different clustering errors by model.
The REP model has 51 clustering errors, REP-FR has 26 clustering errors, REP-FR-DE has 24 clustering errors, REP-DE has 22 clustering errors, REP-POS has 8 clustering errors, and REP-DE-POS has 0 clustering errors.
Table 1 presents ABX errors on data-augmented CPC features (Libri-light dev set) and within- and across-speaker phoneme discriminability scores on the Libri-light clean and other dev sets for CPC training.
The within-speaker phoneme discriminability score for "clean" in the "pitch + reverb" condition is 6.06, and the across-speaker phoneme discriminability score for "clean" in the same condition is 10.99.
The phoneme discriminability score for English is higher than for French in the CPC2-3L+WavAug model.
The phoneme discriminability score for Mandarin is the lowest among the languages in the Superv. topline [dunbar2017] model.
The table presents the performance of different systems on different datasets.
The system with augmentation ("Yes") has lower Phone Error Rate (PER) compared to the system without augmentation ("No") for all four datasets.
Table S1 provides information on architecture ablations and ABX errors on the Libri-light dev set.
The CPC + MH + 2 layers LSTM model has lower ABX errors in the clean condition compared to other models in the Across spk. dev set.
The table presents architecture ablations and ABX errors on the Libri-light dev set for different dataset sizes.
The ABX error rate for the CPC2 + 2 layers LSTM model is 12.82 for the "clean" condition within speaker dev.
The recall percentage for the prealigned data in the CzEng 1.0 dataset is 63.02%.
The precision percentage for the prealigned data in the CzEng 1.0 dataset is 93.74%.
The augmented-baseline training sets result in higher performance measures (accuracy, precision, recall, and F1 score) compared to the baseline training sets.
The augmented-baseline training set improves the recall by a large margin compared to the baseline training set.
Table 2 shows the ROUGE-L scores of data generated by the GPT-2 language model per dataset and class.
The ROUGE-L score for the "DV" dataset in the "Generated hate" column is 0.07.
The accuracy of the augmented-baseline training set is slightly higher than the baseline training set for the combined dataset.
The precision of the augmented-baseline training set is significantly lower than the baseline training set for the WH dataset.
The F1 score of the Bi-LSTM model when using one and two layers is 74.02.
The precision of the Bi-LSTM model for the LOC entity is higher than the precision of the LSTM model for the LOC entity.
The model performs better with two layers compared to one layer, as indicated by the higher F1 score in the "ALL" row.
The model performs better with one layer for the "LOC" and "ORG" entities, as indicated by the higher F1 scores in the "One layer F1" column for these entities.
Table 7 shows the performance of the model when adding more features.
The model achieves an F1 score of 92.05 when using the features Word, POS, Chunk, and Regex.
The ROUGE-1 score for CNN/DM is higher than the ROUGE-1 score for XSum.
The ROUGE-L score for CNN/DM is higher than the ROUGE-L score for XSum.
Table 2 presents experimental results for comparative paragraph generation on the proposed dataset.
The Neural Naturalist – Full model achieved the highest BLEU-4 and ROUGE-L scores in comparative paragraph generation.
The model performs well in predicting the "Species" and "Order" categories.
The categories "Visual" and "Class" have the lowest frequency in the dataset.
The cross-validation score decreases as the values of "wimportance" increase.
The model "MoE-143M" achieves the lowest test perplexity of [BOLD] 28.0 on the 1 Billion Word Language Modeling Benchmark.
The model "MoE-143M" has a total of 4371.1 billion parameters.
Table 2 shows the performance of the individual groups of hand-crafted features.
The Grammatical features have the lowest F1 score among the different types of hand-crafted features.
The word "chemtrails" has a translation of "chemtrails" and a PMI value of 0.92.
The word "следете в" has a translation of "follow in" and a PMI value of 0.97.
The performance of the models improves as more features are added.
The AttNN model has higher precision and recall compared to the TF.IDF model.
The summaries generated by the LenLInit model have a desired length of 25 and a true length of 24.
The summaries generated by the LenMC model have a desired length of 45 and a true length of 45.
The models in the table can be categorized into two groups: "Summarization models" and "Length-control models".
The performance of the "LenEmb" model is worse than the performance of the "LenLInit" model in terms of ROUGE scores.
The shortest path distance between Synset('river.n.01') and Synset('bank.n.01') is 8.
The shortest path distance between Synset('river.n.01') and Synset('bank.n.09') is 10.
The CopyNet model outperforms other models in terms of CRR for Bhagavad Gītā Ins, Bhagavad Gītā Del, and Sahaśranāma Ins.
The table does not provide any information about the System errors Sub metric.
English has the highest combined CRR score among all the languages in Table 1.
The number of deletion errors is higher for Bhagavad Gītā compared to Sahaśranāma for all languages in Table 1.
The table presents the results of different smoothing methods for word weights generation.
The accuracy of the hybrid model is higher for English than for German and Hungarian.
The accuracy of the hard attention model is higher for German and Hungarian than for English.
The "KALE-Joint guo2016jointly" model achieves the highest "hits@10" score in Table 1.
The "CBR (Ours)" model achieves the highest scores in all four metrics in Table 1.
The CBR (ours) model outperforms all other models in terms of hits@1 in the link prediction task on NELL-995.
The Meta-KGR(ConvE) model achieves the highest mean reciprocal rank (MRR) among all the models in the link prediction task on NELL-995.
The table compares the time usage of three different models: Uncompressed, HashNet, and SE.
The Uncompressed model has the highest CPU time usage among the three models, followed by HashNet, and SE has the lowest CPU time usage.
The model "2-layer LSTM-8192-1024 + CNN inputs + CNN softmax" has a perplexity of 39.8.
The model "GCNN-13" has a perplexity of 38.1.
NBT achieves the highest scores for BLEU1, BLEU4, METEOR, and SPICE among all the methods in the table.
"Up-Down" achieves a higher score for the CIDEr metric compared to "NBT".
The NBT method achieves the highest scores for all evaluation metrics on the test portion of the Flickr30k Entities dataset.
The NBT method outperforms the Hard-Attention and ATT-FCN methods in terms of BLEU4 score on the test portion of the Flickr30k Entities dataset.
NBT achieves the highest scores in all metrics (BLEU4, METEOR, CIDEr, SPICE).
NBToracle achieves the highest accuracy score.
NBT†+T2 achieves the highest scores for all out-of-domain test data and in-domain test data metrics.
NBT†+G has the lowest average score among all methods for out-of-domain test data.
The performance of Our ESIM without CoVe + fully-aware + multi-level (d=250) is higher than the performance of Our ESIM without CoVe + fully-aware (d=250) in both Cross-Domain and In-Domain settings.
The performance of Our ESIM + fully-aware (d=250) is lower than the performance of Our ESIM without CoVe + fully-aware (d=250) in the In-Domain setting.
The average performance across all datasets is calculated in Table 2.
The highest performance for each language is marked in bold in Table 2.
Table 1 shows the results on the CELEX dataset.
The "Hard" approach achieves the highest scores in the "2PIE", "2PKE", and "rP" columns.
The average performance of "suffixing+agg.+v.h. FI" is higher than "suffixing+agg.+v.h. TU".
The performance of "templatic AR" is lower than "templatic MA".
The "Full Model" achieves the highest accuracy of [BOLD] 88.69.
The accuracy is higher for "Only P→Q" with 87.74 compared to "Only P←Q" with 87.47.
The BiMPM model has the highest accuracy of 88.17% among all the models in the table.
The models in the table show an increasing trend in accuracy from left to right.
The table shows the performance of different models on the SNLI dataset.
The "BiMPM (Ensemble)" model achieves the highest accuracy among all the models.
BiMPM achieves the highest performance in terms of TREC-QA MAP.
BiMPM achieves the highest performance in terms of TREC-QA MRR.
The TextNAS model achieves the highest scores in both the SST and SST-B datasets.
The TextNAS model achieves a significant improvement in the SST-B dataset compared to the best reproducible model.
The experiment settings for text classification are different for each dataset.
The hidden size used in the experiments varies for each dataset.
Table 7 provides detailed settings for experiments of natural language inference.
The training epoch for SNLI is 8.
The gold label count for "arg0" in Catalan is 2117.
The improvement in F1 score for "arg3" in Spanish when adding English data is -3.40.
The "marcheggiani2017lstm" model does not have scores for the "cat" and "deu" languages.
The reported scores for the "ces" and "spa" languages are in italics.
The table shows the Semantic F1 scores on the English test set for each language pair.
The Semantic F1 scores increase as more languages are added to the English test set.
The table shows the unlabeled semantic F1 scores on the CoNLL 2009 dataset for different languages.
The table shows the improvement in unlabeled semantic F1 scores on the CoNLL 2009 dataset when English data is added to the model.
The values in the "F [ITALIC] rm" column represent the performance of different systems with different stack capacities.
The values in the "TD [ITALIC] rp" column represent the average response time of different systems with different stack capacities.
The multi-task training performance for the BIDAF w/ static memory model is higher than the single-task training performance.
The Hasty Student model performs better on the coherence task than on the cloze task in single-task training.
Our (GPU) system is 11.6 times faster than the CODRA system.
The speed of our (CPU) system is 12.57 sents/s in the "Only Parser" section and 11.99 sents/s in the "End-to-End (Segmenter → Parser)" section.
The precision, recall, and F1 scores for the "Our Segmenter" approach are not provided in Table 1.
The precision score for human agreement is 98.5.
The "ml-VAE" and "Real data" models outperform the "flat-VAE" and "AAE" models in terms of overall performance.
Both the "ml-VAE" and "Real data" models exhibit higher consistency than the "flat-VAE" and "AAE" models.
The table mentions four different models: flat-LM, flat-VAE, ml-LM, and ml-VAE-S.
The ml-VAE-D model has a lower NLL value than the other models.
The ml-VAE-D model outperforms all other models in terms of Yelp B-n scores.
The ml-VAE-S model performs better than the AAE model on the arXiv B-3 score.
ARAE has the lowest self-BLEU scores (B-2, B-3, B-4) compared to other models.
flat-VAE has the highest unique 2-gram percentage among all models.
The ARAE model has the lowest self-BLEU scores (B-n) and unique n-gram percentages (ngr) among all the models.
The [ITALIC] flat-VAE model has the highest self-BLEU scores (B-n) and unique n-gram percentages (ngr) among all the models.
The "Transformer + AD (RL)" model achieves the highest BLEU score for the "De→En" translation.
The "BiDAN (Transformer)" model achieves the same BLEU score as the "Transformer" model for the "En→De" translation.
The BLEU score increases as we move from the "Random Encoder" to the "En→De Encoder" to the "En→De Encoder (BiDAN)" to the "En→Fr Encoder (Original)".
The BLEU score increases as we move from the "Random Encoder" to the "En→De Encoder" to the "En→De Encoder (BiDAN)" but decreases for the "En→Fr Encoder (Original)".
Table 8 provides the ablation results of cross-entropy loss vs. cross-entropy+maxmargin loss for a BiDAF-based generative model on the dev set.
The XE+Max-margin model achieves a higher recall@1 value compared to the Cross-entropy model.
The table presents the performance of baselines, discriminative models, and generative models on the Twitch-FIFA test set.
Table 4 presents the performance of generative models on phrase matching metrics.
The METEOR score for the Seq2seq + Atten. (C) model is 2.59.
Table 7 shows the ablation results of classification loss and max-margin loss on the TriDAF discriminative model.
The max-margin loss performs better than the classification loss in terms of recall@1 on the TriDAF discriminative model.
The Conpono (k=2) model outperforms the BERT-Base, BERT-Large, RoBERTa-Base, and BERT-Base BSO models on average.
The Conpono (k=2) model performs better than the BERT-Base, BERT-Large, RoBERTa-Base, BERT-Base BSO, and Conpono isolated models on the PDTB-I task.
Table 3 provides information about the accuracy of different models on the RTE and COPA benchmarks.
The ALBERT model has an accuracy of 86.6 on the RTE benchmark.
Conpono has a higher accuracy than BERT-Base.
BERT-Large has a higher accuracy than BERT-Base.
The table displays the results of an ablation analysis that investigates the effects of different k values, removing the MLM objective, and training with a small transformer encoder.
The table shows the ROUGE-L scores of different versions of the model for the RT and Yelp datasets.
The accuracy of pragmatics communication using the "Exact copy" virtual opponent is 100% for all virtual opponents.
The accuracy of pragmatics communication using the "SFide" virtual opponent is 97.0%.
The table shows the performance of each of the sound-class detectors measured in terms of equal-error-rates (EERs) for the development data.
The table compares the performance of different readers with a fixed answer verifier using three different configurations.
The "+ Model-III" configuration achieves the highest scores in terms of exact match (EM), F1 score, and no answer accuracy (NoAns ACC).
The "RMR + ELMo + Verifier" model achieves the highest Dev EM and Dev F1 scores.
The "SLQA+" model achieves the highest Test F1 score.
The table compares the performance of readers with different auxiliary losses.
The "RMR + ELMo" configuration achieves the highest exact match (EM) score.
Table 3 compares different architectures for the answer verifier.
Model-III has a higher accuracy than Model-II.
Table 4 compares the performance of different configurations of models with different answer verifiers.
The addition of ELMo improves the performance of Model-III in terms of EM, F1, and NoAns ACC.
The F1 scores calculated by ELDAN are higher than the Chance scores for CPT codes 43239, 45380, and 66984.
The difference between the F1 score calculated by ELDAN and the Chance score is highest for CPT code 12001.
The F1-scores reported by ELDAN for the CPT codes range from 83.33% to 100.00%.
The difference between the F1-scores reported by ELDAN and the Chance scores for the CPT codes range from -6.82 to 42.38.
The BLEU scores increase as we add more components to the baseline model, with the highest score achieved by the Q-WAAE model.
The METEOR scores increase as we add more components to the baseline model, with the highest score achieved by the Q-WAAE model.
The table shows the impact of the noise concatenated to the hidden state of size 512 on the METEOR score.
The increase in noise size is negatively correlated with the METEOR score.
Table 6 shows the effect of different dispersion functions, content coverage, and dissimilarity metrics on the system.
The system achieves the highest ROUGE score for the "Topical" dissimilarity metric in the TAC 2008 dataset.
The left side of Table 3 shows the summaries evaluated by Jensen-Shannon divergence (JSD) on Yahoo Answer for summaries of 100 words and 200 words.
The "Our system" column has a higher value than the other columns in the "Best answer" row.
Table 3 provides information about the evaluation of summaries using Jensen-Shannon divergence (JSD) on Yahoo Answer for summaries of 100 words and 200 words.
The JSD values for 100 words and 200 words in the "Rel + Aut + TM + Pol + Cont(ent Coverage)" row are 0.3102 and 0.1851, respectively.
Table 6 shows the effect of different dispersion functions, content coverage, and dissimilarity metrics on the system.
The Lexical dispersion function has a lower value than the other dispersion functions.
The table includes BLEU scores of the individual and multilingual models on 44 languages→English on the Ted talk dataset.
The multilingual model with their method achieves a BLEU score of 39.75 for the language "Fr".
The MAE values for "Ours with multiple datasets" are lower than the MAE values for "Graph-based (Ours)".
The correlation coefficient values for "Ours with multiple datasets" are higher than the correlation coefficient values for "L-biLSTM(2)-MultiSimp w/UDS-IH2**".
The similarity score between "annexed" and "Brisbane" is 0.836.
Table 1 compares the performance of the proposed TPRN model to BiDAF.
The TPRN model achieves higher EM and F1 scores than BiDAF on both the dev and test sets.
The token "printmaker" has the highest similarity score of 0.9587.
Both "printmaker" and "composer" have similarity scores higher than 0.8.
The token "phrase" has the highest similarity score of 0.817.
The token "wrong" has a higher similarity score than the token "mean".
The similarity scores decrease as the tokens move down the table.
The "+ Elmo" model has higher accuracies on both the development and test sets compared to the "Base" model.
The "+ Elmo" model has a higher accuracy on the development set compared to the "Base" model.
Table 2 provides a summary of the NFH Identification corpus, including the number of positive and negative instances in the train, dev, and test splits.
The total number of instances in the test split of the NFH Identification corpus is 500.
The method "CopyRnn" achieves the highest F1@5 and F1@7 scores for all datasets.
The method "CopyRnn" achieves the highest F1@5 score for the Hulth (500) dataset.
The "CopyRnn" method achieves the highest Rouge-1 score for all datasets.
The "Inject" method achieves the lowest Rouge-F1 score for the Hulth dataset.
The highest recall+ value is achieved at 1 day.
The highest F1+ value and accuracy value are achieved at 1 day.
The term "fucking" appears more frequently in disruptive posts compared to other terms.
The term "you" appears more frequently in constructive posts compared to other terms.
The performance metrics for the classifiers with "FW" are generally lower than those without "FW".
The SVM classifier has the highest F1 score among all classifiers.
The experiment used four different classifiers: SVM, SVM (FW), NB, and NB (FW).
The accuracy of the SVM classifier is 0.890.
The F1 score for constructive contributions of the NB (FW) classifier is 67.26.
The table shows the performance of SVM and NB classifiers using the sliding window approach with stratified sampling for function words classifications.
The SVM classifier has a higher accuracy than the NB classifier.
Table 8 provides the F1 score of char models and their performance on the dev. set for selected languages with different gather strategies.
The DQM model achieved an F1 score of 96.1, and there are 16 xpos tags in the training set.
The system "ours" achieves the highest accuracy of 97.96% on the WSJ test set.
All systems in the table achieve accuracy values above 97% on the WSJ test set.
Table 6 shows the F1 scores for selected languages on word and sentence level character models for the prediction of morphology using late integration.
The F1 score for the "la_ittb" language using the word level character model is 93.22.
The model trained with a morphological constraint performs better than the original model on the Catalan-Portuguese language pair.
The model trained with a morphological constraint performs slightly better than the original model on the Italian-Spanish language pair.
The size of the Polish dictionary is smaller than the size of the French dictionary.
There is no dictionary size provided for the mapping direction from Russian to Czech.
Basic-GRU achieves the highest accuracy score of 0.835 on the Weibo dataset.
CSI achieves the highest recall score of 0.996 on the Weibo dataset.
The table represents 19 hand-crafted emotion features of news content.
The fraction of emotion positive and negative words in the news content is 2.
The HAN model achieves the highest BLEU score in the En-De Europarl dataset.
The SentNmt model has a training speed of 1.0x and a testing speed of 1.0x.
The DocNmt model has a BLEU score of 17.0.
Adding a context fusion gate to the Doc2Sent model improves its BLEU score to 12.4.
The table provides binary accuracy for different variants of C-MFN and training scenarios.
The best performance of C-MFN is achieved when using all three modalities of text, vision, and acoustic.
The "UR-Funny" dataset consists of 8257 positive and negative examples.
The "Big Bang Theory" dataset includes both linguistic and acoustic modalities.
The "r-RNTN f" method has the lowest perplexity score for the PTB dataset.
The "r-RNTN f" method has the highest perplexity score for the text8 dataset.
The CVAE model with the Cyclical schedule (C) outperforms the CVAE model with the Monotonic schedule (M) for all BLEU scores.
The CVAE+BoW model with the Cyclical schedule (C) has a higher recall score than the CVAE+BoW model with the Monotonic schedule (M) for BLEU score B1.
The CVAE model has a lower KL Loss than the CVAE+BoW model.
The CVAE+BoW model has a higher B4 precision than the CVAE model.
The table provides test performance results for the GLOSS-BoW and GLOSS-POS models on unsupervised STS tasks and supervised tasks.
The GLOSS-BoW model performs better on average on unsupervised STS tasks compared to the GLOSS-POS model.
Gloss-BoW performs better on the unsupervised STS tasks than on the supervised tasks.
uSIF with a configuration dimension of 300 achieves a score of 64.9 on the Unsupervised STS-() tasks.
The SCL method outperforms the baseline SVM and baseline MEMM methods in terms of accuracy for the "Early Modern English" training period.
The table shows accuracy results for adapting from the PTB to the PPCMBE and the PPCEME of historical English.
The accuracy for the "ppceme" task with Fema attribute embeddings (error reduction) is 79.05 (19%).
Table 5 provides the tagging accuracies of adaptation experiments for different feature sets in the PPCEME dataset.
Table 7 shows the tagging accuracies of different domain adaptation models from the PTB to the PPCEME.
The proposed method achieved the same WER as the 1st decoding with LM0 and rescoring with LM2 on the Test-clean dataset.
The proposed method achieved a lower WER than the 1st decoding with LM0 on the VLSP2018 dataset.
The table provides information about different sets of audio datasets, including Training and Testing sets.
The table provides information about the number of hours of audio data available for each dataset.
Multilingual LM0 performs better than English LM1 and Vietnamese LM2 on the "Test-clean" dataset.
Multilingual LM0 performs better than Vietnamese LM2 on the "VLSP2018" dataset.
Common Crawl performs better than OpenSubtitles in terms of overall F-score in the Semantic Dissimilarity task.
OpenSubtitles performs better than Common Crawl in terms of precision, recall, and F-score for Non-parallel sentences.
The En-Fr translation performs better than the Fr-En translation based on the Test BLEU scores.
The Overfitted Diversified Model performs better than the Diversified Model based on the E[1N∑Nip^M(yij|yi<j)] values.
Table 4 provides information about the performances on low-resource translations.
The method of data diversification has higher performances in all translation directions compared to the method used by flores.
Table 9 shows the BLEU scores for models with and without back-translation on the IWSLT’14 English-German, German-English, and WMT’14 En-De tasks.
The back-translation baseline performs better than the no back-translation baseline on all tasks.
Table 10 provides information about the diversity performances in BLEU and Pairwise-BLEU scores for WMT'14 English-German (En-De) and English-French (En-Fr) translations.
The "Beam" method has the highest Pairwise-BLEU scores for both English-German (En-De) and English-French (En-Fr) translations.
Table 11 shows the improvements of data diversification under conditions with and without dropout in the IWSLT'14 English-German and German-English.
Our model (Ours) outperforms the baseline model in terms of gain for both the En-De and De-En tasks.
Table 12 shows the improvements of data diversification under the conditions of maximum likelihood and beam search in the IWSLT'14 English-German and German-English.
The performance of the "Ours" model with beam search (beam=5) is higher than the performance of the baseline and the "Ours" model with beam search (beam=1) for both the En-De and De-En tasks.
The BERT classifier has the highest accuracy for the LAMBADA task.
The BERT classifier achieves higher accuracy with unlabeled GPT data for the LAMBADA task.
LAMBADA is statistically superior to all other generative approaches on each classifier and each dataset.
EDA with SVM performs similarly to LAMBADA on the TREC dataset.
The batch size used for the News20 dataset is larger than the batch sizes used for the BBC and BBC-Sports datasets.
The learning rate used for the News20, BBC, and BBC-Sports datasets is the same.
The "News20" dataset has the largest train size among the three datasets.
The "BBC Sports" dataset has the smallest test size among the three datasets.
The accuracy for the News20 dataset with BBC as the target and source using the Bi-LSTM (Target Only) method is 91.33%.
The F1-Score for the BBC dataset with News20 as the target and source using the Instance-Infused Bi-LSTM method with penalty function is 0.9100.
The "News20" dataset has lower accuracy and F1-score compared to the "BBC" and "BBC Sports" datasets in both the single source and multiple sources scenarios.
The proposed model achieves higher accuracy when using instances from multiple source datasets compared to the single source accuracy for all three datasets.
The Multinomial NB-bigram model achieves the highest accuracy score on the News20 dataset.
The Random Forests- tf-idf model achieves the highest F1-score on the BBC Sports dataset.
The dimension of GloVe.840B.300d embeddings is 300.
The hidden size of the Encoder is 512.
Our model achieves a Smatch score of 76.3±0.1.
The van Noord and Bos (2017b) model achieves a score of 86 in the Named Ent. category.
Table 4 shows the results of ablation studies on different components of the model, with corresponding scores for AMR 1.0 and AMR 2.0.
The full model performs better than any individual ablated component, as shown by the higher scores in both AMR 1.0 and AMR 2.0.
Table 5 provides Smatch scores of full models trained and tested based on different node linearization strategies.
The scores for the "Pre-order + Alphanum" and "Pure Alignment" strategies are higher in the "AMR 2.0" column compared to the "AMR 1.0" column.
For the "kNN" induction method and the "full" seed selection, the SVDPPMI method performs better than the SGNS method.
For the "RandomWalkNum" induction method and the "limited" seed selection, the SGNS method performs worse than the SVDPPMI method.
The mean values for Valence, Arousal, and Dominance are different for the three different annotations (goldEN, goldDE, Warriner).
The Valence rating for goldDE is higher than the Valence rating for goldEN.
The "RINANTE+BERT" approach achieves the highest performance scores for aspect and opinion term extraction in both SE14 and SE15 datasets.
The "BERT feature-based" approach outperforms the "BiLSTM-CRF + word2vec" approach in aspect term extraction for the SE14-R dataset.
RINANTE-Double-Pre† achieves the best performance on SE15-R.
The table provides a breakdown of the question types in the COCO-QA dataset, showing the counts for each category in both the train and test datasets.
The table accurately represents the total counts for each category in the COCO-QA dataset, with the train and test counts adding up to the total count for each category.
The models listed in the table are ELMo, BERT-base, and DistilBERT.
The scores listed in the table are 68.7 for ELMo, 79.5 for BERT-base, and 77.0 for DistilBERT.
The table compares the performance of three different models: BERT-base, DistilBERT, and DistilBERT (D).
DistilBERT (D) achieves a higher F1 score on the SQuAD task compared to DistilBERT.
DistilBERT has fewer parameters than ELMo and BERT-base.
DistilBERT has a lower inference time than ELMo and BERT-base.
The "Vote" model performs better on the "Sogou Full" dataset compared to the "Sogou 3k" dataset.
The model trained on S0∪S1 performs better on the "Amazon Full" and "Yelp Full" datasets compared to the other datasets.
Table 1 shows the performance of updated word vectors on the word analogy task for Out of vocab and In vocab questions.
The performance of fine-tuning on the word analogy task is 67.7 for Out of vocab questions and 66.8 for In vocab questions.
The classification accuracy when training on S0 is 74.9% regardless of the size of S1.
The classification accuracy when fine-tuning on S1 is higher than when using RCSLS+Fine. on S1, and the accuracy increases as the size of S1 increases.
The CasRel method achieves the highest precision, recall, and F1 scores on both the NYT and WebNLG datasets.
The CopyR∗ [ITALIC] RL method achieves lower precision, recall, and F1 scores than the CasRel method on both the NYT and WebNLG datasets.
The CasRel method achieves the highest precision, recall, and F1 scores on both the NYT and WebNLG datasets.
The CopyR∗ [ITALIC] RL method achieves lower precision, recall, and F1 scores than the CasRel method on both the NYT and WebNLG datasets.
Table 8 shows market trend prediction using main technical indicators.
The SVM method has a higher accuracy than the LSTM method for all the different types.
The lag values in the table range from 1 to 2 for all stock and model combinations.
In the table, there are certain stock and model combinations where the sentiment attitude granger-causality has a significant effect on the price.
Table 9 provides market trend prediction results using FT news articles and RWNC headlines from 2011-2015.
The Financial Times achieved the highest accuracy for predicting the market trend for DJIA.
Table 10 represents market trend prediction using financial tweets from Twitter.
The baseline model has a higher accuracy for predicting market trends for AAPL compared to DJIA.
The objective functions used for the experiments are "Cosine", "L2", and "L1".
The "L1" objective function achieves the highest accuracy on the test set.
As the number of layers increases, the accuracy on the test for both NLU and ASR tasks generally increases.
The highest accuracy on the test for the NLU task is achieved when there is 1 layer and for the ASR task is achieved when there are 3 layers.
The scores for all languages are higher in the "M-Pre + ASR" row compared to the "Baseline" row.
The scores for the "Es" language are lower in the "C-Final" column compared to the scores for the other languages in the same column.
The baseline performance for the German language is 17.3.
The performance of the multilingual cascade system for the Italian language is 21.5.
Table 5 provides the percentage of sentences in the correct language computed with langdetect.
The percentage of sentences in the correct language for Dutch is 97.9%.
Table 3 shows the Quadratic Weighted Kappa (QWK) scores of different AES models.
The CO-ATTN model has a higher Quadratic Weighted Kappa (QWK) score compared to the Rubric and SG models.
The feature "SPClmh" has different values for different scenarios.
The feedback messages are the same for the values of the feature "SPClmh".
Table 2 shows the test results for Event Coreference with the Singleton and Matching baselines.
"This work" achieves the highest score for B3 in event coreference.
"This work" achieves the highest score for CEAF-E in event coreference.
The test results in Table 4 are for event sequencing.
The F-Score for our model in event sequencing is higher than the Precision and Recall values.
The table shows the results of different stopping methods on 20Newsgroups for batch percentages of 1%, 5%, and 10%.
The F-Measure at the stopping point for the Oracle Method is 75.49 when the batch percentage is 10%.
The XVisionSpeech system has a higher count and percentage of errors compared to the XBoWCNN system.
The XVisionSpeech system has a higher count and percentage of errors compared to the XBoWCNN system in the "Semantically related" error type.
The table shows the results of different models used for cross-lingual keyword spotting.
The table provides the Equal Error Rate (EER) values for each model used in cross-lingual keyword spotting.
Table 4 shows the cross-lingual keyword spotting results for different variants of XVisionSpeechCNN on development data.
OracleXVisionSpeechCNN has the highest Equal Error Rate (EER) among the three models.
"GenDS-Single" has the highest scores for Grammar, Context Relevance, and Correctness.
The scores for Grammar, Context Relevance, and Correctness of "GenDS-Single" are higher than the scores of other models.
Table 3 shows the automatic evaluation results on the Music dataset.
The "GenDS" model achieves a BLEU score of [BOLD] 0.122 on the Music dataset.
Table 5 represents the automatic evaluation on the QA dataset.
The GenDS model has the highest BLEU score among all the models.
RETURNN and Sockeye are two different toolkits used for training.
The BLEU scores for Sockeye in 2017 are higher than in 2015.
The table compares the performance of different toolkits on German→English translation.
The BLEU scores for RETURNN and Sockeye have increased from 2015 to 2017.
Table 4 provides the BLEU scores for different translation systems on the WMT 2017 English→German task.
The LMU + reranking system achieves a higher BLEU score than the Systran and Edinburgh systems on the WMT 2017 English→German task.
The "hybrid" model outperforms other models in terms of WER on both Hub5'00 and Hub5'01.
The "frame-wise" training approach generally leads to lower WER values compared to other training approaches.
The MET Fixed feature set has a higher reading speed compared to the TOEFL Fixed feature set.
The MET Any feature set has a higher S-Clusters coefficient compared to the TOEFL Any feature set.
The feature "WFC" has the lowest MAE values for both MET and TOEFL scores.
The "S-Clusters" feature has a higher "r" value for MET scores compared to TOEFL scores.
The table compares different languages and their corresponding data, and shows the differences in entropy and repetition of unigrams/trigrams in references and translations.
The average entropy of translations (unigrams and trigrams) is higher than that of references, and the average repetition rate of translations (unigrams and trigrams) is lower than that of references.
The table provides test correlations for two different models (GRAN and Avg) on two different datasets (SimpWiki and NMT).
The test correlations for both models (GRAN and Avg) are lower in the length range 20-30 compared to the other length ranges.
Table 6 shows the length filtering test results after tuning length ranges on development data for the NMT and SimpWiki models.
The length range for the NMT models is [0,10] and for the SimpWiki models is [0,15].
The quality filtering test results are averaged over languages and data sources for the NMT rows.
The average quality score for the filtering method "None (Random)" is 66.9.
The LSTM model performs better on the FR language compared to the CC language.
The EP dataset has the highest test accuracy for both the CC and DE languages.
Table 12 shows the diversity filtering test results after tuning filtering hyperparameters on development data.
The BLEU score for SimpWiki Avg is 66.5.
The table shows test results when using more training data, and it demonstrates that more data helps improve the performance of both Avg and GRAN models, bringing them close to or surpassing the performance of training on SimpWiki and surpassing the PPDB baseline.
The table demonstrates that the Avg model achieves the highest performance when trained on the CC-CS (100k) dataset compared to other datasets mentioned in the table.
The "SparseAttention" model performs better than the "NoAttention" model on the natural language inference task.
The "SoftAttention" model performs better than the "LogisticAttention" model on the natural language inference task.
Our model outperforms models S, A, and B in terms of BLEU score.
The highest average score on the NIST Chinese-English translation task is achieved with the information unit treated as a segment and discarding 4 tokens.
The scores on the NIST05 subset of the NIST Chinese-English translation task are consistently higher when the information unit is treated as a sub-sentence compared to when it is treated as a segment.
Our model outperforms the 5-LM and RNN models in terms of precision, recall, and F-score.
Our model has lower average latency compared to the 5-LM and RNN models.
The overall results on the BSTC Chinese-English translation task show that as the number of discarded tokens increases in the ASR input fine-tuning, the translation scores generally decrease, except for the "+discard 6 tokens" case, where the score increases.
The overall results on the BSTC Chinese-English translation task show that the translation scores are generally higher for the clean input during the pre-training phase compared to the ASR input.
The performance of ViLBERT + augmentation is better than ViLBERT in the image retrieval task on Flickr30k.
The top-1 scores in R@1, R@5, and R@10 for the image retrieval task on Flickr30k are achieved by ViLBERT + augmentation.
As the percentage of remaining weights decreases, the MaP decreases for all tasks (SQuAD, MNLI, QQP).
As the percentage of remaining weights decreases, the F1 score increases for the SQuAD and QQP tasks.
Table 2 compares the performance of (Soft) movement pruning with current state-of-the-art pruning methods at high sparsity levels.
For the MNLI dataset, at a sparsity level of 10%, (Soft) movement pruning outperforms the remaining weights pruning method in terms of MaP and soft MvP scores.
The "pg + cbdec" model achieves the highest ROUGE F1 score.
The "pg + cbdec" model achieves the highest ROUGE L score.
The "RL + pg + cbdec" model achieves higher scores than the "pg + cbdec" model for ROUGE F1, ROUGE 2, and ROUGE L.
The "pg + cbdec" model achieves higher scores than the "previous works" models for ROUGE F1, ROUGE 2, and ROUGE L.
The ROUGE 1 score for the "pg" model in the previous works is 39.53.
The ROUGE L score for the "RL + pg + cbdec" model is 37.06.
Table 6 shows the ROUGE F1 scores of ablation studies evaluated on the CNN/Daily Mail validation set.
The table shows the effect of source languages for comparable corpus creation using different News Crawl corpora.
The precision values decrease as we move from "Fi News Crawl" to "De News Crawl".
The table shows M2 and GLEU results, with bold scores representing the best score in unsupervised SMT and underlined scores representing the best overall score.
The F0.5 score for CoNLL-14 (M2) is highest in the iteration with forward refinement.
Table 4 shows the GEC results with W&I+LOCNESS test data.
The team "UEDIN-MS" has the highest number of true positives (TP).
The table shows the hyper-parameter settings for the generation-based baselines and the generation module in the proposed hybrid neural conversation model.
The learning rate for Seq2Seq-Facts is higher than the learning rate for Seq2Seq.
"HybridNCM-RS" and "HybridNCM-RSF" are two different models.
The BLEU-1 ROUGE-L score is higher for "HybridNCM-RS" with k’=3 compared to "HybridNCM-RSF" with k’=3.
The table shows the response generation performance for two different types of supervision: HybridNCM-RS and HybridNCM-RSF.
The highest BLEU-1 score in the table is 1.3450.
CGA+KG+8[min] achieves the highest AUC score on the Bio dataset.
CGA+KG+1[min] shows a relative improvement of 2.31 in AUC compared to GQE on the Bio dataset.
Table 3 presents the results of human evaluation for the data collected with each MR, including informativeness, naturalness, and phrasing.
The mean scores for pictorial MR in the informativeness, naturalness, and phrasing categories are higher than the mean scores for textual MR.
The mean value of the textual MR increases as the number of attributes increases.
The standard deviation of the pictorial MR decreases as the number of attributes increases.
The precision scores of the proposed model are higher than the precision scores of both Baseline 1 and Baseline 2.
The F1-Score of the proposed model is higher than the F1-Score of the hua2016shortest model.
The table compares different document-level model architectures and complexity.
The Multi-Encoder (Para.) approach with Transformer architecture and 1 layer achieves the highest Bleu score in the en-de translation task.
Removing stopwords improves the performance of en-it Bleu and en-it Ter.
Retaining only named entities significantly decreases the performance of en-de Ter.
The subreddit "depression" has the highest number of posts in the training, validation, and test sets.
The subreddit "bipolar" has the lowest percentage of posts in the training set compared to the other subreddits.
The linguistic clue "positive emotion" has a higher association with label 1 compared to label 0 and label -1.
The linguistic clue "work" has a stronger association with negative emotions (label -1) compared to positive emotions (label 1).
The RN model has the highest accuracy, precision, recall, and F1 score among all the models.
The fastText model has the same accuracy, precision, recall, and F1 score.
The RN model has the highest performance in terms of accuracy, precision, recall, and F1 score compared to the other models.
The BiLSTM model has a higher accuracy than the LSTM model.
The F1-score for class -1 is higher for SSA compared to BiLSTM.
The recall for class 1 is higher than the recall for class -1.
The variant "BiLSTM+RN+sent.+topic" has the highest performance among all the models.
The "BiLSTM" model outperforms the "BiLSTM+concat" model in terms of accuracy, precision, recall, and F1 score.
The AWD-LSTM-MoS + Alleviated TOI model achieves the lowest test perplexity among all the models.
The AWD-LSTM-MoS + Alleviated TOI model performs better than the Simple-LSTM + Alleviated TOI model in terms of test perplexity.
Table 2 compares the perplexity scores of the AWD model on three datasets with different levels of Token Order Imbalance (TOI).
The perplexity scores for PTB, WT2, and WT103 are lower for the Standard TOI level compared to other TOI levels.
The experiment is comparing different values of K on the PTB dataset and the AWD model.
The table compares different types of Token Order Imbalance (TOI) for the IEMOCAP dataset using a simple LSTM model.
The alleviated TOI method shows higher weighted accuracy (WA) and unweighted accuracy (UA) compared to other TOI methods.
Table 6 presents the results of different experiments conducted on the IEMOCAP dataset using the Transformer model.
The experiment with Alleviated TOI 10 shows higher WA and UA values compared to other TOI values.
The Transformer model achieved an accuracy of 89.24% on the IMDB dataset.
The BCN+Char+CoVe model achieved an accuracy of 53.7% on the SST-5 dataset.
The table shows the BLEU score vs context length on different models.
The BLEU score for the Transformer model with a context length of 128 is 15.55.
The "BPT (k=4)" model has the highest BLEU score among all the models listed in Table 7.
The "Transformer" model has a higher BLEU score than the "Transformer (our implementation)" model.
Table 1 shows the performance (WERs) of different variations of binary weight networks on WSJ1.
The experiments in Table 1 compare the performance of binary weight networks with different types of softmax on WSJ1.
The table compares the performance of different models with binary weights and activations on the WSJ1 dataset.
The table shows the WERs (%) of neural networks with binary weights and activations at different sizes on the WSJ1 dataset.
The table shows the Word Error Rates (WERs) of a neural network with binary weights and activations on the AMI dataset.
The "animalThatFeedOnInsect" and "animalSuchAsInvertebrate" relations have higher Hits@10 scores for ZSGAN [ITALIC] KG compared to the other relations.
The "produceBy" relation has a higher MRR score for ZSGAN [ITALIC] KG compared to the other relations.
ZSGAN KG (DistMult) achieves the highest NELL-ZS MRR among all the models.
ZS-TransE achieves the lowest NELL-ZS Hits@1 among all the models.
The highest accuracy for the verb-past-tense task is achieved in the following languages: English, German, Spanish, Italian, Czech, and Croatian.
The number of word pairs for the "state-currency" analogy type is consistent across all languages.
The number of word pairs for the "adjective-comparative" analogy type is highest among all the "Syntactic" analogy types.
The M-OT technique yields the highest accuracy for the "Monoling." experiments.
The B-CCA technique yields the highest accuracy for the "Cross-lingual" experiments.
The language pair "Es" (Spanish) has the highest score in the table.
The English scores are consistently higher than the German scores in the table.
English (En) has the highest accuracy score among all languages in the capital-common-countries task.
German (De) performs better than Croatian (Hr) in the capital-common-countries task across all languages.
The highest score for each language is on the diagonal from the top left to the bottom right.
English (En) has higher scores compared to other languages in the table.
The highest percentage for each language is in the diagonal from the top left to the bottom right.
The percentage for each language decreases as we move from left to right in each row.
The highest percentage for each language is in the corresponding language column.
The "Es" row has the highest percentages among all the rows for each language.
English has higher performance than German in all rows.
Table 4 displays the F1-score, Precision, and Recall with different IVA parameters.
The F1-score for R=3 is 0.807.
SBN-IVA has the highest F1-score, Precision, and Recall values among all methods in the Sports category.
DN-GAA has a higher F1-score in the Movies category compared to the TV-series category.
The table compares the prediction results of different models based on encoder using NYT122, NYT71, and NYT27 datasets with different KB regularization strategies.
The model "RegDVAE+D" achieves the highest F1 score and NMI score on the NYT122 dataset compared to other models.
There are four different models compared in the table.
The F1 score for RegDVAE (Euclidean at encoder) is higher than the F1 scores for DVAE, RegDVAE (KL at encoder), and RegDVAE (JS at encoder).
The F1 score increases as we move down the table.
The "Ours + BERT" model has the highest F1 score among all the models listed in the table.
The F1 score for "Ours" is higher than the F1 scores for the other models.
The average F1 score for all models is higher than 85.
The F1 score for "Ours" is higher than the F1 scores for the other models listed.
The F1 score for "Ours + BERT" is higher than the F1 score for "Ours".
Adding sentence-level and document-level information improves the performance on all three benchmark datasets.
Incorporating all types of information results in the highest performance on all three benchmark datasets.
The F1 score for the "ALL" strategy is higher than the F1 score for any other strategy.
The ERR value for the "mean-pooling" strategy is lower than the ERR value for the "label-embedding" strategy.
Table 7 provides detailed results on the CoNLL-2003 dataset for IV, OOTV, OOEV, and OOBV.
Our method outperforms the baseline method in terms of F1 score for all categories in the CoNLL-2003 dataset.
GPro achieves the highest MRR score on the FB15k dataset.
GRank achieves the highest fdMAP score on the WN18RR dataset.
The "wFST" model has the lowest WER and PER scores among all the models in the table.
The "wFST" model has the lowest WER 100 score among all the models in the table.
The experiment involved multiple models.
The "LangID-All" model achieved the lowest WER at 100.
Table 6 shows the results on languages not in the training corpus.
The WER 100 score for the NoLangID-All model is 56.29.
RWMN achieves the highest performance scores for the video+subtitle task on the MovieQA public validation and test datasets.
RWMN outperforms RWMN-noRW, RWMN-noR, RWMN-noQ, and RWMN-noVid on the video+subtitle task for the MovieQA public validation and test datasets.
The "Human" translation achieves the highest BLEU score.
The "Google" translation achieves the highest RCQ score in the simple weighting.
The "Graphs only" method has the lowest average monetary gains.
The "Multi-modal" method has the highest confidence score.
Table 2 provides the average monetary gains and confidence scores for different approaches.
The multi-modal approach has the highest monetary gains.
The overall performance of HAN+sum increases as the percentage of cells increases.
Pairwise considers all cells for aggregation.
HAN+pairwise has the highest number and other values among all the techniques.
The test accuracy for the feature set {Word identities, word shapes, word embeddings} is higher than the test accuracy for the feature set {Word identities, word shapes}.
The training accuracy for the feature set {Word identities, word shapes, word embeddings} is higher than the training accuracy for the feature set {Word identities, word shapes}.
The table shows the performance of LSTMs for PoS tagging using word identities and word shapes.
The test accuracy for "n≤25" is highest for 200 hidden units.
The ME model used by Le-Hong team achieved the highest performance of 88.78 at VLSP 2016.
Both Le et al. and [Anonymous] teams used the CRF model at VLSP 2016.
The table shows the results of different models tested on the WMT 2017 data for correcting a phrase-based system.
The "+Shared encoder" model shows an improvement in translation quality from the dev 2016 to test 2017.
The "Transformer x4 (all above)" model achieves the best performance in terms of TER↓ and BLEU↑ scores for all three test sets.
The "Transformer 2M" model performs better than the "Transformer 1M" and "Transformer 4M" models in terms of TER score for the test 2017 set.
MS-UEdin (Ours) outperforms other systems in terms of TER↓ and BLEU↑ scores.
FBK performs better than POSTECH in terms of TER↓ score.
The system "MS-UEdin (Ours)" outperforms all other systems in terms of TER and achieves a high BLEU score.
The system "DFKI-MLT" performs the worst among all the systems in terms of both TER and BLEU.
The "Composite Document Vector" method achieves the highest accuracy of 93.91 on the IMDB Movie Review Dataset.
The "Weighted WordVector+TfIdf(Our Method)" method achieves a higher accuracy of 90.67 compared to the "Weighted WordVector+Wiki(Our Method)" method on the IMDB Movie Review Dataset.
The table shows the results of Vector Composition with different Operations.
The Idf Graded Weighted Average composition method achieves the highest accuracy among all composition methods.
The method "Weighted Word Vector + tfidf" with "ANOVA-F" feature selection achieves the highest accuracy of 90.37.
The accuracy of the method "Document Vector + tfidf" increases when using feature selection techniques, such as "ANOVA-F".
The LSTM model has the highest accuracy for the DSTC 4 dataset.
The interlabeler agreement for the SwDA dataset is 84.0%.
Table 2 shows the WER comparison on CHiME-4 single-channel track evaluation sets with adversarial examples (AdvEx) (ϵ=0.1).
The WER for the Baseline system is lower on the et05_simu BUS dataset compared to the et05_real BUS dataset.
Table 1 shows the WER comparison on the Aurora-4 evaluation set with adversarial examples (AdvEx) (ϵ=0.3).
The WER reduction percentages in Table 1 are calculated by comparing the values in the "AdvEx" row with the values in the "Baseline" row.
The "Proposed Speech2Code" model performs better than the "Baseline Tacotron with MFCC input" model in terms of BLEU and METEOR scores.
Increasing the size of the codebook improves the BLEU and METEOR scores.
The LAS score for predicting language ID and POS tags with MaLOPa is 70.8 for the target language de.
The average LAS score for the target language sv is 84.2.
The method "WECT" achieves the highest Micro, Macro, and Rare classes accuracies compared to other methods.
The method "WECT" achieves the highest accuracy for the 50% rarest classes compared to other methods.
The perplexity of the trained language model is lowest for the target domain "WECT".
The perplexity of the trained language model is higher for the target domain "Physics" compared to the other target domains.
The table shows the accuracies of different models or combinations of models in a question answering system.
The addition of NQS, NTP, and IR components improves the accuracy of the question answering system.
Our system achieved the highest accuracy among the top three QA systems.
The baseline system achieved the lowest accuracy among the top three QA systems.
VLV-GM (MMI) has the highest accuracy score among all the models.
VLV-GM (MMI) performs better than LDA-HMM-GM (MMI) in terms of accuracy.
The table shows the adversarial success for different models.
The adversarial success for adver-3 is 0.054.
The table shows the performance comparison of different versions of m-RNN models on the Flickr30K dataset.
The m-RNN-EmbOneInput model achieves a score of 0.274 on the B-2 metric.
Among all the models listed in the table, "Ours-m-RNN-AlexNet" achieves the highest performance in terms of Sentence Retrival at R@1.
"DeepFE-RCNN" performs better than "Ours-m-RNN-AlexNet" in terms of Image Retrival at R@10.
The m-RNN-shared-NNref-BLEU-Orcale model performs better than the m-RNN-shared model on all metrics.
The m-RNN-shared-NNref-CIDEr model performs better than the m-RNN-shared-NNref-BLEU model on the CIDEr metric.
The proposed method of OCD + MTL with RNN dec. achieves the highest maF1 score.
The models using SVM achieve a high accuracy of 0.9967.
The proposed methods perform better than the baselines in all metrics.
The proposed methods have a higher accuracy than the baselines.
Table 5 provides a performance comparison on a resplited AAPD dataset, including different models and their performance metrics on both seen and unseen test sets.
The OCD model outperforms all other models in terms of miF1 and ebF1 scores on both the seen and unseen test sets.
The "Proposed methods" show better performance than the "Baselines" in terms of all performance metrics.
The "Logistic rescore" and "Logistic joint dec." methods achieve the highest accuracy among all the models.
The "HiCE + Morph + MAML" method achieves the highest F1-score in the Named Entity Recognition (F1-score) Rare-NER task.
The "HiCE + Morph + MAML" method achieves the highest accuracy in the POS Tagging (Acc) Twitter POS task.
Table 1 shows the performance on the Chimera benchmark dataset with different numbers of context sentences, measured by Spearman correlation.
The method "HiCE + Morph" achieves the highest Spearman correlation of 0.4053 on the Chimera benchmark dataset with 4-shot.
The F1 scores for the claims with context length values of 1, 2, 3, and 4 are increasing gradually.
The F1 score for the claim with a context length value of 3 is higher compared to the F1 scores for the claims with context length values of 1, 2, and 4.
Table 2 provides the number of claims with at least 5 votes above a certain threshold of agreement percentage for both the 3-class and 5-class cases.
As the agreement score threshold increases, the number of claims decreases for both the 3-class and 5-class cases.
The majority of the votes in the dataset belong to the "Medium impact" category.
The total number of votes in the dataset is 241,884.
Table 4 shows the number of claims for different ranges of context length, specifically for claims with more than 5 votes and an agreement score greater than 60%.
The number of claims decreases as the context length increases.
RoBERTa achieves the highest F0.5 score on the CoNLL-2014 test dataset among all the encoders.
BERT achieves a higher precision score on the BEA-2019 dev dataset than ALBERT.
The XLNet model achieves the highest F0.5 score on the CoNLL-2014 test dataset among the single models.
The ensemble model of RoBERTa and XLNet achieves the highest precision score on the BEA-2019 test dataset among the ensembles.
The table shows the performance of trained language models using different combinations of coarse-grained, fine-grained, and dense training methods.
The table demonstrates that certain combinations of training methods result in improved performance, while others lead to a decrease in performance.
The "3/4 fine + 1/4 dense" training method has the highest BLEU score among all the methods.
The "3/4 fine + 1/4 dense" training method has a positive improvement compared to the "Fine-grained" method.
MarCo has the highest F1 score among all the act generation methods.
Transformer (GEN) has a higher F1 score than Transformer.
The MarCo (BERT) model achieves a higher success rate compared to the MarCo model.
The Structured Fusion model achieves a higher BLEU score and combined score compared to other models in the "Without Act" row.
The ensemble of the Pretrained seq2seq system with 5 models achieves a BLEU score of 24.7 on newstest2014.
The ensemble of the Backtranslation system with 12 models achieves a BLEU score of 24.7 on newstest2014.
The models in Table 2 are divided into two groups: intrinsic models without using extra information and models that make use of additional information.
The model ITransF (domain sampling) performs better on FB15k dataset compared to WN18 dataset.
The "Sparse" method outperforms the "Dense" and "Dense + ℓ1" methods in terms of MR and H10 on both WN18 and FB15k datasets.
The "Sparse" method has the shortest training time per epoch compared to the "Dense" and "Dense + ℓ1" methods on both WN18 and FB15k datasets.
ITransF method outperforms Sparse Encoding method in terms of both "WN18 MR" and "FB15k MR".
ITransF method performs better than Sparse Encoding method in terms of both "WN18 H10" and "FB15k H10".
"GCN and memory networks" has the highest average rating among all the models.
"GCN and memory networks" has the highest percentage gain over the baseline model among all the models.
The method "AMUSED(DSTC)" has the highest precision@1 among all the methods listed in the table.
The method "IR Baseline" has a lower precision@1 compared to both "AMUSED(Persona Chat)" and "AMUSED(DSTC)".
The BRDNN model has the lowest test CER among all the models.
The BRDNN model has the lowest number of parameters among all the models.
Table 1 compares the WER and CER results of a BDRNN trained with the CTC loss function using different decoding methods.
The use of a dictionary constraint and bigram language model improves the CER and WER results compared to the baseline (No LM).
The table provides performances over different memory types.
The table provides performances for both referenced and unreferenced memory types.
The BLEU and ROUGE scores generally increase as the number of interlocutor's dialogue turns increases.
The "ICRED (ours)" model generally outperforms the "Persona Model" in terms of BLEU and ROUGE scores, except for the interval (1000, 5000].
The "ICRED" model has the highest BLEU and ROUGE scores compared to the other models.
As the main components are removed from the model, the BLEU and ROUGE scores decrease.
BERT SQ− Bmul achieves the highest F1 score for Japanese.
BERT SQ− Bmul achieves the highest EM score for French.
The "Dual BERT" system outperforms the "P-Reader (single model)" system on the CMRC 2018 dataset.
The "GNMT+BERT SQ− Len+Aligner" system performs better than the "r-net (single model)" system on the DRCD dataset.
The "Exact" method has a lower BLEU score compared to the "Greedy" and "Beam-10" methods.
The "Beam-10" method has a lower ratio compared to the "Greedy" method.
Using the SP-MTL-SC transfer model reduces the error rates for all tasks compared to using a single task model.
Using the SP-MTL-BC and ASP-MTL-SC transfer models reduces the error rates for the Baby task compared to using a single task model.
Table 2 shows the error rates of our models on 16 different datasets for various tasks.
The MT-DNN model performs better than other models in terms of error rates across multiple tasks.
The table presents an ablation study on the VQAv2 val-set, categorized into three different categories: I, II, and III.
The ablation study on the VQAv2 val-set employs different methods for each category: RAF-I(ResNet) for category I, Up-DownAnderson et al. (2018) for category II, and RAF-IO(ResNet-ResNet) for category III.
Table 2 compares the performance of different methods with our single model performance on VQAv2.0 test-dev and test-standard server.
The RAF (Ours) method has the highest overall performance on the test-dev dataset.
The ensemble model outperforms other models in terms of all metrics.
The table presents an ablation analysis on the development set for the DSTC7 Advising dataset.
The ensemble model outperforms the individual models in terms of R@1, R@10, R@50, and MRR.
The "w/o LSTM" model performs worse than the other models in terms of accuracy in both the R52 and Reuters21578 datasets.
The "Proposal" model achieves the highest accuracy score in both the R52 and Reuters21578 datasets.
The DRCN model achieves the highest MAP score among all the models in the SelQA dataset.
The DRCN model achieves the highest MRR score among all the models in the SelQA dataset.
LM-Transformer has the highest accuracy among all the models.
The accuracy for DRCN is higher than the accuracy for DIIN.
The "DRCN" model performs better in terms of MAP and MRR scores in the clean version compared to the raw version.
The "Comp.-Aggr." model outperforms the "BiMPM" model in terms of MAP score in both the raw and clean versions.
Table 6 compares the accuracy (%) of models ESIM, DIIN, CAFE, and DRCN on different linguistic categories.
Table 1 provides information about the InDEE-2019 dataset for five languages: Marathi, Hindi, English, Tamil, and Bengali, including the number of documents and sentences for each language in the train, validation, and test sets, as well as the number of labels for each dataset.
The number of English documents in the train, validation, and test sets are 456, 56, and 131, respectively.
The BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores for "Our system" are higher compared to the other methods.
The METEOR score for "Image + sentences" is higher compared to the other methods.
The German side has a larger vocabulary compared to the English side.
The "BL+3Features" system performs better than the "BL+4Features" system on the test set in terms of METEOR (norm) score.
The "Monomodal NMT" system performs better than the "Multimodal NMT" system on the validation set in terms of BLEU score.
ResNet-50 and ResNet-152 have similar BLEU scores for all four metrics.
VGG-19 has a lower BLEU-1 score compared to ResNet-50 and ResNet-152.
Table 5 presents the BLEU and METEOR scores of NMT-based submissions for Task 2.
The Test METEOR score for the multimodal system is 32.3 and the Test BLEU score is 19.2.
There are 8,909 instances labeled as neutral in the training set.
There are 2,600 instances labeled as stance in the test set.
In the test set, there are 18,349 instances labeled as "Unrelated".
The training set contains 49,972 instances.
The train set has 4,518 instances and the test set has 2,600 instances.
The number of instances labeled as "Disagree" is 840 in the train set and 697 in the test set.
The majority vote system has the lowest F1 [ITALIC] m score among all the systems.
The CombNSE system has the highest F1Neutral score among all the systems.
In Stage1, there are 17,668 unrelated instances and 681 related instances.
In Stage3, there are 1,436 instances where the stance is agree and 467 instances where the stance is disagree.
The F1-score decreases as different modules are removed from our method.
The method "HD + AT(seed abr. + comb.)" achieves the highest F1-score.
The "AT (seed abr. + comb.)" method achieves the highest F1-score among all the SLU systems on the DSTC3 evaluation set.
The "HD" system consistently outperforms other SLU systems in terms of F1-score on the DSTC3 evaluation set.
Transfer learning improves performance compared to the baseline in the enFI - ETen language pair.
Transfer learning improves performance in the enET - ETen language pair but does not improve performance in the ETen - enET language pair.
Table 2 presents the results of transfer learning with English reused either in the source or target, along with baselines for training on only the child or parent language.
Table 2 includes language pairs where the parent language is unrelated to the child language.
The transfer BLEU score decreases as the number of child training sentences decreases.
The transfer BLEU score is higher than the baseline BLEU score.
Table 7 provides a summary of vocabulary overlaps for different language sets.
The BLEU n-gram precisions for the three candidates (Base ENET, ENRU+ENET, ENCS+ENET) are increasing.
The brevity penalty for the three candidates (Base ENET, ENRU+ENET, ENCS+ENET) is close to 1.
Table 1 compares the performance of different models (vgg16, vgg16-frcn, and res101-frcn) on ground-truth MS COCO regions.
The model used for both the "val" and "test" splits in RefCOCOg is MAttNet.
The precision at the IoU threshold of 0.5 is higher for the "test" split compared to the "val" split in RefCOCOg.
Table 7 presents object detection results.
The "res101-mrcn" network achieves a higher APbb50 score than the "res101-frcn" network.
The table provides instance segmentation results for different networks.
The network "res101-mrcn (ours)" performs worse than the network "res101-mrcn" in terms of instance segmentation results.
Among the three models, GNMT has the lowest BLEU score and the highest TER score.
CMT2 has higher adequacy and fluency scores compared to CMT1.
The "Machine Translation" module has a contribution of 25.
The "Language Tagger" module has a contribution of 36.
The model with length encoding has a lower word error rate (WER) compared to the baseline and the model with length embedding.
The performance of MNLI (two-class) is higher than MNLI in the "Self" and "GLUE Diag." rows.
The performance of MNLI8.5k is the same in the "Self" and "MNLI" columns.
The table displays the annotated relations between entities, categorized into four different types of relations: "case has condition", "case has finding", "case has factor", and "modifier modifies finding".
The table shows that the majority of annotated relations between entities are "Inter-sentential" relations, accounting for 81.9% of the total relations.
The F1 score for BiLSTM CRF is higher than the F1 score for CRF and factor.
BioBERT has higher precision, recall, and F1 scores than CRF, BiLSTM CRF, and MTL.
The table compares the results for different dataset sizes (1% and 2%) for both the "Top" and "Bottom" strategies.
The "Top" strategy consistently outperforms the "Bottom" strategy in terms of "Validation Seen PL" for both dataset sizes (1% and 2%).
Table 3 shows the AUC performance of different combinations of the two tasks when evaluated on a dataset containing only PR and RW negatives.
The AUC performance is [BOLD] 84.0 for Val. Seen and [BOLD] 79.2 for Val. Unseen when using the combination of cma + nvs.
The ALTR (Ours) model achieves a lower Normalized Error (NE) than The Regretful Agent model.
The ALTR (Ours) model achieves the same Success Rate (SR) as the Self-Monitoring model.
When both "cma" and "nvs" are "✗", the value in the "Validation Unseen SPL ↑" column is 29.0.
When both "cma" and "nvs" are "✗", the value in the "Validation Seen SR ↑" column is 47.9.
CGExpan-MRR achieves the highest Mean Average Precision values on both Wiki and APR datasets.
MCTS achieves the highest Mean Average Precision value at MAP@10 on the Wiki dataset.
Table 6 presents the experimental results of different models in semantic matching between e-commerce concepts and items.
The "Ours + Knowledge" model outperforms the other models in terms of AUC, F1, and P@10 in the semantic matching between e-commerce concepts and items.
The table shows the experimental results of different sampling strategies in hypernym discovery.
The labeled size is reduced by 150k, 100k, and 175k for the US, CS, and UCS strategies, respectively.
The precision of the model increases as more components are added to the model.
The highest precision is achieved when the model includes the components Wide, BERT, and Knowledge.
The table presents the experimental results of three different models in shopping concept tagging.
The +Fuzzy CRF & Knowledge model achieves the highest precision, recall, and F1 scores among the three models in shopping concept tagging.
The F1 score for E2E NER is higher when using LM compared to without LM.
Both precision and recall scores for E2E NER are higher when using LM compared to without LM.
The accuracy of the LSTM model with Bert word embedding is 88.00%.
The F1-score of the SDM model with FastText word embedding is 90.92%.
The SVM model has the lowest accuracy among all the suicide risk detection models.
The SDM model has the highest F1-score among all the suicide risk detection models.
Adding user's features to the text improves the accuracy and F1-score compared to using only the text.
Adding user's features to the combination of text and image improves the accuracy and F1-score compared to using only text and image.
The table presents the BLEU and length ratio of two models, Seq2Seq and This work, on the Zh→En validation set.
The models used in the experiments are AWD-LSTM-MoS-BERTVocab, BERT, BERT-CAS (Our), BERT-Large-CAS (Our), AWD-LSTM-MoS-GPTVocab, GPT, and GPT-CAS (Our).
The BERT-Large-CAS (Our) model has the lowest perplexity score of 20.42 on the WT-103 dataset.
The search cost for BERT-CAS on PTB is 0.15 GPU days.
The search cost for DARTS (second order) is 1 GPU day on WT-2.
Table 6 compares the model parameter size and results with GPT-2.
The parameter size for BERT-Large-CAS is 395M.
The proposed pretrained DNN approach achieves higher scores than both the baseline CRF/MaxEnt and baseline DNN approaches in all evaluated metrics.
The proposed pretrained DNN approach shows a statistically significant difference in SER compared to both the baseline CRF/MaxEnt and baseline DNN approaches.
The training set size varies for each method and domain.
The "CMLM + Mask-Predict" model has 4 and 10 rescored candidates.
The BLEU score for the "DisCo + Easy-First" model in the de\rightarrowen direction is 31.31.
Table 4 provides the BLEU scores for different models in translation tasks across different language pairs.
The difference in BLEU scores between the "en\rightarrowde raw" and "en\rightarrowde dist." tasks for the "CMLM + MaskP" model is 2.8.
Table 6 shows the dev results from bringing training closer to inference.
The "Random Sampling" variant has a higher BLEU score than the "Easy-First Training" variant for the en\rightarrowde translation task.
The Parallel Easy-First decoding strategy achieves the highest BLEU score for the en\rightarrowde translation task.
The Mask-Predict decoding strategy requires 10 steps for the ro\rightarrowen translation task.
Table 8 shows the results of an extremely large Q&A dataset.
TMKD has the highest performance among all the models.
The TKD model performs better than the KD model on all four datasets.
The TKD model achieves a higher accuracy than the KD model on the MNLI dataset.
Table 9 compares different metrics for multiple datasets.
The accuracy of the models improves as the number of transformer layers increases for all datasets.
Bill Clinton has the highest frequency of co-occurrence with skip-grams among the president entities in the table.
Bill Clinton has the highest sum of frequencies among the president entities in the table.
Table 3 shows the Mean Average Precision (MAP) across all queries on Wiki and APR for different methods.
The Set-CoExpan method performs better than other methods in terms of Mean Average Precision (MAP) at different cut-off values.
The table presents the results of an experiment using different fusion strategies.
The fusion strategies used in the experiment are "RNN", "Concat", and "Sum".
The table presents the results of an experiment using different attention mechanisms.
The Top Down (σ=ReLU) attention mechanism achieves a validation score of 64.12 with the GRU model.
The value 0.4982 represents the performance of FT_T on BERT.
The F1 score decreases as the degree of the polynomial kernel increases.
The precision (P) decreases as the degree of the polynomial kernel increases.
The table shows the performance of TempRelPro in terms of coverage, precision, recall, and F1-score for all domains, compared with systems in QA TempEval augmented with TREFL.
The F1-score of TempRelPro + coref is higher than the F1-score of TempRelPro without coreference resolution.
The F1 score is highest for "S2" when using the "Word2Vec" feature vector ((→ w1⊕→ w2)⊕→ f).
The F1 score is lower when using traditional features compared to any of the feature vectors.
The F1-scores for each TLINK type are higher with the feature vector (→w1⊕→w2) compared to the other feature vectors.
The F1-scores are higher for the feature vector (→w1−→w2) compared to the other feature vectors.
The F1 score for TLINK type "AFTER" is 0.5043.
The precision (P) for TLINK type "IDENTITY" with feature vector "f" is 0.7333.
The table provides the micro-averaged scores for the system "CauseRelPro-beta" in the task "CauseRelPro-beta".
The system "CauseRelPro-beta" has higher precision, recall, and F1 score compared to the system "mirza-tonelli:2014:Coling".
The system that includes increased training data, EMM-clusters, and propagated CLINKs (Causal-TimeBank + EMM-clusters + prop. CLINKs) achieves the highest F1 score among the three systems.
The system that includes increased training data, EMM-clusters, and propagated CLINKs (Causal-TimeBank + EMM-clusters + prop. CLINKs) achieves higher precision and recall values than the other two systems.
Table 7 shows the macro-averaged F1 score for multi-class author classification on large datasets using different types of discourse features.
The nnet-768 model takes more than real-time to process on a Raspberry Pi 3.
The nnet-768 model has a larger size than the nnet-256 model.
API.ai* has the lowest F1-score among all NLU providers in the web apps category.
The F1-score for the NLU provider "Snips" with a train size of 70 is [BOLD] 0.790.
The precision for the NLU provider "API.ai" with a train size of 2000 is 0.905.
Table 16 shows the precision, recall, and F1-score averaged on all slots in an in-house dataset, run in June 2017.
The F1-score for the "SearchCreativeWork" intent is higher for Luis than for Wit, API.ai, Alexa, and Snips, regardless of the train size.
Table 17 provides precision, recall, and F1-score for different intents in an in-house dataset.
Table 2 presents the results of Wikipedia in terms of bits-per-character.
The validation score for Wikipedia in terms of bits-per-character is 1.67.
The "adaptive weight noise" regularization technique results in a higher log-loss value compared to the "none" regularization technique.
The "none" regularization technique results in a lower SSE value compared to the "adaptive weight noise" regularization technique.
The table shows the WER of various models on the WSJ corpus.
The EEMMI trigram model achieves a WER of 7.05% on the dev93 dataset.
The table shows the results on WMT’14 English-German translation.
Our model (Ours) with the Trans.-Big model achieves a BLEU score of 30.01 on WMT’14 English-German translation.
The method trained with BackTranslation performs better than the baseline methods trained on different backbone models.
The Miyato:17 method achieves a performance of 45.11 on the MT06 dataset.
The table shows the results of two different methods, "Vaswani:17" and "Ours", on NIST Chinese-English translation.
The table shows the results of two different models, "Trans.-Base", for the "Ours" method on NIST Chinese-English translation.
The "M2 (proposed)" model has the highest F-score among all the models in the table.
The F-score of the "SVM + n-grams + sentiment" model is 78.90.
The confusion matrix shows that there are 88 instances correctly predicted as negative.
The model correctly predicted the sentiment as NO 388 times.
The model correctly predicted the sentiment as YES 1002 times.
The table represents the actual and predicted sentiment analysis results.
The actual sentiment is "NO" in 445 cases.
The table represents the confusion matrix for sentiment analysis.
The number of actual NO sentiments predicted as YES is 277.
The table shows the confusion matrix for sentiment analysis, with actual sentiment values of NO and YES, and predicted sentiment values of NO and YES.
The total number of instances predicted as NO in the sentiment analysis is 911.
The table represents the confusion matrix for sentiment analysis.
The number 420 in the table represents the count of actual YES sentiments that were predicted as YES.
The number of actual NO sentiments predicted as NO is 413.
The number of actual YES sentiments predicted as YES is 824.
The table represents the confusion matrix for sentiment analysis.
The total number of actual "NO" sentiments is 1738.
The number of actual NO sentiments predicted as NO is 1032.
The number of actual YES sentiments predicted as YES is 316.
The table presents the readmission prediction performance of two models: LSTM and CC-LSTM.
CC-LSTM has the highest average recall (A.R.) among the two models.
The total number of examples in the table is 100.
The percentage of hits at 10 for LI is higher than for Non-LI.
The table shows the post-discharge mortality prediction performance of three different models: Model, LSTM, and CC-LSTM.
The post-discharge mortality prediction performance improves for all three models when predicting mortality at the 1-year mark compared to the 30-day mark.
The table provides examples from the validation splits of VQA 1.0 and VizWiz datasets.
The table includes the model's prediction for each example from the VQA 1.0 and VizWiz datasets.
The table presents results for four different VQA datasets: VQA 1.0, VQA 2.0, VQA-abstract, and VizWiz.
The MaSSeS metric is reported for two different thresholds: 0.7 and 0.9.
The table provides accuracy and macro-F1 (%) scores for aspect-based sentiment analysis on three popular datasets in different domains.
The BERT-PT-LSTM method achieves the highest accuracy and macro-F1 (%) scores for aspect-based sentiment analysis on the "Laptop" and "Restaurant" domains.
BERT-LSTM achieves the highest accuracy on the dev and test sets.
MT-DNN outperforms BERT-Attention in terms of accuracy on the dev and test sets.
Table 3 shows the test performance (classification error) as polarity classifiers using different recommender systems and hybrid models.
The hybrid model with the recommender systems A_U200_I120 has the lowest classification error among all subsets.
Table 2 shows the precision of retrieval for the set-follow operation with various count-min sketches.
The precision of retrieval for the set-follow operation with a count-min sketch of width=2000 is 100.0 at k=1, k=10, k=100, and 99.98 at k=1000.
The precision, recall, and F1 scores for the "Set" reasoning task are consistently 100.0 for all values of k.
The recall, precision, and F1 scores for the "Intersect" reasoning task generally increase as the value of k increases.
The recall values for set-follow operation increase as the value of k increases for both single task and multi task settings.
The recall values for set-follow operation are higher in the multi task setting compared to the single task setting.
Table 5 presents the Hits@1 results of WebQuestionsSP and MetaQA (2-hop and 3-hop) datasets.
PullNet achieves a Hits@1 score of [BOLD] 99.9 on the WebQuestionsSP dataset.
Table 10 presents an ablated study on WebQuestionsSP, evaluating different variations of EmQL.
Among the different variations of EmQL, EmQL (no-sketch) achieves the lowest score on WebQuestionsSP.
The "Ours" system outperforms the "Hier" system on the Chinese-English task (see Table 5).
The performance of the system combination methods improves as the number of single MT systems increases (see Table 5).
The method labeled "Ours" achieves the highest scores in all NIST evaluations.
The method labeled "Ours" achieves a higher BLEU score compared to the other methods.
Table 3 presents the results on the English-German task, including the case-sensitive tokenized BLEU scores for different methods.
Our method achieves a BLEU score of 30.52 on the English-German task.
Table 2 shows the results of human evaluation.
The SMAE model performs better in sentiment and fluency compared to the CAE and MAE models.
Table 1 provides the performance of the proposed method and state-of-the-art systems.
The proposed method outperforms the baseline with an SMAE score of 76.64 (+2.05).
Table 1 shows the results of direct bias using different methods for comparing Art-Maths, Arts-Sciences, and Career-Family.
The direct bias is stronger for Arts-Sciences and Career-Family compared to Art-Maths.
Table 3 shows the evaluation results for models trained on Daily Mail and their generated texts.
The model with λ=0.5 has a perplexity of 0.000 for CB| o and EBd.
The "+Bag-of-Words" model achieves the highest translation performance among all the models tested.
The "+Bag-of-Words" model achieves better translation performance than the other models on all tested datasets.
The CDME model has the highest F1 score among all the models.
The "Simple Concat - 3 (en + code + config)" model has the highest precision, recall, and F1 score among all the "Simple Concat" models.
The "Segmented Question - Predicted" method has the highest MRR value among the three methods.
The "Segmented Question - Gold" method has a higher MRR value than the "Full Question" method.
The BLEU score improves as the model complexity increases in the WMT14 English⇒German translation task.
The improvement in BLEU score between consecutive models decreases as the model complexity increases in the WMT14 English⇒German translation task.
The Our-single model achieves higher accuracy than the model without method b†.
The model without subtask level transfer has lower accuracy than the Baseline model.
ECM outperforms Seq2Seq and Emb in terms of both overall content and overall emotion evaluation.
Seq2Seq performs worse than ECM in terms of disgust content evaluation.
The method "ECM" has the highest accuracy score of 0.773.
The method "w/o EMem" has the lowest perplexity score of 61.8.
The ECM method has the highest percentage of responses with a content score of 2.
The Seq2Seq method has the highest percentage of responses with an emotion score of 0.
The table shows the confusion matrices for using heuristic h1 with τ=25%n, heuristic h1 with τ=50%n, and heuristic h2 with flexible τ.
The number of correct predictions is higher when r > 25% compared to when r ≤ 25%.
Our model achieves an accuracy of 94.44% in the global setting.
The global model proposed by Pershina et al. in 2015 achieves an accuracy of 91.8%.
The local accuracy of our model is 85.73%.
The global accuracy of our model is 87.9%.
For heuristic h1 with τ=50%, there are 2071 correct predictions and 331 incorrect predictions.
For heuristic h1, the number of incorrect predictions when r > 50% is 81.
When using heuristic h2 with flexible τ, there are 805 correct predictions for label 0.
When using heuristic h1 with τ=50%n, there are 79 incorrect predictions for label 1.
Table 5 presents the results of four different models on the CoNLL 2003 Test-b dataset.
The model "Global + LDS" with "h2" heuristic and "flexible" depth has the highest accuracy of 94.44%.
The "ReMine" method achieves the highest F1 score on both the NYT and Wiki-KBP datasets.
The "Liu. et al. (2017arXiv170904109L)" method achieves the highest F1 score on the NYT dataset.
ClausIE performs the worst among all methods in terms of Precision@K, MAP, NDCG, and MRR on the NYT dataset.
The table provides accuracies of normalization and POS tagging on test data for different datasets and tasks.
The "Gold" value is 100.00 for both the "LAI" and "Multilingual" tasks in the "Language-aware" column.
Table 5 provides accuracies for POS tagging using a variety of normalization strategies.
The accuracy for POS tagging using the "Gold" normalization strategy is higher than the accuracies for other normalization strategies.
The model "CompClip-LM-LC" has the highest dev MAP score.
The model "PS-rnn-elmo" has the highest train MRR score.
The model "PS-rnn-elmo-s" has the highest dev MAP and dev MRR scores compared to the other types.
The train MAP and train MRR scores for all types are higher than the corresponding dev scores.
Table 6 shows the performance of different models with different methods for computing node representation.
The PS- [ITALIC] rnn-elmo model has the highest dev MAP score among all the models.
Table 3 compares the performance of the BiLSTM baseline with SentModel on the Dev set.
The BiLSTM model has a higher F1 score compared to the SentModel.
BERT-Two-Step outperforms other baselines in terms of F1 score on both the Dev and Test sets.
BiLSTM achieves a higher F1 score on the Test set compared to the LSTM model.
The models are listed in ascending order based on their performance on the "Restaurant Acc" metric.
The models have higher macro-F1 scores on the "Laptop" aspect compared to the "Restaurant" aspect.
Table 2 provides comparisons of baseline models on the Restaurant dataset and Laptop dataset.
SDGCN-BERT performs better than AEN-BERT on both the Restaurant and Laptop datasets.
Table 3 shows the MAP performance on three BMASS relations with at least 100 unigram analogies.
CBOW UniM performs better than CBOW MWE on the L4 relation.
The proposed method "Ours:SG+RWS+WR" achieves the highest scores for BLEU-1, BLEU-4, METEOR, ROUGE-L, CIDEr, and SPICE among all the models.
The proposed method "Ours:SG+RWS+WR" outperforms the "Up-Down" model in terms of cross-entropy loss for BLEU-1, BLEU-4, METEOR, ROUGE-L, CIDEr, and SPICE.
Table 2 compares the performance of the proposed methods to other state-of-the-art models after cider optimization training.
The proposed method "att2in+SG+RWS+WR" outperforms the baseline method "att2in [Rennie_2017_CVPR]" in all metrics (B-4, M, R, C, S).
The table shows the performance of the text-retrieval model on the MSCOCO Karpathy validation set.
Increasing the value of K does not significantly affect the Cross-Entropy Loss for most metrics, except for the "Cross-Entropy Loss C" metric, which shows a slight increase.
The Cross-Entropy Loss for the "K=5" experiment is higher than the Cross-Entropy Loss for the "K=15" experiment.
The F1-Score for Turkish using BERT is 0.816.
The average F1-Score for all languages using SVM with TF-IDF is 0.760.
The "LR (this work)" method achieves the highest accuracy of 87.84 on the 20 Newsgroups dataset.
The "Distributed structured output" method achieves an accuracy of 84.00 on the 20 Newsgroups dataset.
The method "Paragraph vector" achieves the highest accuracy score of 87.8 on the Stanford sentiment treebank dataset.
The "LR (this work)" method and the "Recursive neural networks" method achieve the same accuracy score of 82.4 on the Stanford sentiment treebank dataset.
LR (this work) achieves the highest accuracy among all the methods on the Amazon electronics dataset.
Sequential CNN outperforms NN-{1,2,3}-grams in terms of accuracy on the Amazon electronics dataset.
The method "LR (this work)" achieves the highest accuracy of 78.59 on the U.S. congressional vote dataset.
The "Min-cut" method achieves an accuracy of 75.00 on the U.S. congressional vote dataset.
Table 3 provides results for identifying the Arg1 and Arg2 subtree nodes for the SS case under the GS + no EP setting for the three categories.
The F1 scores for Arg1, Arg2, and Rel in the "All" category are 86.63, 93.41, and 82.60, respectively.
Table 1 provides the results for the connective classifier.
The accuracy for the "GS" classifier is higher than the accuracy for the "Auto" classifier.
The F1 score for Arg1 is 86.67 when using GS + no EP.
The F1 score for Rel is 86.24 when using GS + no EP.
The table shows the results for three different classifiers: GS + no EP, GS + EP, and Auto + EP.
The F1 score for GS + no EP is higher than the F1 scores for GS + EP and Auto + EP.
Table 6 shows the results for the non-explicit classifier.
The precision for the attribution span labeler with the "GS + no EP" setting is 79.40.
The F1 score for the attribution span labeler with the "Auto + EP" setting is 42.59.
The "MultiSpan" model has the highest English LF score and Chinese LF score.
The "BiaffineRule" model has a higher English LR score than the "BinarySpan" model.
The value for English word embeddings is 100, the value for Chinese word embeddings is 80, and the value for character embeddings is 20.
The maximum training epochs is 50 and the dropout rate is 0.5 for English and 0.3 for Chinese.
The LF score of the charniak2005rerank parser is lower than the LF score of the petrov2007unlex parser.
The LF score of the zhu2013acl parser is higher than the LF score of the zhang2009tran parser.
As the number of permutations P increases, the FOM (Median Example) and OTWV (Median Example) also increase.
The Best Example performance for P@10, FOM, and OTWV is consistently higher than the Median Example performance for the corresponding metrics.
As the number of permutations P increases, the median example FOM and OTWV also increase.
For P=16, the best example P@10 is 84.8.
As the number of permutations P increases, the Median Example FOM and OTWV values also increase.
For each number of permutations P, the Best Example FOM, OTWV, and P@10 values are higher than the Median Example values.
The Arabic monolingual model has a higher precision than recall.
The recall of the French monolingual model is 61.0.
The CNN model performs better than the RF model in the cross-lingual experiments.
The recall score is higher than the precision score for the Fr→Ar cross-lingual experiment using the CNN model.
The "TbTQT" model achieves the highest R2@1 score among the unsupervised models.
The "CLMN" model achieves the highest R10@5 score among the cross-lingual information retrieval models.
The CLMN model outperforms all other models in terms of R2@1, R10@1, R10@2, R10@5, and MRR scores.
Among the models initialized with BERT, the CLMN model performs the best in terms of R2@1, R10@1, R10@2, R10@5, and MRR scores.
The F1 score for the Hinglish dataset is higher than the F1 score for the Spanglish dataset.
The Macro Precision for the Spanglish dataset is higher than the Macro Recall.
The accuracy of humans on the 3-Sent validation set is 0.258*.
The accuracy of the AttCon-LSTM model on the 3-Sent test set is 0.288.
The highest value in the "more than half" column is in the row labeled "many".
The intercept coefficient for the variable [ITALIC] z11 is -0.0571.
The F-value for the smooth term "s(zValuePerc): [ITALIC] z14" is 36.6899.
The AIC values of five fitted models with corresponding degrees of freedom (df) are listed in Table 2.
The AIC value for the "Modified" model is lower than the AIC value for the "Excluded" model.
The t value for the coefficient of the interaction term "#sTV:[p]" is -3.9979.
The estimate for the coefficient of the variable "#TV vs. #sTV" is -0.8389.
The intercept ([ITALIC] z11) is statistically significant with a p-value less than 0.0001.
The smooth term s(zValuePerc): [ITALIC] z14 is statistically significant with an F-value of 5.7228 and a p-value less than 0.0001.
The coefficient for [ITALIC] z14 is not statistically significant (p-value > 0.05).
The smooth term for [ITALIC] z14 is statistically significant (p-value < 0.05).
The intercept ([ITALIC] z11) in the generalized additive model is statistically significant.
The smooth term s(zValuePerc): [ITALIC] z49 in the generalized additive model is not statistically significant.
The variable [ITALIC] z14 has a significant effect on the dependent variable.
The smooth term fs(zValuePerc,sameValues,m=1,k=10) has a significant effect on the dependent variable.
[ITALIC] z26 has a statistically significant effect on the dependent variable.
fs(zValuePerc,trajectoryZ,m=1,k=7) has a statistically significant effect on the dependent variable.
CNN2 achieves the highest test accuracy of 80.19% among all ML models.
BoW/SVM achieves a test accuracy of 80.10%.
The model correctly learns to focus on the original API call and the changes demanded by the user in Personalization task 2 (Updating API calls). It does not require multiple hops to solve this task.
The values in the "Hop #3" column are all 0.
The model successfully solves the personalization task 1 (Issuing API calls) by predicting the correct answers.
In the third hop, the model's attention is focused primarily on the "price range" field of the API call.
The predicted answer for all the entries in the "User Input" column is "what food are you looking for".
The correct answer matches the predicted answer for all the entries in the "Predicted Answer" column.
The table compares the performance of different models such as HREDL, HREDG, HREDLG, HREDL-CR, HREDG-CR, HREDLG-CR, HREDL-AR, HREDG-AR, HREDLG-AR, HREDL-CAR, HREDG-CAR, and HREDLG-CAR.
The proposed method achieves the highest accuracy for the Literature category in the factoid QA task.
The proposed method achieves an accuracy of 88.1 in the factoid QA task.
The table provides hyper-parameter settings for different datasets in EL and QA tasks.
The hyper-parameter "dropout" has different values for each dataset in the EL and QA tasks.
NTEE (w/o strsim) has a lower accuracy than NTEE.
SG-proj-dbp has a higher accuracy than SG-proj.
Table 8 provides performance results for translation in both directions, Zh-En and En-Zh.
Table 8 compares the performance of different translation models using different basic units.
Table 5 shows the performance of P2P-Tran with different combinations of basic units for the source and target languages.
P2P-Tran achieves higher performance in the Zh-En direction compared to the En-Zh direction for all combinations of basic units.
Table 7 shows the performances of P2P-Tran compared with T2T-Tran.
The performance of P2P-Tran in the En-Zh direction is higher in the Text Space Test.
The Hadamard fusion method has the highest CIDer score for the noun tag.
The Hadamard fusion method has the highest Meteor score for the verb tag.
The DAN(K=4)+LQIA model has the highest rank-correlation score on the HAT val dataset.
The LSTM Q+I+Attention(LQIA) model has the highest score on the VQA1.0 Open-Ended (test-dev) dataset.
The table presents the analysis of network parameters for different variations of the DCN model.
The DCN Mul_v2(K=4)+MCB model achieves the highest performance across all question types in the VQA1.0 Open-Ended (test-dev) dataset.
The table shows the VQA2.0 accuracy on the validation set for different variations of DCN and DAN models.
The DCN Mul_v2(K=1) +LQIA and DCN Mul_v2(K=4)+MCB models have the highest accuracy among their respective groups.
The "MDN-Joint (Ours)" method achieves the highest METEOR score among all the methods in the table.
The "MDN-Joint (Ours)" method achieves the highest CIDEr score among all the methods in the table.
The Tag-q3-con combination achieves the highest scores for the Meteor and Rouge metrics.
The Tag-v3-con combination outperforms the Tag-n3-con combination in terms of BLEU-1 and CIDer metrics.
The "AM" model with "IE(K=5)" achieves a BLEU-1 score of [BOLD] 24.4.
The "HM" model with "IE(K=4)" achieves a CIDer score of [BOLD] 33.8.
The TP-N2F model achieves the highest accuracy for both Operation Accuracy and Execution Accuracy on the MathQA dataset testing set.
The SEQ2PROG-orig model has the lowest accuracy for both Operation Accuracy and Execution Accuracy on the MathQA dataset testing set.
The TP-N2F model achieves the highest accuracy in all three testing sets.
The LSTM2TPR model outperforms the LSTM2LSTM+atten model on the cleaned testing set.
As the value of K increases, the clean accuracy decreases.
The certified accuracy is highest when K is 100.
The F1 score for Span SRL is 87.8 and the F1 score for Dependency SRL is 87.9 in the baseline model.
The precision for Span SRL is 88.1 and the recall for Span SRL is 87.5 in the baseline model.
Our system with gold syntax achieves an F1 score of 87.2 on the CoNLL05 WSJ dataset.
Our system with predicted syntax achieves an F1 score of 87.9 on the CoNLL09 WSJ dataset.
The CAAE model has a higher percentage agreement with human scores compared to the ARAE model.
The Neural Adv. Clf. has a higher percentage agreement with human scores compared to the Unigram Adv. Clf.
The content preservation of style-masked texts is consistently higher than unmasked texts for all models.
On average, the content preservation of style-masked texts is higher than unmasked texts.
Table 11 shows the performance on previously unobserved Xs and Zs.
The performance of augmented-InferSent is higher than InferSent for all categories.
The performance of [ITALIC] C1 [ITALIC] T∗ is higher than the performance of [ITALIC] C2 [ITALIC] T∗.
Table 7 shows the sentiment analysis results on an RNN model pre-trained with aligned Uzbek word embeddings.
The sentiment analysis model performs better on the translated dataset compared to the manual dataset.
The "whole pip." method achieves the highest scores for Ans EM, Ans F1, Sup EM, Sup F1, Joint EM, and Joint F1 on both the Dev set and Test set.
The "whole pip." method outperforms the "MUPPET" method in terms of Ans EM, Ans F1, Sup EM, Sup F1, Joint EM, and Joint F1 on the Dev set.
The full system (single) achieves the highest performance in terms of F1 score, LA, and FS on the FEVER dataset.
The performance of the Hanselowski system is higher on the dev set compared to the test set.
The "Yes/No" answer type has the highest accuracy among all the answer types.
The accuracy for the "Common noun" answer type is 27.3%.
The retrieval filtering hyper-parameter "k" can be set to either 2 or 5 for paragraph-level retrieval.
The retrieval filtering hyper-parameter "k" can be set to either 2 or 5 for sentence-level retrieval.
Table 10 provides the means of the absolute differences in Pearson's r between each pair of models over all 25 STS competition datasets.
The mean absolute difference in Pearson's r between the "Word / LSTM" model and the "Trigram / LSTM" model is 2.89.
Our Model outperforms Tree-LSTM in terms of accuracy.
Our Model (Pre-parsed) performs better than LSTM (No KG) in terms of accuracy.
The FG Reader achieves the highest accuracy on the CBT-NE Test dataset.
The AS Reader performs better than the MemNets on the CBT-CN Valid dataset.
The SAW Reader model achieves the highest accuracy on the CMRC-2017 Valid and CMRC-2017 Test datasets.
The GA Reader model outperforms the AS Reader model on the CMRC-2017 Valid dataset.
The table presents the results of a case study on CMRC-2017 using three different models: [EMPTY], Word + Char, and Word + BPE.
The table shows the performance of different operations (concat, sum, and mul) on the CMRC-2017 dataset.
Table 4 shows the accuracy results on the PD and CFT datasets for different models.
The SAW Reader model achieves the highest accuracy on both the PD Test and CFT Test-human datasets.
The loss stride for GramCTC is 21.46.
The WER for GramCTC at epoch time 4 is 18.27.
GramCTC has higher CER and WER than CTC on the development set.
The Mix-3 model outperforms the Baseline and the 2×Baseline* models on the overall dev set.
The Baseline model has the lowest serving latency among all models.
The performance of DCMN_BERT is lower than BERT.
The performance of DCMN_XLNet is higher than XLNet.
The performance of the models generally improves as we move down the table.
The performance of the models with additional features (DCMN, PSS, AOI) is higher compared to the models without these features.
The "adapted" method has a higher success rate than the "best prior" method for both SFR Name and SFH.
The "adapted" method has a higher reward than the "best prior" method for both SFR Name and SFH.
The "SFR+SFH" and "SFR+L11" committee policies have higher rewards than the "SFR" and "SFR+SFH+L11" committee policies.
The "SFR+SFH" and "SFR+L11" committee policies have lower success rates than the "SFR" and "SFR+SFH+L11" committee policies.
Table 2 displays the BLEU scores of different approaches on the validation set.
The NMT Ensemble approach achieves a BLEU score of [BOLD] 42.54 for En-De.
Table 7 represents BLEU scores on the validation set for task 1B.
The highest BLEU score on the validation set for task 1B is achieved by the NMT model in the "(En+Fr+De)-Cs" language pair.
The Method w2v[c+w]norm has a Pu F1 score of 76.68 and a B3 F1 score of 68.10.
The Winner method has a Pu F1 score of 78.15 and a B3 F1 score of 70.70.
The "Agglomerative Clustering" method achieves a Pu F1 score of 62.10 and a B3 F1 score of 49.49 in Subtask B.2 with ID.
The "Logistic Regression" method achieves a Pu F1 score of 68.22 and a B3 F1 score of 58.61 in Subtask A with ELMo[c+w]norm.
Table 7 presents the results of an ablation study on the English development set, where different variations of the system were tested.
The system variation without predicate-specific encoding has the lowest F1 score among all the tested variations.
The system "Ours (local)" achieves the highest F1 score among all the systems listed in Table 2.
The system "roth-lapata:2016:P16-1 (global)" has a higher precision than the systems "lei-EtAl:2015:NAACL-HLT (local)" and "fitzgerald-EtAl:2015:EMNLP (local)".
The OOTV word rate is lower for WSJ, OntoNotes, and Avg. compared to Answers, Emails, Newsgroups, Reviews, Weblogs, and Avg.
The UAS is higher for WSJ, OntoNotes, and Avg. compared to Answers, Emails, Newsgroups, Reviews, Weblogs, and Avg.
The table shows the average Web Treebank development UAS at different threshold settings.
The UAS scores at threshold settings t1, t3, t5, and t∞ are 84.89, 84.97, 84.81, and 84.14, respectively.
The accuracy for the "Nationality-adjective" relation is the highest among the three relations.
The sense difference for the "Capital-country" relation is the lowest among the three relations.
The highest precision value in the "prec@1" column is in the "Compositional sense" row with a value of 0.482.
The lowest precision value in the "prec@3" column is in the "Aspectual global" row with a value of 0.042.
The table shows the number of labeling functions and the relation types they can annotate for two kinds of information: "Wiki-KBP" and "NYT".
The "NYT" kind of information has 25 relation types.
The "NYT" dataset has more relation mentions annotated as None compared to the "Wiki-KBP" dataset.
The "NYT" dataset has more conflicts involving None compared to the "Wiki-KBP" dataset.
The Ori method performs better than the TD and US methods on the Wiki-KBP dataset in terms of F1 score.
The Wiki-KBP dataset has a higher accuracy than the NYT dataset.
The LSTMavg model performs better than the avg model in most sentence pairs, except for sentence pair 5.
The gold similarity scores are higher than both the LSTMavg and avg similarity scores in most sentence pairs.
Table 4 provides the results on SemEval textual similarity datasets for LSTMavg, avg, and GRAN models.
The LSTMavg model achieves the highest score of 80.3 on the "headline" dataset.
The model GRAN with scrambling regularization achieves a score of [BOLD] 83.4 on the SICK dataset.
The model avg with word dropout regularization achieves an average score of 80.6 on the two datasets.
Table 8 shows the impact of initializing and regularizing toward universal models in supervised training.
Table 8 compares the performance of different models in supervised training.
The character level ELMo model outperforms other models on all datasets.
The CKIP toolkit performs better than other word segmentation toolkits on the AS dataset.
The char-level ELMo model outperforms the baseline model in terms of accuracy for all the datasets.
The PKU dataset has the lowest OOV rate among all the datasets.
The F1 scores on the failure set for syntactic parsing increase after the syntactic parsing conversion.
The F1 scores on the failure set for SRL increase after the SRL conversion.
The failure rate for SRL-NW on the BC genre is 26.86%.
SRL-100 has 25.6k examples.
The highest absolute difference in BLEU score for each head of the encoder's self-attention mechanism is -0.53 in LayerHead 6.
The change in BLEU score is statistically significant in LayerHead 5 and LayerHead 6.
Adding gold mentions improves the performance of the Our Model on both the CoNLL and i2b2 test sets.
The Our Model with gold mentions performs better than the End2end model with gold mentions on both the CoNLL and i2b2 test sets.
The Our Complete Model has the highest F1 scores for all three categories (Third Personal, Possessive, Demonstrative).
The Our Complete Model has higher precision and recall scores for the Demonstrative category compared to the other models.
The F1 scores for Third Personal, Possessive, and Demonstrative are higher for the Our Complete Model compared to the other models.
The F1 scores for Third Personal, Possessive, and Demonstrative are lower for the Without KG Model compared to the other models.
Table 6 provides information on the cross-domain performance of different models.
Our Model performs better on the i2b2 test data compared to the CoNLL test data.
The performance of the GloVe embeddings is lower for MWE-based evaluation metrics compared to token-based evaluation metrics.
The NERima+ system performs better on MWE-based evaluation metrics compared to token-based evaluation metrics.
BERT (Gold) has a higher combined accuracy than GloVe (Gold).
Kirilin et al. has a higher precision than BERT (Pred.).
There are three different methods used in the experiment: OnlyText, Frozen-WT, and Adaptive-WT.
The Adaptive-WT method achieves the highest BLEU score for the EN→DE translation task.
The table presents the experimental results of four different Seq2Seq-based systems: OnlyText, DAMT, Imagination, and Proposal (Hard).
Among the Seq2Seq-based systems mentioned in the table, the Proposal (Hard) method achieves the highest BLEU score for the DE→EN translation.
The Proposal (Hard) method achieves the highest BLEU score for the DE→EN translation task.
The Proposal (Soft) method achieves a METEOR score of 70.1 for the EN→FR translation task.
The table presents the results of incremental analysis using different methods.
The Proposal (Hard) method achieves the highest BLEU score for the EN→DE translation task.
The F1 scores for the combination "RD → RD + RG + LM" and "RD (8) → RD (8) + RG (8) + LM" are higher than the F1 scores for other combinations.
The F1 score for the "RD → RD + RG + LM" combination is higher than the F1 score for the "Dyer et al. (2016)-generative" model.
Table 1 shows the F1 on the development set for word-synchronous beam search when searching in the RNNG generative (RG) and LSTM generative (LM) models.
Increasing the word-synchronous beam size, Kw, leads to higher F1 scores in both the RNNG generative (RG) and LSTM generative (LM) models.
Table 2 shows the development F1 scores on section 22 of the PTB when using various models to produce candidates and score them.
The F1 score for the RD model is 92.22.
Table 2 provides the development F1 scores on section 22 of the PTB when using various models to produce candidates and score them.
The scoring model RD consistently achieves the highest F1 scores in all three scenarios.
The cosine similarity between the preposition pair (above, below) is 0.85 using Word2vec embeddings.
The cosine similarity between the preposition pair (after, before) is 0.70 using GloVe embeddings.
Our method (WD) achieves a higher F1 score than the state-of-the-art method for both the CoNLL and SE datasets.
Our method (ALS) and Our method (WD) achieve the same precision, recall, and F1 score for both the CoNLL and SE datasets.
The accuracy of FNN with Our method (WD) embedding method is 0.892 in prepositional attachment disambiguation.
The accuracy of LRFR with Word2vec embedding method is 0.903 in prepositional attachment disambiguation.
Table 1 provides information about LM training parameters.
The value for the "Dropout Mult" parameter is 1.0 for both LM and Fine-Tune LM.
BERT-LeaQuR outperforms all other methods in terms of MAP, Recall@10, Recall@20, and Recall@30.
The performance of BERT-LeaQuR is statistically significantly better than all the baselines in terms of MAP, Recall@10, Recall@20, and Recall@30.
NeuQS outperforms all other methods in terms of MRR, P@1, nDCG@1, nDCG@5, and nDCG@20 in both the Qulac-T Dataset and the Qulac-F Dataset.
The performance metrics (MRR, P@1, nDCG@1, nDCG@5, and nDCG@20) are consistent between the Qulac-T Dataset and the Qulac-F Dataset for the "OriginalQuery" method.
The "base+E2Tcnn" model performs the best in the 1st and 4th evaluation metrics.
The "base+E2Tcnn" model has the lowest mean score among all models.
Examples E1.1 and E1.2 are both related to the United States.
Example E2.1 is about the Olympic Winter Games, while example E2.2 is about the price of gold.
Multi-passage BERT achieves the highest F1 score on the SQuAD-Open dataset.
DenSPI + Sparc achieves a higher EM score than DenSPI on the SQuAD-Open dataset.
The BERT model achieves the highest EM and F1 scores on the SQuAD development set.
The "DenSPI + Sparc" model achieves a higher EM and F1 score compared to the "LSTM + SA + ELMo" model on the SQuAD development set.
Table 5 shows the exact match scores of Sparc in different search strategies.
The exact match scores of Sparc are higher in the "SQuAD-Open + Sparc" and "CuratedTREC + Sparc" datasets compared to the "SQuAD-Open DenSPI" and "CuratedTREC DenSPI" datasets.
Table 5 shows the effect of the threshold parameters, τfocus and τcov, on the hybrid approach on the e-commerce English→Russian task.
Each row in Table 3 represents a different system configuration by either removing or limiting one model compared to the full system.
Table 4 shows the effect of the beam size on the hybrid approach for the e-commerce English→Russian task.
As the beam size increases, the BLEU scores for item descriptions decrease.
Our Method outperforms both Bi-LSTM-CRF and BERT-FF for all classes.
Our Method achieves the highest F1 score for the "Nat" class.
The count for the label O is 88791.
The count for the label Geo is 37644.
The RNN-CNN method has the highest accuracy for both the 0-Class and 1-Class.
The LSTM-Attention method has a lower accuracy compared to the Self-Attention method.
The model DualVD-DAM outperforms the other models in terms of M1, M2, and Repetition.
The model WDL performs the best in terms of Richness.
LF-DAM (ours), MN-DAM (ours), and DualVD-DAM (ours) are the models developed by the researchers.
DualVD-DAM (ours) achieves the highest MRR score among all the models.
The DualVD-DAM model performs better than the LF-DAM model in terms of MRR, R@1, R@5, R@10, Mean, and NDCG.
The DualVD-DAM model performs better than the MN-DAM model in terms of MRR, R@1, R@5, R@10, Mean, and NDCG.
The DualVD-DAM model achieves the highest MRR, R@1, R@5, R@10, Mean, and NDCG values among all the models in the table.
The DualVD-DAM model achieves the highest R@1 value among all the models in the table.
The "CBOW" method achieves the highest accuracy for the "Sentiment analysis" task with a value of [BOLD] 75.83.
The "RNN" method achieves the highest accuracy for the "Syntactic analogies" task with a value of [BOLD] 54.64.
The table shows the results on the syntactic and semantic analogies tasks with a bigger corpus.
The forest model performs better on the syntactic analogies task.
Our model has a lower training time compared to the standard BERT model for all datasets.
The standard BERT model has a higher training time compared to our model for all datasets.
The table presents the evaluation results of different models using QA-based and criteria-based human evaluation.
The table includes the evaluation scores for different criteria, such as information, novelty, and relevance.
Table 6 provides ROUGE Recall scores on the CNN/DailyMail dataset for different explainable aspects under different thresholds.
The ROUGE Recall scores for the aspect of Novelty when the threshold ϵn is set to 0.45 are 47.31 (R-1), 20.90 (R-2), and 43.53 (R-L).
The bilingual system (bi) is trained on only the eval source language, while the other systems are trained on two similar source languages.
The alignment score is highest for the Gl + Pt system.
Table 1 provides the number of sentences for each language pair.
The number of training sentences for the Tr → En language pair is 182,450.
The multilingual model performs better in the "multi:std here" and "multi:pre+align on the" scenarios compared to the "multi:std 6/0" and "multi:pre+align 0/14" scenarios, respectively.
The multilingual model performs better in the "multi:std here" scenario compared to the "multi:pre+align on the" scenario, and it also performs better in the "multi:std 6/0" scenario compared to the "multi:pre+align 0/14" scenario.
The "Proposed" method achieves a higher F1 score than the "C&C", "EasySRL", and "depccg" methods.
The "ELMo" method outperforms the "depccg" method in terms of precision and F1 score.
The "+ ELMo" method outperforms the "depccg" method in terms of UF1 and LF1 scores.
The "Converter" method achieves the highest UF1 and LF1 scores on WSJ23.
The proposed method achieves the highest F1 score among all the methods listed in the table.
The proposed method achieves higher precision, recall, and F1 score than the C&C method.
The table presents the results of three different methods on speech conversation texts.
The "+ Proposed" method achieves the highest F1 score on the whole test set.
The method "+ Proposed" has the highest UF1 and LF1 scores compared to the other methods.
The UF1 and LF1 scores for the "+ Proposed" method are higher than the scores for the "depccg" and "+ ELMo" methods.
Table 7 shows the test performance for various metrics on the dodecaDialogue tasks comparing the multi-task and multi-task + fine-tuned methods to existing approaches.
The F1 score for the Ubuntu task using the existing independent approach is 17.1.
The "Reddit+ConvAI2" model has the lowest perplexity score for the ConvAI2 dataset.
The "Reddit+Empathetic Dialog" model has a lower perplexity score compared to the "Reddit+Wiz. of Wikipedia" model for the EmpatheticDialog dataset.
The table provides human evaluations on the Wizard of Wikipedia (unseen) test set, comparing various decoding schemes for the Image+Seq2Seq model trained on all tasks MT, as well as comparisons with human outputs using ACUTE-Eval.
The Image+Seq2Seq model outperforms human outputs on the Wizard of Wikipedia (unseen) test set.
The "JAPE SE+AE" method achieves the highest score of [BOLD] 41.18 in the ZH→EN translation task for Hits@1.
The "JAPE SE+AE" method achieves a high score of [BOLD] 86.18 in the EN→ZH translation task for Hits@50.
JAPE with semantic expansion and abbreviation expansion achieves the highest performance in terms of Hits@1 and Hits@10 for the JA→EN translation direction.
JAPE with semantic expansion and abbreviation expansion achieves the highest mean performance for both the JA→EN and EN→JA translation directions.
JAPE SE+AE achieves the highest performance in terms of Hits@1, Hits@10, Hits@50, and Mean scores for FR→EN translation.
JAPE SE w/o neg. achieves lower performance in terms of Hits@1, Hits@10, Hits@50, and Mean scores for EN→FR translation.
JAPE outperforms both DBP100K and MTransE for both ZH→EN and EN→ZH translations.
DBP100K performs worse than both MTransE and JAPE for both ZH→EN and EN→ZH translations.
The translation accuracy for JA→EN is higher than EN→JA.
The translation accuracy for EN→JA is lower than JA→EN.
The translation accuracy from French to English is higher than the translation accuracy from English to French.
The translation accuracy from English to French is lower than the translation accuracy from French to English.
Table 9 summarizes the results in the 4S corpus.
Louvain has the highest score in the 4S corpus.
The table describes different aspects of AVL trees.
The number of segments for tree rotation is 13.
The algorithm "Spectral" has the highest F1 score of 0.41.
The algorithm "Agglomerative" has the highest Adjusted Rand Index (ARI) score of 0.52.
The "Walktraps" algorithm has the highest values for both "A" and "R" in the document relationship identification task.
The "A" metric consistently outperforms the "R" metric in the document relationship identification task.
The clustering algorithm "Spectral" achieves the highest F1 score in the 4S corpus.
The clustering algorithm "Agglomerative" achieves the highest Adjusted Rand Index (ARI) score in the 4S corpus.
The "WMT-Bio." dataset has the highest percentage of unseen words in both the DE-EN and EN-DE directions.
The "commoncrawl" dataset has the lowest perplexity in the EN-DE direction.
As the number of encoder layers increases, the performance of the model generally improves.
The model with 5,000 PreN+T tokens performs better than the model with 30,000 PreN+T tokens.
The average performance for each operation is higher when using the "replace" operation compared to the other operations.
The performance for each operation is consistently higher when using the "all" test set compared to the "clean" test set.
The average performance for the "delete" operation is 29.0.
The "switch" operation has the highest performance among all operation types.
The highest value in each row of the "train \test" column is marked in bold.
The values in the "avg" row represent the average of each column.
Table 5 shows the results of discourse parsing at different levels: intra-sentential, multi-sentential, and text level.
Our approach achieves a labeled F-score of [BOLD] 66.8 in the intra-sentential level.
Table 2 provides characteristics of the training and test set in RST-DT.
The training set in RST-DT contains 347 documents and the test set contains 38 documents.
LR achieves the highest precision score among all models.
SVM achieves the highest recall score among all models.
BERTLARGE has the highest accuracy on the SST-2 dataset.
The Distilled BiLSTMSOFT model has the highest accuracy on the MNLI-m dataset.
The "Ours" update rule achieves the highest accuracy for the transductive setting with m=3.
The "Sab." update rule performs better than the "Ours" update rule for the inductive setting with m=7.
Our Caps2NE achieves the highest Micro-F1 score on the PPI dataset.
Our Caps2NE achieves the highest Macro-F1 score on the BlogCatalog dataset.
The "Our Caps2NE" model achieves the highest accuracy score on the Cora dataset among the unsupervised graph embedding models.
The "Transductive BoW" model achieves an accuracy score of 58.07 on the Cora dataset.
The Pearson correlation coefficient between Spanish and Portuguese is 0.89.
The Pearson correlation coefficient between English and Indonesian is 0.88.
The correlation coefficient between Spanish and Portuguese for translation-stable words is 0.85.
The correlation coefficient between English and German for translation-stable words is 0.78.
There is a significant correlation between average word happiness and word usage frequency rank for all languages in the table.
There is a negative correlation between average word happiness and word usage frequency rank for most languages in the table.
The correlation coefficient for Portuguese: Twitter is +0.090.
The p-value for English: Music Lyrics is 4.87×10−20.
There are 57 instances where BERT+Entity trained in Setting I on CoNLL’03/AIDA (testa) did not make any predictions.
There are 12 instances where the gold annotation was wrong in the evaluation of BERT+Entity trained in Setting I on CoNLL’03/AIDA (testa).
The strong F1 score for entity linking using BERT+Entity in Setting I is 82.8.
The strong F1 score for entity linking using BERT in the AIDA/testb dataset is 52.4.
As the number of parameters decreases, the FLOP value increases.
As the number of parameters decreases, the AGP value increases.
The ProQA (90k) method has the highest Recall@5, Recall@10, and Recall@20 scores among all the methods.
As the size of the dataset decreases, the Recall@5, Recall@10, and Recall@20 scores of the ProQA (no clustering) method also decrease.
ProQA has the highest EM score among all the models listed in the table.
T5 has the largest model size among all the models listed in the table.
The models BERT BASE, BERT LARGE, XLNet BASE, XLNet LARGE, RoBERTa BASE, and RoBERTa LARGE can be trained with or without inputting context, question, and answer options.
The accuracy of the GPT-2 model on the test set is 47.2%.
Figure 4 displays the effectiveness in disarming the obfuscation attack on the obfuscation-50 and obfuscation-99 datasets.
The TP+GP method is effective in disarming the obfuscation attack on the obfuscation-99 dataset.
The table shows the test and dev accuracy of different models on WikiSQL data.
The model "Pointer-SQL + EG (5)" has the highest test and dev accuracy among all models.
The most frequent error category in the CoNLL development set is "Nn" with a count of 400.
Data augmentation improves the recall for the "ArtOrDet" error category in the CoNLL development set, with an increase from 20.08% to 29.14%.
Table 1 shows the performance on the Lang-8 test set.
Adding a language model to the RNN results in a test BLEU score of 61.70.
The conventional distillation type achieves an accuracy of 92.7 on the SST-2 task.
The inplace distillation type achieves a higher accuracy of 84.3 on the MRPC task compared to the conventional distillation type.
The score for the STS-B task using BERTBASE is 69.6.
The score for the QNLI task using DynaRoBERTa (mw, md=1,1) is 87.9.
The average accuracy for CoLA 1.0x is higher than the average accuracy for CoLA 0.5x.
Table 8 provides an inter-language comparison of results.
The scores in the "Increase on SG" column represent the difference between the mentioned systems and the scores of SkipGram embeddings trained on the target languages - fr-SG, de-SG, and ur-SG respectively.
The term "None" achieves the highest MRR and Hit@10 scores for both WN18RR and WIN18.
The term "None" has a higher MRR and Hit@10 score compared to the other terms for both WN18RR and WIN18.
S4 performs the best among all individual terms in MDE on FB15k-237 in terms of MRR.
S4 performs the best among all individual terms in MDE on WN18RR in terms of MRR.
Table 1 provides information on the training and test samples, as well as the training and test accuracy for various languages.
The LSTM model outperforms the traditional model in terms of accuracy for most languages.
The LSTM model consistently has higher accuracy than the traditional model across different languages, except for Arabic.
Messages with a "Prediction Score" of 0.59 and above are classified as "ACTIONABLE".
The message "@PGE4Me’s new program allows customers to go 100% #solar without having to install rooftop solar panels!" is classified as "NON-ACTIONABLE".
Table 2 shows the SERs (%) for three different models: Combined v1, Combined v2, and Overlapping Speech Model.
K-BERT (CN-DBpedia) achieves higher F1 scores than Google BERT in Finance_Q&A, Law_Q&A, and Finance_NER tasks.
K-BERT (MedicalKG) achieves the highest precision, recall, and F1 score in the Medicine_NER task.
Table 2 shows the results of various models on NLPCC-DBQA and MSRA-NER datasets.
Our BERT performs better on the MSRA-NER dataset compared to the NLPCC-DBQA dataset.
Table 4 compares various measurements between ACL'17 and ICLR'17.
The reviews in ACL'17 section have a higher average rating for clarity compared to the reviews in ICLR'17 section.
The mean score for "Oral" presentation format is higher than the mean score for "Poster" presentation format in all categories.
The mean score difference between "Oral" and "Poster" presentation formats is positive for all categories.
Table 5 shows the test accuracies for acceptance classification.
Our model outperforms the majority classifiers in all cases.
Table 1 provides information about the accuracy achieved by different methods in controlled generation and text style transfer tasks.
The DE-VAE method achieves high accuracy in both controlled generation and text style transfer tasks across all three datasets.
The category "Electronics" has the highest number of products, QA, and reviews compared to other categories.
The average length of questions is higher than the average length of answers for each category.
The F1 score for RAHP-Base in the Electronics category is 0.651.
The AUC score for RAHP-NLI in the Health category is 0.794.
The table shows the results of three ablation experiments conducted on different models.
The ablation experiments conducted on the "Phones" category consistently improve the F1 scores compared to the baseline.
The probability of the topic z given the word w is higher for "Word Gore" compared to "Word 770".
The probability of the topic z given the word w is higher for "Nero" compared to "246".
HCLM with Cache has the lowest values in both the Dev and Test columns.
The HCLM with Cache model performs better on the EN dev dataset compared to the LSTM and HCLM models.
The HCLM with Cache model performs better on the FR test dataset compared to the LSTM and HCLM models.
Table 1 shows BLEU scores on three translation tasks.
Our Method performs better than Transformer in all translation tasks.
The table compares the performance of the translation and evaluation modules on the CN\rightarrowEN and EN\rightarrowDE translation tasks.
The evaluation module performs worse than the translation module on all translation tasks.
The table provides information about N-gram accuracy and cosine similarity on CN\rightarrowEN translation.
Our method achieves higher N-gram accuracy than the Transformer model on CN\rightarrowEN translation.
The baseline model has a distance of 1.000 from the word "_monopoly" in the embedding space.
The word "_monopolies" has a distance of 0.509 from the word "_monopoly" in the embedding space.
DPQ-SX achieves the lowest perplexity (PPL) values among all the methods for all three model sizes.
DPQ-VQ achieves the highest compression ratio (CR) among all the methods for the Large model size.
DPQ gives a compression ratio of 37x on the embedding table.
The model's performance on the CoLA task is slightly lower for the "DPQ-SX" embedding compared to the "Full" embedding.
The table compares the performance of PQ and DPQ-VQ methods for the NMT task on WMT19 (En-DE).
The DPQ-VQ (K=32, D=128) and DPQ-SX (K=32, D=128) methods achieve the same BLEU score of [BOLD] 38.8.
The DPQ-SX _evolve has a higher similarity score than the baseline (Full) _evolve.
The city "_Chicago" has the highest distance value for the baseline model.
The city "_Hamilton" has the lowest distance value for the DPQ-VQ model.
The table shows the BLEU scores for different setups in English-German translation.
The best results in English-German translation are obtained using the Edit distance transducer based combination setup.
The "Combined translation" method achieves the highest BLEU score among all the methods.
The "Hiero baseline" method outperforms the "NMT baseline" method in terms of BLEU score.
The average number of UNK insertions per sentence in the NMT and Hiero comparison is 0.16.
61.7% of the sentences are affected by other edit operations (Type III) in the NMT and Hiero comparison.
As the vocabulary size increases, both the Pure NMT BLEU and NMT+Hiero BLEU scores increase.
As the vocabulary size increases, the percentage of unknown words in the Pure NMT system decreases.
The MIARN model has the highest F-score among all the models.
The F-score for manual labelling is the highest among all the models.
There are 345 instances of sarcastic annotations when the data has sarcasm tags.
There are 975 instances of non-sarcastic annotations when the data does not have sarcasm tags.
Table 5 presents the agreement between intended labels and perceived labels on the iSarcasm test set.
The number of intended sarcastic labels on the iSarcasm test set is 61, while the number of intended non-sarcastic labels is 50.
The table includes results from different datasets such as Riloff, Ptacek, SARC-balanced, SARC-unbalanced, and SemEval-2018.
The table includes results from different models such as LSTM, Att-LSTM, CNN, SIARN, MIARN, and Dense-LSTM.
The Joint-BERT-RCNN (best) method outperforms the baseline method in terms of F1 score in scenario 2.
The Voting LSTMs (ours) method achieves higher Precision than the baseline method in scenario 3.
The precision, recall, F1, and accuracy scores for the "BERTEnsemble+DistillBERT" ensemble model are 0.731, 0.610, 0.665, and 69.36% respectively.
The precision, recall, F1, and accuracy scores for the "RoBERTalarge" single model are 0.723, 0.681, 0.702, and 71.09% respectively.
The clustering performance improves as we move from Elmo to TFIDF (Baseline) based on the percentage of claims in clusters.
The number of claims included in the clustering process decreases as we move from Elmo to Infersent.
The "Full tree matching NTI-SLSTM-LSTM global attention (Ours)" model achieves the highest test accuracy of [BOLD] 87.3 on the natural language inference task.
The "SPINN-PI encoders" model achieves the highest train accuracy of 89.2 on the natural language inference task.
The "NTI (Ours)" model outperforms the "Paragraph Vector le2014" model in terms of MAP score.
The MRR score of the "NTI (Ours)" model is 0.6884.
The cluster size for "This work" is 30.
The relative standard deviation for "DUC 2006" is 0.74.
The model "xrenner" performs better than the model "Joshi2019BERTFC" in terms of average F1 score.
The model "xrenner" achieves the highest F1 score in the CEAFϕ4 evaluation metric.
VILLALARGE outperforms other methods on the RefCOCOg test dataset.
UNITERBASE performs better than other methods on the Flickr30k Text Retrieval (TR) task with a recall rate of 10.
VILLABASE outperforms all other methods in terms of scores on both RefCOCO+ and RefCOCO datasets.
VILLABASE achieves the highest score on the RefCOCO+ testB dataset when evaluated using detected proposals.
VILLA achieves the highest performance on the RefCOCO testB [ITALIC] d dataset among all the methods.
The average performance of VILLA is higher than the average performance of UNITER (reimp.) and VILLA-pre.
The VILLABASE model performs better than the UNITERBASE model in the Visual Coreference task for scenes in the Flickr30k dataset.
The VILLABASE model performs better than the UNITERBASE model in the Visual Relation task for objects on the Visual Genome dataset.
The table presents the results of three different methods: LXMERT, LXMERT (reimp.), and VILLA-fine.
VILLA-fine achieves the highest performance on the VQA test-dev dataset.
The average performance of the methods is increasing from UNITER (reimp.) to VILLA-pre to VILLA-fine to VILLA.
VILLA performs better than UNITER (reimp.), VILLA-pre, and VILLA-fine on VCR (val) Q→A.
The word "opposing" has the highest score among all the words in the table.
The word "instrument" has the highest frequency among all the words in the table.
The hypothesis-only model performs better than the majority baseline on the SNLI dataset.
The hypothesis-only model performs better than the majority baseline on the SciTail dataset.
The word "tall" has the highest SNLI score of 0.93.
The word "competition" has the highest SNLI frequency of 44.
The word "Nobody" has the highest score among all the words in the table.
The word "sleeping" has the highest frequency among all the words in the table.
The word "outside" has the highest score of 0.67.
The words "an", "gathered", "girl", "trick", "Dogs", "watches", "field", "singing", and "something" have a frequency of 10 or less.
The word "smile" has the highest MPE Score of 0.83.
The word "boy" has the highest score among all the words in the table.
The word "sitting" has the highest frequency among all the words in the table.
The difference in accuracy between the Hyp-Only and MAJ models is positive for both the "entailed" and "not-entailed" labels.
The accuracy for the "not-entailed" label is higher than the accuracy for the "entailed" label for both the Hyp-Only and MAJ models.
Our model outperforms all the compared baselines in both slot filling and intent detection on the two multi-intent datasets.
Stack-Propagation model achieves the highest overall accuracy on the DSTC4 dataset.
The Full Model outperforms the other models in terms of slot F1, intent F1, intent accuracy, and overall accuracy on both the DSTC4 and MixSNIPS datasets.
The MixSNIPS dataset is more challenging than the DSTC4 dataset, as all models achieve lower overall accuracy on MixSNIPS compared to DSTC4.
Our model outperforms other models in terms of F1 scores for both ATIS Slot and ATIS Intent.
Our model achieves the highest overall accuracy for the ATIS dataset.
The table shows the accuracy of three different methods: Majority, GPPooled Brown, and GPPooled BOW.
The experiment used three different training methods: CE, SGD, HF, and NG.
The phone accuracy for the CE method is 0.6549, for the SGD method is 0.7044, for the HF method is 0.6967, and for the NG method is 0.7089.
The table shows the %WER differences between different optimizers on dev.sub2 with 158k trigram (200 hr).
The %WER difference for all optimizers on dev.sub2 with 158k trigram (200 hr) is around 30.
The "NMT + Moses + image" system achieves the highest BLEU score in the cross-lingual captioning task.
The "NMT + Moses" system achieves the highest METEOR score in the multimodal translation task.
The CNN model performs better than the MLP model in terms of Hits@10 (%) on FB4M-desc dataset.
The MLP model has higher mean ranks than the CNN model on FB4M-desc dataset.
The "Ours" model has a lower mean rank average than the "TransE" model.
The "Ours" model has a higher Hits@10 average than the "TransE" model.
The number of IV-VQA test samples is smaller than the number of CV-VQA test samples.
The number of IV-VQA train samples is smaller than the number of CV-VQA train samples.
The model "Trained by us CNN+LSTM" has a higher accuracy than the model "For comparison d-LSTM Q + norm I".
The model "SAAA" has a higher accuracy than the model "SAN".
The accuracy of the model when fine-tuned with real data only for the question "is there a" is 63.61%.
The flipping rate of the model when fine-tuned with real data only for the question "how many" is 9.77%.
The FKGL score is lower for the dynamic mixing ratio compared to the static mixing ratio in both the Newsela and WikiSmall datasets.
The BLEU score is higher for the static mixing ratio compared to the dynamic mixing ratio in both the Newsela and WikiSmall datasets.
Table 5 shows the multi-task layer ablation results on Newsela.
The "Both lower-layer" model has a higher BLEU score compared to the "Final (High Ent + Low PP)" model.
Our model outperforms Georgiev2009FeatureRichNE in terms of F1 score.
Georgiev2009FeatureRichNE is the previous state-of-the-art model.
Table 3 shows the evaluation results for Bulgarian POS tagging, including different combinations of POS tags and morphological features.
The overall precision, recall, and F1 score of the named entity recognition system are 93.31, 91.12, and 92.20, respectively.
The precision for recognizing organizations is 75.57, and the recall is 96.34.
Each model in Table 7 is a combination of different components.
The F1 score increases as more components are added to the model in Table 7.
The best model predicts the label "O" most accurately.
The best model predicts the label "B-ORG" with the highest number of correct predictions.
The BLEU score improves as additional components are added to the Transformer-Base model.
The "+ BiARN Encoder (Ours)" model achieves the highest BLEU score compared to the other re-implemented related work models.
The table compares the Word Error Rate (WER) of a 6-layer-BLSTM direct PIT-CE-ASR model on mixed speech generated from different combinations of speakers.
The WER of the 6-layer-BLSTM direct PIT-CE-ASR model decreases as the SNR increases.
The WER decreases as the SNR increases for all models in the IHM-2mix dataset.
The "4L-BLSTM" model has a higher WER compared to other models at all SNR conditions in the IHM-2mix dataset.
The Transformer (PATECH) system has a higher accuracy than the RNN system.
The table displays the BLEU scores of the RNN, Transformer (PATECH), and Transformer (KSAI) MT systems on the WMT 2019 news test set.
The Transformer (PATECH) MT system achieves a higher BLEU score than the RNN MT system on the WMT 2019 news test set.
Table 3 presents the inter-annotator agreement (Cohen's κ values) for the MQM calibration set and evaluation set.
The Transformer (PATECH) model has a lower error ratio than the RNN model.
The Transformer (KSAI) model has a higher error ratio for the "Entity" error type compared to the RNN model.
The average UAS score across all languages is 84.2.
The LAS score for Russian is 83.3.
The ELAS score for Arabic is higher than the ELAS score for Bulgarian.
The UAS score for English is higher than the UAS score for Estonian.
Table 6 provides the hyperparameters for the baseline models.
The word embedding dimensions is 100, the char embedding dimensions is 32, and the char BiLSTM dimensions is 100.
The small models achieve higher UAS and LAS scores compared to the dist models for all languages.
On average, the small models outperform the dist models in terms of UAS and LAS scores.
The average UAS score across all languages is 82.5.
The LAS score for Russian is 76.7.
The table describes the performance of a model on three different datasets: MEN, WS-353, and Bib100.
The performance on Bib100 is higher than the performance on MEN and WS-353.
Table 1 provides an overview of all Human Intuition Datasets (HIDs) and includes information on the number of word pairs, unique words, and judgments per word pair.
The Bib100 dataset has 100 matchable pairs where both words are present in the BibSonomy vocabulary.
Table 2 provides Spearman correlation scores for both the high-dimensional representation (ρhigh) and word embeddings (ρemb) of both tagging datasets.
The word embeddings (ρemb) have higher correlation scores than the high-dimensional representation (ρhigh) for both tagging datasets.
The similarity score between word pairs in the MEN dataset increases from 0.533 to 0.604 when using the WikiGloVe embeddings.
The similarity score between word pairs in the WS-353 dataset increases from 0.658 to 0.700 when using the WikiGloVe embeddings.
As the maximum history length decreases, the BLEU4, ROUGE-L, and CIDEr scores also decrease.
When the source sequences are concatenated into one, the BLEU4, ROUGE-L, and CIDEr scores decrease.
Table 2 compares the MTN (Base) model to other state-of-the-art visual dialogue models on the test-std v1.0 dataset.
The MTN (Base) model achieves the highest NDCG score among all the visual dialogue models compared in Table 2.
The proposed approach achieves a higher BLEU4 score than the third-best entry in the DSTC7 submissions.
The proposed approach achieves a higher METEOR score than the MTN (Large) baseline.
Table 6 shows the performance of operator detection using gold sequence labels and system-generated labels.
The system's precision and recall for the "Preference" category are lower than for the "Negations" and "Disjunctions" categories.
BERTLarge has the highest accuracy, F1 score, and recall among all the methods.
FastText has a higher accuracy than LSTM.
The label for the class "Mention of Non-Tobacco Drugs" is -1.
The annotation class for the class "Advertisements" is 3.
There are five different categories in the table.
The category with the highest average number of retweets is PM.
Table 3 provides statistical information about different datasets, including the number of negative instances, positive instances, average sentence length, and vocabulary size.
The RT-C dataset consists of 436,000 instances, with an equal number of negative and positive instances (218,000 each).
SVM-3 achieves the highest score in the PL05-C dataset.
LM-3 achieves the highest score in the IMDB-U dataset.
Table 6 compares a dictionary rule learning method with naive counting and demonstrates that the dictionary rule learning method learns more intuitive results than counting directly.
The s.parser method assigns more positive polarities to the fragments compared to the naive counting method (see "s.parser Polarity" column in Table 6).
The accuracy of the model is higher on the train set than on the test set for all languages.
The F-Score of the model is higher than the Precision and Recall for all languages.
The FNR for high-quality subs in French is higher than the FNR for high-quality subs in German, Italian, Portuguese, and Spanish.
The number of sentences in EuroParl for French is lower than the number of sentences in EuroParl for German, Italian, Portuguese, and Spanish.
German has the highest accuracy in classification.
Portuguese has the highest accuracy in scoring.
Table 6 shows the test accuracy for various model architectures.
The LASER CNN model achieves the highest test accuracy among all the model architectures.
The train accuracy for all languages in the dataset is above 99.9%.
The number of test samples for each language is less than the number of training samples.
Table 2 shows the performances of Dir, DirV, and JMV2 in terms of MAP, Pr@5, and Pr@10.
DirV and JMV2 perform better than Dir in terms of Pr@10 for TREC7.
The strategies used for VoiceMask, VTLN-based VC, and Disentangl.-based VC are different.
The WER (%) achieved using Disentangl.-based VC with random strategy is significantly higher than the WER (%) achieved using VoiceMask, VTLN-based VC, and Disentangl.-based VC with const and perm strategies.
The COPYNET (C) model achieves the highest ROUGE-1, ROUGE-2, and ROUGE-L scores among all the models.
The SRB (C) model outperforms all other models in terms of ROUGE-1, ROUGE-2, and ROUGE-L scores.
The SRB (C) model achieves the highest ROUGE-1, ROUGE-2, and ROUGE-L scores among all models.
The "SRB (our proposal)" model achieves the highest BLEU score among all the models in the PWKP dataset.
The "SRB (our proposal)" model achieves the highest BLEU score among all the models in the EW-SEW dataset.
SRB achieves the highest BLEU score on the PWKP dataset compared to other systems.
SRB achieves the highest BLEU score on the EW-SEW dataset compared to other systems.
TGM with the prediction of multimodal latent topic achieves the highest scores in all metrics for both MSR-VTT and Youtube2Text datasets.
TGM with the assigned expert-defined topic achieves the highest score in the MSR-VTT ROUGE-l metric.
The BLEU4 score for MSR-VTT dataset using M&M TGM model is 44.33.
The CIDEr score for Youtube 2Text dataset using M&M TGM model is 80.45.
Table 4 compares the generalization ability of the vanilla model and the M&M TGM model on the MSR-VTT training set and the Youtube2Text testing set.
The M&M TGM model outperforms the vanilla model in terms of BLEU4, METEOR, ROUGE l, and CIDEr scores.
Table 5 provides a performance comparison of different topic prediction losses.
The loss value for MSR-VTT is lower for KL loss compared to l2 loss.
The table shows the influence of human interactions on the Youtube2Text dataset with noisy and clean expert-defined topics assignment.
The clean topic has higher values for all metrics (BLEU4, METEOR, ROUGE-l, CIDEr) compared to the noisy topic.
Table 1 shows the BLEU scores computed on the development and test sets using different combinations of approaches.
The CSLM + RNN + WP model achieves a higher BLEU score on the test set compared to the baseline and RNN models.
Table 2 compares decoding methods and their WER for Hub5’00, SWB, and CH datasets on Switchboard 300h.
Table 2 provides information on the WER values for different decoder implementations (Simple, Adv., and LSTM) on Hub5’00, SWB, and CH datasets.
The WER[%] Hub5’00 for the Phoneme unit is 26.0.
The number of labels for the BPE type is 151.
The work "Ours" has the highest WER[%] Hub500 value compared to other works.
The label unit "Phon." has the lowest WER[%] Hub500 value compared to other label units.
The "charagram (large)" model achieves the highest Spearman's ρ score of 70.6 on the SL999 dataset.
The "charagram (large)" model has the highest number of character embeddings with 173,881.
The highest score in each row is marked as boldface in Table 3.
The scores for STS 2012, STS 2013, STS 2014, and STS 2015 are higher for charagram-phrase compared to charCNN and charLSTM in Table 3.
The model "charagram (2-layer)" has the highest accuracy of [BOLD] 97.10%.
All models in the table have accuracy scores above 96%.
The classification task is more imbalanced for the Krapivin, NUS, SemEval, KP20k, KP Training, and KP Validation datasets compared to the Inspec dataset.
The coverage of keyphrases by candidate phrases is higher for the KP20k, KP Training, and KP Validation datasets compared to the Inspec, Krapivin, NUS, and SemEval datasets.
CopyRNN outperforms KEA in terms of F1@5 and F1@10 for all benchmark datasets.
BERT-Base-Pair outperforms Maui, KEA, and Tf-Idf in terms of F1@5 and F1@10 for all benchmark datasets.
The table provides information on the training time and performance of BERT-Base-Pair and SKE-Base-Rank methods.
SKE-Large-Cls method performs better in terms of F1 score at the top-10 positions compared to other methods mentioned in the table.
SKE-RNN-Cls outperforms Maui and CopyRNN in terms of F1 score at both @5 and @10 for KP20K.
SKE-RNN-Cls achieves the highest F1 score among all methods for KP20K.
The highest accuracy on the "Aus Acc" dataset is achieved when the weights are learned and the embedding layer is initialized with the Normal distribution.
The highest accuracy on the "US Acc" dataset is achieved when the weights are frozen and the embedding layer is initialized with the Cauchy distribution.
The one-hot encoding technique achieves the highest F1 score on the Twitter dataset.
Table 6 provides the F1-Scores of a word-level LN model initialized with different embedding techniques.
The F1-Score for the ELMo embedding technique is 0.3307 for the Twitter dataset.
Table 7 shows the F1-Scores of each model on each of the three lexical normalization datasets.
The table presents an ablation study on the DailyMail test dataset, evaluating the impact of different variations on Rouge-1, Rouge-2, and Rouge-L scores.
The "ITS" variation performs better than the "w/o selective reading" variation in terms of Rouge-1 score.
The ITS model outperforms other baselines in terms of Rouge-1 and Rouge-2 scores on the DailyMail test dataset.
The SummaRuNNer model achieves the highest Rouge-L score among the baselines on the DailyMail test dataset.
The ITS model achieves the highest Rouge-1 score on the CNN test dataset.
The REFRESH model achieves a Rouge-L score of 26.9 on the CNN test dataset.
Table 3 compares different baselines on the DUC2002 dataset using Rouge recall score with respect to the abstractive ground truth at 75 bytes.
The ITS baseline achieves the highest Rouge-1, Rouge-2, and Rouge-L scores on the DUC2002 dataset.
The table compares the performance of different models on the DailyMail corpus.
The model "Hybrid MemNet" performs better than the model "ITS" on the DailyMail corpus.
The POS tagger system "Owoputi et al., 2013 (CRF)" has a higher accuracy than the system "Stanford CoreNLP".
The POS tagger system "Ma and Hovy, 2016" has a lower accuracy than the system "Owoputi et al., 2013 (CRF)".
Table 5 shows the POS tagging performance with automatic tokenization on the Tweebank v2 test set.
The F1 score for the our bi-LSTM tokenizer is 93.3.
The table compares the performance of different dependency parsing systems on the Tweebank v2 test set, with automatic POS tags.
The ensemble system achieves the highest LAS score of 79.4.
The DPN-S2S model achieves the highest accuracy in terms of ROUGE-1, ROUGE-2, and ROUGE-L on the Gigaword text summarization task.
The ConvS2S model has the lowest accuracy in terms of ROUGE-1 on the Gigaword text summarization task.
DPN-S2S has the highest performance on the Nist04 Chinese-English translation task.
DPN-S2S has the second-highest performance on the Nist12 Chinese-English translation task.
The number of paragraphs in the "SciFi" category is higher than the number of paragraphs in the "Fantasy" category.
The method "Stem-Based" is not compatible with the filtered tasks.
The method "Form-Context" has a higher Definitional Nonce MRR than the method "Selective FastText".
The "Additive + Window/Sub/Neg" method has the highest "Filtered CRW Number of Context Sentences" for all values of "Method" except 0.
The "Neural FCM" method has the highest "Filtered CRW Number of Context Sentences" for all values of "Method".
The BLEU1 score for Layer 0 is 33.1.
The increase in BLEU score from Layer 0 to FULL is 19.81.
The rule "Noun-Adjective" has the highest percentage of 40.10%.
The rule "Adjective-Adverb" has a percentage of 3.2%.
The table compares the performance of SGNS alone and SGNS+PVN/PDE model on different word similarity and analogy datasets.
The SGNS+PVN/PDE model outperforms SGNS alone on all the word similarity and analogy datasets.
Table 2 compares the performance of SGNS, PPA, and PVN models on different word similarity datasets.
PVN (ours) performs better on average than SGNS and PPA on the word similarity datasets.
The performance of the Semantic type with the Add operation is better than the performance of the Semantic type with the Mul operation.
The performance of the MSR type with the Mul operation is better than the performance of the MSR type with the Add operation.
The table compares the performance of SGNS alone and SGNS+PDE against word similarity and analogy datasets.
The SGNS+PDE method (PVN(ours)) consistently outperforms the SGNS method in all categories.
The Pearson's correlation coefficient between assigned scores and quality categories for the "Human" metric is 0.701.
The Spearman's correlation coefficient between assigned scores and quality categories for the "BLEU-1", "ROUGE-L", and "METEOR" metrics are lower than their respective Pearson's correlation coefficients.
The model "LaBSE" achieves the highest P@1 scores on the UN parallel sentence retrieval task for all languages.
The model "m-USETrans." performs worse than the model "Yang et al. (2019a)" on the en-zh language pair in terms of P@1 score.
The model "LaBSE" achieves the highest precision score for the fr-en translation task in the forward direction.
The model "LaBSE" achieves the highest F-score for the de-en translation task in the backward direction.
The languages with the highest Tatoeba Accuracy are nb, tl, and ia.
The languages with non-zero percentage of unknown tokens are ber and kab.
Table 7 shows the performance of different models on the STS benchmark.
The ConvEmbed model performs better on the STS benchmark compared to the InferSent model.
Table 4 provides information on the vocabulary sizes and Spearman ρ correlation values for the three datasets (C1, C2, and C3) in the Twi and Yorùbá languages.
The Yorùbá language has a larger vocabulary size and a higher Spearman ρ correlation value compared to the Twi language in the "C2: Curated Small Dataset (Clean + some noisy text)" dataset.
The Yorùbá Spearman ρ correlation is lower for the Curated Small Dataset with clean + some noisy text compared to the Curated Large Dataset with all clean + noisy texts.
The Twi Spearman ρ correlation is higher for the Pre-trained Model with Wiki compared to the other models.
The F1-score for the "Fine-tuned uncased Multilingual-bert (Multilingual vocab)" is higher than the F1-score for the "Fine-tuned uncased Multilingual-bert (Yorùbá vocab)" for all entity types.
The F1-score for the "Fine-tuned uncased Multilingual-bert (Yorùbá vocab)" is higher than the F1-score for the "Pre-trained uncased Multilingual-bert (Multilingual vocab)" for all entity types.
The logistic regression model achieves the highest recall for Class-2 (Palestinian) when using the content as the classification subject.
The SVM model achieves a high recall for Class-1 (Israeli) when using the title as the classification subject.
The Lasso method has a percentage increase of +41% compared to the Co-occurrence method.
The L1LR method takes 104.9 seconds for phrase selection.
There are 8 different emotions listed in the Plutchik-8 Emotion column.
The number of instances for each emotion in the test set is the same as the number of instances in the validation set.
The model without pretraining achieves an accuracy of 67.6% on the AGR task.
The model trained with binary EmoNet tasks achieves an accuracy of 62.8% on the CNT task.
Table 10 shows the results of pre-training using different numbers of labeled samples from Sentiment.
The highest value for LOV is 69.1, which corresponds to the "200 K" row.
The No-Pretrain model has the highest AGR score among all the models.
The OPT score increases as the number of unlabeled samples used for pre-training increases.
Table 4 presents the results of numeral prediction tasks using different metrics for different models and datasets.
The GMM model performs better than the D-LSTM model in terms of AVGR on the Numeracy-600k dataset with dim 300 S_A.
The accuracy of the model decreases as the training data size decreases.
The F1 score is higher for the augmented test sets compared to the original test sets.
The values of "embedding dim" and "context window" are the same for all experiments in Table 7.
The values of "epoch" and "batch size" for the "Numeral Prediction (Numeracy-600k)" experiment are different compared to the other experiments in Table 7.
The table provides information on the training speed for each of the five methods.
The NumAsTok method has the highest training speed among all the methods.
The highest value in the "Decoding MLP1" column is 77.73.
The highest value in the "Subtraction MLP1" column is 15.52.
The DNN-NDA system outperforms the DNN-LDA system for all three senone sizes (2k, 4k, and 10k).
The DNN-NDA system with 10k senones achieves the lowest minDCF10 value.
The ROUGE scores for the PGNet model with n=1 hint words are 26.88 (ROUGE-1), 08.19 (ROUGE-2), and 22.63 (ROUGE-L).
The GPT-2 model with n=1 hint words achieves higher ROUGE scores compared to the PGNet model with n=1 hint words.
GPT-2 with n=1 has the highest number of True Positives and the lowest number of False Negatives compared to other systems.
The accuracy of GPT-2 (n=1) is higher than the accuracy of PGnet (n=1).
GPT-2 with n=1 has the highest quality score among all systems.
The "Target" has the highest overall score among all systems.
Table 5 shows the average accuracy on EPOSG for different models.
The model CNN-G achieves the highest accuracy for all categories in the EPOSG dataset.
The table presents the Phone Error Rate (PER) for different network architectures of a Feed-Forward DNN.
The performance error rate generally decreases as the network size (number of units) increases.
Network configurations with larger network sizes generally have lower average performance error rates.
The Genus "Baltic" belongs to the Family "Indo-Eur".
The language "af" has the highest mBERT vocabulary coverage with 51.3%.
The "Monolingual BERT (§ 5.2)" models have a larger model size compared to the "Baseline" models.
The "Monolingual BERT (§ 5.2)" models have a higher LAS and UAS score compared to the "Baseline" models.
Table 2 compares the language model performance on the 3-question trust score vs. the 10-question trust score.
There is a significant difference in the language model performance between the 3-question trust score and the 10-question trust score, as indicated by the bolded Pearson rdis values in Table 2.
The feature set "ngr_r + ngr_b + topics" performs the best in terms of both Pearson rdis and MSE.
The sentiment baseline performs the worst in terms of both Pearson rdis and MSE.
The performance of the Unsupervised (Ours) model decreases as the noise level increases.
The Supervised (Upperbound) model outperforms the Unsupervised (Ours) model at all noise levels.
The WEAN system achieves the highest fluency score on the EW-SEW dataset.
The DRESS-LS system achieves the highest adequacy score.
The RNN-cont model performs better than the RNN model in terms of ROUGE scores.
The WEAN model achieves the highest ROUGE scores on the LCSTS test set.
The "X-Y" method shows the highest improvement in performance compared to other methods.
The "X-X-mono" method performs better than the "En-En" method in terms of mAP for the same language target answer removal.
The number of questions is the same for all languages in XQuAD-R.
The number of candidates is different for each language in MLQA-R.
Table 2 provides the mean average precision (mAP) of baseline models on XQuAD-R and MLQA-R.
The mAP for X-X-mono is 0.52 on XQuAD-R and 0.49 on MLQA-R.
The model "X-X-mono" performs the best in both "angle=90,lap=0pt-(1em)Zero-shot" and "angle=90,lap=0pt-(1em)LAReQA".
The "Translate-Test" model has a higher average mAP than the "X-X" model.
The "Original" model has a higher interpretability score than the "SPINE" and "SPOWV" models in the GloVe column.
The "SPOWV" model has a higher interpretability score than the "Original" model in the SPINE column.
The SPOWV SG vector has the highest SimLex score.
The SPOWV GloVe vector has a higher Bio-SimLex score than the SPINE GloVe vector.
SPOWV SG has the highest average performance among all the vectors.
GloVe performs better in terms of PC than SPINE GloVe.
Our approach has a higher BLEU score compared to the baseline system.
The system without modeling structural information of indirectly connected concept pairs has a lower BLEU score compared to our approach.
The L-SW-DA configuration achieves the highest BLEU score for both EN-ZH and ZH-EN translation tasks.
The L-SW-DA configuration has the lowest translation time for both EN-ZH and ZH-EN translation tasks.
The number of training examples for the L-SW-DA preprocessing method in the EN-ZH translation task is 3493, and the BLEU score achieved is 21.05.
The alignment method "Zh → En" has the highest (Ter-Bleu)/2 score among the different alignment methods.
The "MT08 Web" dataset has higher Bp and Bleu scores compared to other datasets.
Table 2 shows the Alignment F1 scores of different models.
The precision and F1 score for the MaxEnt system are 74.86 and 75.96, respectively.
Table 2 shows the performance of different multi-stage systems: Baseline, Two-stage, and Three-stage.
The Two-stage system achieves the highest performance among the Baseline, Two-stage, and Three-stage systems.
The BLSTM with 2 layers and 256 nodes achieves the highest accuracy of 88.88%.
The BLSTM with 3 layers and 256 nodes achieves the highest accuracy of 89.80% for sentences longer than 3 seconds.
The "Three-stage" system takes more epochs to converge compared to the "Baseline" and "Two-stage" systems.
The "Three-stage" system requires more epochs in the first stage compared to the "Baseline" and "Two-stage" systems.
The model predicts more instances as PD (Primary Domain) compared to PK (Primary Keyword) and UN (Unknown).
The actual instances are more in PD (Primary Domain) compared to PK (Primary Keyword) and UN (Unknown).
The dataset *DDI2013 has the highest proportion of interactions that are Pharmacodynamic.
TR22 has the highest average number of words per sentence among the datasets.
The "Bert-transformer Encoder + Bert-transformer Decoder" model outperforms other models in terms of ROUGE-1, ROUGE-2, and ROUGE-L scores.
The "Bert-transformer Encoder + Transformer Decoder (full text)" model performs better than the "Transformer (full text)" model in terms of ROUGE-1, ROUGE-2, and ROUGE-L scores.
The table presents an ablation study of a model with Convolutional Self-Attention on the CNN/Daily Mail dataset, comparing the performance of different methods.
The "+ 2D conv." method achieves the highest ROUGE-2 score among all the methods tested in the ablation study.
The "Bert-transformer Encoder + Transformer Decoder" model outperforms other models in terms of ROUGE-1, ROUGE-2, and ROUGE-L scores.
The "Bert-transformer Encoder + Transformer Decoder" model performs better when using the full text version of the dataset compared to the truncated version.
The BERT+CopyTransformer (enc.) method has the highest ROUGE-1 score among the models.
The BERT+CopyTransformer (enc.) method has the highest ROUGE-L score among the models.
Table 2 shows the WER (%) of nine recognizers on the evaluation data.
The WER (%) for recognizer B is 12.26.
The "+ OR-NMT" system achieves the highest BLEU score in both RNNsearch and Transformer systems.
The BLEU score of the Transformer system is higher when the "+ OR-NMT" addition is included.
Table 1 shows case-insensitive BLEU scores (%) on the Zh→En translation task.
The this work + OR-NMT system achieves higher BLEU scores than RNNsearch, SS-NMT, and MIXER.
The "+ noise" system has a higher average BLEU score than the "RNNsearch" system.
The average BLEU score of the "+ sentence oracle" system is higher than the average BLEU score of the "+ word oracle" system.
Table 1 provides information about the training, development, and test data for PTB, CTB, and PDT datasets.
The number of sentences in the PTB development set is 22, and in the CTB development set is 886-931; 1148-1151.
The dependency parser in this paper achieves an Eng UAS of 93.0.
The dependency parser in this paper achieves a Chn CM score of 37.0.
The table shows the test results for English-Czech translation using different models.
The addition of the "+ GCN" component improves the BLEU4 scores for English-Czech translation.
"ARedSum-Ctx" achieves the highest Rouge F1 score in all three metrics (RG-1, RG-2, RG-L) among all the models.
"BertSumExt" achieves the highest Rouge F1 score for RG-2 among all the models.
ARedSum-Ctx achieves the highest Rouge F1 score for RG-L.
The Oracle model achieves the highest Rouge F1 score for RG-1.
The "CNLM LSTM" model performs the best on the "Adj. Gender" experiment.
The "WordNLM" model achieves 100% accuracy on the "Adj. Number" experiment.
Table 1 provides the performance of different language models for English, German, and Italian.
The WordNLM model has the highest perplexity among the language models for English, German, and Italian.
Table 5 shows results on MSR Sentence Completion for different models and training data.
The RNN model achieves an accuracy of 24.3% for Wikipedia training and 24.0% for in-domain training on MSR Sentence Completion.
The table compares the accuracies of different models on Wikipedia and in-domain training for MSR Sentence Completion.
The accuracy of the Word RNN model for in-domain training is 58.9.
The precision and recall values for the baseline model on the CamRest676 dataset are 0.885 and 0.786, respectively.
The precision and recall values for the Assembled Augmentation model on the CamRest676 dataset are 0.883 and 0.875, respectively.
The assembled augmentation method achieves the highest success F1 score on the CamRest676 dataset.
The machine utterance augmentation using synonym substitution achieves a success F1 score of 0.775 on the KVRET dataset.
"Bert-Base-KagNet" and "Bert-Large-KagNet" models outperform their corresponding "Bert-Base-FineTuning" and "Bert-Large-FineTuning" models in terms of accuracy.
The accuracy of all models improves as the amount of training data increases.
Table A.2 compares text encoders trained on different textual inputs and evaluated on three different prediction tasks.
The accuracy at 1 for the "Init Query + Tags" text input is higher than the accuracy at 1 for the "Init Query" text input.
Table A.2 compares text encoders trained on different textual inputs and evaluated on three different prediction tasks.
The accuracy at 1 for the "Init Query + Tags" text input is higher than the accuracy at 1 for the "Init Query" text input.
Table 3 shows the BLEU scores on a subset of examples predicted in the right language by zero-shot translation through the multilingual model (newstest2012).
The pivot translation from German to French has a BLEU score of 19.71.
The Zero Shot translation from German to French performs worse than the Zero Shot translation from French to German for both the vanilla and adversarial models.
The supervised translation from English to French performs better than the supervised translation from English to German.
The table shows the average BLEU scores for a multilingual model on the IWSLT-2017 dataset for three different groups: "en↔xx (8)", "xx↔yy (12)", and "All (20)".
The average BLEU score for the "All (20)" group using the vanilla direct method is 22.2.
The highest Kendall's τ value for the French language is 29.92.
The average Kendall's τ value across all languages is 29.92.
The Kendall's τ score for the "es" language is 23.81.
The Kendall's τ score for the "syntax25" model is 7.42.
Table 2 shows Kendall's τ on WMT12 for neural networks using BLEUcomp.
The Kendall's τ for the French language using BLEUcomp, syntax25, and Wiki-GW50 is 26.15.
The table shows the average Kendall's τ on WMT12 for semantic vectors trained on different text collections.
The table shows the results when combining the semantic vectors with 4 metrics and syntax25.
"On-the-fly sentence embeddings + full fine-tuning" achieves a higher Kendall’s τ score for the cz language compared to "Pre-computed sentence embeddings".
The variant "On-the-fly sentence embeddings + full fine-tuning" has the highest average Kendall’s τ score across all languages.
Table 7 describes the Kendall's τ on WMT12 for different variants of LSTM-RNNs.
Bidirectional LSTM performs better on Kendall's τ for the French language.
The table shows the results for baseline and LSTM-based model without syntactic embeddings.
Table 9 compares the performance of different translation systems on the WMT12 dataset.
NNRKMean has the highest correlation score for the cz language.
SEMPOS has the highest average correlation score across all languages.
NNRKMean has a higher Kendall's τ score on French (44.1) compared to NNRKZero (43.7) and DiscoTK discoMT:WMT2014 (43.3).
ReVal has the lowest average Kendall's τ score (30.4) compared to all other systems.
The system "ReVal" achieves the highest average Pearson correlation for the language pair "de" (German).
The system "NNRKMean" achieves a higher Pearson correlation for the language pair "cs" (Czech) compared to the language pair "ru" (Russian).
The MASS+finetune model performs better in terms of fluency, consistency, and engagingness on the Cornell Movie dataset compared to the Dailydialogue and Switchboard datasets.
Table 7 shows the performance of systems and iterative combinations of them.
The iterative combination of systems 1, 2, and 3 achieves a higher F0.5 score.
Table 4 shows the performance of the Nematus system on the W&I dev set based on different training data.
The use of more data improves the performance of the Nematus system on the W&I dev set.
The table provides information on the size of synthetic datasets and Nematus scores when trained on them.
The size of the Gutenberg Books dataset used in the experiments ranges from 650,000 to 7,000,000 sentences.
The filtered versions of Language Tool, Grammarly, and Nematus have higher F0.5 scores compared to their non-filtered versions.
The filtered version of Nematus has the highest F0.5 score among all the systems.
The table shows the performance metrics (precision, recall, and F0.5 score) of different systems.
The combination of systems 1, 2, and 3 achieves the highest F0.5 score among all combinations listed in the table.
The "1+2+3+4 ensemble" system has the highest F0.5 score among all the systems.
The "1+2+3+4 combination" system has the highest precision and F0.5 score among all the systems.
As the maximum sequence length increases, the validation accuracy on BERT-LARGE generally decreases.
The highest validation accuracy on BERT-LARGE is achieved with a learning rate of 1.5e-6.
The Bleu score for sl-en translation decreases as the number of BPE merges increases.
The Bleu score for be-en translation decreases as the number of BPE merges increases.
Table 2 shows the translation results of different transfer learning setups.
The "+ Cross-lingual word embedding", "+ Artificial noises", and "+ Synthetic data" systems outperform the "Baseline" and "Multilingual Johnson et al. (2017)" systems in terms of Bleu scores for the "be-en" language pair.
Freezing the parameters of the target self-attention component in the decoder improves the Slovenian→English translation results.
Adding the encoder-decoder attention component to the decoder improves the Slovenian→English translation results more than adding the feedforward sublayer component.
Table 1 shows the accuracies of different models compared to the (tanh) LSTM-RNN.
The LSTM-RNN model with GloVe-300D achieves an accuracy of 49.9% on the fine-grained sentiment classification task.
HAN+L achieves higher F1-macro and UAR scores than HCAN, HLGAN, and HAN on the DAIC-WoZ corpus development set.
HLGAN achieves the same F1-macro and UAR scores on the DAIC-WoZ corpus development set.
The table shows the results of different architectures on the GPC dataset.
The architecture HAN+L performs the best when considering only the therapist's input.
The hate vocabulary is distributed across eight different categories: archaic, ethnicity, nationality, religion, gender, sexual orientation, disability, and social class.
The ethnicity category has the highest percentage of hate vocabulary distribution.
Batch 1 has more sentences than batch 2.
The inter-annotator agreement is slightly higher for batch 1 compared to batch 2.
The "Att. Cap2Img †" method has a higher CR score than the "Cap2Img" method.
The "Cap2All" method has a higher MPQA score than the "Cap2Cap" method.
The "mix-all" model has the highest BLEU score of [BOLD] 32.7 on the En-Es WMT Test Set clean.
The "clean+prep" model has the second highest average BLEU score of 31.1±1.5 on the En-Es WMT Test Set.
The mix-all model performs better than the clean model on both noisy input and preprocessed input with a GEC model.
The mix-all model performs better than the clean model on noisy input, but the clean model performs better on manually corrected input.
Preprocessing the noisy input with a GEC model improves the BLEU score for the mix-all model.
Table 5 provides a comparison of different models based on their WER percentages.
The CHiME-6 TDNN-F baseline model has a higher WER percentage compared to other models in Table 5.
The WER(%) for the "Char NNLM" is higher than the WER(%) for the "Word NNLM".
The "+SACT" model has the highest BLEU score among all the models.
The NPMT model has a higher BLEU score than the RNNSearch and Seq2Seq models.
The BLEU score of the Unsupervised Phrase Table improves with each iteration of back-translation.
The improvement in BLEU score of the Unsupervised Phrase Table starts to plateau after the 6th iteration of back-translation.
The Model Ensemble with the combination of Best Word-level + Subword-level achieves the highest BLEU score.
The Model Ensemble with the combination of Best Word-level + Subword-level achieves the highest BEER 2.0 score.
The table provides perplexity scores for each of the models.
The ARAE (condit.) model has the lowest perplexity score.
The AER scores of the different systems vary.
"Cseq2seq-I (MP)" has the highest AER score among all the systems.
The accuracy of the Oracle + DA model decreases after applying the rule-based transformation.
The FEVER score of the UCL model is higher before applying the rule-based transformation.
Table 4 provides a breakdown of the potency of the three classes of transformation for each model.
The accuracy before and after the transformations are provided in Table 4.
Table 5 shows the results on all four Bakeoff-2005 datasets.
Our model achieves a score of [BOLD] 95.4 on the PKU dataset.
The table shows the effect of different update methods on the PKU and MSR datasets.
The Early update method outperforms the Standard and LaSO update methods in terms of F1 scores on both the PKU and MSR datasets.
Pre-training improves the F1 score on the PKU dataset.
The test time is longer than the training time on the PKU dataset.
Table 2 provides the average number of acceptable realizations out of 3 for different models.
The syntax-aware model has a higher average number of acceptable realizations out of 3 compared to the baseline s2s model.
The highest accuracy score for English is 96.8%, for Bulgarian is 98.9%, and for Japanese is 96.5%.
The accuracy score for English is higher than the accuracy scores for Bulgarian and Japanese.
The highest F1 score for English is 90.70, for German is 82.00, for Dutch is 89.86, and for Spanish is 87.18.
The F1 score for English fine-tuning is 90.70, for German fine-tuning is 82.00, for Dutch fine-tuning is 89.86, and for Spanish fine-tuning is 87.18.
The highest accuracy for the English language is 96.82%.
The highest accuracy achieved through fine-tuning for the Italian language is 98.11%.
The table shows the performance scores for "hi" and "ur" in some context.
The performance score for "hi" is 97.1.
The SOV order has a performance of 64.22.
Our Best Result has the highest F1 score among all the parsers in Table 5.
Cross and Huang (2016) achieved an LR score of 90.5 and an LP score of 92.1.
Table 1 provides the development F1 scores using word-level search with various beam sizes k and two choices of word beam size kw.
As the beam size k increases, the F1 scores also increase.
The F1 scores increase as the beam size (k) increases for both [ITALIC] kw= [ITALIC] k and [ITALIC] kw= [ITALIC] k/10.
The F1 scores are slightly higher for [ITALIC] kw= [ITALIC] k/10 compared to [ITALIC] kw= [ITALIC] k for all beam sizes.
The table provides results when the best setting from Section 6 is rerun with Open action pruning with context size c=2 and various pruning fractions p.
The F1 score increases as the pruning fraction p decreases.
The base model has the highest ΔSDR (dB) value among all the models in Table 9.
Both the model without FiLM and the model without distance regularization have lower ΔSDR (dB) values than the base model.
In the validation set, the majority of examples have a Δ SDR (dB) less than 5.0.
A significant portion of the examples in the test set involve confusing speakers.
The table shows the performance of Wavesplit when the number of speakers varies.
The performance of Wavesplit improves as the number of active speakers decreases.
Table 5 provides information about the accuracy of models and humans on different datasets.
The accuracy of Turkers on RACE-M, RACE-H, and RACE datasets is 85.1%, 69.4%, and 73.3% respectively.
Table 13 compares the Word Error Rate (WER) for two speech systems and human level performance on read speech.
The Word Error Rate (WER) for the two speech systems is higher than the WER for human performance on all datasets.
The architecture "7 RNN, 9 total" has the highest number of RNN layers among all the architectures in the table.
The architecture with 2400 hidden units has a higher WER on the development set compared to the architecture with 1280 hidden units.
Table 2 compares the Word Error Rate (WER) on a training and development set with and without SortaGrad, and with and without batch normalization.
The Word Error Rate (WER) is lower for the "Sorted" condition compared to the "Not Sorted" condition.
Table 5 compares the WER with different amounts of striding for unigram and bigram outputs on a specific model.
The WER is higher for bigram outputs compared to unigram outputs for all stride values.
The performance gain decreases as the number of GPUs increases.
Our all-reduce implementation performs better than the OpenMPI all-reduce implementation for all numbers of GPUs.
The speedup for English is higher than the speedup for Mandarin.
The GPU CTC time for English is higher than the GPU CTC time for Mandarin.
The Baidu dataset consists of both "read" and "mixed" speech types.
The total number of hours for all datasets used to train DS2 in English is 11,940.
As the fraction of data increases, the WER decreases for both the Regular Dev and Noisy Dev sets.
The Noisy Dev set has a lower WER compared to the Regular Dev set for the "Hours 120" training duration.
Both Noisy Speech DS1 and Noisy Speech DS2 perform worse than the noise-free baseline on the CHiME eval clean dataset.
The table compares the performance of different models on different datasets.
The OOV performance is the lowest for the MSR dataset compared to the other datasets.
The "Sparse-CRF" model achieves the highest F1 score of 95.08 on the NER CoNLL03 test.
The "Sparse-CRF" model achieves the highest F1 score of 95.85 on the CWS CTB6 dev.
The table compares the performance of different models on multiple datasets.
The +SEmb-Hetero model outperforms other models on all datasets.
The table compares the performance of NN and non-NN models on the CoNLL03 dataset.
The table shows the performance of the best NN and non-NN models.
The model with "our best" configuration achieves the highest performance in all three genres: CTB6, PKU, and MSR.
The model with "our best" configuration achieves a performance of 95.48 in the CTB6 genre.
The performance of LM is higher than the performance of AR and SAR for both "Idiom Embedding" and "Average Character Embedding" representations.
The performance of "Average Character Embedding + MLP" is higher than the performance of "Average Character Embedding" for both LM, AR, and SAR.
The performance of the "Synergistic (ours)" model varies based on the values of "N" and "M".
The highest value of R@1 is achieved by the "Synergistic (ours)" model when "M" is equal to 20.
The Technion model has the highest performance in terms of NDCG, MRR, R@1, R@5, and R@10 among all the models.
The Single (ours) model outperforms the LF, HRE, and MN models in terms of NDCG, MRR, R@1, R@5, and R@10.
Table 2 shows the performance of a discriminative model on a validation dataset.
The MRR values increase as the value of "N" increases for a fixed value of "M" and "τ".
The word similarity scores for "cbow" models are generally higher than the scores for "sg" models.
The word similarity scores generally increase as the size of the training corpus increases.
Table 1 provides information about the training corpora.
The ivLBL model achieves the highest accuracy of 55.5% on the MSR sentence completion task.
The LSA model performs better than the SG model on the MSR sentence completion task.
The table compares the results of the "sg" and "cbow" models on the MSR sentence completion task.
The table presents the scores of different word representation models and approaches on the MSR sentence completion task.
The Pearson's r score for the Reddit+SNLI tuned model on the dev set is 0.835.
The Pearson's r score for the CNN (HCTI) model on the test set is 0.784.
Table 1 shows the Precision at N (P@N) results of a Reddit model with different encoders on the test set.
The Transformer encoder achieves the highest Precision at N (P@N) results for all values of N.
The KIM Ensemble model has the highest accuracy of 89.0 on SNLI classification.
The Reddit+SNLI model has an accuracy of 84.1 on SNLI classification.
Table 4 provides the Pearson's r of the proposed models on the STS Benchmark with a breakdown by category.
The Reddit+SNLI tuned model outperforms the Reddit tuned model in both the dev and test sets for all categories.
Table 6 shows the Mean Average Precision (MAP) of the proposed models on CQA subtask B.
KeLP-contrastive1 has the highest Mean Average Precision (MAP) among the proposed models on CQA subtask B.
The average UAS/LAS scores for mBERT with zero-shot transfer and POS information are higher than the average UAS/LAS scores for mBERT with supervised learning.
The word order distance to English for Chinese is 0.23 and the UAS/LAS scores for mBERT with supervised learning are 88.3/81.2.
The models perform better on average in English and Chinese compared to the other languages.
The mBERT model performs the best in the English language compared to the other languages.
The average accuracy for low resource cross-lingual transfer is higher than the average accuracy for zero-shot cross-lingual transfer.
The F-score and accuracy for the "All" feature set are both 78.06 and 77.68, respectively.
The "All" feature set performs significantly better than the "Baseline" feature set, with an F-score and accuracy of 78.06 and 77.68, respectively, compared to 50.00 for both metrics in the "Baseline" feature set.
The "All" feature has the highest F-score and accuracy.
All the features have an F-score above 80.
The accuracy for the "Only metadata" feature is higher than the accuracy for the "Only POS" feature.
The feature set "Sent,bad,pos,NE,meta,punct" has a lower F-score than the feature set "Only metadata".
The table presents the results for individual feature groups in a classification task of paid troll vs. non-troll comments.
The accuracy of the classification task decreases as more specific features are included in the feature combination.
Table 2 provides the zero-shot performance on non-English centric language pairs, comparing different approaches and providing a baseline for translation quality with parallel data.
The multilingual model trained on all English-centric language pairs performs better than the multilingual model without monolingual data.
The translation quality of the Multilingual NMT model for the language pair "fr_en" is 34.9.
The translation quality of the Mono. Only model for the language pair "de_en" is 7.4.
Table 1 compares the impact of different domain similarity measures on the student's performance when used for interpolating the predictions of source domain teacher models.
The Jensen-Shannon divergence performs better in the "Book" and "DVD" domains compared to the other similarity measures.
The sentiment analysis performance of the Student model with a general teacher is higher on DVD and Electronics domains compared to the Book domain.
The sentiment analysis performance of the Teacher-only model is higher on the Kitchen domain compared to the Book and Electronics domains.
Table 1 provides the performances of Seq2Seq models with and without attention on the RCV1-v2 test set.
The F1 score is higher for the Seq2Seq model with attention compared to the model without attention.
Our model achieves the lowest hamming loss (HL) among all the models.
Our model achieves the highest F1 score among all the models.
The "hybrid" model performs better than the other models in all metrics.
All models have an F1 score higher than 0.866.
Our model outperforms the hierarchical model on the RCV1-V2 test set at different sentence lengths.
Our model achieves the same recall as the hierarchical model for sentence length of 15 words on the RCV1-V2 test set.
Table 14 shows the performance of different query strategies on different datasets.
The chance baseline has a performance of 9.4±0.0 on the SGN dataset.
The chance intersection for b=39 queries is 100.0 for all datasets.
The intersection of samples obtained with different seeds (ModelD) is higher than the intersection of samples obtained with the same seeds (ModelS) for the FastText model.
Table 5 shows the intersection of samples obtained with different values of b in FastText and NaiveBayes models.
The FastText model with 9∩19∩39 intersections performs better than the FastText model with 39∩39∩39 intersections in terms of the SGN dataset.
Table 13 provides metrics measured after training the FastText (FTZ-Ent) model on a sample with 39 queries using the entropy query strategy.
The table presents the average label entropy for six different datasets across multiple query iterations.
The average label entropy for FTZ (∩ Q) is higher than MNB (∩ Q) across all datasets.
Table 6 provides the baseline performance of correctly predicting the label of tweets with a negative stance.
The best ML system has higher precision, recall, F1 score, and AUC values compared to the other methods.
The agreement scores for "Relevance", "Subject", "Stance", and "Sentiment" are the same.
The category "Relevant" has the highest Mutual F-score.
The classifier accurately classified the majority of positive tweets.
The classifier misclassified the majority of neutral tweets as positive.
The "Best ML system" performs better than the "Other" system for both "Pattern" and "Negative" categories.
The "Pattern" category has more predictions than the "Negative" category for both the "Best ML system" and the "Other" system.
Table 11 provides the performance of the ensemble system on correctly predicting tweets labeled as 'Negative'.
The precision of the ensemble system in correctly predicting tweets labeled as 'Negative' is 0.18, the recall is 0.61, the F1-score is 0.28, and the AUC is 0.62.
The LSTM-Word-Large model has the lowest perplexity score of 85.4, indicating better performance in language modeling compared to other models.
The LSTM-Char-Large model has the largest size of 19 m, indicating a larger number of parameters in the model compared to other models.
The table shows test set perplexities for Data-s, with the first two rows from Botha2014b and the last six rows from this paper.
The perplexities for the "Large Char" model are the highest among all the models in the table.
Table 5 shows the test set perplexities on Data-l for different models.
The perplexities for the Char model are the lowest among the Small models on Data-l.
Table 7 shows the perplexity on the Penn Treebank for small/large models trained with/without highway layers.
The perplexity of the small model trained without highway layers is 100.3.
The perplexity of the LSTM-2x1500 model is lower than the perplexity of the LSTM-2x200 model.
The training time of the discrete TRF model is longer than the training time of the KN5 model.
"KN5+LSTM" performs better than "KN5+neural TRF" on the test set.
"neural TRF" has a longer training time than "LSTM-2×200".
Table 2 provides a performance comparison between the baseline and the best system.
The best system has a higher human rating compared to the baseline system.
Table 2 compares the results of different models to the literature.
The model "ACCESS: NbChars0.95 + LevSim0.75 + WordRank0.75" achieved the highest SARI score and the lowest FKGL score.
The accuracy of the FBANK representation with 100 hr 1 Hidden pre-training is 46.9%.
The accuracy of TERA-base with time + channel auxiliary objective and 360 hr Linear pre-training is 77.4%.
The table compares different types of representations.
TERA-large has the highest performance in terms of #ParamPre-train and #L.
The frame-wise linear classification accuracy increases with longer pre-training durations for the TERA-base: time + channel representation.
The MFCC representation has a low utterance-wise linear classification accuracy for the 100 hr pre-training duration.
The liGRU + TERA-base model with time + channel + mag pre-training has the lowest WER among the different pre-training combinations.
The liGRU + TERA-base model with time + channel pre-training has a lower rescore score than the other pre-training combinations.
The table presents the results of an ablation study comparing different methods for pre-training on the LibriSpeech train-clean-100 subset.
TERA-base: time + channel + mag (WS) achieves the highest performance in phone classification among all the methods tested.
The table provides MSE values for different performance monitoring techniques on various datasets/domains.
The entropy-based decision (Dec), MCD for decision (Dec), and autoencoder (AE) techniques have the lowest MSE values in the "All Together" dataset/domain.
The table shows the results and comparison of different models on the DEBIN-Test subset.
The Transformer model outperforms the other models in terms of precision, recall, and F1 score.
Table 1 shows a total of 441 NLP features.
The "Structural Features" category in Table 1 contains 51 features.
Our Models outperform eventAI in both MacroF and RMSE metrics according to Table 4.
Model E (Longformer + Transformer + NLP Features) achieves the highest MacroF score among Our Models in Table 4.
Our Proposed Method - Top Ns using (D + E + F) achieved the highest rank among all systems in Task B.
The data sources mentioned in the table are Datorium, AHD, Diabetes, Breast, Colon, Liver, Lung, Melanoma, Prostate, and Stomach.
The years mentioned in the table are 14/15, 13, and 13/14.
The table provides the performance of different models on the VQA 2.0 Test-dev Split dataset.
The table provides the performance of the MFH (Yu et al., 2017b) model and MFH w/RTAU model on the VQA 2.0 Validation Split dataset.
Simple Net (Conv. Visual Attn.) has the lowest score in the "VQA 2.0 Validation Split Y/N" category among all models.
DRAU has the highest score in the "VQA 2.0 Validation Split All" category among all models.
The performance of the models on the VQA 1.0 Open Ended Task Test-dev improves as we move down the table.
The performance of the MCB model on the VQA 1.0 Open Ended Task Test-standard improves when using 7 models for prediction compared to using only 1 model.
In Version-1, there are 175 tweets with a favor stance for Target-1.
In Version-3, there are a total of 1,879 named entities.
The LSTM models have higher validation accuracy compared to the GPT-2 models.
The GPT-2 models have lower distributional discrepancy compared to the LSTM models.
The language model score for 5GPT-2 is [BOLD] 1.0.
The BLEU-5 score for 5LSTM is 0.0.
Among the five LSTM-based and five GPT-2-based generators, the 5GPT-2 model has the highest LM score.
All the generators, including 5LSTM, 5GPT-2, and 10Gs, achieve a perfect score of 1.0 on the DD metric.
GPT-21.0 has the best performance among all the generators based on the FED score.
GPT-20.8 has the second-best performance among all the generators based on the FED score.
The HLSM model has the highest test classification accuracy among all the models.
The asymmetric LDA model has a higher test classification accuracy compared to the LDA model.
The table provides the test F1 scores of different models on the CoNLL-03 and OntoNotes 5 datasets.
The models perform better on the CoNLL-03 dataset compared to the OntoNotes 5 dataset.
Table 7 presents the results of binary sentiment classification on different domains.
The precision and recall values for the "True" class are consistently higher than those for the "Fake" class across all domains in Table 7.
The table shows the results of language modeling measured by perplexity for four different domains: Book, Electronic, Movie, and Hotel.
The gC2S(P+S) model consistently achieves the lowest perplexity scores compared to the other models in each domain.
The evaluation data for fake review detection includes four different domains: Book, Electronic, Movie, and Hotel.
The total number of reviews/ratings in the evaluation data for fake review detection is calculated by multiplying the number of products by the number of reviews/rating for each domain.
The true positive rate for fake review classification in all domains is 77.9%.
The false negative rate for fake review classification in the hotel domain is 14.8%.
Table 6 shows the results of fine granularity sentiment classification on different domains.
In the Hotel domain, the precision, recall, and F1 scores of fake reviews are lower than those of true reviews.
The F1 score for the second component using the SubSpace (CBOW) method on the ENC dataset is 73.44.
The F1 score for the second component using the ALLDEFS method on the EVPC dataset is 69.2.
The "SubSpace (JJ+ RB)" model outperforms the "Baseline" model in terms of F1 score.
The "SubSpace (VB)" model has a higher recall than the "SubSpace (JJ)" model.
The state-of-the-art models (SVO and AN) outperform the SubSpace model in terms of accuracy and f1 score.
The AN model has more features than the SVO model.
The ANSP method achieves the highest accuracy score of 0.750.
The DTR method has the lowest negative rate of 0.273.
The dropout rate for the LIAR dataset is 0.6.
The learning rate for the Weibo dataset is 0.01.
ANSP performs the best among all models on Weibo.
Hybrid-CNN achieves the highest accuracy among all models on LIAR.
The precision score for the LIAR dataset with the basic+Adv model is 0.366.
The F1 score for the Weibo dataset with the basic+Adv+Ind+Diff model is 0.462.
The method with reinforcement learning performs better than the method without reinforcement learning for common features extraction.
The F1 score of the method with reinforcement learning is higher than the F1 score of the method without reinforcement learning for common features extraction.
OMDb LDA and OMDb RBM consistently outperform Reuters LDA and Reuters RBM across all mAP metrics.
20NewsGroup LDA and 20NewsGroup RBM consistently outperform OMDb LDA and OMDb RBM across all mAP metrics.
Table II provides the performance evaluation of fixed total feature number word/word pair on three different datasets: OMDB, Reuters, and 20NewsGroup.
The F-score for word pair is lower than the F-score for word in the OMDB dataset with a fixed total feature number of 11K.
Table 5 shows the performances of the BiLSTM-CRF and Deformable stacked structure models without character embedding on the test set.
The Deformable stacked structure model achieves a slightly higher F1 score on the CoNLL-2003 dataset compared to the BiLSTM-CRF model.
The experiment in Table 6 compares the effects of different structure settings (1, 2, and 3) on the OntoNotes 5.0 dataset.
The F-score improves as the structure setting changes from 1 to 3 in the experiment.
Table 1 provides F1 scores, precision, and recall for different models for the multi-label classification task.
ResNet-18 with BXENT (single) method has an F1 score of 0.566.
BaseVocab performs worse than FinVocab on the PhraseBank corpus.
Table 6 provides WSD evaluation results using F1 scores (micro) for different models.
The F1 scores (micro) for the "LTA-NLM" model are higher than the F1 scores (micro) for the "BiLSTM-LM" model in all test datasets.
VSE-C (ours) outperforms GloVe and VSE++ in all categories of the fill-in-the-blank task.
VSE-C (ours) performs better than VSE++ in the "All (n. + prep.) R@1" category of the fill-in-the-blank task.
VSE-C (+all) is the least robust model against caption-specific adversarial attacks compared to the other models.
VSE++ achieves the highest accuracy in retrieving the correct caption when only considering the top-ranked caption.
VSE-C (+num.) has the highest R@1 score in the MS-COCO Test (+num.) category.
VSE-C (+rel.) has the highest Mean r. score in the MS-COCO Test (+rel.) category.
MAD-Attn achieves the highest accuracy in both the DA type and slot-value metrics compared to the other models.
MAD achieves an accuracy of 0.1 at the turn level.
The "Food" slot has the highest number of values among all the informable slots.
The "Res_name" slot can be requested along with the "Postcode" and "Signature" slots.
MAD achieves the highest accuracy on the ALDM dataset, with a value of 76.7.
MAD-Attn and MAD-EM both achieve a perfect accuracy of 100.0 for the "DA type" metric on the ALDM dataset.
The "MAD" model has the highest prediction accuracy on both the departure city and arrive city slots.
The "MAD-Attn" and "MAD-EM" models have the same prediction accuracy on both the departure city and arrive city slots.
The No Atten model performs worse as the depth increases, while the Simple model performs better as the depth increases.
The Structured model outperforms the Simple model for all depths.
Table 3 shows the translation performance of three different models on character-to-word and word-to-word Japanese-English translation.
The Simple model has better translation performance than the Sigmoid and Structured models for both character-to-word and word-to-word translation.
The MemN2N model performs well on the Time Reasoning task, with an answer accuracy of 99.9% and a supporting fact selection accuracy of 77.6%.
The Binary CRF model performs consistently on the different tasks, with an average answer accuracy of 81.4% and a supporting fact selection accuracy of 39.6%.
The models listed in Table 5 show an increasing trend in accuracy.
Adding intra-sentence attention to the decomposable attention model improves its accuracy.
Adding "+wiki" to the initialization improves the F1 score on the dev and test sets for all three subjects (MS, Phy, CS) compared to the SE initialization.
The F1 score on the test set for CS is highest when using the "+wiki+ACM" initialization compared to the other three initialization methods.
The combination of GraphFeat* with ULM achieves the highest F1 score on the test set.
The combination of GraphInterp with ULM performs better on the dev set than the combination of GraphInterp without ULM.
The table displays the model performance on the official subtask B development set and test set.
The model without any additional modifications has a development accuracy of 91.6%.
The embedding algorithm MC has the highest correlation score for the Eigenspace Instability measure.
The embedding algorithm GloVe has the highest correlation score for the 1−k-NN measure.
The CBOW and GloVe embeddings have the lowest errors in the SST-2 and Subj downstream tasks for the Eigenspace Instability measure.
The MC embeddings have the lowest errors in the CoNLL-2003 downstream task for the PIP Loss measure.
The table provides hyperparameters for three different embedding algorithms: CBOW, GloVe, and MC.
The learning rate for CBOW is 0.05, for GloVe is 0.01, and for MC is 0.2.
The maximum number of training epochs for training TransE knowledge graph embeddings is set to 1000.
The optimizer used for training TransE knowledge graph embeddings is SGD.
The CBOW embedding algorithm has the highest error for the SST-2 downstream task.
The GloVe embedding algorithm has a higher error for the NER (CoNLL-2003) downstream task compared to the SST-2 downstream task.
The precision and recall values generally increase as the dependency length increases.
The LSTM model consistently outperforms the F - T and F - C models in terms of precision and recall.
The table provides LAS results for different categories in the Google Web Treebank dataset.
The difference in F-Cubic accuracy between the "no dropout" and "with dropout" conditions is 0.4.
The F-Cubic accuracy for the LSTM model with dropout is 89.5.
The Bleu score for German→English translation improves as the size of the synthetic data increases.
The translation quality for German→English is the highest in 2016.
Table 2 shows the English→German translation quality (Bleu) of systems using forward and reverse models for generating synthetic data.
The size of the dataset increases when synthetic target or synthetic source data is added.
The size of the + synthetic dataset is double the size of the baseline dataset.
The + ground truth dataset has a higher Bleu score than the baseline dataset in 2017.
The "Difficulty criterion" system achieves a higher Bleu score than the "Random" system for the De-En translation in the test year 2016.
Table 4.4 shows the PER(%) obtained in distant-talking scenarios with two different pre-training methods: standard RBM pre-training (Stand-PT) and supervised close-talking pre-training (CT-PT).
The proposed supervised close-talking pre-training (CT-PT) achieves a PER(%) of 34.7 in the Real-Rev scenario.
The table shows the performance (in PER%) of different types of GRU models with and without batch normalization on the TIMIT dataset using MFCC features.
The Li-GRU model with batch normalization has the lowest PER% among the three types of GRU models.
The second row in Table 5.6 uses batch normalization while the first row does not use batch normalization.
Table 6.1 compares the performance of the proposed joint training approach with other competitive DNN-based systems.
The proposed joint training approach outperforms other competitive DNN-based systems in terms of performance.
The "Network of DNNs" system performs worse than other DNN-based systems on all three datasets (TIMIT rev, WSJ rev, WSJ rev+noise).
The "Network of DNNs" system has the lowest performance on the WSJ rev dataset compared to other DNN-based systems.
The table shows the performance of the proposed network of DNN achieved at various levels of the architecture.
The performance achieved at Level 2 is better than the performance achieved at Level 0 and Level 1.
There are four different models in the table: PLTE, -LASA, -PM, and -LASA-PM.
The performance of all models is higher on the Weibo dataset compared to the other datasets.
The number of test data for the Cornell dataset is 8,310.
The total number of constituting utterances for the SEMAINE dataset is 4,386.
The "neu" category has the highest number of dialogues and utterances in the Iemocap dataset.
The "surp" category has the highest number of utterances in the Dailydialog dataset.
The table shows the average performance on ERC using different pre-trained weights adaptation strategies.
The F-Score for Iemocap is higher than the F-Score for DD across all pre-trained weights adaptation strategies.
The table shows the average performance on ERC with pre-trained weights: {θBERT+θcornellcxt}.
The DialogueRNN model achieves the highest F-Score and r values among all the models in the table.
The accuracy of the CNN model is higher when trained with SentiLR-B augmentation compared to when trained with C-BERT augmentation.
The RNN model has a higher accuracy on the full SST dataset compared to the full MR dataset.
Table 1 shows the accuracy on sentence-level sentiment classification benchmarks for different models.
The F1 score for BERT on the ATSC Lap14 task is 77.48.
The F1 score for RoBERTa is 87.72.
The SentiLR-B model outperforms other models in all tasks.
The SentiLR-B model performs better than other models in aspect-level sentiment analysis.
RoBERTa achieves the highest accuracy on the SCT dataset in the story ending prediction task.
Both SentiLR-B and SentiLR-R have higher accuracy than BERT on the SCT dataset in the story ending prediction task.
The number of aspect-polarity tuples for the OpeNER dataset in the Multiclass − category is 556.
The number of aspect-polarity tuples for the SemEval dataset in the EN language is 2268.
Table 4 shows the macro F1 scores of four models trained on English and tested on Spanish (ES), Catalan (CA), and Basque (EU).
Blse has the best results for each metric per column in Table 4.
The table displays the average length of tokens of correctly and incorrectly classified targets on the OpeNER Spanish binary corpus.
The RMSE value of the "AutoMOS Raw" metric is higher than the RMSE value of the "AutoMOS Quantized" metric.
The "Pearson r" correlation value for the "nfold=600,643,663,1235,1593" folds is higher than the "Pearson r" correlation value for the "nfold=6000,6424,6624,12348,15924" folds.
"Davidson et al." has the highest accuracy score among all the models.
"Our Model with embeddings trained on Glove" has a higher accuracy score than "Our Model with embeddings trained on Word2Vec".
"Our Model with embeddings trained on Glove" has the highest accuracy score.
"Our Model with embeddings trained on Glove" has a higher accuracy score than "Our Model with pre-trained Word2Vec embeddings".
The average silhouette score for the term-based clustering approach with 39 clusters is 0.0000.
The term-based clustering approach was performed with 39 clusters.
Table 1 shows the ROUGE scores derived from each feature in the Salient Sentence Selection task.
The ROUGE-N score for R-2 derived from the COS_TPS feature is 0.3550.
The precision@10 for the "T" transfer language is higher than the precision@10 for the "S" transfer language.
Table 2 shows the performance of typology prediction using hidden states of the parser's encoder for different WALS IDs.
The Logreg model performs better than the Majority baseline in typology prediction for all WALS IDs.
The "skip-thought-pr" method achieves the highest accuracy score in the 2-class sentiment classification task.
The "CNN-pool-k" method achieves the highest accuracy score in the 5-class sentiment classification task.
The "CNN-pad" model has the highest inference speed among all the models.
The "CNN-pad" model has a higher inference speed than the "CNN-pool" model.
The "CNN-pool-k" method has the highest accuracy of [BOLD] 90.64.
The "skip-thought-pr" method has the lowest accuracy of 82.57.
The "SRL-C" model achieves the highest performance when injecting external syntax information.
The "Full-C" input achieves the highest performance when injecting external syntax information.
"Our best model" outperforms all other models in both the "CoNLL'05 WSJ" and "CoNLL'12 Test" datasets.
"He et al. (2018)" performs better than "Ouchi et al. (2018)" on the "CoNLL'05 Brown" dataset.
The table provides the performance of three different models on the CoNLL'05 WSJ, CoNLL'05 Brown, and CoNLL'12 Test datasets.
The "Dep" model performs consistently well on the CoNLL'05 WSJ, CoNLL'05 Brown, and CoNLL'12 Test datasets.
The table provides the results of different methods on the considered benchmark data sets.
The table shows the accuracy percentages of different methods on the CCAT-10 dataset.
Each class in Table 1 is assigned a base 4 representation.
The class {␣, ∖t, ∖n} is assigned the base 4 representation 10.
The LR classifier has the highest accuracy on the test set.
The SVM classifier has a higher accuracy than the LR classifier on validation set 1.
The "FCGR+FTT+PCA" method has the lowest accuracy among the proposed methods.
The "FCGR+LR" method has a next accuracy of 4.1.
The consonants "b, f, p, v" have a corresponding number of "1" in the Soundex encoding table.
The consonants "c, g, j, k, q, s, x, z" have a corresponding number of "2" in the Soundex encoding table.
The parsing speed of "Our Model" is 220 sents/sec.
The parsing speed of "DBLP:conf/acl/ZhouZ19{}^{\spadesuit} (w. Cython)" is 159 sents/sec.
The "Our Model" outperforms the "PSN Model" for all phrase types.
The phrase type "NP" has the highest count among all the listed types.
The table provides evaluation results on the SRE18 (CMN2) dev set.
The MP model with λ=0.001 has the lowest EER and DCF(0.01) values compared to other models.
The table provides DCF scores for the x-vector, B, and MP models on the SRE16 (pooled) test set for different combinations of λ and p values.
The DCF scores for the MP model with λ=0.001 and p=0.01 are higher than the scores for the other combinations of λ and p.
The F1 score for "BP 0" is higher than the F1 score for "BP 1" and "BP 2".
The precision score for "Nulls P" is higher than the recall score for "Nulls R".
The Reschke Best system has the highest F1 score among all systems.
The Mention-CNN system with Sum aggregation has a precision of 0.0 for nulls.
The experiments used two different models: T-ED and T-DMCA.
The accuracy of the models increases as the model architecture becomes more complex.
The F1 score for ChemProt using the BERT+balanced bagging configuration is 0.70.
The F1 score for CDR using the DARE configuration is 0.73.
The models in the table are categorized into different scenarios based on their performance.
The F1 score for the G+J model is 83.68.
Increasing the amount of pre-training data improves the performance of the model, as shown by the decreasing CER values in Table 3.
Pre-training on multiple languages (EN, ES, DE) and using a gate mechanism results in the best performance, as indicated by the lowest CER value in Table 3.
Table 2 shows the performance of the language-universal modeling approach with and without language-specific gating, as well as the baseline monolingual performance.
The language-universal modeling approach with language-specific gating achieves the lowest CER and WER percentages for all three languages.
The WESPAD model consistently outperforms the other models across all topics in the PHM2017 dataset.
The CNN model performs well on most topics in the PHM2017 dataset, but has a lower F1-measure for the "Cancer" topic compared to the other models.
The WESPAD model has the highest F1 score and recall compared to other models in the PHM2017 dataset.
The "ME+lex+cen" model has a precision of 0.827 in the PHM2017 dataset.
The "WESPAD" model has the highest F1 score and recall compared to other models in the FLU2013 dataset.
The "ME+lex+emb" model has higher precision and recall values compared to the "ME+lex" and "ME+cen" models in the FLU2013 dataset.
Table 5 shows the feature ablation results of WESPAD on the PHM2017 dataset.
The FC audio encoder has the lowest PESQ score for both the "A" and "AV" setups.
The "MAE+Cosine" loss function has the highest SNR score for both the "A" and "AV" setups.
The recall percentages for each viseme decrease from top to bottom, except for the "Chance" row which has a much lower recall percentage.
The average recall percentage for all visemes is 33.5%.
The accuracy of the re-trained pronoun scoring model on the "WMT13-18" training data is 86.76%.
The re-trained pronoun scoring model was trained on the "WMT13-18" data.
The model "Concat" has the best performance in anaphora according to the evaluation model.
The model "Concat" has the highest BLEU score among all the models.
Table 10 ranks models according to their performance in anaphora, with smaller model scores indicating better performance.
The BLEU score for the "Sen2Sen" model is 56.6195.
Table 11 ranks models based on their performance in coherence, with smaller scores indicating better performance.
The Concat model has a BLEU score of [BOLD] 31.96.
The "Concat" model has the highest rank in coherence according to the evaluation.
The "Concat" model has the highest BLEU score.
The Concat model has the highest coherence score among all the models.
The values in the "main" column are higher than the values in the other columns for each row.
The values in the "cutler xx ecab" row are lower than the values in the other rows for each column.
As the value of r increases, the value of C∗ also increases.
For a fixed value of r, the value of C∗ decreases as the value of μ increases.
The table provides the results of a comparison study on OSHA reports and ECAB legal decisions using different methods.
The lasso method has the highest average time (101.1 sec) among all the methods listed.
The table presents word list characteristics of different rescaling norms for three methods: textreg.1.2, textreg.2, and textreg.4.
The average word frequency is highest for textreg.4, followed by textreg.2, and then textreg.1.2.
The phrase "were using a gasoline" appears in all 5 reports.
The phrase "exhaust fumes" appears in 18% of the reports.
"propan+ power+" has the highest values for q=1.5, q=2, q=3, and q=4.
The phrase "poison+ at+" has 100% of reports tagged.
The "Average feature vector" model input has the highest accuracy of 73.96%.
The "Concatenated vector" model input has a lower accuracy of 73.18%.
The table presents the accuracy of the dialogue act identification with various model setups.
The RNN model with 3 utterances in context achieves the highest accuracy in the dialogue act identification task.
Table 5 presents the multi-class classification results for different baseline models on three different datasets: Original, Balanced, and Translation.
The Deep Learning models with Keras Embedding and FastText Embedding outperform the NB, SVM, SGD, B-LSTM, and CNN models in terms of F1 scores.
The POS tagger architecture has 2 Char-BiLSTM layers and 2 BiLSTM layers.
The parser architecture has 3 BiLSTM layers and an Arc MLP size of 500.
The result of "Our Best Model" is higher than the results of "rosa-marecek-2018-cuni" and "tyers-etal-2018-multi".
The result of "Our implementation" is lower than the result of "Our Best Model".
UH-RiTUAL achieved the highest F1 score for entities among all the participants in the WNUT-2017 shared task.
UH-RiTUAL achieved the highest F1 score for surface forms among all the participants in the WNUT-2017 shared task.
The class "person" has the highest precision, recall, and F1 score among all the classes.
The "Entity" class has higher precision, recall, and F1 score compared to the average precision, recall, and F1 scores of all classes.
The best model performs better than the other models in terms of Parse and Disfl F1 scores.
The best model performs better than the Berkeley model in terms of Parse F1 score.
The CL-attn model achieves the highest F1 score and flat-F1 score among all the text-only models on the dev set.
The CL-attn model performs better in terms of fluent sentences compared to the Berkeley and C-attn models on the dev set.
Table 2 shows the F1 scores for parse and disfluency detection of different models on the dev set.
The model "+ p + δ + f0/E-CNN" achieves an F1 score of 88.59 for parse and disfluency detection on the dev set.
The table compares the parse and disfluency detection F1 scores on the test set of different models.
The best model achieves an F1 score of 88.50 for parse and disfluency detection on the test set.
Table 5 provides human ratings comparing the performance of "Our Model Fine-Tuned" and "Our Model Emo-prepend" to the "Ground Truth" on the metrics of readability, coherence, and emotional appropriateness.
The "Ground Truth" responses have a higher rating for emotional appropriateness compared to both "Our Model Fine-Tuned" and "Our Model Emo-prepend."
Our model outperforms the baseline model in terms of perplexity.
Our model outperforms the baseline model in terms of BLEU score.
Our model achieves a success rate of 51.5% on the unseen test set in a single run.
The Reinforced Cross-Modal model achieves a success rate of 0.02 in beam search.
The highest score in the dependency arc prediction task is achieved by the model trained with GloVe embeddings.
The model trained with the SRL layer performs better in the dependency arc prediction task compared to the models trained with the DP and MT layers.
The F-measure values increase from Model 1 to Model 9, and then decrease for Model 10 and Model 11.
Model 9 has the highest Precision, Recall, and F-measure values.
Table 4 presents the mean reciprocal rank for co-author retrieval using ATM, NEA embeddings, Tf-idf, and NEA smoothing methods.
NEA smoothing has the highest mean reciprocal rank for co-author retrieval among the methods mentioned in Table 4.
The table shows the worst four topics produced by LDA and their corresponding NEA topics, with LDA trained on the NIPS corpus for K=7,000.
The table provides words related to topics and features for LDA and NEA, respectively.
The table compares the improvement of NEA topics over the original LDA topics in terms of per-topic coherence score.
Figure 2 displays the four topics that had the highest improvement by NEA compared to the original LDA topics.
The LDA model in Figure 8 is trained on the NIPS corpus for K=2,000.
The NEA model in Figure 8 is applied to images for visual recognition and pixel matching.
The NEA model outperforms the MMSG model in document categorization on the Reuters-150 corpus.
The NEA model outperforms the MMSG model in document categorization on the Ohsumed corpus.
RankGAN achieves the highest BLEU-4 score among all the GAN models.
LeakGAN achieves the lowest perplexity score among all the GAN models.
The "PF-HIN +BFS" model performs worse than the "PF-HIN" model in terms of LR AUC, LR F1, SS AUC, MC MIC-F1, MC MAC-F1, NC NMI, and NC ARI.
The "PF-HIN" model performs better than the "PF-HIN +random" model in terms of LR AUC, LR F1, SS AUC, MC MIC-F1, MC MAC-F1, NC NMI, and NC ARI.
PF-HIN outperforms all other models in terms of AUC and F1 scores on the link prediction task.
The models achieve higher AUC and F1 scores on Freebase compared to DBLP, YELP, and YAGO.
PF-HIN achieves the highest F1 scores in all four datasets (DBLP, YELP, YAGO, Freebase).
DeepWalk achieves the lowest F1 score in the DBLP dataset.
The PF-HIN model achieves the highest NMI score on the node clustering task.
The HINE model outperforms the LINE model in terms of ARI on the node clustering task.
The table provides information about the computation cost of various graph embedding models on different tasks.
The computation time for all graph embedding models is higher for the "SS" task compared to the "LR" task.
Table VII provides an ablation analysis over pre-training tasks.
The PF-HIN-ANP model has lower LR AUC, LR F1, SS AUC, MC MIC-F1, MC MAC-F1, NC NMI, and NC ARI values compared to the PF-HIN model.
The LR F1 score for the PF-HIN +CNN model is 0.396.
The NC NMI score for the PF-HIN +attention model is 0.712.
The "PF-HIN" model achieves the highest LR AUC score among all the models.
The "PF-HIN" model achieves the highest NC NMI score among all the models.
The value of the BERTScore-KPQA metric is 0.652.
The value of the NarrativeQA metric after stopwords removal is 0.748.
MS MARCO has a higher inter-annotator agreement (Krippendorff's α) compared to AVSD.
The correlation between BERTScore-KPQA and AVSD is higher than the correlation between BERTScore-KPQA and SemEval.
The correlation between BLEU-4 and NarrativeQA is higher than the correlation between BLEU-4 and MS-MARCO.
Table 2 provides the mean average precision of tf-idf (TFIDF), BM25, word vector centroid (CENTROID), and the semantic approach (SEM) on the TREC set.
The SEM method has a higher mean average precision compared to the TFIDF, BM25, and CENTROID methods on the TREC 2006 dataset.
As the size of the parallel corpus increases, the BLEU scores for the single-pair model, single-pair model with monolingual corpus, and the multi-way, multilingual model also increase.
The single-pair model with monolingual corpus achieves higher BLEU scores compared to the single-pair model.
Table 1 represents the macro-averaged F1-score of automatic essay classification into the big five dimensions of personality.
The combination of MB, CoarseAff, FineEmo, and BasicEmo achieves the highest F1-score for the conscientiousness (CON) dimension.
The "Seq" model has the same accuracy for all categories of contrastive errors.
The "Graph" model has a higher accuracy than the "Seq" model for the "Gender" category of contrastive errors.
Table 1 shows BLEU and Meteor scores on the development split of LDC2015E86.
The GCNSeq model achieved the highest BLEU score among all the models in Table 1.
The Graph model achieves the highest BLEU score for the LDC2015E86 dataset.
The Graph model achieves the highest Meteor score for the LDC2017T10 dataset.
There are 619 sentences in the development split for 0 reentrancies.
There are 679 sentences in the test split for 1-5 reentrancies.
The "IMOJIE" extraction method has a lower Mean Number of Occurrences (MNO) compared to the "CopyAttention+BERT" method.
The "IMOJIE" extraction method has a lower number of tuples compared to the "CopyAttention+BERT" method.
The system "IMoJIE" has the highest AUC score among all the OpenIE systems.
The system "OpenIE-4" has the same Last F1 score as its Opt. F1 score.
The "Score-And-Filter" technique has the highest F1 score and Last F1 score among all the filtering techniques.
The "Score-And-Filter" technique has a higher F1 score compared to the other filtering techniques.
The road representation for transportation is 0.74 ± 2.34.
The similarity for TimeOut representation is 0.44.
The pretraining tasks perform better on the VOC07 dataset compared to the IN-1k dataset.
The Token Classification pretraining task performs the same as the Bicaptioning pretraining task.
The table shows the results of three different pretraining tasks: Bicaptioning, Multi-Label Classif., and Instance Segmentation.
Bicaptioning has the highest VOC07 score among the three pretraining tasks.
ResNet-101 performs better than ResNet-50 on VOC07.
ResNet-101 (IN) performs better than ResNet-50 on IN-1k.
The ResNet-101 backbone achieves the highest CIDEr and SPICE scores compared to other backbones.
Increasing the depth of the ResNet-50 backbone does not significantly impact the CIDEr score, but it decreases the SPICE score.
Table 2 shows the perplexity on the test set for different language models.
The models "Ours (LSTM-2048)" and "Ours (2-layer LSTM-2048)" have lower perplexity values on the test set compared to other models.
The "Ours" approach has a perplexity of 147 and a training time of 30 minutes.
The "Ours" approach is significantly better than other published approximate strategies.
The "Ours" method has the lowest perplexity values for all languages.
The perplexity values for "bg 50k" and "da 128k" are the same.
The Contextualized CLBT (SVD) model performs better on average than the mBERT model.
The Contextualized CLBT (GD) model outperforms the Contextualized mBERT model.
Table 2 shows the performance on three tasks of the countries dataset with the AUC-PR metric.
MINERVA significantly outperforms all other methods on the hardest task (S3).
Table 9 shows example 1-to-M relations from fb15k-237 with a high cardinality ratio of tail to head.
The relation "/people/marriage_union_type/unions_of_this_type./people/marriage/location_of_ceremony" has a cardinality ratio of 129.75.
The FR-Neural model performs better than the FR-Identity and FR-Linear models in all word similarity tasks and the syntactic relation task.
The FR-Linear model has a higher correlation score than the FR-Identity model in the Word Similarity MTurk-771 task.
The "associated_clinical_finding" edge type has a higher count than the "child" edge type in the SNOMED-CT knowledge graph.
There are more instances in the SNOMED-CT knowledge graph that have a "has_method" edge type compared to the "has_procedure_site" edge type.
As the feature set size decreases, the mean squared error (MSE) increases.
The largest feature set size has the highest mean squared error (MSE).
The participant "Icarfish (this paper)" has a rank of "-" and an MSE of "0.0351".
The participant "Clickbait17-baseline" has a rank of "7" and an MSE of "0.0435".
The feature category "CNG" has the highest score in the table.
The feature category "CNG" has the highest score among all the feature categories in the table.
The scores for "how to" and "as" are the same.
The scores in the "Word lists" category range from -0.29 to 2.60.
The highest score in the table is 2.60 for the "Easy Words" feature.
The feature "Abbreviation count" has a positive score.
The feature "Character sum" has a score of 6.70.
The "MathSum" model performs the best in terms of edit distance on the EXEQ-300k test set.
The "Tail" model performs the worst in terms of exact match on the OFEQ-10k test set.
The MathSum model outperforms all other models on both the EXEQ-300k and OFEQ-10k test sets.
The Lead model performs better than the Random model on all metrics for both the EXEQ-300k and OFEQ-10k test sets.
The table compares the performance of two models: Disfluent and Fluent.
The Fluent model has a higher BLEU score than the Disfluent model on the development set with a single reference.
The rank of the English News data set improved from 12th during the competition to 6th after the competition.
The linear kernel achieved an accuracy of 0.8653 and an F1-score of 0.8547 on the English News data set.
The table shows regression results on three different data sets: English News, English WikiNews, and English Wikipedia.
The table shows the post-competition ranks for different kernel types on each data set.
The lexical diversity between generated paraphrases and reference sentences is higher for French-English than for Czech-English.
The semantic similarity between paraphrases and reference sentences is higher for longer Czech-English sentences.
The IDF of the token "proud" is 11.1.
The IDF of the token "told" is 7.9.
Table 6 shows the results of multimodal sentiment analysis experiments with different number of MTL layers.
The multimodal sentiment analysis experiment with 8 MTL layers achieved an MAE value of 0.837 and a Corr value of 0.744.
The model with summarization networks replaced by simple addition performs slightly worse than the original FMT model.
Using only audio factors performs better than using only visual factors.
The FMT [6] model outperforms the other FMT models in all metrics (BA, F1, MAE, Corr).
The FMT [6] model has a higher BA score compared to the other FMT models.
The table shows the results of experiments with different numbers of heads for the OTF model.
The MAE values for the OTF model with different numbers of heads range from 0.930 to 1.174.
Table 1 shows the performance of different model variants based on four different strategies: W2V, LDA, RE-ID (rand. init), and RE-ID (W2V init).
The RE-ID (W2V init) strategy has the highest performance in terms of 1/MRR among all the model variants mentioned in Table 1.
The "Ultramarathon Runners" community has a size of 28.
The "Hedge Fund Managers" community has a 1/MRR value of 25.7.
The standard deviation of the chosen C value decreases as the standard deviation of the accuracy estimate decreases.
The standard deviation of the chosen gamma value decreases as the standard deviation of the accuracy estimate decreases.
The table shows the standard deviation (SD) performance of J-K-fold tuning on the IMDB dataset over 1,000 different partitions.
The standard deviation of the accuracy in the J-K-fold tuning on the IMDB dataset ranges from 0.367% to 0.550%.
The table shows the performance of J-K-fold tuning for different widths of the LSTM.
The standard deviation of the accuracy estimate decreases as the chosen input's standard deviation decreases.
The accuracy of BERT without any defense is 89.0% for the "No Attack" case and drops to 60.0% for the "All attacks 1-char" case.
The accuracy of BERT with the Pass-through defense is 89.0% for the "No Attack" case and increases to 84.5% for the "All attacks 1-char" case.
The Word Error Rates (WER) decrease as we move from "Swap" to "All" in the "Word Recognition Spell-Corrector" category.
The Word Error Rates (WER) for the "Background" strategy are lower compared to the other strategies in the "Word Recognition Spell-Corrector" category.
The "Ours-MPA+Ortho.+GAM" model outperforms other models in all metrics (RC@1, RC@5, RC@10, MRR, MR) in the Seq2Seq Architecture.
The "Ours-MPA+Ortho.+GAM" model includes the Ortho. component.
BiDAF performs better than DocQA on the test set.
BERT performs better than DrQA on correctly spelled questions.
The percentage of the test set is not provided in Table 3.
The R-Net model performs better on the "non-R" type compared to the other models.
The R-Net model uses the Adadelta optimizer.
The BiDAF model uses a batch size of 60.
The performance of the models on SberQuAD EM increases from the simple baseline to BERT.
The performance of the models on SQuAD F1 is higher than the performance on SberQuAD F1 for all models.
The category "Incomplete answer" has the highest percentage with 29%.
The percentages of all the categories in the table sum up to 100%.
"RE3QA LARGE" achieves the highest scores in both Full EM and Full F1 on the TriviaQA-Wikipedia test set.
The models in the table show an improvement in performance in terms of Verified EM and Verified F1 scores.
The table presents results on the SQuAD-document dev set for three different models: S-Norm Clark and Gardner (2018), RE3QA BASE, and RE3QA LARGE.
The RE3QA LARGE model achieves an F1 score of 87.20 on the SQuAD-document dev set.
Table 5 shows the results of different models on the TriviaQA-unfiltered test set and SQuAD-open dev set.
The RE3QA [ITALIC] LARGE model achieves the highest scores for TriviaQA-unfiltered EM, TriviaQA-unfiltered F1, SQuAD-open EM, and SQuAD-open F1.
The RE3QA model achieves an F1 score of 84.81 on the SQuAD-document dataset.
The BERT \textsc [ITALIC] pipe model achieves an F1 score of 71.13 on the TriviaQA-Wikipedia dataset.
The performance of the model decreases when the "yhard" and "ysoft" labels are removed.
The model achieves higher exact match (EM) scores on the SQuAD-document dataset compared to the TriviaQA-Wikipedia dataset.
The Women's Institute was founded in Ontario, Canada in 1897 by Adelaide Hoodless.
Candidate [1] has the highest score in the retrieving component.
The final answer is chosen based on the reranking score, and in this case, candidate answer [1] is selected as the final answer.
The final answer is not necessarily chosen based on the retrieving and reading scores, as candidate answer [2] has higher scores in those categories but is not selected as the final answer.
The negative mean density of polar words in novels is lower than the negative mean density of polar words in fairy tales.
The positive mean density of polar words in novels is lower than the positive mean density of polar words in fairy tales.
The LM (D{HR}) has an average MAE of 0.80, MSE of 0.97, and R2 of -1.34.
The LM (D{HR}) has a total MAE of 4.02, MSE of 4.85, and R2 of -6.72.
The Linear Model (LM) achieves the highest average accuracy for both D{HR} and D{LR}.
The "DUC+TGSum" training data performs better than the "DUC" training data in terms of both ROUGE-1 and ROUGE-2 scores for all test sets.
The method "UB-1" performs better than "UB-2" and "UB-Combination" in terms of both ROUGE-1 and ROUGE-2 scores.
The method "UB-Combination" has the same ROUGE-2 score as the method "UB-2".
TGSum is rated higher in terms of informativeness compared to TWEET.
TGSum is rated higher in terms of readability compared to both OK and Bad.
SMT (our SR) performs better on our segmentation than on TACoS gt intervals.
Human descriptions have a higher BLEU score per description than per sentence.
There are four different versions: Base, Rand, Distract, and Full.
The "Full" version has the highest Bleu-1 score.
The "Golden" model is included in the table.
Table 4 presents the Fleiss’ Kappa of Human Agreement for the Appropriate and Informativeness testing scenarios.
ConceptFlow has the highest Bleu-1 score among all the models.
The Concept-PPL value is not provided for the ConceptFlow model.
The Seq2Seq model has the lowest value of Novelty w.r.t. Input(↓) Bleu-1.
The CCM model has the highest value of Novelty w.r.t. Input(↓) Rouge-1.
ML (Dynamic) has the highest Wiki Ma-F1 score and Ontonotes Ma-F1 score among all the models.
ML (None) has the highest accuracy score on the Ontonotes dataset among all the models.
BERT outperforms Uniform, GloVe, and Word2Vec in terms of accuracy and macro-F1 scores on the modified Wiki, Ontonotes, and BBN datasets.
BERT achieves the highest accuracy score among Uniform, GloVe, and Word2Vec on the modified Wiki dataset.
The "HRED + attn + pointer (ours)" model has been tested with different encoders.
Table 2 shows the test perplexity of trained and random hierarchical encoder models on the CNN / Daily Mail dataset.
The perplexity of the Random LSTM model is higher than the perplexity of the Trained model for all combinations of encoder and decoder hidden sizes.
TeSAN+ achieves the best performance in terms of test loss, test PR-AUC, and test ROC-AUC among all the models.
CBOW+, Sg+, med2vec, and MCE+ have similar test losses, while GloVe+ has a slightly higher test loss.
The TeSAN+ model outperforms all other ablation models in terms of test loss, test PR-AUC, and test ROC-AUC.
The Interval+ model has a lower test loss than the Normal_SA+ and Multi_SA+ models.
The "Posterior-GAN" model performs the worst in both "DailyDialog Coherence" and "DailyDialog Informativeness".
The "Posterior-GAN" model performs the worst in both "Coherence" and "Informativeness" on the OpenSubtitles dataset.
The model "Posterior-GAN(A)" outperforms all other models in all metrics for the DailyDialog dataset.
The model "Posterior-GAN(A)" outperforms all other models in all metrics for the OpenSubtitles dataset.
There are four different models in the table.
Adver-REGS and DP-GAN have higher Frequency-based Similarity values than Seq2Seq-att.
The frequency of the phenomenon "Anaphora" is 196 out of 343 tuples.
The frequency of binary relations is 254 out of 343 tuples.
Table 1 compares the initialization with phone CTC vs. AWE/AGWE in terms of SWB dev WER (%).
The SWB dev WER (%) for the A2W Segmental with AWE init and a vocab size of 10K is 16.0.
Table 2 shows the WER (%) results on the SWB/CH evaluation sets for different systems.
The system with speed perturbation and a vocabulary size of 20K achieves a WER (%) result of 11.4/20.8 on the SWB/CH evaluation sets.
The table provides accuracy, precision micro, and recall micro for the MLP and KNN models.
The KNN model has a higher accuracy than the MLP model.
The table compares three different models: GCC-Dec, GCC-Nrc, and GCC-Dec with pre-trained initialization.
The batch size for GCC-Dec is 256, for GCC-Dec with pre-trained initialization is 128, and for GCC-Nrc is also 128.
The values for the "Entail" label are higher than the values for the "Contradict" label in both the train and dev sets.
There are 4 different data splits in the table.
The number of premise-hypothesis pairs is the same for each data split.
Table 5 shows the accuracy of test splits with structured representation of premises using RoBERTaL trained on SNLI and MultiNLI training data.
RoBERTaL trained on SNLI achieves higher accuracy for WMD-1 compared to when it is trained on MultiNLI.
Table 1 provides the total number of test instances for deixis, lex. cohesion, ellipsis (infl.), and ellipsis (VP).
The number of instances for deixis, lex. cohesion, ellipsis (infl.), and ellipsis (VP) is equal for the "distance 1" condition.
The "DocRepair" model has the highest BLEU score among all the models.
The "CADec" model has a lower BLEU score compared to the "baseline" model.
Table 5 presents the human evaluation results comparing DocRepair with a baseline.
The total number of evaluations in the human evaluation is 700.
The "MorphMine" method achieves the lowest perplexity values for English, German, and Turkish compared to other methods.
The "MorphMine" method achieves the lowest perplexity value for the German language compared to other methods.
The "MorphMine" method achieves the highest scores in all English and German word similarity datasets.
The "MorphMine" method achieves a score of 0.62 on the German WS-353 dataset.
The table shows the performance of different methods on English Semantic, English Syntactic, German Semantic, German Syntactic, and Turkish Semantic+Syntactic datasets.
MorphMine has the highest performance on the German Semantic and German Syntactic datasets.
The table provides information on the label distribution and distribution into train, validation, and test sets for the Riloff and Ptacek datasets.
The Riloff dataset has a higher proportion of sarcastic data compared to non-sarcastic data, while the Ptacek dataset has a higher proportion of non-sarcastic data compared to sarcastic data.
The EX-ED model achieves the highest F1 score on both the Riloff dataset and the #Riloff dataset.
The F1 score on the #Riloff dataset is higher than the F1 score on the Riloff dataset for both the EX-CASCADE and EX-W-CASCADE models.
The Stacked AE model has a higher error rate than the Denoising AE model on within-speaker discriminability in English data.
The Siamese DNN model has a lower error rate than the Correspondence AE model on across-speaker discriminability in Xitsonga data.
The table shows the average precision (AP) scores of different feature extraction techniques on the test set.
The performance of the English NN improves with an increase in the amount of training data.
As the number of gold standard word pairs N decreases, the average precision (AP) on the test set using the partitioned UBM and correspondence AEs also decreases.
The partitioned UBM performs better in terms of average precision (AP) on the test set compared to the correspondence AEs for all values of N.
The table presents the performance of the unconstrained segmental Bayesian model on TIDigits1 over iterations in which the reference set is refined.
The word error rate decreases over iterations in the performance of the unconstrained segmental Bayesian model on TIDigits1.
The "Segmental Bayesian" model has different performance when it is constrained and when it is not constrained.
The "Segmental Bayesian" model with "no" in the "Constrained" column does not have a value for the "UTD init." column.
The Constrained model has a lower WER than the Unconstrained model in both TIDigits1 and TIDigits2 datasets.
The Constrained model has a higher Cluster purity than the Unconstrained model in both TIDigits1 and TIDigits2 datasets.
Table 5.2 provides the same-different performance of different representations.
The "cosine" metric has the highest average precision (AP) for Xitsonga in the "Downsampled cAE embeddings" row.
Table 5.4 provides a breakdown of the errors on English2 for the speaker-dependent models in Table 5.3.
The "Del." (deletion errors) percentage is lower for the "cAE" embedding compared to the "MFCC" embedding in the "BayesSegMinDur" model.
The model "BayesSegMinDur" outperforms the model "BayesSeg" in terms of Precision, Recall, and F-score for Word boundary detection.
The model "BayesSeg" has a lower percentage of Substitution Errors, Deletion Errors, Insertion Errors, and Word Error Rate (WER) compared to the model "BayesSegMinDur".
The Many-to-one WERm (%) for English1 is higher than the Many-to-one WERm (%) for English2 across all models and datasets.
The One-to-one WER (%) for Xitsonga is higher than the One-to-one WER (%) for English1 and English2 across all models.
The ResCNN model outperforms the GRU model on the text-independent speaker recognition task.
Increasing the size of the training set improves the performance of the speaker recognition models.
Each layer in the ResCNN architecture has a specific structure and dimension.
The total number of parameters in the ResCNN architecture is 24M.
The "ResCNN, triplet" system achieves the lowest EER among all the systems.
The "ResCNN, softmax (pre-train) + triplet" system achieves the highest accuracy among all the systems.
The finetuned GRU system has the highest accuracy among all the systems.
The DNN i-vector system has the lowest accuracy among all the systems.
Table 2 shows the evaluation results of coreference algorithms on the test dataset.
The F1 scores for MUC, B3, and CEAF [ITALIC] e are all above 95 for the Seq2Seq+Copy algorithm.
The Seq2Seq+Copy model achieves the highest precision, recall, and F1 scores among all the models.
The Pipeline model achieves the lowest precision, recall, and F1 scores among all the models.
The table describes the Precision@k of semantic alignment on the test set.
The Precision@1 for 160 hours in semantic alignment is 1.85.
The ME values are higher than the CD values for all combinations of hours and test/dev sets.
The CS values are lower than the ME values for all combinations of hours and test/dev sets.
Increasing the number of features improves the performance of the system in terms of WER(%).
Combining High, Mid, and Low frequency features leads to the lowest WER(%) value.
The pooling stride of 2 performs better than the spectrogram/convolution stride of 2.
The pooling stride of 40 performs better than the spectrogram/convolution stride of 10.
Increasing the number of features improves the performance of the models, as shown by the decreasing WER.
Adding additional filters at the same stride improves the performance of the models.
The Hybrid approach performs better than the BayesTopic and BestCluster approaches in terms of P_k for both BayesTopic WD and BestCluster WD.
The BayesTopic approach has the highest variability in performance, as indicated by the highest standard deviation for P_k, compared to the BestCluster and Hybrid approaches for both BayesTopic WD and BestCluster WD.
The meeting with the highest number of segments used for the Hybrid approach is Bmr018.
Deep Voice 2 has a higher MOS than Deep Voice 1.
Tacotron (WaveNet) has a higher sampling frequency than Tacotron (Griffin-Lim).
The h-HRL agent has a higher success rate than the RL agent.
The m-HRL agent receives a higher reward than the RL agent.
The table shows the comparison results of different categories of reminders and offers.
The accuracy of the Hybrid Hierarchical CNN-LSTM model is different for each category of reminders and offers.
The "Hybrid" model has the highest accuracy among all the models.
The "All-CNN-LSTM" model has a higher accuracy than the "All-LSTM" model.
The table provides the optimal value of t for each corpus.
The optimal value of t for the Canon Camera corpus is 110.
The table compares the performance of different products in terms of their original precision and recall, as well as the precision and recall of our approach.
The average precision and recall of our approach are lower than the average precision and recall of the original data.
The table shows the training batches for the WMT'14 English-German translation task.
The proposed method provides an easier curriculum at the beginning of the model training.
Table 3 shows the effects of different λm of the norm-based model competence function and λw of the norm-based sentence weight function.
The model achieves the same development score for λm 2.5 and λw 1/2.
KV-MemNN performs best for the "KB" knowledge representation.
The addition of conjunctions and coreference resolution decreases the performance of KV-MemNN.
The Key-Value Memory Network method achieves the highest MAP and MRR scores among all the methods listed in the table.
The Key-Value Memory Network method outperforms the Word Cnt and Wgt Word Cnt methods in terms of MAP and MRR scores.
The denkowski2017stronger + Pre-translation model does not have a score for the en-fr Word translation.
The Our baseline + ReWE (CEL) (λ=20) model has the highest score in the eu-en BPE translation.
The table shows the results of the Contrastive A experiment for three different language pairs: en-fr, cs-en, and eu-en.
The en-fr language pair has the highest BLEU Word score among the three language pairs.
The F1 score for "our parser (no prior)" is higher than the F1 scores for both "CCG" and "our parser (KB prior)".
The precision for "our parser (type-correct prior)" is higher than the precision for both "CCG" and "our parser (no prior)".
The precision of our parser with KB prior is higher than the precision of the other parsers evaluated.
The recall of our parser without prior is higher than the recall of the other parsers evaluated.
Table 5 shows the results of the NER evaluation task using different embeddings for various languages.
The F1 scores for the NER task using different embeddings for English are 0.89 for fastText, 0.90 for EFML, and 0.92 for EMBEDDIA.
The "Proposed method" performs better than the "Baselines" and "Previous studies for controlling output length" in terms of ROUGE scores.
The ensemble of 5 models achieves the highest ROUGE scores among all the models.
Table 5 provides evaluation results for different measures such as Precision, Recall, F-measure, and Accuracy.
Ensemble Modeling consistently outperforms other methods in terms of Precision, Recall, F-measure, and Accuracy.
The "HNr" model performs the best in terms of Recall@10.
The "HNr+c" model has the highest value in the "rankcontext" metric.
SGNS and Poincaré are two different methods used for the WordSim-353 task.
The correlation coefficient values increase as the dimension increases for both SGNS and Poincaré methods.
The word "bank" is more frequent but has a lower norm than the word "bankruptcy".
The word "dog" is more frequent but has a lower norm than the word "dogs".
The norms of the sentences in the Movie Reviews dataset are increasing from top to bottom.
The norm of the sentence "a trifle of a movie, with a few laughs ... unremarkable soft center" is 6.86.
The encoder dimensions in the table range from 10 to 2400.
Higher encoder dimensions result in lower perplexity values.
Table 5 compares different document-level NMT methods on Chinese-English translation.
The "Our work" method outperforms the "Baseline" and "(Tiedemann and Scherrer [Tiedemann2017Neural])" methods in terms of translation performance for the 900k dataset.
The table compares the performance of three different methods: Baseline, Joint Training, and Explicit Contextual Integration, on Chinese-English translation.
The table provides the average performance scores for each method: Baseline, Joint Training, and Explicit Contextual Integration.
The "Pre+Next+Input Embedding" method achieves higher BLEU scores than the "Input Embedding" method for both the 900k and 2.8M corpora.
The "Pre+Next+Input Embedding" method achieves a higher BLEU score than the "Baseline" method for the 900k corpus.
The table compares the performance of four different methods: Baseline, Joint Training, Pre-training & Fine-tuning, and No Pre-training.
The table compares the number of parameters used in the baseline and our method, as well as the performance comparison.
The baseline LSTM model has a lower similarity at the target rhyme density than the LSTM iteration model for most artists.
Sage Francis has the highest average rhyme density among all the artists.
The maximum length of generated verses varies for different artists.
The average percentage of training completed for generating verses is 77.8%.
The table presents the EDS parsing test set results for three different models: AE RNN, EDM P, and EDM A.
The EDM model with italics (EDM P) has a higher score than the EDM model without italics (EDM A) on the EDS parsing test set.
The table presents the results of attention-based encoder-decoder models with alignments encoded in the linearization, for top-down (TD) and arc-eager (AE) linearizations, and lexicalized and unlexicalized predicate prediction.
The EDM with unlexicalized predicate prediction achieves the highest performance among all the models mentioned in the table.
Table 2 shows the DMRS development set results of encoder-decoder models with pointer-based alignment prediction, delexicalized predicates, and hard or soft attention.
The EDM [ITALIC] P score for the AE stack model is 85.28.
The ACE model outperforms the TD RNN and AE RNN models in terms of EDM performance.
The ACE model outperforms the TD RNN and AE RNN models in terms of Start EDM performance.
Table 5 shows the development set results for AMR parsing using different models.
Table 6 shows the Smatch F1 scores of different models used for AMR parsing.
The FlaniganTCDS14 model achieves a Smatch F1 score of 56 in AMR parsing.
Table 1 shows the BLEU scores of DeepAPI and related techniques.
Lucene+UP-Miner has the lowest Top1 score among the listed tools.
The "Full action space" performs better than "Kf(k)" and "Kp(k)" in terms of BLEU scores.
The "Full action space" performs better than "Kf(k)" and "Kp(k)" in terms of AP scores.
The concatenation + 2 FC fusion approach with all three modalities (V, A, S) achieves the highest value of 1.81 in the Metric B@4.
The concatenation + 2 FC fusion approach with all three modalities (V, A, S) has the highest number of parameters (179 × 106).
The learned proposals using MDVC with no missings achieve higher scores than the learned proposals using MDVC on the full dataset.
The GT Proposals M score is higher for MDVC with no missings compared to MDVC.
The "Feature Transformer" has the highest number of parameters among all the models.
The "Feature Transformer" achieves the highest scores for both BLEU-4 (B@4) and METEOR (M) metrics.
Table 2 provides information about the final untranslated term count and the number of I-tags after filtering based on the relevancy constraint.
The S-rank has 812 untranslated terms, which accounts for 7.0% of all untranslated terms.
The table provides information about translated, non-literally translated, and raw untranslated term annotations obtained in the annotation process using the NAIST STC for (T)ranslator, and {B,A,S}-rank SI.
The "T" row has a higher number and percentage of translated terms compared to the other rows.
The average precision score for class B is highest for the "− word freq" method.
The pitch detectors "VDE" and "GPE" have decreasing accuracy values, while the pitch detectors "FPE" and "FFE" have increasing accuracy values.
The "DIO" pitch detector has the lowest accuracy values among all the pitch detectors.
Table II presents the voicing decision error rate for different baselines and proposed approaches.
Logistic Regression has the lowest voicing decision error rate among all the algorithms.
The table provides the gross pitch error rate (in %) for various F0 estimation techniques.
The gross pitch error rate for Linear Regression is 3.89%.
The "Hierarchical-PSV (full model)" method achieves the highest Macro-F1 and Accuracy scores among the tested methods.
Adding CNN to the GRU model improves the Macro-F1 score compared to using only the GRU model.
The table presents results of veracity prediction in both single-task and multi-task settings.
The Hierarchical-PSV method achieves the highest Macro-F1 and Acc. scores among all the methods in the table.
The coverage percentages for labeled and unlabeled data are different for each dataset.
The coverage percentages for expanded data are higher than the coverage percentages for non-expanded data for each dataset.
The subgraph prediction performance for "parses" is better than the subgraph prediction performance for "gold-".
The summarization performance with expansion is better than the summarization performance without expansion.
The table shows the performance of different models on forecasting market movement as the daily latest time up to which news headlines are collected varies.
The MSIN model performs the best on forecasting market movement when the daily latest time up to which news headlines are collected is 23:59.
The attention mass for the headlines varies for each model (GRUtxt, LSTMw/o, MSIN) on different dates.
On Date: 2013-04-26, the attention mass for the headlines by MSIN is higher compared to GRUtxt and LSTMw/o.
The SVM model has the highest precision for predicting an up market.
The MSIN model has the highest accuracy.
The accuracy for GloVe with Orthonormal is 82.10.
The accuracy for retro word2vec with random is 78.46.
The model detected 241 errors.
The model made 80523 correct predictions.
The table provides information on the number and recall of errors detected in the Bounded and Unbounded categories.
The recall for the Bounded category is higher than the recall for the Unbounded category.
Table 5 provides a class-wise comparison of model accuracy for ProBERT and GREP.
GREP has a higher overall accuracy than ProBERT.
ProBERT (bert-base-uncased) achieves a bias ratio of 0.98.
GREP (bert-large-uncased) achieves an F1 score of 94.0 for the masculine gender.
The ensemble model with both bert-large-uncased and bert-large-cased performs the best on all data.
The ensemble model with bert-large-uncased performs slightly worse on the gap-test data compared to the ensemble model with both bert-large-uncased and bert-large-cased.
ProBERT assigns a probability of 0.405 to event A.
GREP assigns a probability of 0.038 to event B.
ProBERT assigns a higher probability to event A compared to event B.
GREP assigns a higher probability to the event of neither A nor B compared to ProBERT.
Table 3 provides the gold standard evaluation on the domain-specific subtask.
The "CNN" model achieves the highest MRR score in the medical domain.
The RCNN model achieves the highest MAP score for both the "Word" and "Sense" embeddings.
The CNN model achieves a P@3 score of 9.00 for the "Word" embedding.
The LIWC scores provided in Table 5 are for the categories of "analytic", "authentic", and "tone".
The LIWC scores for the "analytic" and "authentic" categories are higher in the "HappyDB" corpus compared to the other text corpora.
Table 8 shows the percentage of sentences in each topic for both the 24 Hours Reflection and 3 Months Reflection.
The percentage of sentences in the "people" topic is higher for the 3 Months Reflection compared to the 24 Hours Reflection.
Table 10 provides precision, recall, and F1 scores for each category, as well as the percentage of moments per category for each reflection period.
The F1 score for the "Enjoy the moment" category is higher for the 24 Hrs reflection period compared to the 3 Months reflection period.
The word "I2 made" appears more frequently than the word "I1 got" in the instruction sets.
The word "great" appears more frequently than the word "family" in the instruction sets.
The CVAE (BOW) model performs better than the CVAE model in terms of BLEU-4 score.
The BN-VAE model has a lower KL divergence than the CVAE (BOW) model.
As the temperature increases from T=1.0 to T=5.0, the CER% decreases.
The table shows the effect of varying T on the CER% for RNNLM softmax with λ=0.9.
Table 2 shows the perplexities on transcriptions of the AISHELL-1 development set.
The perplexity of the RNNLM model on external text is 34.
The CER% decreases as the value of λ increases until λ=0.9, after which it increases again.
Table 4 provides comparisons of different variations of the Seq2Seq model on the test set.
The addition of shallow fusion (SF) to the Seq2Seq + Proposed LST model results in a lower CER% on the test set.
The "Our ARNN + GBRT" model outperforms the "Yamada (2016) local" model on both Micro P@1 and Macro P@1 metrics.
The "Yamada (2016) global" model outperforms the "Chisholm (2015) global" model on both Micro P@1 and Macro P@1 metrics.
The Our Attention-RNN model outperforms the Baseline (MPS), Cheng (2013), and Yamada (2016) models in both the Sampled Test Set and the Full Test Set.
The Our RNN, w/o Attention model performs slightly worse than the Our Attention-RNN model in both the Sampled Test Set and the Full Test Set.
The "PPRforNED" model is used for evaluation in the CoNLL-YAGO test-b - Training Steps Eval.
The performance on the CoNLL training set improves when GBRT is applied.
Table 4 presents the results of corrupt-sampling and initialization on the Wikilinks Evaluation-Set.
The model achieves a higher micro accuracy when using initialization compared to random initialization for near-misses in the Wikilinks Evaluation-Set.
Table 5 presents the overall accuracy top1 on VQA-CP v1 for different models.
The addition of RUBi improves the overall accuracy and the accuracy on Yes/No questions compared to the baseline model.
The model "RUBi (ours)" achieves the highest scores in all answer types.
The model "Baseline architecture (ours)" has a lower overall score compared to the model "RUBi (ours)".
The baseline model achieves an accuracy of 64.75 on the test-dev split.
The RUBi learning strategy achieves an accuracy of 61.16 on the val split.
The table presents the results of an ablation study of the question-only loss LQO on VQA-CP v2 for four different models.
The models in Table 5 are categorized into three types: discriminative, generative, and semisupervised.
The generative model with the correct composition function has the highest F1 score in Table 5.
The table includes parsing results of two types of models: D and G.
The model type G achieves the highest F1 score among all the models in the table.
The "RNNG - correct" model has a test ppl (PTB) value of 105.2.
The "RNNG - buggy†" model has a test ppl (CTB) value of 171.9.
The F1 score for the BTS* model on English is 84.57.
The F1 score for the Carreras model on Dutch is 77.05.
The average accuracy for all languages is higher for CRF+ compared to CRF.
The average accuracy for all languages is higher for BTS compared to BTS*.
The batch size used for training the sentiment model is 28.
The Adam optimizer is used for training the sentiment model.
The table presents the results of the clinical sentiment extraction task using three different models.
The BiLSTM-CRF + ELMo (L=2) model achieves a precision of 89.14, a recall of 88.59, and an F1 score of 88.87.
The DGLSTM-CRF + ELMo (L=2) model achieves a precision of 89.59, a recall of 90.17, and an F1 score of 89.88.
The model "BiLSTM-CRF ( [ITALIC] L=1) + ELMo" achieves a precision of 88.87%.
The model "DGLSTM-CRF ( [ITALIC] L=2) + ELMo" achieves an F1 score of 89.88.
The BiLSTM-GCN-CRF model outperforms the BiLSTM-CRF model on both the Catalan and Spanish datasets.
The DGLSTM-CRF model with L=2 and ELMo outperforms the DGLSTM-CRF model with L=0 and ELMo on the Catalan dataset.
The table shows the performance of two models, BiLSTM-CRF + ELMo (L=2) and DGLSTM-CRF + ELMo (L=2), on the CoNLL-2003 English dataset.
The F1 score of the DGLSTM-CRF + ELMo (L=2) model is higher than the F1 score of the BiLSTM-CRF + ELMo (L=2) model on the CoNLL-2003 English dataset.
Table 7 shows the low-resource NER performance on the SemEval-2010 Task 1 datasets.
The BiLSTM-CRF (L=1) model performs better on Spanish than on Catalan in terms of precision, recall, and F1 scores.
DGLSTM-CRF with predicted dependencies performs better than BiLSTM-CRF in all languages.
DGLSTM-CRF with predicted dependencies shows a higher improvement in F1 performance compared to BiLSTM-CRF in Catalan and Spanish.
The GFT-1 method performs better than the EncDec method in all navigation scenarios.
The FiLM method has the highest success rate in the "nav_avoid 2D" navigation scenario.
The "GFT-2" model has the highest success rate in the "nav 3D" task.
The "FiLM" model performs better in the "nav_avoid 2D" task compared to other models.
GFT-2 has the highest success rate on the "nav_avoid 3D" task.
The Concat model has the lowest success rate on the "nav_bw 2D" task.
The GFT-2 model achieves the highest navigation success rates among all models in both 2D and 3D scenarios.
The Gated model outperforms the Concat model in 2D navigation scenarios, while the Concat model outperforms the Gated model in 3D navigation scenarios.
The termination condition for both the 15K and 100K datasets is to early stop when the Hits@1 score begins to drop on the validation sets, checked every 10 epochs.
The maximum number of epochs for both the 15K and 100K datasets is 2000.
The "news" category has a higher frozenness value than any other data source.
The "r/politics" category has a larger size than any other data source.
The table represents the covariance values for movement in the subreddits r/nba, r/nfl, and r/politics.
The covariance between movements in r/nba and r/nfl is 0.00240.
The predictor performs best on r/The_Donald subreddit.
The predictor performs better on the "Phonemes" feature for the r/politics subreddit.
The subreddit "r/food" has the highest accuracy among all the subreddits.
The accuracy of the subreddit "r/Conservative" is 0.83.
The F1 score for the "Material-∗" condition is higher than the F1 score for the "Property-∗" condition in the rule-based system.
The F1 score for the "both sub-levels" condition is lower than the F1 score for the other conditions in the rule-based system.
Mat-ELMo has the highest F1 score in the Material F1 and Material R metrics.
Mat-ELMo has a higher F1 score than SciBERT in the Operation F1 metric.
The macro-averaged F1 score for all types combined is 0.754.
The F1 score for the "Material-Start" type is higher than the F1 score for the "Material-Final" type.
The rule "Po-OM" has the highest coverage among all the rules.
The rule "P-O" has the highest accuracy among all the rules.
The table shows the results of different models for Chinese word segmentation.
The Joint-Multi-BERT model achieves the highest F1 score for segmentation on the CTB-7 dataset.
The Joint-Multi-BERT model performs better than the Joint-Multi model on CTB-9 for both segmentation and udep.
The Hatori et al. (2012) model performs better on CTB-5 for segmentation compared to udep.
The F1 score for segmentation is higher for CTB-7 than for CTB-5 in the Joint-Multi models.
The UAS score for dependency parsing is higher for CTB-7 than for CTB-9 in the Joint-Multi models.
Table 5 presents the experimental results on Visual Genome in terms of F1 score for different phenomena.
The F1 score for the "Neg" phenomenon is higher in the "Phenomena (#) Con" column compared to the "Phenomena (#) (17)" column.
Table 5 compares the correlation scores of different encoding methods on a subset of the HyperLex lexical split.
The TX + ReZero [ITALIC] α=0 model has 34M parameters.
The BPB for the TX + ReZero [ITALIC] α=1 model is 1.17.
Table 5 shows the performance on 1000 human-recorded questions.
TextMod performs better on Original Text than SpeechMod on Recorded.
The output dimensions of the convolutional layers decrease as we move from the first convolutional layer to the fifth convolutional layer.
The input dimensions of the convolutional layers are much larger than the output dimensions.
As the noise percentage increases, the Word Error Rate (WER) also increases.
The Word Error Rate (WER) increases as the noise percentage increases.
The accuracy of the baseline model is 53.74.
The accuracy of the TextMod model with blind noise is 48.76.
The system "with all KB + RACE (selected)" achieves the highest accuracy score of 85.5 on the MCScript 2.0 test set.
The system "BERT [ITALIC] LARGE" achieves an accuracy score of 82.3 on the MCScript 2.0 test set.
The Word error rate decreases as the number of hours of adaptation increases for both the Multidomain and Voicesearch-Dictation models.
The Multidomain model performs better than the Voicesearch-Dictation model in terms of Word error rate for all levels of adaptation.
The word error rate is higher with noise than without noise for the Voicesearch test set.
The word error rate without noise is lower for the Dictation test set at 8 kHz compared to the Dictation test set at 16 kHz.
The "YouTube" test set has the highest word error rate for both the "Voicesearch-" and "Dictation" models.
The "Call-center" test set has the highest word error rate for the "YouTube" model.
The Phoneme Error Rate (PER) increases as we move from Wav2Vec to ResDAVENet to MockingJay to PASE.
The Phoneme Error Rate (PER) is highest for the "en-de" language pair.
The "en-es" language pair has the highest Text → Speech R@1 score.
The "en-pt" language pair has the lowest Speech → Text R@10 score.
The table shows phone recognition results with features extracted from different layers of CSTNet on different language pairs.
The phone recognition performance for en-it is 25.5.
The method "S+L+R" performs the best on RefCOCO, RefCOCO+, and RefCOCOg when using detected proposals.
The method "Ours CMRIN" performs the best on RefCOCO testA, RefCOCO+ testA, and RefCOCOg test when using detected proposals.
Table I compares the state-of-the-art methods on RefCOCO, RefCOCO+, and RefCOCOg datasets using VGG-16 and ResNet architectures.
The Ours CMRIN method performs better when using the ResNet architecture compared to the VGG-16 architecture.
Table V shows the results of ablation studies on different variants of the spatial relation graph of the proposed CMRIN model on RefCOCO, RefCOCO+, and RefCOCOg datasets.
The CMRIN model consistently outperforms existing state-of-the-art models on all three benchmark datasets.
Table 2 shows the CER (%) results of multilingual ASR using DARTS-ASR under different fine-tuning approaches.
The CER (%) results for Vietnamese and Tamil are lower under the Fine-tuning of DARTS-ASR Adapt pruned approach compared to the other fine-tuning approaches.
The CER (%) results of monolingual ASR using the CNN Module VGG-Large are consistently higher than the results using the CNN Module VGG-Small for all languages.
The CER (%) results of monolingual ASR using the CNN Module DARTS-ASR Only Conv3x3 are consistently lower than the results using the CNN Module DARTS-ASR Full for all languages.
The CNN Module DARTS-ASR consistently performs worse than the CNN Module VGG-Small and CNN Module VGG-Large across all languages.
Swahili consistently has the lowest CER (%) among all the languages.
The "CERTAIN" word class is bookmarked less frequently than it is cited.
The "WE" word class is downloaded less frequently than it is used.
The "Reward-RefNet" model outperforms the "RefNet" model in terms of BLEU4 score and percentage preference for both fluency and answerability.
The "Reward-RefNet" model achieves a higher BLEU4 score than the "RefNet" model.
The table compares the performance of different models on various datasets.
The RefNet model outperforms other models on the SQuAD dataset at the passage level.
The ensemble models have the highest MAR@6 score among all the models.
The official majority baseline has a MAR@6 score of 0.4065.
The count of "pensive" is higher than the count of "dependable".
The count of "Scotty" is higher than the count of "backhand".
The Qbot (RL) team performs better than the Qbot (SL) team.
The evaluation campaigns in Trentino assessed the L2 linguistic competences of pupils at different levels, including A1, A2, and B1.
The number of pupils participating in the evaluation campaigns decreased from 2016 to 2017 but increased again in 2018.
Both the ADE and CoNLL04 datasets use GRU as the BiRNN type.
Both the ADE and CoNLL04 datasets have a label embedding size of 25.
The model "Ours" achieves the highest F1 score on both the ADE and CoNLL04 datasets.
The "Micro" metric type generally achieves higher F1 scores than the "Macro" metric type on both the ADE and CoNLL04 datasets.
The table presents results from an ablation study using the CoNLL04 dataset with two types of models: Baseline and BiRRNs.
The table includes results from an ablation study using the CoNLL04 dataset, with metrics evaluated using both Macro and Micro averaging.
The average score for the Fine-tuned method is 39.3.
The score for the LAMOL0.2GEN method on the WOZ task is 78.4.
Table 1 provides a summary of various tasks and their corresponding datasets, dataset sizes, and metrics.
The metric used for sentiment analysis is EM, which represents an exact match between texts.
GPT-2 outperforms other methods on the WikiSQL, SST, SRL, WOZ, AGNews, Amazon, DBPedia, Yahoo, and Yelp datasets.
Other methods outperform GPT-2 on the SQuAD dataset.
Table 4 provides a summary of the averaged scores on five tasks for different models.
The model with LAMOL0.2GEN has a higher average score than the multitasked model.
The "KS-BiGRU (ours)" method outperforms all other methods in terms of Recall, Precision, and F1 score.
The "KS-BiGRU (ours)" method has a higher Recall value than the "BiLSTM[petrochuk2018simplequestions]" method.
The BERT based Entity Embedding outperforms the ganea2017deep entity embedding in terms of F1mi score for the FIGER typing system.
The BERT based Entity Embedding achieves higher accuracy than the ganea2017deep entity embedding for the OntoNotesfine typing system.
The F1 score on AIDA-B for the "BERT-Entity-Sim (local)" method is [BOLD] 90.06±0.22.
The highest F1 score on AIDA-B is achieved by the "BERT-Entity-Sim (local & global)" method, which is [BOLD] 93.54±0.12.
Table 2 shows the F1 scores of different methods on five out-domain test sets.
The F1 score for the "Oracle type (Ultra-fine)" is higher for the "MSNBC" dataset compared to the "AQUAINT" dataset.
Figure 7 displays the input for the sentence "I am not that rich" without and with POS-tags, where the POS-tags are inserted as super characters and the +-symbols represent spaces (word boundaries).
The table represents the sentence "I am not that rich" without and with POS-tags, where each cell represents a word or a POS-tag in the sentence arranged in the same order.
The value for the "Parameter RNN type" is "brnn".
The value for the "Epochs" is "20–25".
Table 4 provides the baseline results for semantic parsing on LDC2016E25.
The post-processing technique of "Pruning" improves the performance by 0.7 on the development set and 0.6 on the test set.
The input for the AMR (t / thing :quant 1 :polarity -) representing the sentence "Not one thing" is shown with and without super characters.
The symbols "+-" represent spaces in the input for the AMR (t / thing :quant 1 :polarity -) representing the sentence "Not one thing".
The F-score decreases as the ratio of CAMR AMRs increases and the ratio of JAMR AMRs decreases.
When there are 100,000 CAMR AMRs and 0 JAMR AMRs, the F-score is 65.8.
The baseline model has a dev accuracy of 54.8% and a test accuracy of 53.1%.
Introducing super characters improves the model's performance.
The "Ours" model has the highest RG P% and CS P% values.
The "Ours" model has the highest BLEU score.
The "Ours" model performs the worst in all categories except for "#Gram" where it performs the best.
The "Gold" model performs the best in all categories except for "#Sup" where it performs the worst.
Table 1 provides ablation results of baseline models and fusion strategies on the VQA-CP v2 test set.
Adding the CF-VQA fusion strategy improves the performance of the baseline models.
Table 1 shows the ablations of baseline models and fusion strategies on the VQA-CP v2 test set.
The baseline model UpDn achieves an accuracy of 37.69% on the VQA-CP v2 test set.
The Sum fusion strategy achieves an accuracy of 12.54% on Yes/No questions in the VQA-CP v2 test set.
The "GVQA agrawal2018don" method performs better on the "VQA-CP v2 test" set compared to the "VQA-CP v2 train" set.
The "CF-VQA (Sum)" method performs better on the "VQA-CP v2 train" set compared to the "VQA-CP v2 test" set.
Table 5 shows the ablations of baseline models and fusion strategies on the VQA-CP v1 test split.
The RUBi model outperforms the SAN model in terms of the "Num." metric.
Table 5 presents the ablations of baseline models and fusion strategies on the VQA-CP v1 test split.
The RUBi model achieves a score of 68.97 in the "All" category.
Table 5 shows the ablations of baseline models and fusion strategies on the VQA-CP v1 test split.
The RUBi fusion strategy achieves the highest performance in terms of the "All" column.
The baseline model SAN has a score of 33.18.
The model with the fusion strategy of Sum and CF-VQA has a score of 49.85.
The RUBi model outperforms the UpDn model in the baseline models.
The + CF-VQA strategy improves the performance of the baseline models.
Table 6 shows the ablations of baseline models and fusion strategies on the VQA-CP v2 test split using the simplified VQA causal graph.
The RUBi model achieves a score of 69.11 on the VQA-CP v2 test split using the simplified VQA causal graph.
The table shows the improvement of RUBi cadene2019rubi and Learned-Mixin (LM) clark2019don using CF-VQA on the VQA-CP v2 test split.
The CF-VQA method improves the performance of both RUBi cadene2019rubi and Learned-Mixin (LM) clark2019don.
The DER for EEND is higher in the "Simulated mixtures 5" condition compared to the "Simulated mixtures 2" and "Simulated mixtures 3" conditions.
The DER for x-vector is the lowest in the "Simulated mixtures 5" condition compared to the other conditions.
The use of both "PIT loss" and "DPCL loss" results in a lower DER (%) compared to when no loss function is used.
The use of "PIT loss" results in a lower DER (%) compared to when no loss function is used.
Table 2 shows the effect of the number of training mixtures on simulated speech generated with β=2. The models are trained with β=2.
As the number of training mixtures increases, the DER(%) decreases.
The EEND (proposed) method has the lowest DER (12.28%) among the three methods.
The x-vector method has the highest false alarm rate (1.90%) among the three methods.
The table shows the ROUGE F-measure results on the French, Portuguese, and Spanish corpora.
The ROUGE-1 score for Spanish using the ILP:∞+LM method is 0.5500.
The mean accuracy decreases as the mutual information cutoff increases.
The accuracy for the "Pairs MI>0" is higher than the accuracy for the "Pairs MI>4".
The number of instances predicted correctly for Group 1 is 277.
The number of instances predicted correctly for Group 2 is 265.
The accuracy of the model in classifying Group 1 vs. Group 2 is 0.9748.
The false positive rate of the model in classifying Group 1 vs. Group 2 is 0.0107.
Table 6 represents the test confusion matrix for Group 3 vs. Group 2.
The expected count for Group 1 is 43 and the expected count for Group 2 is 47.
The model achieved an accuracy of 0.6475, correctly classifying 90 out of 139 instances.
The model achieved a recall of 0.6812, correctly identifying 47 out of 69 instances of the positive class.
As the value of "Cut" increases, the mean values for both "thresh=1" and "thresh=3" decrease.
The standard deviation values for both "thresh=1" and "thresh=3" are relatively consistent across different values of "Cut".
The performance of the model increases as the number of dynamic features increases for all types of n-grams.
The model achieves the highest performance when the number of dynamic features is 50 for word pairs.
The table shows the mean accuracy and standard deviation of two different word source methods: Ensemble and Feature Selection.
The mean accuracy for the "All Pairs MI>0" is higher than the mean accuracy for "Single Words".
The standard deviation for the "All Pairs MI>4" is lower than the standard deviation for "All Pairs MI>2".
The highest accuracy is achieved when the "Word Count Cut" and "Pair Count Cut" values are both 4.
The standard deviation is lowest when the "Word Count Cut" and "Pair Count Cut" values are both 6.
As the number of evaluations increases, the evaluation scores generally increase.
The highest evaluation score is achieved when the number of evaluations is 110K.
The table presents the results of experiments on the number of layers for encoders and decoders.
The translation performance improves as the number of layers increases.
The table shows the translation performance on English-German, English-French, and Chinese-to-English test sets.
The proposed approach performs better on the English-German translation.
Table 3 presents the results of an ablation study on English-German, English-French, and Chinese-to-English translation tasks.
The full model outperforms all other configurations in terms of translation accuracy for English-German, English-French, and Chinese-to-English tasks.
The MRC system achieves higher precision than the SRC system when classifying the "Conjunction" relationship type.
Both the SRC and MRC systems have lower recall values when classifying the "Part-Of" relationship type.
The average accuracy and F1 scores for all the models and datasets are 84.10 and 80.22, respectively.
The "Scibert uncased" model performs better than the "Scibert cased" model on all datasets.
The accuracy increases after fine-tuning the models for all datasets.
Humans prefer gradient-based reduced examples over randomly reduced ones for all datasets.
ReDAN+ (Diverse Ens.) has the highest NDCG score among all the models listed in the table.
Table 5 provides an analysis of the NDCG score achieved by different models on different question types.
The percentages in the "Percentage" row represent the distribution of question types in the dataset.
The BERT model outperforms the BiDAF model in terms of overall performance.
The BERT model outperforms the BiDAF model in terms of performance on civil cases.
The performance of unanswerable questions is lower than the performance of "NO" questions in both the civil and criminal datasets.
The unanswerable+ dataset contains more unanswerable questions than the civil dataset.
The table shows the influence of unanswerable questions by implementing BERT and BiDAF on the development set and test set, with different adjustments.
CJRC+Train performs better in the Civil category compared to Development Bert and Test Bert.
The table provides information about the number of UNK (unknown) tokens after using the revision memory for the RNNSearch and +RM models.
The +RM model shows a decrease of 16.1% in the number of UNK tokens compared to the RNNSearch model.
The table shows the average BLEU score of CAMIT's bi-directional decoder after multiple interactions on the NIST ZH-EN task.
The RNNSearch model consistently achieves a BLEU score of 35.45 across all revisions.
There are four different models: RNNSearch, +OL, +RM, and CAMIT.
The +RM model shows a performance gain of +0.59 for NIST04 and +0.26 for NIST05 compared to the RNNSearch model.
The variable "Anxiety" has a higher level of internal consistency compared to the variable "Variable context availability".
The level of depression is relatively low compared to other variables in the table.
The variable "Insecurity" has the highest level of agreement among annotators with a K-alpha value of 0.3123.
The variable "Illusion" has the second highest level of agreement among annotators in the "Variable Anxisety" column with a K-alpha value of 0.3063.
The K-alpha value for Variable Anxisety is 0.3884.
The value for Depression is 0.5536.
Table 6 provides information on the number of words (original, after lemmatization or stemming) in the DISCO PAL corpus for various psychological categories.
The number of words after lemmatization is higher for the "Vulnerability" category compared to the "Depression" category in the DISCO PAL corpus (see Table 6).
The table shows the fraction of words from the original DISCO PAL corpus in different source corpora, categorized by psychological categories.
The fraction of words from the original DISCO PAL corpus that are annotated in all three source corpora is 0.12.
The fraction of words from the original DISCO PAL corpus in the guasch2016spanish stem source corpus is 0.08.
The fraction of words from the original DISCO PAL corpus in the ferre2017moved lem source corpus is 0.18.
The r2 value for the feature anger in the label all category is 0.84.
The coefficient value for the feature anger in the label Compulsión category is 0.04.
The table shows the naturalness and speaker similarity scores for the ground truth (self-similarity) in English, Spanish, and Chinese.
The mean opinion score (MOS) for speaker similarity between English speakers is 4.40±0.07.
The mean opinion score (MOS) for speaker similarity between Spanish speakers is 4.39±0.06.
The table shows the naturalness MOS (Mean Opinion Score) of monolingual and multilingual models synthesizing speech in different languages.
The table compares the effect of EN speaker cloning with no residual encoder using two different models.
The BERT-Large model outperforms all other models in terms of accuracy.
All models underperform humans in terms of accuracy.
Method (8) IBT+Back achieves the highest translation accuracy in the DE to EN LAW domain.
Method (5) DAFE performs better than method (6) IBT in the DE to EN MED domain.
The BLEU score for the IBT model without any additional data is 30.88.
The BLEU score for the IBT+Back model with additional data from the TED domain is 40.92.
The RMFN model outperforms the other models in terms of A2, F1, and A7 metrics.
The RMFN model achieves a lower MAE and higher Corr compared to the other models.
RMFN-R3 outperforms RMFN-R4, RMFN-R5, and RMFN-R6 in all metrics.
The task for all the rows in the table is sentiment analysis.
The "STL + GU" strategy achieves the highest F1 score among all the strategies.
The "All three" strategy achieves the lowest loss among all the strategies.
FinBERT outperforms Yang et. al. (2018) and Piao and Breslin (2018) in terms of both MSE and R2 score on the FiQA Sentiment Dataset.
FinBERT achieves a lower mean squared error (MSE) than Yang et. al. (2018) and Piao and Breslin (2018) on the FiQA Sentiment Dataset.
Table 4 shows the performance of three different models: Vanilla BERT, FinBERT-task, and FinBERT-domain.
FinBERT-task achieves the highest F1 score among the three models mentioned in the table.
The accuracy decreases as we move from the lower layers to the higher layers in the neural network.
The training time decreases as we move from the higher layers to the lower layers in the neural network.
The performance of the dnc models after finetuning varies depending on the (sub-)meeting length.
The performance of the SC models varies depending on the (sub-)meeting length.
The performance of all three tokenizers decreases as the error rate increases.
The WordPiece tokenizer consistently outperforms both the WhiteSpace and N-gram(n=6) tokenizers across all error rates.
Table 7 assesses the influence by code on four different datasets: MPUART, 1MWDFS, MRRG, and GOCS.
Among the four datasets, MPUART achieves the highest average precision (AP) score.
The optimizer used for SNLI is Adam.
The word embeddings used for SNLI are 300D Glove embeddings that are fixed.
The table shows the accuracy results for three different models on the SNLI dataset.
The DA (reimplementation) model achieves an accuracy of 86.9% on the "Dev" dataset and 86.5% on the "Test" dataset.
The optimizer used for training is Adam and the learning rate is 0.0004.
The word embeddings used are 200D embeddings from Wikipedia and they have a hidden size of 200.
The optimizer used for training the SST model is Adam with a learning rate of 0.0002.
The SST model uses 300D Glove embeddings (fixed) and LSTM cells.
There are four different models in the table.
The Neighbor Reg. [ITALIC] x [BOLD] ΦΦ model has the highest hit@1 value.
The table presents the performance of four different models for Russian for k=20 clusters compared to [Fu et al.2014].
The "Neig. Reg. x Φ" model achieves the highest hit@1 score among the four models.
The "Self Persona" and "Their Persona" conditions use different methods for dialog utterance prediction.
The "Profile Memory" method outperforms the "Seq2Seq" method in terms of F1 score in both the "Original" and "Revised" conditions.
The error rate for predicting the persona of speaker 1 (Profile 1) is lower than the error rate for predicting the persona of speaker 0 (Profile 0) using the KV Profile Memory model.
The error rate for predicting the persona of speaker 1 (Profile 1) is higher than the error rate for predicting the persona of speaker 0 (Profile 0) using the KV Memory model without considering the profile.
Table 14 shows the error rates for predicting the persona of speaker 0 (Profile 0) or speaker 1 (Profile 1) given the dialogue utterances of speaker 0 (PERSON 0) or speaker 1 (PERSON 1) using the KV Profile Memory model for different dialogue lengths.
The accuracy of predicting the persona improves as the dialogue length increases for both speaker 0 (Profile 0) and speaker 1 (Profile 1) using the KV Profile Memory model.
The LC_Att method outperforms the Baseline1[fu2018] and L_1DCNN methods in terms of LCC.
The LC_Att method outperforms the Baseline1[fu2018] and L_1DCNN methods in terms of F1.
The pseudo score is positively correlated with the SNR (Signal-to-Noise Ratio).
The pseudo score for the "original clean" is higher than the pseudo scores at all other SNR levels.
The table presents the results of an ablation study comparing the performance of three different methods: Transformer (base), Trm + BERT l2 r, and Trm + BERT sm.
Trm + BERT performs better than Transformer (base) and Trm + BERT l2 r on the De-En translation task.
The "Abstract+situated" approach performs better than the "Situated-only" approach in terms of F1 scores.
The "GDSE-CL" model performs better than the "GDSE-SL" model in terms of F1 scores.
GDSE-CL performs better than GDSE-CL-text in both the abstract and situated attribute prediction scenarios.
DeVries-RL performs better than DeVries-SL in both the non-dialogue and open-domain zero-shot gameplay scenarios.
The value of the hyperparameter "CNN window size" is 3.
The hyperparameter "Dropout ratio" has values of 0.1, 0.3, and 0.5.
The table shows the BLEU scores of non code-switched (original) input on En-Ru test sets.
Our system outperforms the baseline system in terms of BLEU scores on all En-Ru test sets.
The "Neural Belief Tracker (NBT-CNN)" model achieves the highest "Joint Goal Acc." score among all the DST models compared in Table 4.
The "Delexicalization-based Model + Semantic Dictionary" achieves a higher "Joint Goal Acc." score than the "Scalable Multi-domain DST" model.
The table provides joint goal accuracy results on different datasets.
The GRU-based feature extraction has a higher accuracy than the baseline for both DSTC2 and Sim-M datasets.
The model used for all the classes is C5.
The average precision, recall, and F1 score for all classes combined is 0.92, 0.91, and 0.91, respectively.
Table 5 shows the crowdsourced validations of samples identified by models C0, C1, C2, and C3.
The average F1 score for all classes in Table 5 is 0.65.
The average f-score obtained using GloVe embeddings with the RNN model is 73.60.
The average training time in minutes using the Hashing method with m=1000 and the CNN model is 40,22.
The similarity test score for YP130 using the Our-Method with unweighted vectors and a dimension of 500 is 0.24.
The arithmetic mean of the similarity tests using the Our-Method with PPMI-weighted vectors and a dimension of 4000 is 0.69.
The "Hash - CBOW" method has the highest Spearman's ρ value in the "YP" column.
The "Hash - CBOW" method has the lowest Spearman's ρ value in the "RG65" column.
The emotions listed in the table are the ones considered in the work.
The values for the "Unpleasant", "Responsibility", "Uncertainty", "Attention", "Effort", and "Control" dimensions for the emotion "Interest" are -1.05, -0.13, -0.07, 0.70, -0.07, and -0.63 respectively.
The GMA_MCB-att model achieves the highest R@5 score among all the models.
The GMA model achieves a MRR score of 0.6045.
The "CorrNet(123)" model is used for both "Left to Right" and "Right to Left" optimizations.
The "CorrNet" model has the highest accuracy for both the "Left to Right" and "Right to Left" tasks.
The "MAE" model has a higher accuracy for the "Right to Left" task compared to the "Left to Right" task.
The CorrNet model achieves the highest accuracy in the "EN → DE" and "EN → ES" language pairs.
The Majority Class model achieves the lowest accuracy in the "EN → FR" language pair.
The table shows the cross-lingual classification accuracy for 3 different pairs of languages when merging the bag-of-words for different numbers of sentences.
The accuracy of the CorrNet model with 5 sentences in the DE → EN language pair is 72.78.
The F1-measure for the CorrNet model on the NEWS 2010 En-Hi Transliteration Mining Dataset is 81.56%.
The MAE for the MAE model on the NEWS 2010 En-Hi Transliteration Mining Dataset is 72.75.
The DCCAE model has the highest score in the AN column.
The SplitAE model has the highest average score.
Each language included in AlloVera has a different number of phonemes.
The sources for each language's phonology included in AlloVera are different.
The average phoneme error rate for the full dataset is 73.8.
The phoneme error rate for the low-resource dataset with private phonemes is 55.4.
The phone error rate for Tusom is higher than Inuktitut.
The CNN method achieves the highest F1 score among all the methods.
The "Brychcín et al." method has the highest recall score among all the methods.
The table describes the F-measure of CNN with different numbers of convolutional kernels.
As the number of convolutional kernels increases, the F1 score of the CNN also increases until it reaches 44, after which it remains constant.
Table 1 presents the F1 scores of domain, intent, and slot classification tasks modeled by character-level and word-level models for various sizes of training data.
The F1 scores for intent classification increase as the training size increases for character-level models.
Table 2 presents the F1 scores of domain, intent, and slot classification tasks modeled by single-task vs. multi-task models for various sizes of training data.
The F1 scores for slot classification tasks are consistently lower for single-task models compared to multi-task models.
As the training size increases, the F1 scores for all tasks increase for both the fastText-init model and the randomly initialized model.
The fastText-init model performs better than the randomly initialized model for the domain classification task for all training sizes.
The "PRPN-UP" model has the highest F1 score with respect to strictly left-branching trees.
The "300D ST-Gumbel" model has the lowest F1 score with respect to the Stanford Parser trees.
The table shows the performance results for two different document representations.
Word2Vec has a higher precision than Bag of Words.
BERT-FlowDelta achieves the highest F1 score on CoQA Test.
SDNet zhu2018sdnet achieves a HEQ-D score of 76.8 on QuAC Test.
BERT-FlowDelta achieves the highest CoQA F1 score among the models listed.
BERT-FlowDelta has a slightly higher QuAC F1 score than BERT-Flow.
The accuracy of the FlowQA model on the Tangrams task is [BOLD] 76.4.
The accuracy of the FusionNet model on the Scene task is 58.2.
The inclusion of bigrams improves the MAP scores for the models.
The inclusion of bigrams improves the NDCG@10 scores for the models.
The best binary result for code dimensionality 32 is 0.32.
The MAP score for PV-DBOW model with bigrams included is 0.43.
The MAP and NDCG@10 scores vary for different models and inclusion of bigrams, depending on the value of dimensionality.
The MAP and NDCG@10 scores for the "Binary PV-DBOW" model with inclusion of bigrams and dimensionality 300 are [BOLD] 0.2 and [BOLD] 0.54, respectively.
The table compares the performance of different binary units for a 32-bit model trained on the 20 Newsgroups dataset.
The "Krizhevsky & Hinton binarization" approach has the highest MAP and NDCG@10 scores among the mentioned approaches.
The table provides information retrieval results for transfer learning on the 20 Newsgroups and RCV1 datasets.
The NDCG@10 score is higher for the RCV1 dataset compared to the 20 Newsgroups dataset in the information retrieval results for transfer learning.
As the code size decreases, the NDCG@10 values for all three datasets (20 Newsgroups, RCV1, English Wikipedia) also decrease.
As the radius increases, the NDCG@10 values for all three datasets (20 Newsgroups, RCV1, English Wikipedia) also decrease.
The word "apple" is predominantly associated with Sense 1.
The word "rock" is predominantly associated with Sense 2.
The nearest neighbors for Sense 2 of the word "bank" are "fdic" and "lending".
The value of γ for the word "mouse" is 0.0.
As the value of γ increases, the average number of senses per word decreases.
The average number of senses per word decreases as the value of γ increases.
The adjusted rand index for the Disambiguated Skip-gram model with a dimension of 50 and 5 senses is higher when using a penalty term of δ=0.005 compared to δ=0.0001.
The "Disambiguated Skip-gram" model has the highest adjusted rand index for the SemEval-2013 dataset.
The "AdaGram" model has the highest adjusted rand index for the WWSI dataset.
The "MSSG" model has the highest average similarity score (avgSimC) among all the models.
The "AdaGram" model has the lowest maximum similarity score (maxSimC) among all the models.
The NP-MSSG model performs worse than the Skip-gram model on the avgSim metric.
The bag-of-senses model has a higher MAP score compared to the bag-of-words model.
The bag-of-senses model has a higher NDCG@10 score compared to the bag-of-words model.
The bag-of-senses model outperforms the bag-of-words model in terms of MAP score.
The TF-IDF-MR model outperforms the bag-of-words model in terms of NDCG@10.
The TF-IDF-MR model performs better than the bag-of-words model in terms of MAP score.
The bag-of-words model and the TF-IDF-MR model have the same performance in terms of NDCG@10 score.
The table shows the classification accuracy of different prediction methods based on gender and age.
The baseline method has a classification accuracy of 0.51 for gender prediction.
The systems in Table 14 are ordered by their Kendall's τ score, with the highest score at the top and the lowest score at the bottom.
The systems with higher Kendall's τ scores have higher Spearman's ρ coefficients.
The GMM WAE (ours) model has the highest classification accuracy among all the models.
The Dinentangled VAE model has the highest entropy value.
The performance of the FFNN model decreases as the number of hidden layers increases.
The performance of the KN model is higher than the performance of the FFNN model.
The "FFNN [M*200]-600-600-80k" and "FOFE [M*200]-600-600-80k" models have lower performance compared to the other models.
The values for the "model 1", "model 2", and "model 4" columns are not provided for the "NoP 4" row.
The BERT-U model outperforms human performance in most categories, except for the "Other" category.
Both the BERT-U model and human classifier have the lowest performance in the "Data Retention" category.
GP-GloVe performs the best on the GBWR-Association task.
Post-Processing HSR-GloVe performs the worst on the GBWR-Correlation task.
The models with post-processing and the GP-GloVe word vectors achieve the highest average Pearson correlation coefficient for each year.
The STS-2014 task has the highest average Pearson correlation coefficient among all tasks.
The model using GloVe embeddings with Post-Processing achieves the highest average F1 score for the coreference resolution task.
The model using GP-GloVe with Post-Processing has the highest difference in F1 score compared to the other models.
The table shows the BLEU results of different variations of LSTM and Transformer models.
The "+ marginal scores in encoder and decoder, and forward / backward scores only in encoder self-attention layer 0 and layer 1" model achieves the highest BLEU score on the Fisher test dataset.
The table provides information on the different architectures used for the translation task.
The table provides information on the different types of inputs used for the inference.
The table presents the results of ranking experiments on CACM using different models.
The model "paragram-counterfit-affect" achieves the highest NDCG and MAP scores among all the models in the table.
The WP dataset has the highest affect scoring for formality.
The ClueWeb09 dataset has the highest affect scoring for frustration.
Table 2 shows the results of ranking experiments on the New York Times (NYT) dataset.
The BM25 model achieves the highest NDCG score among all the models in the ranking experiments on the NYT dataset.
The performance of the enriched models is generally higher than the performance of the non-enriched models.
The GloVe-retrofitted-affect-555 model achieves the highest performance in both MAP and NDCG metrics.
The "w2v-GoogleNews-300" model achieves the highest NDCG score in the query expansion experiments on CACM.
The "GloVe-retrofitted" model performs better than the "counterfit-GloVe" model in terms of MAP in the query expansion experiments on CACM.
Table 5 shows the performance of the model on the test set for subtask 1 (binary classification).
The accuracy of the model on the test set for subtask 1 is 70.153%.
The TaLK Convolution (Deep) model has the highest Rouge-1, Rouge-2, and Rouge-L scores among all the models.
The TaLK Convolution (Deep) model has 83M parameters.
The model "wu2019pay" achieves the highest BLEU scores for both WMT En-De and WMT En-Fr.
The model "DBLP:journals/corr/abs-1804-09849" has the highest parameter count for WMT En-De.
The test perplexity of "TaLK Convolution (Ours)" is lower than the test perplexity of "Dynamic Convolutions".
The parameter size of "DBLP:journals/corr/abs-1809-10853" is larger than the parameter size of "DBLP:journals/corr/DauphinFAG16".
Table 1 provides details about the model architecture.
The fourth convolutional layer has 525 filters.
The learning rate of the main network in the deep reinforcement learning text readability assessment model is set to 0.0001.
The exploration policy used in the deep reinforcement learning text readability assessment model is ϵ−greedy.
The table provides information about three different datasets: Weebit, Cambridge, and Persian.
The number of words per text is different for each dataset in the table.
The table shows the experimental results (Pearson's r × 100) on textual similarity tasks for different models.
The model "P-SIF (Doc2VecC)" achieves the highest accuracy score of 86.0.
The model "LDA" has the lowest recall score of 70.7.
The "GloVe" method achieves the highest score on the "headlines" task, with a score of 75.6.
The "Tree LSTM" method achieves the highest score on the "post editing" task, with a score of 83.7.
For all the sentences in Table 12, the difference between the P-SIF error and the SIF error is positive.
Table 13 provides example sentences and their corresponding scores for SIF [ITALIC] sc, P-SIF [ITALIC] sc, SIF [ITALIC] err, and P-SIF [ITALIC] err.
The difference between SIF [ITALIC] err and P-SIF [ITALIC] err is consistently positive, indicating that P-SIF scores are closer to the ground truth compared to SIF scores.
As the size of the head or tail increases, the number of training triplets decreases.
As the size of the head or tail increases, the number of auxiliary entities increases.
The table compares the accuracy of the simple baseline and proposed models.
The proposed method achieves higher average accuracy than the baseline method across all pooling techniques and dataset sizes.
The BLEU score of the GAT model increases as the number of layers increases, except when there are 4 layers.
The GAT model with 2 layers has the highest BLEU score among all the layer settings.
The model with 4 heads has the highest BLEU score among all the models.
The models with 4 and 5 heads have the same BLEU score.
The "GAT-enc+dec" model achieves the highest BLEU score and exact F1 score among all the models.
The "Liu:19" model performs better than the "Liu:18" model in terms of BLEU score and exact F1 score.
The gap between humans and the best tested model is larger on SQuAD 2.0 compared to SQuAD 1.1.
The "DocQA + ELMo" system achieves the highest scores in both EM and F1 for the SQuAD 2.0 development set.
The "DocQA + ELMo" system outperforms the "BNA" and "DocQA" systems in terms of EM and F1 scores on the SQuAD 2.0 development set.
The "BNA" system has the highest exact match (EM) score among all systems.
The "DocQA" system has the highest F1 score among all systems.
The number of propositions decreases as the sentence length increases.
The majority of propositions have a sentence length between 10 and 29.
The table shows the performance of the model with different discount coefficients η on the English development set.
The F-score varies depending on the combination of discount coefficient η and number of iterations.
As the number of arguments increases, the number of propositions decreases.
The highest number of arguments is associated with the lowest number of propositions.
The Baseline Model outperforms the Previous Best Single Model in terms of labeled F1 score on the CoNLL-2009 in-domain test sets.
On average, the Baseline Model performs better in terms of labeled F1 score on the CoNLL-2009 in-domain test sets compared to the Previous Best Single Model.
The table provides accuracy rates for different methods used in multi-source cross-domain polarity classification.
The K0/1 + TKC method achieves an accuracy rate of [BOLD] 84.1* for the DEK→B domain transfer.
Table 2 shows the single-source cross-domain polarity classification accuracy rates of different methods compared to a state-of-the-art baseline and other approaches.
Table 3 shows the Arabic dialect identification accuracy rates of different methods compared to the 2017 ADI Shared Task winner and the first runner up.
The accuracy rates for the method "K0/1+, K∩+, KLRD+, Kivec + TKC" are significantly better than the accuracy rates of the 2017 ADI Shared Task winner according to a paired McNemar's test.
The BGT model outperforms other models on all datasets.
The BGT model achieves a performance of 74.9 on the en-tr dataset.
The Kendall τ value for the "P@1" metric on TREC-QA-Dev is 1.000.
The p-value for the Kendall τ test on the MRR metric for WikiQA-Test is 0.017.
The AVA evaluator performs better than the ADS evaluator on S1, S2, S3, and S4.
The ADS split performs better than the AVA split on S1, S2, and S3 in the development set.
The F1 score for the CoNLL-05 development set with 3% RoBERTa2 is 70.48.
The F1 score for the AM-MOD label on the CoNLL-12 development set with 100% +U,F,O is 98.04.
The F1 score of RoBERTa2 with the ablation tests "+U,F" on WSJ is [BOLD] 88.07.
The model "w/ Attention" performs the best among all models for Task A, Task B, and Task C.
The model "Parallel LSTM" performs the best for Task B based on the F1 score.
The model "w/ Attention + aug features" outperforms the model "w/ Attention" in Task B.
The model "w/ Attention + aug features" outperforms the model "w/ Attention" in Task C.
The "Attention & IR" model outperformed other systems in all three tasks.
The "Attention & IR" model and the "IR" model achieved the same MAP score in Task A.
The Cross-Aligned model achieves an accuracy of 83.78% on the Yelp test set.
The NS-BiGRU model achieves a BLEU score of 49.64 on the Amazon test set.
The highest score for ROUGE-1 (RG-1) is 36.92.
The model "Reinforced-Topic-ConvS2S" achieves the highest accuracy on the RG-1 score.
The model "Reinforced-Topic-ConvS2S" achieves the highest accuracy on the RG-L score.
The "Reinforced-Topic-ConvS2S" model achieves the highest RG-1 score on the DUC-2004 dataset.
The "RNN+MRT" model achieves the highest RG-2 score on the DUC-2004 dataset.
Table 6 provides accuracy scores for different models on the LCSTS dataset using both word-level and character-level ROUGE metrics.
The RNN context model performs better with character-based preprocessing compared to word-based preprocessing.
The F1 score for pronoun gender classification using BERT (Contexts) + Data on possessives is 87.4.
The F1 score for pronoun gender classification using Context MT (Contexts) on prodrop is 65.2.
In WMT'13 (development), masculine entities have a higher prodrop rate than feminine entities.
In WMT'13 (development), the pronoun "he" appears more frequently than the pronoun "she".
The table presents the BLEU score and pronoun accuracy by gender on the WMT’13 Spanish-to-English test set for three different models: Baseline MT, + Gender Tags, and Context MT.
The pronoun accuracy for the Baseline MT model is 95.2%.
Adding more data consistently improves validation performance for both VisualBERT(MLM) and ViLBERT(MLM) when trained for the same number of epochs.
ViLBERT(MLM) has higher validation losses than VisualBERT(MLM) when pretrained for the same number of iterations on the VQA 2.0 dataset.
The hyperparameters for each stop consonant model are tuned separately.
The dropout percentage is different for each stop consonant model.
There are 35 linguistic constructs addressed by DisSim.
There are three different clausal disembedding constructs addressed by DisSim.
The table shows the impact of training regime and vocabulary overlap on agreement accuracy (FR→IT).
The agreement accuracy is higher in the "Bilingual (FR+IT small) Joint Training" column compared to the "Bilingual (FR+IT small) Pre-Training" column.
The NN+WLL+LM2 approach achieves the highest F1 score for the NER task.
The system with the NN+SLL approach achieves the highest per-word accuracy (PWA) for the POS task.
The best performing system achieved an F1 score of 97.35 for POS, 94.32 for CHUNK, and 89.86 for NER.
The worst performing system achieved an F1 score of 97.29 for POS, 93.99 for CHUNK, and 89.35 for NER.
The table compares the performance of neural network architectures trained with both non-convex and convex approaches.
The "Non-convex Approach" outperforms the "Convex Approach" in terms of NER performance.
SENNA has the lowest RAM consumption among the state-of-the-art systems.
SENNA has the lowest runtime among the state-of-the-art systems.
The ODPCV+ WV method achieves the highest precision at k=1, k=2, k=3, k=4, and k=5 among all methods in the table.
ODPWV achieves higher precision at k=1, k=2, k=3, k=4, and k=5 compared to ODPCV.
The table compares the precision, recall, and F1-score of category vector generation methods ODPCV(Algebra) and ODPCV(Embedding) on the ODP dataset.
ODPCV(Algebra) has a higher F1-score (0.453) compared to ODPCV(Embedding) (0.230) on the ODP dataset.
The combination of ODPCV+ WV achieves the highest F1-score.
The Precision and Recall values for ODPCV and ODPWV are similar.
The table shows the performance metrics (precision, recall, and F1-score) of the ODP and CNN models on a moderate-scale dataset with 13 categories.
The F1-score for the ODP model on the moderate-scale dataset with 13 categories is 0.687.
The dLCE word embeddings outperform GloVe embeddings in terms of Adjective F1 score for both the Pattern-based Model and the Combined Model.
The Combined Model with dLCE word embeddings achieves the highest precision score for nouns.
The "Combined AntSynNET" model outperforms the baseline models in terms of F1 scores for both adjectives and nouns.
The "Combined AntSynNET" model achieves the highest precision score for nouns compared to the other models.
Table 4 compares the performance of the novel distance feature with Schwarz et al.'s direction feature across different word classes.
The pattern-based approach for the distance feature has a higher F1 score for nouns compared to verbs and adjectives.
Table 2 provides language model perplexities on the validation and test sets of the Penn Treebank.
The table shows the language model test perplexities of LSTM and LSTM + Behavior gating on Couples Therapy and Cancer Couples Interaction Dataset.
LSTM + Behavior gating performs better than LSTM on the Cancer Couples Interaction Dataset.
Models that use both root and leaf labels have higher performances than models trained with only root labels.
S-LSTM models outperform RNTN models in terms of performance for both root labels and root + leaf labels.
S-LSTM outperforms other models in terms of accuracy at both the sentence level and the phrase level.
RNTN performs better than NB, SVM, and RvNN at the sentence level.
There are three different variations of the S-LSTM model: S-LSTM-LR, S-LSTM-RR, and S-LSTM.
The performance of the S-LSTM model with both root and leaf labels is higher than the performance of the S-LSTM model with only root labels.
Table 2 provides comparisons between baselines and best adversaries.
TP (Denmark) has a higher accuracy than TP (France) for both Gender and Age.
FASum has the highest factual score among all the models.
UniLM has the highest informativeness score among all the models.
The table includes the performance scores of BottomUp, Corrected by FC, UniLM, and FASum models.
UniLM achieves the highest ROUGE-1 score among all the models.
The "Corrected by FC" model has higher RMR1 and RMR2 values compared to the "UniLM" model.
The "BottomUp" model has lower RMR1 and RMR2 values compared to the "Corrected by FC" model.
Table 5 presents the results of an ablation study with full training set by freezing model parameters for generator or parser during learning.
The DIM model performs better than the Super model on the Django dataset when the generator parameters are frozen.
The table provides information on the average annotation time per text and speed-up for two different phases: MeD and TeD.
The speed-up achieved in the MeD phase ranges from -6% to 54%, while in the TeD phase it ranges from -8% to 34%.
The average intra-annotator agreement for MeD is 0.77.
Suggestion A2 has a value of 0.76 for MeD.
The NN Cluster Ranker achieves the highest F1 score on the CoNLL 2012 Chinese Test Data.
The NN Cluster Ranker achieves the highest CEAF ϕ4 score.
The model "Meemi (VecMaportho)" has the highest P@1 score for the English-Italian pair.
The model "VecMapmultistep" has the highest P@10 score for the English-German pair.
VecMapmultistep performs the best in terms of word similarity for EN-DE and EN-FA.
Meemiw (VecMaportho) performs the best in terms of word similarity for EN-FA.
The model "Meemi-multi (VecMaportho)" has the highest correlation coefficient (r) for English.
The model "Meemiw (MUSE)" has a higher correlation coefficient (ρ) for Farsi compared to the model "VecMapuns".
Table 5 shows the accuracy on the XNLI task using different cross-lingual embeddings as features.
The lower bound accuracy for both EN-ES and EN-DE is lower than the accuracy of any other model.
The table presents the entity classification performance with and without augmented features for the OntoNotes and Elected Reps datasets.
The entity classification performance improves when augmented features are used, as indicated by the higher Micro F1 scores for NFGEC+ compared to NFGEC for both OntoNotes and Elected Reps datasets.
The table shows the number of failed pivot translation tasks for different categories.
The "Emb" category has the highest number of failed pivot translation tasks.
The table shows XNLI dev accuracy for English, Russian, Chinese, and Urdu for BERT-BASE models trained either on Wikipedia or CommonCrawl.
The average XNLI dev accuracy across English, Russian, Chinese, and Urdu is shown in the "Δ" cell.
The "3 best" relations have higher accuracy, P@2, and P@5 scores compared to the "3 worst" relations.
The "3 worst" relations have higher average |Cq| values compared to the "3 best" relations.
The "full (ensemble)" model performs better than the "full (single)" model in both the unmasked and masked settings.
The performance of the "GloVe with R-GCN" model is lower in the masked setting compared to the unmasked setting.
Table 3 presents the performance of various models using full-length ROUGE metrics.
Model SD2 achieves the highest ROUGE-1 score among all the models.
The TRADE model achieves the highest joint accuracy in both the MultiWOZ and the MultiWOZ (Only Restaurant) datasets.
The Joint Slot model performs better on the Hotel domain than on the Train domain.
The Training 1% New Domain model performs better on the Taxi domain than on the Train domain.
The model achieves a joint goal accuracy of 60.58% in the "Taxi" domain without training on any samples from that domain.
The slot filling accuracy is higher for the "Restaurant" domain (93.28%) compared to the "Hotel" domain (92.66%) when trained using single-domain data.
Table 6 shows the character recognition accuracy on 100 random frames.
The precision, recall, and F1-score values are lower after the filter is applied.
The ROLL model with BERT as the encoder achieves an accuracy of 0.754 on the KnowIT VQA test set for human descriptions.
The TVQA model with LSTM as the encoder achieves an accuracy of 0.612 on the KnowIT VQA test set for subtitles.
The Full Model performs better than the Read-Recall model in terms of Text and Knowledge branches.
The Recall model performs the best in terms of the Temporal branch.
Table 6 shows the Character Recognition accuracy on 100 random frames.
The accuracy of Character Recognition increases after applying a filter.
Table 6 shows the average speaker-independent cluster, speaker, and gender purity for BayesSegMinDur on three datasets.
The average speaker purity for BayesSegMinDur on the English1 dataset using MFCC features is 31.5%.
The system "Reference-HT" has the highest average percentage at 67.4%.
The system "Online-A-1710" has the lowest average percentage at 55.3% and the lowest average z-score at -0.351.
The combination of systems SV1, SV2, SV3, ARJT1, ARJT2, ARJT3, DLDN2, DLDN3, DLDN4 achieved a BLEU score of 28.46.
The system combination ARJT2, ARJT3, ARJT4 + 3 identical systems with different initialization achieved a BLEU score of 27.82.
The system "Combo-6" has the highest Ave % value of 69.9.
The system "Online-B-1710" has the lowest Ave [ITALIC] z value of -0.494.
The table provides information about the performance of different systems in Subset-1, second iteration.
The systems listed in the table show a decreasing average percentage performance in Subset-1, second iteration.
The system "Reference-HT" has the highest average percentage.
The system "Online-B-1710" has the lowest average z-score.
The system "Reference-HT" has the highest average percentage.
The system "Online-B-1710" has the lowest average Z.
The system "Reference-HT" has the highest average percentage at 66.6%.
The system "Online-B-1710" has the lowest average z-score at -0.455.
The system "Combo-6" has the highest average percentage at 0.237.
The average percentage for "Reference-WMT" is -0.115.
The error category "Incorrect Words" has the highest fraction of sentences with a percentage of 7.64%.
The fractions in the "Fraction" column add up to 100%.
Table 4 shows the reconstruction BLEU scores of MAD and MACD in different languages for the WMT unsupervised tasks.
MAD achieves higher reconstruction BLEU scores than MACD in the En-Fr-En and En-Ro-En tasks.
The BLEU scores on the unsupervised IWSLT’13 English-French (En-Fr) and IWSLT’14 English-German (En-De) tasks increase as the values of k and n increase.
Adding MACD improves the BLEU scores on the unsupervised IWSLT’13 English-French (En-Fr) and IWSLT’14 English-German (En-De) tasks.
MACD (2nd level, n=2) outperforms other methods in terms of BLEU scores for all language pairs.
MACD (2nd level, n=2) performs better than the Baseline (lample2018phrase_unsup) (NMT) method in terms of BLEU score for the English-French language pair.
The MACD with NMT (2nd level, n=2) method achieves the highest BLEU scores in all translation tasks.
The MACD with NMT (2nd level, n=2) method outperforms the Baseline NMT (lample2018phrase_unsup) method in the En-Fr translation task.
The model "MALA-S3" achieves the highest scores for both SMD Entity-F1 and SMD BLEU.
The model "LIDM" outperforms the model "LaRL" in terms of MultiWOZ Entity-F1 and MultiWOZ BLEU.
The ablation studies in Table 3 test different models in the proposed architecture.
Table 3 provides the macro-average and micro-average scores for each model in the ablation studies.
The model "This work: NCET + ELMo" outperforms the model "KG-MRC Das et al. (2019)" in both Task-1 and Task-2.
The model "ProGlobal Dalvi et al. (2018)" performs the best in Task-1 according to both macro-average and micro-average scores.
The hashtag #1savetatas has the highest rank among all the breast cancer hashtags listed in the table.
Table 1 shows the accuracy rate and p-value obtained for the classification subtasks.
The "children × philosophy" combination has the highest accuracy rate among the four subtasks.
The table shows the accuracy rate and p-value obtained for different classification subtasks.
The accuracy rate for the subtask "children × investigative × philosophy" is 62.2%.
The Oracle (BertScore) has the highest ROUGE-1 score.
The SummaRuNNer* has the lowest ROUGE-2 score.
Table I displays the ROUGE and Human evaluation scores of oracle summaries built on BertScore and ROUGE.
The ROUGE-1 F1 Score for the ROUGE Oracle is 51.84.
The BertScore Candidates perform better in the Similarity Evaluation than the ROUGE Candidates.
The Entity and Event Questions have a higher Accuracy in the QA Paradigm Evaluation compared to the Extended Questions.
The recall score for R-2 in the Oracle model is 8.70*.
The F1 score for R-1 in the Code [1,0,0] model is 34.81.
The CL class has the highest cosine distance between conference vectors and mean class vectors of languages.
ACL has a higher number of class 1 papers compared to COLING.
TACL has a lower number of class 3 papers compared to COLING.
The "+ BoW" model has a lower accuracy compared to the "baseline" model for both MNLI and CWB Bal.
The "baseline" model has a higher accuracy_hr compared to the "+ BoW" model for both CWB Stress* and WOB Stress.
The table presents the performance for reducing the CWB via data enhancement/augmentation under three different conditions: baseline, + origin, and + synthetic.
The addition of synthetic data improves the Stress* accuracy compared to the baseline condition.
The "emb_basic" model has a higher accuracy than the "baseline" model on the CWB Balanced dataset.
The "sgl_sent" model has a lower accuracy than the "baseline" model on the WOB Stress dataset.
The baseline model has an accuracy of 82.3% on MNLI.
Adding 20,000 additional synthetic data improves the accuracy of BERT for reducing the CWB.
The table shows the performance of BERT for reducing the WOB via data enhancement/augmentation under three different conditions: baseline, + origin, and + synthetic.
The highest stress accuracy is achieved when using synthetic data augmentation.
The table shows the WER (%) performance of unadapted and T/S learning adapted LSTM acoustic models for robust ASR on the real noisy channel 5 test set of CHiME-3.
The table includes two different adaptation data: "clean-noisy" and "clean-noisy, clean-clean".
Table 3 provides the BLEU scores of NMT models trained on bitext data mined from aligned documents on TED Talk test sets, along with the volume of distinct aligned sentence pairs for each language.
The Albanian language has a higher BLEU score for English-to-x translation (28.9) compared to the Macedonian language (26.3).
Vietnamese has the highest BLEU score for the En–x translation direction.
Romanian has the highest volume among the languages.
The table presents the results of content-based document alignment recall using two different methods: DDE Similarity and SAE Similarity.
Both the DDE Similarity and SAE Similarity methods show higher content-based document alignment recall in the "Mid" and "High" categories compared to the "Low" category.
The "bigru-att" model has a hidden units size (HU) of 300, a dropout rate across dimensions (Dd) of 0.2, a dropout rate of word embeddings (Dwe) of 0.02, and a batch size (BS) of 12.
The "bigru-lwan" model performs better on all labels compared to frequent labels in terms of RP@5 and nDCG@5.
The "bigru-lwan-l2v" model performs better than the "bigru-lwan" model on both all labels and frequent labels in terms of Micro-F1.
The MAP value for the SRS method is 0.709.
The R10@5 value for the Synergistic method is 0.798.
Table 4 presents the evaluation results of different ablation models.
The SRS model outperforms the other ablation models in terms of MAP.
Table 1 shows the BLEU scores on Paracrawl and WMT En-Fr datasets with uniform, heuristic, and learned curricula.
The BLEU score for the WMT dataset with the uniform (6-bins) curriculum is not available.
Table 2 shows the BLEU scores on ablation experiments with default and fixed rewards or observations.
The default reward/observation setting has a higher BLEU score compared to the fixed reward/observation setting.
The attack success rate for the targeted adversarial attack using AdvChar with 10 changes and 10 characters is 0.945 for the THUCTC dataset.
The average edit distance for the untargeted adversarial attack on the Wechat dataset is 2.573.
The classifier accuracy after the transferability-based blackbox attack is higher for the THUCTC dataset than for the Wechat dataset.
The classifier accuracy after the transferability-based blackbox attack is higher for the Wechat dataset than for the THUCTC dataset.
The human performance on the clean dataset is 0.84 \pm 0.04.
The performance of the BERT classifier on the adversarial dataset is 0.00.
The table shows the untargeted attack success rates on a Chinese BERT-based classifier for the THUCTC dataset.
The untargeted attack success rates for the THUCTC dataset are higher when using the l_{2} norm compared to the l_{1} norm.
The targeted attack success rate for THUCTC dataset is higher than the untargeted attack success rate.
The success rate for the l_{2} targeted attack decreases when the number of modified characters increases.
The untargeted attack success rate for THUCTC dataset is higher than the targeted attack success rate using strategy 1.
The targeted attack success rate for THUCTC dataset using strategy 2 is higher than the untargeted attack success rate.
Table 6 compares different channels.
M1 outperforms M2 and M3 in terms of R2@1, R10@1, R10@2, R10@5, and ACC.
KEHNN has the highest accuracy score among all the models evaluated on answer selection.
The DeepMatch model with a specific topic has an accuracy score of 0.682.
KEHNN achieves the highest performance in terms of R2@1, R10@1, R10@2, and R10@5 among all the models evaluated.
The performance of LSTM and LSTM a is the same in terms of R2@1, R10@1, R10@2, and R10@5.
The number of pairs decreases as the length of the pairs increases.
The performance of the MV-LSTM model increases as the length of the pairs increases.
The "Ubuntu dataset" is divided into four length categories: [0,30), [30,60), [60,90), and [90,∞).
The performance of the "MV-LSTM" model decreases as the length of the dataset increases.
The context model has the highest accuracy among the baseline and word model.
The context model has the highest F1 score among the baseline and word model.
The number of training examples for Bn and Hi is the same for all three datasets.
The average length of the test examples for Bn and Hi is similar for all three datasets.
The context model has a higher accuracy than the word model.
The context model has a higher F1 score than the baseline.
The correlation values for the "Target word" context are generally higher than the correlation values for the "Full sentence" context.
The "BERT av 4" embeddings have higher correlation values compared to other embeddings.
cdec (SMT) has the highest 1-gram BLEU score among the systems listed in the table.
RNNSearch* has a lower 1-gram BLEU score than Direct bridging.
Table 2 shows the percentage of target side eos translated from source side eos on the development set.
The direct bridging system has a percentage of 81.30 for target side eos translated from source side eos on the development set.
The ROT percentages for RNNSearch* are higher than those for Direct bridging for all POS categories.
The ROT percentage for Direct bridging is higher than that for RNNSearch* for the noun category (NN).
"TFIDF-LDA (Avg.Sig.)" has the highest "Avg. c-stats" value among all the methods.
"TFIDF-LDA (Avg.Sig.)" shows the largest improvement compared to the baseline among all the methods.
The vanilla LSTM model with 2048 hidden units has a perplexity of 40.27.
The LD-Net with Layer-wise Dropout model has 10 layers.
LD-Net (9, pruned) achieves a higher speed than LD-Net (9, origin) in all cases.
LD-Net (9, pruned) has a significantly lower number of FLOPs compared to LD-Net (9, origin).
The AVeer method has the lowest c@1 score among all the AV methods.
The GLAD method has the highest AUC score among all the AV methods.
AVeer has the lowest c@1 score and AUC score among all the methods.
The "coupled ensemble" model performs on par with the baseline on Ainu.
The "+shared" model outperforms the baseline on Spanish.
Multi-Source models achieve lower Character Error Rates (CER) on all three target languages, even in extremely low resource settings (Ainu, Mboshi).
The coupled ensemble model has a Character Error Rate (CER) of 42.2 on Spanish transcription.
SC and Style-SC models outperform the other models in terms of fluency, coherence, meaningfulness, poeticness, and entirety.
Human-generated poetry is rated higher in terms of fluency, coherence, meaningfulness, poeticness, and entirety compared to the poetry generated by the models.
The GLM (BERT-large) method achieves a higher F1 score compared to both BERT-large implementations.
The GLM (BERT-large) method is the only method that stands out in the table.
Table 2 compares the performance of different models on the Dev and Test sets of CommonsenseQA.
The RoBERTa model combined with information retrieval (IR) during finetuning achieves a Dev score of 78.9 and a Test score of 72.1 on CommonsenseQA.
GLM (BERT-base) achieves the highest test accuracy on the WN11 triple classification task.
KG-BERT (BERT-base) outperforms DistMult-HRS in terms of test accuracy on the WN11 triple classification task.
The table presents different methods used for the CKBC triple classification task.
GLM using RoBERTa-large achieves a test accuracy of 94.6% on the CKBC triple classification task.
Table 2 presents the results of an ablation study of MuRel, examining the pairwise module and the iterative processing on the VQA 2.0 val, VQA-CP v2, and TDIUC datasets.
The iterative processing of MuRel achieves higher performance on the VQA 2.0 val, VQA-CP v2, and TDIUC datasets compared to the pairwise module.
The MuRel strategy outperforms the Attention baseline on the VQA 2.0 dataset.
The MuRel strategy outperforms the Attention baseline on the VQA CP v2 dataset.
The "MuRel" model achieves the highest score in the "test-dev All" column.
The "Counter" model outperforms the "Bottom-up" model in the "test-dev Num.", "test-dev Other", and "test-dev All" columns.
The MuRel model performs the best in the Color Attr. task.
The MuRel model performs the best in the Pos. Reasoning task.
Table 5 provides a state-of-the-art comparison on the VQA-CP v2 dataset.
The "All" value for the HAN model is 28.65.
Table 5 shows the overall accuracy for the original model and the agent, split by action type.
The average human achieves an F1 score of 49.8 in the Context task.
The MMA AnswerOnly system achieves a higher F1 score on the Short Answer Dev Span task compared to the BERT system.
The MMA Context system achieves a higher precision score on the Short Answer Test task compared to the BERTWWM system.
The table compares the performance of Lithium EDL and Google Cloud NL API.
The table provides the total counts of entities for each language in the dataset.
The table provides precision, recall, F-score, and accuracy for both context independent and context dependent features.
The combination of all features has a precision of 0.63, recall of 0.87, F-score of 0.73, and accuracy of 0.64.
Japanese has the highest precision, recall, f-score, and accuracy among all the languages.
German has the highest recall and f-score among all the languages.
The table compares the performance of Lithium EDL and Google Cloud NL API for different languages.
Lithium EDL processed a higher number of entities than Google Cloud NL API.
The NMF_robustW + HEQ method outperforms the NMF_plain + HEQ method for all noise levels.
The performance of all methods decreases as the noise level increases.
The baseline method performs better than the NMF_plain and NMF_robustW methods across all noise levels.
The NMF_robustW method outperforms the NMF_plain method at an SNR level of 10.
The robust weighted NMF combined with HLDA performs better than plain NMF combined with HLDA at SNR 10.
The combination of robust weighted NMF and HLDA performs better on average than HLDA alone.
The table presents human evaluation results of zero-shot English-Chinese question generation for the Xlm and Pipeline (Xlm) models.
The Xnlg model shows a significant improvement in fluency compared to the other models.
The Xlm model has the lowest score in the BL-4 metric.
The supervised approach outperforms the zero-shot approach for both English-to-English and Chinese-to-Chinese question generation when using the BL-4 metric.
The zero-shot approach outperforms the supervised approach for both English-to-English and Chinese-to-Chinese question generation when using the RG-L metric.
The "DiffStk-MRNN (Ours)" model has the highest mean accuracy with noise.
The "StackRNN" model has a lower mean accuracy compared to the "DiffStk-RNN (Ours)" model.
Table 2 provides the percentage of correctly classified strings for various RNN models with and without noise, as well as carry forwarding state on D2 language.
The DiffStk-MRNN (Ours) model achieves the highest accuracy among all the RNN models mentioned in the table.
"liang2011learning" has the highest accuracy among all the models in Table 5.
The accuracy of "jia2016data" is 85.0.
The table presents the F1 scores of different models on GraphQuestions.
The "ScanneR" model achieves the highest F1 score among the models evaluated on GraphQuestions.
Table 6 provides SacreBLEU scores for data selection experiments.
The "Subtitles" domain has the highest average SacreBLEU score among all domains.
The F1 score is higher with pre-ranking compared to without pre-ranking for all categories.
The precision, recall, and F1 score are higher with pre-ranking compared to without pre-ranking for the "Koran" category.
The precision for the D-Finetune model is 0.63.
The recall for the Moore-Lewis model on the Koran dataset is 0.985.
The model with the "DropAttention(c)" configuration achieves the highest F1 score for NER and Chunking.
The model with the "DropAttention(e)" configuration achieves the highest accuracy for POS.
The model "DropAttention(c)" with normalized rescaling has a classification rate (CR) of 82.75%.
The model "DropAttention(e)" with traditional rescaling achieves a higher accuracy on the AG's News dataset compared to the Yelp13 dataset.
The table provides information about the sizes of different sequence labeling datasets in terms of the number of tokens.
The CoNLL 2000 dataset is used for the task of chunking.
The model "DropAttention(e)" with a window size of 2 achieved the highest BLEU score of 28.32.
Changing the dropout rate from 0.1 to 0.2 does not significantly affect the BLEU score.
The best result for SNLI is achieved with the "DropAttention(e)" hyperparameter setting.
The result for SNLI with the "DropAttention(c)" hyperparameter setting is 84.38.
RQRF outperforms the other models in terms of NLL, MAP, MRR, and NDCG.
BM25 performs worse than the other models in terms of NLL, MAP, MRR, and NDCG.
The model "RQRF" outperforms the other ablation models in terms of NLL, MAP, MRR, and NDCG.
The model "RQRF" achieves the lowest NLL value among the ablation models.
The Annotated Model performs better than the Text Similarity Model in terms of NDCG score.
The Annotated Model performs better than the Text Similarity Model in terms of τap score.
Table 2 provides results for ranking by each individual feature value, including similarity features, age, mention count, and citation impact.
The NDCG value for the similarity between the abstracts of the annotated document and the referenced document is 0.79.
Table 4 provides results for the model trained using the Google Scholar Related Articles ranking.
The model trained using the Author Annotations performs better in terms of NDCG compared to the model trained using the Google Scholar Related Articles ranking.
The average phoneme error rate for the full dataset is 73.8.
The phoneme error rate for the low-resource dataset is 55.4.
The table presents the results of an ablation experiment comparing the performance of the Full system, Global information only, and Local information only.
The Full system achieves the highest F-score among the Full system, Global information only, and Local information only.
The table presents the results of an ablation experiment comparing the performance of the Full system, Global information only, and Local information only.
The Full system achieves the highest F-score among the Full system, Global information only, and Local information only.
The feature set "All" consistently performs better than other feature sets across all agreement thresholds.
The agreement threshold of 80% (4/5) consistently yields higher scores than other agreement thresholds for all feature sets.
The BERT-base model achieves the highest F1-score among all the models in the table.
The LogReg model with fine-tuned BERT LM and features achieves a precision of 0.7704, recall of 0.8184, and F1-score of 0.7937.
The precision, recall, and f1-score for the "avg/total" row are calculated by averaging the metrics for each label.
The precision, recall, and f1-score for the "LOC" label are 0.648, 0.872, and 0.744 respectively.
The performance of BoSsNet with 1-Hop Encoder on bAbI Dialog Tasks T1 is 100.
The BLEU score for CamRest using BoSsNet with Multi-Hop Encoder is 15.2.
BoSsNet outperforms all other models in terms of entity F1 on both the CamRest and SMD datasets.
Mem2Seq and Mem2Seq* have the same BLEU score on the CamRest dataset.
The table represents AMT evaluations on CamRest and SMD datasets.
BoSsNet performs better in terms of "Info" score on CamRest and "Grammar" score on SMD.
The Seq2Seq model performs better on the CamRest Info dataset compared to the CamRest Grammar dataset.
The BoSsNet model performs significantly better on the CamRest Info dataset compared to the CamRest Grammar dataset.
Table 4 shows the MOS similarity scores for voice ID pairs in English and Spanish.
The range of answer lengths in the dataset is from 14.56 to 25.33.
The similarity score for within answers using the "similar" method is 21.71.
MODEL-DP outperforms the baselines in terms of RD-RL, RD++-RL, and FD-RL.
The RD-RL score for MODEL-DP is 0.8288.
The mean Spearman's rank correlation for the baseline model (CBOW-S 750) in the FD-R dataset using Kernel Regression is 0.3972 with a standard deviation of 0.0590.
The mean Spearman's rank correlation for the FD-F dataset with CBOW-S (750) vectors using the MODEL-DP-S model is 0.4083.
The LSTM + 4-gram model has the lowest perplexity score among all the models.
The table shows BLEU scores for two scoring variants of the attention function of the proposed decoder.
The "Content+Scope" variant of the attention function has a lower BLEU score for the "En-Zh" language pair compared to the "Es-En" language pair.
The Hybrid-RL system has the highest win rate among the three systems.
The RL system has the highest average turn among the three systems.
The GCNN (best) model has the highest Overall F1-score.
The GCNN (best) model has the highest F1-score for inter-sentence pairs.
The GCNN model achieves the highest F1 score on the CHR test set.
The zhou2016cdr model achieves the highest recall percentage on the CDR test set.
The "Question Content + Ontology" model achieves the highest precision, recall, and F-score among all the feature models.
The "Ontology + External" model achieves the lowest F-score among all the feature models.
The Support Vector Machine (SVM) has the highest accuracy, precision, recall, and F-score among the three classifiers.
The Support Vector Machine (SVM) has a higher accuracy than the Random Forest and Naive Bayes classifiers.
Table 7 presents the evaluation results for Question 1 using three different evaluation methods: Merge, Neighbor, and Unmerge.
The Merge evaluation method has the highest performance in terms of Majority Voting for Question 1.
Both mlp and rnn outperform the static threshold baseline by a large margin.
The optimal buzzer has an ACC of 1.0, EW of 0.523, and Score of 2.19.
The rnn model achieves the highest accuracy among all the models.
The rnn model has a longer training time compared to the dan model.
Table 4 shows the average WikiAnn NER F1 scores on the development sets of 265 languages with shared vocabularies of different sizes.
The F1 score on the development sets of 265 languages increases as the BPE vocabulary size increases.
Table 3 shows NER results on WikiAnn for different languages and settings.
Table 5 shows the NER F1 scores for the 101 WikiAnn languages supported by all evaluated methods.
The MultiBPEmb +char+finetune method achieves the highest NER F1 score for the intersection of all languages and BERT.
The "+Full BiRNN" model has a higher BLEU score than the "+Full Baseline" model.
The "+Full BiRNN" model was trained on more sentences than the "+Full Baseline" model.
The precision of the BiRNN model is higher at a noise ratio of 0% compared to a noise ratio of 90%.
The recall of the BiRNN model is higher at a noise ratio of 0% compared to a noise ratio of 90%.
Table 2 shows the effect of ablating gedge and gkb from XPAD on the test set.
Ablating gkb from XPAD results in an F1 score of 34.5.
The "ProGlobal" model achieves the highest Precision score on the dependency task.
DocTag2Vec and Doc2Vec have higher recall scores for NCT (general tags) compared to DocTag2Vec (incremental) and SLEEC on the News Content Taxonomy dataset.
The MAE values decrease from HLR+ to C-HLR+ to N-HLR+ to CN-HLR+, with N-HLR+ and CN-HLR+ having the lowest MAE values.
N-HLR+ and CN-HLR+ have the same lowest MAE value of 0.105, which is an improvement compared to the previous methods of Pimsleur and Leitner.
HLR-lex has the lowest MAE value among all the models.
Pimsleur and Leitner are previous methods of modelling the forgetting curve.
The table shows the effect of the number of attention heads on scaling with 8.3 billion parameters and 8-way model parallelism.
As the number of attention heads decreases, the hidden size per head increases.
The speedup obtained for the 1.2 billion parameters model using model parallelism increases as the number of GPUs increases.
The speedup obtained for the 1.2 billion parameters model using model parallelism is higher with 8 GPUs compared to 4 GPUs.
The models in the table are listed in descending order of their accuracy scores.
The models perform better on the laptop domain compared to the restaurant domain.
The table presents an ablation study on Japanese-to-English and French-to-English translation methods, evaluating the performance of different translation methods.
The BLESS dataset has 1,337 instances of the hypernymy relation.
The EVALution dataset has 3,156 instances of the antonymy relation.
The model HyperVec achieves the highest accuracy scores for all three metrics (BLESS, WBLESS, BIBLESS).
HyperScore has the highest value among all the measures in the table.
Paragram has the highest value among all the word embedding models in the table.
The AP result for Hyp/All using DE- SGNS is 0.28.
The AP result for Hyp/Syn using IT→EN HyperVec is 0.57.
Table 1 provides the Pearson correlation for inter-annotator agreement on the SemEval-2007 Affective Text Corpus.
The frequency-based average for inter-annotator agreement on the SemEval-2007 Affective Text Corpus is 43.
Table 1 shows the Pearson correlation for inter-annotator agreement on the SemEval-2007 Affective Text Corpus.
The Pearson correlation for inter-annotator agreement on the SemEval-2007 Affective Text Corpus for the emotion "joy" is 59.91.
The model with BERT transferring to MNLI has a lower accuracy than the CE loss baseline.
The model with BERT transferring to SCITAIL has a higher accuracy than the DFL loss baseline.
The CE loss has a higher value compared to other models.
The PoE model performs better than other models on the test set.
There are no results available for the AdvCls* and AdvDat* models in the BERT and Hard conditions.
RUBi and DFL models show an improvement in performance compared to the CE loss in both the BERT and InferSent conditions.
The "DFL-Joint" method performs better than the "DFL ♥" method on the HANS dataset.
Table 9 shows the accuracy results of models with InferSent transferring to new target datasets, where the models are trained on SNLI and tested on the target datasets.
The average relative difference in percentage with respect to CE loss is -2.36%.
Table 1 provides macro-averaged scores for ICO span prediction at both the token and clinical entity level.
The F1 score for clinical entities in ICO span prediction is 0.67, the Precision is 0.55, and the Recall is 0.87.
Table 4 shows the performance for predicting an article's exact MeSH terms using the rule-based system, run on both the automatically extracted spans and the expert-provided test spans.
The precision for predicting an article's exact MeSH terms using the rule-based system is higher for the extracted spans compared to the expert spans.
The NER task in the CoNLL dataset has a higher complexity compared to the other tasks.
The Sentiment Analysis task in the MPQA dataset has a high relative ambiguity compared to the other tasks.
BERT outperforms both random and GloVe embeddings on the GLUE task.
BERT outperforms random embeddings on the NER task and requires 16 times less data to achieve better performance.
The table shows the performance of BERT, random, and GloVe embeddings across four linguistic categories defined by the GLUE diagnostic task.
The overall performance of BERT is higher than that of random and GloVe embeddings.
Sent. (SST) has the highest complexity among all tasks.
Sent. (MPQA) has the highest unseen prevalence among all tasks.
The table shows the top-1 validation accuracy of ImageNet/ResNet-50 training using different optimizers.
The momentum optimizer has the highest accuracy of 0.7520.
Lamb with a batch size of 512 achieves an F1 score of 91.752 on the dev set and takes 82.8 hours to train.
Lars stops scaling at the batch size of 16K.
Lamb achieves a higher performance (F1 score) than Lars for all the batch sizes.
The Lamb optimizer achieves the highest test accuracy among all the optimizers.
The AdamW optimizer outperforms the Adagrad optimizer in terms of test accuracy.
The experiment used five different optimizers: Momentum, Addgrad, Adam, AdamW, and Lamb.
The AdamW optimizer achieves the highest average accuracy over 5 runs with a value of 0.9945.
The table shows the tuning information of AdamW.
The F1 score on the dev set varies for different combinations of batch size, warmup steps, and learning rate.
The highest validation accuracy achieved is 0.6604.
The crowd annotated more instances in the Q&A domain compared to experts 1 and experts 3.
The number of instances annotated by the crowd and experts 1 and experts 3 is the same in the Debate domain.
The total number of instances annotated by experts and the crowd in the Q&A domain is 2,085, in the Debate domain is 2,100, and in the Review domain is 1,100.
The number of instances annotated by experts and the crowd in the Q&A domain for training is 1,109, in the Debate domain is 1,093, and in the Review domain is 700.
The agreement between expert and crowd annotations is higher for cognitive aspects than for reasoning aspects.
The agreement between expert and crowd annotations is higher for effectiveness aspects in Q&A.
Table 2 shows the tagging accuracy on various test set varieties, including domains, languages, and age groups.
The tagging accuracy for the Portuguese language using the Bilty model with word and character embeddings is 47.33.
The table compares the performance of single-task learning (STL) and multi-task learning (MTL) frameworks for sentiment and emotion tasks.
The MTL framework outperforms the STL framework in terms of F1-score and accuracy for sentiment and emotion tasks when using both text and visual features.
The total number of posts in the Reddit dataset is 3239.
The emotion with the highest number of posts in the flickr dataset is "contentment" with 2389 posts.
The total number of posts in the flickr dataset is 9337.
The Common space fusion model achieves the highest accuracy on both the Reddit and Flickr datasets.
The Image-based classifier model achieves a lower F-Macro score on the Reddit dataset compared to the Flickr dataset.
The accuracy of the single-input classifier is higher for the "reddit image" modality compared to the "reddit text" modality.
The accuracy of the joint fusion is lower for the "flickr image" modality compared to the "flickr text" modality.
Table 3 presents the results of domain adaptation on three tasks: CWS, POS, and NER.
The method "FGKF" achieves the highest F-score for the CWS task.
Table 4 shows the results of baselines and fine-grained knowledge fusion methods on CWS.
The target-only method achieves the highest performance on the Zhuxian F dataset.
The table presents the results of four different methods on the Twitter test set.
The NER performance is stronger when using strongly relevant elements compared to weakly relevant elements on the Twitter test set.
Table 7 shows the ablation results of the Twitter test set.
Removing shared embeddings results in a decrease of 0.58 in POS F-score and a decrease of 1.34 in NER F-score.
The macro-averaged LAS F1 score for UDPipe 1.2 on all 81 test sets is 69.528.
The system rank for MQuni, excluding the 4 surprise language test sets, is 10.
The proposed system L3 outperforms all the single systems and ensembled systems on the microblog dataset.
The proposed system E1 outperforms all the single systems and ensembled systems on the news dataset.
The proposed method, DistillCombined, achieves higher test denotation accuracy than the supervised method.
The domain "Calendar" has the lowest test denotation accuracy, while the domain "Housing" has the highest test denotation accuracy.
The empathy score for "leukemia" is 6.90.
The distress score for "somewhere" is 1.05.
The highest faithfulness scores are achieved with the "Filtered+L [ITALIC] f" method for all three values of K.
The faithfulness scores improve as we move from the "Random" training explanations to the "Filtered" explanations, and further improve when using "Filtered+L [ITALIC] f" method for all three values of K.
Using justification improves the BLEU-4 score and the Visual EMD score compared to not using justification.
Using explanation improves the ROUGE-L score and the CIDEr score compared to not using explanation.
The accuracy of CNN+B1 is lower than the accuracy of CNN for all tasks (MR, SUBJ, CR, TREC).
The accuracy of CNN+MCFA is higher than the accuracy of CNN+B2 for the CR task.
The table shows the diagnostic intelligibility and naturalness rates for different cases, including the baseline and with PSF and WRF techniques.
The Baseline model performs better on the Common Test Set compared to the Complex Test Set.
The table presents the ablation study results of question answering performances in the development set of HotpotQA in different settings.
The performance of the model is better when using only gold paragraphs for question answering in the HotpotQA dataset.
Table 1 shows the performance comparison on the private test set of HotpotQA in the distractor setting.
DFGN(Ours) has a higher F1 score than the Baseline Model.
The MT-DNN model performs better than the BERT model on the MRPC test dataset.
The Reptile model achieves the highest score on the RTE test dataset.
The Reptile model is used for all the experiments in the table.
The Reptile model achieves the highest score in the "STS-B" task when the number of update steps is 5 and the inner learning rate α is 1e-3.
Table 5 presents the interaction between POS tagging and dependency parsing on the Chinese Penn Treebank development set.
When both interaction tags are enabled, the LAS (labeled attachment score) is 81.76.
The method "this work (Joint)" achieves the highest scores in all metrics for both PTB and CTB5 datasets.
The "Joint models" consistently outperform the "Pipeline models" in the CTB5 UAS metric.
The "MRR" metric measures the Mean Reciprocal Rank and the "P@10" metric measures the Precision at 10.
Approach (iii) with SKLD + normalization has the highest MRR and P@10 scores for both translation directions.
The values in the "Overall" column represent the overall perplexity on the testing set for different methods and settings.
The "1-grams + AVR" feature set outperforms the other feature sets in terms of IG, TU, and TW.
The "1-grams + AVR" feature set has the highest performance in terms of the TW metric.
IG has the lowest average number of words and emojis compared to TU and TW.
TW has the highest average number of tags.
The feature set "combination + VSF" has higher values for "IG" and "TU" compared to "TW".
The "IG" feature set has a higher value for "subjectivity" compared to "TU" and "TW".
The table compares the performance of different machine translation models.
The table shows the BLEU scores of different machine translation models.
Table 5 shows the BLEU score of transformer-large models trained with different regularization methods.
The combination of different regularization methods achieves the highest BLEU score.
The "Scheduled DropHead" model has the highest BLEU score of 29.4.
The "Anti-curriculum DropHead" model has the lowest BLEU score of 28.7.
The offense rate for White individuals is slightly higher than the offense rate for Black individuals.
The percentage of positive sentiment is slightly higher for Black individuals than for White individuals.
The table provides results for the task 3 model.
The precision for task 1 on the training data is 0.99.
Table 2 provides results for task 1 and 2 models.
The Task 1 F1 score for the test dataset is 0.84.
The "trigger" role has the highest F1 score on the task 3 dev set.
The "participant" role has precision, recall, and F1 scores above 0.6 on the task 3 dev set.
Table 3 compares human evaluation in terms of informativeness, coherence, and overall ranking.
The overall ranking of RNES with coherence is lower than its informativeness and coherence scores.
The table provides performance comparison on the CNN/Daily Mail test set for different models.
The RNES model without coherence performs better than the Lead-3 and NES models on the CNN/Daily Mail test set.
The NMI value for SpCoA is higher than the NMI value for SpCoSLAM.
The EAR value for SpCoA is lower than the EAR value for SpCoSLAM.
The highest Macro F1 score is achieved when both Speaker and Tags metadata are included.
The highest Micro F1 score is achieved when no metadata is included.
Table 10 provides information about the number of instances and labels per domain.
Each domain in Table 10 is associated with a set of labels.
BART-Large outperforms all other models in terms of METEOR, chrF++, BS (F1), and BT on the AGENDA test set.
T5-Small performs the worst in terms of BT on the AGENDA test set.
BART-Large achieves the highest scores for all metrics compared to other models.
BART-Base has a higher BLEU score than T5-Small and T5-Base.
T5-Large achieves the highest BLEU A score and METEOR A score among all the models on the WebNLG test set.
BART-Large achieves a higher METEOR S score than T5-Large on the seen partition of the WebNLG test set.
BART-TSS outperforms other models in terms of BLEU, METEOR, and chrF++ scores on the AMR17 dataset.
BART-TSS performs better than T5-TSS in terms of the chrF++ score on the AGENDA dataset.
The "RBF" classifier performs better than the other classifiers in both the GloVe and Word2vec setups.
The "Word2vec MLP" setup performs better than the other setups in both the GloVe and Word2vec setups.
There are four different setups in the table.
The combination of linear classifiers (LR, LIN) with the "Mult" operation consistently improves performance.
The NMT model outperforms the SS-NMT model in terms of BLEU scores for all language pairs and all training epochs.
The en-fr language pair has a larger number of training pairs compared to the other language pairs.
The SS-NMT L1-L2 system outperforms the SS-NMT L2-L1 system in terms of BLEU scores for all language pairs.
The SOTA L1-L2 system outperforms the SOTA L2-L1 system in terms of METEOR scores for all language pairs.
The multitask model outperforms the pipeline baselines in terms of BLEU score.
The multitask model outperforms the pipeline baselines in terms of SARI score.
The table provides adjacency ARI accuracy within grade level for two different adjacency levels, 1 and 2.
The multitask model performs better than the pipeline model in capturing the target grade when the difference between the source and target grade is greater than 3.
Table 7 presents data ablation experiments showing the impact of different types of training examples on a multi-task model.
The highest BLEU score in the evaluation metrics column is 22.75, which corresponds to the "Newsela S" row.
The table provides evaluation results for the tasks of English Simplification and Machine Translation.
The "Translate and Simplify" approach improves the performance for both the English Simplification and Machine Translation tasks.
The model "JESSI-B" achieves the highest F-Score of 87.31 in Subtask B.
The model "- DomAdv" achieves the lowest F-Score of 47.48 in Subtask B.
OleNet has the highest rank among the models in Subtask A.
OleNet has the highest F-Score among the models in Subtask A.
The model "NTUA-ISLab" achieves the highest F-Score in Subtask B.
The model "JESSI-B" is marked with an asterisk in the table.
Table 1 provides attribute values for the texts of Section 4.2.
The attribute values for Text #1 are provided in Table 1.
The dataset "MatInf-Summ" is in the "Health" domain.
The dataset "CNN / DM cnndm" is in English.
The method "MTF-S2S" outperforms the other baseline methods in all three metrics on MatInf-QA.
None of the baseline methods achieve scores that reach the upper bound in any of the three metrics on MatInf-QA.
BERTabs achieves the highest scores in all three metrics (R-1, R-2, R-L) on the LCSTS dataset.
Global Encoding achieves the highest scores in all three metrics (R-1, R-2, R-L) on the LCSTS dataset.
The table shows precision, recall, and F1 scores on the binary classification of pseudo-alignments on the held-out test set.
The F1 score for the "GB" model in the "all" category is 97.6.
Table V compares the Pearson correlation obtained by context vectors at different epochs of the training and word embeddings on the test set of the "Semantic Textual Similarity Task" at SemEval 2017.
As we add more components to the model, the joint goal and slot accuracy increases.
The accuracy of the joint goal and slot is consistently higher than the accuracy of the baseline model.
Table 5 provides F1 Score and Average Slot Accuracy for different domains.
The F1 Score for the Restaurant domain is 0.94.
Three different types of features are used in TDNN training.
The DB index decreases as we move from State 0 to State 2 for all three types of features.
The system "ecnuc" achieves the highest F1 score in both the development and test sets among all systems.
The system "gtnlp" (this paper) achieves an F1 score of 0.639 in the blind dataset.
The hyperparameter "lstm1" has a range of 64-320 and the best value is 259.
The dropout hyperparameter "dropout2" has a range of 0-0.9 and the best value is 0.57.
BERT outperforms other approaches in terms of MAP score for the phrase query subset and performs better than all approaches except Prob. CLIR for the entire query set.
Dot-Product performs the worst among all approaches in terms of MAP score for both the phrase query subset and the entire query set.
The table presents the performance of two different approaches - BERT and Dot-Product.
BERT achieves an accuracy of 95.3%.
The table provides MQWV scores on the Lithuanian analysis and development sets for different approaches.
Table 5 shows the development set accuracy of varying models based on the hidden size (H) and the number of LSTM layers (L).
The model with a hidden size (H) of 64 and 2 LSTM layers (L) achieves the highest development set accuracy.
The "Chinese BERT" system achieves the highest test accuracy among all the Chinese g2p systems.
The "Majority vote" system achieves the lowest test accuracy among all the Chinese g2p systems.
The model "Our baseline" performs equally well in terms of precision, recall, and F1-score for English, but performs differently for Chinese.
The model "+ AP + BERT" outperforms the model "+ AP + ELMo" in terms of F1-score for Chinese.
The vector size parameter is the same for both samples.
The sample parameter is different for the two samples.
The number of instances in the validation set of the "Tratz-fine" dataset is smaller than the number of instances in the training set.
The number of instances in the test set of the "Tratz-coarse" dataset is smaller than the number of instances in the training set.
The method "DBS" outperforms the method "BS" in all metrics on the COCO image captioning task.
The method "DBS" outperforms the method "BS" in all metrics on the PASCAL-50S image captioning task.
The method "DBS" outperforms the other methods in all the metrics.
The scores for all methods are higher in the "Distinct n-Grams 4" metric compared to the "Distinct n-Grams 1" metric.
The Wait-3 policy has the highest BLEU score on the Test set.
The Wait-1 policy has the lowest WER score on the Dev set.
As the value of λ increases, the Dev WER and Test BLEU scores increase, while the Dev BLEU and Test WER scores decrease.
The best performance is achieved when λ is 0.3, as it has the highest Dev BLEU and Test BLEU scores.
The Siamese model achieves the highest F1-score for the Positive case.
The LSTM model has the lowest F1-score for the Negative case.
The F1 scores for the "Full" and "w/o RSD" cases are higher than the F1 scores for the "w/o Tokenization" and "Tokenization" cases.
The accuracy for the "Full" case is higher than the accuracy for the "w/o RSD" case.
The combinations CJHIF + Chemical.AI-Real-1 and USPTO + Chemical.AI-Real-1 have the highest precision scores for both positive and negative cases.
The combination CJHIF + Chemical.AI-Rule has the highest F1-score for the negative case.
MDAM has the highest test score among all the models listed in the table.
The test score of MDAM is 41.41.
MDAM has the highest test score compared to other models.
MDAM has a higher test score than MDAM-EarlyFusion.
The table presents the results of different methods on 1,000 samples and full training sets.
MixDA achieves the highest scores in the ASC@full laptop and AE@full rest categories.
The table presents an ablation analysis of MixMatchNL, evaluating the performance of different methods.
The highest score for ASC on the laptop dataset is 80.47.
The table presents BLEU scores for low-resource and high-resource settings, including the scores of bert-fused NMT models and Hictl∗ after pre-training.
Hictl∗ achieves higher BLEU scores than Hictl in all language pairs.
RoBERTa performs better on the GLUE dev dataset on average compared to BERT Large, XLNet Large, and XLM-R Large.
The XLM-R Base model has fewer parameters than the XLM-R Large model.
The "ess-Ad. Multi." system performs better than the baseline and multilingual systems for the "eng–ess" language pair.
The "esu-Ad. Multi." system performs better than the baseline and multilingual systems for the "eng–esu" language pair.
The highest BLEU score is achieved with the "iku segmentation" in the "5000 BPE" row.
The highest chrF score is achieved with the "iku segmentation" in the "5000 BPE" row.
The "Transf. BPE" vocabulary performs better than the "RNN BPE" vocabulary in the "ess→eng" machine translation experiment.
The combination of FST and BPE performs better than FST alone in the "grn→spa" machine translation experiment.
The BLEU score for the "eng–ess" language pair in the baseline system is 4.4.
The BLEU score for the "eng–esu" language pair in the system fine-tuned on esu-Ad. Multi. is 6.0.
The language "spa" has the highest word-level perplexity for all three encoding methods.
The encoding method "Character" has the lowest word-level perplexity for the language "eng".
The language "eng" has the lowest perplexity for the Morfessor, BPE (V=500), and BPE (V=5k) methods.
The language "ess" has the highest perplexity for the Morfessor method.
Table 6.1 provides information about the WER and CER for different learning rates in Crow speech recognition.
Lower learning rates result in lower WER in Crow speech recognition.
The majority class accuracy is consistent across all three regimes.
The accuracy of the Information Clusters (IC) model improves when trigrams are included in addition to unigrams and bigrams.
The table provides information about the number of participants and the mean MET English score by native language group.
There are 37 participants in the English language group.
The STKRL (LSTM+ATT) model performs the best in predicting the head in the N-to-N relation category.
The TransE model performs better in predicting the tail in the 1-to-1 relation category compared to other models.
The method "STKRL (LSTM+ATT)" achieves the highest score in the "FB15K" evaluation.
The method "STKRL (RNN+P+ATT)" outperforms the method "STKRL (RNN+ATT)" in the evaluation.
The Hits@10 score for STKRL (RNN+P+ATT) is 52.2%.
The mean score for TransE is 215.
The test accuracy for the SSSAE method is 67.03%.
The test accuracy for the LP method is 65.47%.
The test accuracy of the SSSAE method on the TIMIT dataset increases as the value of α increases.
The test accuracy of the neural network method on the TIMIT dataset is consistently higher than the validation accuracy.
The table compares the performance of two different methods: Variant w/o {S, att_0} and Full Model.
The BLEU-4 score for the Full Model is 0.207.
The table shows the results of different models and initialization setups on the WikiSplit dataset.
The roberta2gpt model achieved an SARI score of 87.1% on the DiscoFuse dataset.
The Transformer model achieved a BLEU-4 score of 28.1 on the newstest2014 English to German translation task.
The bert2bert model achieved a BLEU-4 score of 34.6 on the newstest2016 German to English translation task.
The Transformer model achieved a BLEU-4 score of 28.1 on the newstest2014 English to German translation task.
The bert2bert model achieved a BLEU-4 score of 34.6 on the newstest2016 German to English translation task.
The SARI scores for bertShare and gpt are lower than the SARI score for embeddings from checkpoint on the DiscoFuse dataset.
Adding task specific SentencePieces or pre-training SentencePieces does not affect the SARI score for bertShare on the DiscoFuse dataset.
The questions in the table are related to defect reports and improvement requests.
The inter-annotator agreement is higher for the question "Does this review contain a defect report?" compared to the question "Does this review contain an improvement request?".
The table presents the in-domain results of various models on the CTB6, PKU, and MSR datasets.
The L-SAN + CRF + BERT model outperforms other models on the CTB6, PKU, and MSR datasets.
The F1 score does not change with different numbers of layers.
The F1 score tends to increase with an increase in the number of heads, up to a certain point, and then may decrease.
The models "L-SAN + CRF + BERT" and "L-SAN + CRF + BERT + t" outperform the models "Liu and Zhang (2012)" and "Qiu and Zhang (2015)" in all three domains (ZX, FR, DL).
The model "L-SAN + CRF + BERT + t_b" achieves the highest performance among all models listed in the table in all three domains (ZX, FR, DL).
The word "唐三" has the highest frequency with a count of 273.
All the proper names and person names have a precision score of 1.00.
The C Score is positively correlated with the dialogue quantity.
The mean fine-tuning epochs are positively correlated with the BLEU score.
The "Transformer/CNN-F" method achieves the highest Persona BLEU score on Weibo.
The "MAML" method achieves the highest FewRel accuracy.
The table shows the alignment distribution entropy for selected de-en models under different conditions.
The alignment distribution entropy values for each model increase as the value of σ increases under the "force" condition.
Table 7 compares the performance of different methods in a setting without opinion term labels.
The method "IMN−d" achieves the highest F1-a score among all the methods compared in the table.
Table 7 compares the performance of different methods in a setting without opinion term labels.
The method "IMN−d" achieves the highest F1-a score among all the methods compared in the table.
The table shows the F1-I scores of different model variants.
The F1-I scores generally increase as we move down the table.
The F1 scores increase as the T value increases for all three datasets.
D1 has the highest F1 score among the three datasets.
Table 7.2 shows the phoneme error rates (%) for end-to-end training from random initialization comparing CTC and segmental models trained with marginal log loss.
The phoneme error rate for the regular LSTM model on the dev set is 17.4.
The proposed system has two passes with error rates of 33.6% and 21.5% on the dev set.
The proposed method has a lower total decoding time compared to the baseline method.
The proposed method has a lower total overall time compared to the baseline method.
The total decoding time for the proposed system is 2.8 hours.
The total overall time for the baseline system is 109.5 hours.
Table 5.1 compares the frame error rates (FER) and phoneme error rates (PER) of the CNN and LSTM encoder on the development set.
The frame error rate (FER) for the CNN on the development set is 22.3% and the phoneme error rate (PER) is 22.2%.
For the "FC" weight function, the loss used is "hinge".
The phoneme error rate for the "FC" weight function with "hinge" loss on the 2s dev set is 19.7%.
The frame error rate is the lowest for the FC loss compared to the hinge and MLL losses.
The frame-wise cross entropy is highest for the MLL loss compared to the hinge and FC losses.
Table 5.4 compares the phoneme error rates (%) on the development set for multi-stage training followed by end-to-end fine-tuning (2s+ft) and end-to-end training from random initialization (e2e).
The phoneme error rate (%) for the weight function "FC" and loss "hinge" is 16.9.
The regular LSTM takes longer to train for one epoch compared to the pyramid LSTM.
Using the hinge loss function takes less time for one epoch compared to using the log loss function in the seg FC model.
Table 7.4 compares real-time factors for decoding between CTC and segmental models.
The regular LSTM has a lower real-time factor for decoding compared to the pyramid LSTM, and the seg MLP has a higher real-time factor for decoding compared to both the regular LSTM and pyramid LSTM.
Table 7.5 shows letter error rates (%) for signer-dependent models averaged over ten folds.
The letter error rate for the CTC model on the Andy test set is 7.8% and the average letter error rate on the test sets is 8.8%.
The table presents the letter error rates (%) for signer-independent models averaged over eight folds.
The letter error rate for the two-stage seg FC model on the Andy test dataset is 53.3 (%).
Table 7.7 compares the letter error rates (%) of the CTC and seg FCB models averaged over eight folds.
The average letter error rate for both the CTC and seg FCB models is displayed in the "Avg" column of Table 7.7.
Table 3 shows the average runtime per iteration (in ms) for different models using TensorFlow and a GeForce RTX 2080 Ti GPU on Amazon.
The average runtime per iteration for the CAHAN-SUM-Σ LR model is 37 ms.
The Full MVN has a higher accuracy score than Voting by 8 independent, 1-view MVNs.
The MVN with length-1 horizontal links has a higher accuracy score than the MVN with no horizontal links.
BERT-base with PR (ours) performs better on the SEA test set compared to the original and SDA test sets.
R-Net with Feature-Input has a higher F1 score compared to R-Net with Original and Adv-Training.
The table is an ablation test to investigate the effect of different constraints.
The SDA EM score is lower than the SEA F1 score.
The BLSTM - 1 layer of 500 cells has the highest accuracy in the action detection task for GPSR.
The LSTM - 1 layer of 500 cells has a higher accuracy than the DLSTM - 2 layers of 500 cells in the action detection task for GPSR.
Table 5 provides the performance of our model and the baseline in evidence extraction on the development set in the distractor setting.
The QFE model outperforms the baseline model in precision, recall, and correlation in evidence extraction on the development set in the distractor setting.
Malon (2018) achieved a precision of 92.2% and an F1 score of 64.9%.
The QFE ensemble achieved a recall of 76.3% and an F1 score of 77.7% on the test set.
The METEOR score is the same for the "Exact" and "DAgger" methods, but different for the "lols" and "Nucleus" methods.
The Slot Error is the same for the "Exact" and "DAgger" methods, but different for the "lols" and "Nucleus" methods.
The "Beam" and "FWC" methods outperform the "Greedy" method in terms of BLEU score.
The "FWC" method has the lowest slot error rate compared to the other methods.
The Greedy method has a higher Fluency raw score than the Nucleus method.
The Exact method has a lower Adequacy z-score than the DAgger method.
The fluency scores for the different text sampling methods are: Exact - 49.258, DAgger - 53.593, LOLS - 54.324, Nucleus - 39.762.
The adequacy z-scores for the different text sampling methods are: Exact - 0.012, DAgger - -0.006, LOLS - 0.065, Nucleus - -0.017.
The "S2SChar" model has a different value for the "dropout" hyper-parameter compared to the other models.
The table compares the performance of different models, including HS2S, S2S, Dict1, Dict2, S2SMulti, and S2SSelf.
The table provides descriptions of the method highlights for each model.
The HS2S model is a Hybrid word-char Seq2Seq model.
The F1 score of the S2SBPE model is 28.90.
The term "lol" was correctly normalized 197 times and the term "laughing out loud" was correctly normalized 272 times.
The term "n" appears 335 times and the term "and" appears 40 times.
Table 8 shows OOV words that our secondary character model has normalized correctly or incorrectly.
The confidence of the model's predictions decreases as the words become more misspelled or have additional characters.
Table 8 provides the confidence percentage for each prediction made by our secondary character model.
Table 8 shows the OOV words that our secondary character model has normalized correctly and incorrectly.
The Typed DT-LSTM model has a Pearson's r value of 0.8731.
The Mean vectors model has a Pearson's r value of 0.7577.
Table 3 provides the three most similar sentences retrieved by Typed DT-LSTM from the SICK test set for each query sentence, along with their scores assigned by Dependency Tree-LSTM and Typed DT-LSTM.
The scores for the sentence "The turtle is following the fish" are 4.48 for SDT and 4.81 for STyped−DT.
Table 6 compares the performance of different LSTM models for binary sentiment classification on the SST dataset.
The "Typed DT-LSTM" model has a higher accuracy than the other LSTM models.
The MCAN (FM) model achieves the highest MAP score on the TrecQA (clean) dataset.
The MCAN (FM) model achieves the highest MRR score on the TrecQA (clean) dataset.
MCAN (FM) achieves the highest R10@1 score among all the models.
ARC-II achieves a score of 0.736 for R10@5.
MCAN (FM) has the highest P@1 score among all the models.
MCAN (FM) has the highest MAP score among all the models.
MCAN (FM) achieves the highest MRR and P@1 scores among all the models in the Reply Prediction on Tweets dataset.
AP-BiLSTM achieves a higher MRR score than AP-CNN in the Reply Prediction on Tweets dataset.
Table 5 shows the ablation analysis results on the TrecQA dataset, where different components are removed from the original model.
Removing components from the original model negatively impacts the MAP performance on the TrecQA dataset.
Table 2 compares the performance of the CWS between "The past scheme" and "The future scheme".
The future scheme outperforms the past scheme in terms of F-score.
The model "Tcn-with-CRF" has the highest F-score of 92.34.
The model "Bi-LSTM-with-CRF" has a size of 2080.
The table compares the performance of different models for Chinese Word Segmentation (CWS).
The model presented in the current work achieves a precision of 93.8, recall of 94.4, and F1 score of 94.1 for Chinese Word Segmentation (CWS).
Table 2 presents the clustering results on three datasets using both unsupervised and semi-supervised methods.
The CDAC+ method achieves the highest NMI, ARI, and ACC scores on all three datasets.
The RNN model performs worse than the transformer model on the Dailydialog dataset in terms of WL2 and l-DIMEN scores.
The RNN model with pre-attn performs better than the RNN model with uniform weights on the Movie Dialogs dataset in terms of BLEU score.
Table 3 shows the Word Error Rates (WERs) with or without AE layers.
Model B2 with LAS + AE has a WER of 5.8.
Table 2 shows the WERs (%) of the baseline RNN-T model and deliberation models with different attention setup.
The WER (%) is lower for the deliberation model with both acoustics and text compared to the deliberation models with only acoustics or only text.
The B5 model with LAS and Beam search has the lowest WER (%) in all categories.
The E10 model with + Joint training and Beam search has a lower WER (%) than the E9 model with Deliberation and Beam search.
There are 500 triplets in the Small Train stage of CAIL2019-SCM.
The total number of triplets in all stages of CAIL2019-SCM is 8,964.
Table 7 shows fine-tuning results on open-domain datasets.
The reproduced DrQA model achieves a score of 20.1 on the WebQuestions dataset.
The "Conv BiDAF (5-17-3)" model outperforms the "BiDAF" model in terms of exact match score on the development set.
The "Conv BiDAF (5-17-3)" model has a significantly shorter training time until convergence compared to the "BiDAF" model.
Table 3 compares variants of the BiDAF and DrQA models with different numbers of layers.
The Conv BiDAF (11-51-3 conv layers) model achieves the highest scores for both exact match and F1.
The table provides performance results for the Wikipedia and Web datasets.
Transfer Learning (M7) achieves the highest accuracy among all setups.
The setups evaluated in the table include BiLSTM (M1), Semi-supervised Training (M6), Transfer Learning (M5), and Transfer Learning (M7).
The "BiLSTM with kNN" model achieves the highest accuracy among all models on most datasets.
The "char-CRNN" model achieves the highest accuracy on the Yelp F. dataset.
Adding derivatives to the i-vectors decreases the WER from 14.0% to 13.9%.
Using MFCC as UBM features results in a WER of 14.3%.
The Square root normalization method has the lowest WER (%) for Dimension 200.
The i-vector normalization method has the highest WER (%) for Dimension 50.
The use of affine transformation (AT) improves the performance, as indicated by the lower WERs when "yes" is selected in the "AT" column.
Using LSTM as the language model results in better performance, as indicated by the lower WERs when "LSTM" is selected in the "LM" column.
Table 9 shows the F-scores of the baseline and both-retrained models relative to role types on two data sets using the PCFGLA-parser-based system.
The both-retrained models have higher F-scores than the baseline models for all role types and data sets.
The inter-annotator agreement is higher for L1 than for L2 in each language.
The PCFGLA-parser-based SRL system achieves an F-score of 73.81 on the English L1 data.
The neural syntax-agnostic SRL system has a decrease in F-score of 5.41 on the overall data compared to the syntax-based SRL system.
Table 7 displays the accuracies of different PCFGLA-parser-based models on two test data sets.
The F-score for the L1 model with both retrained components is 74.83.
Table 8 provides accuracies of different neural-parser-based models on two test datasets.
The accuracy for the L1 model with classifier retraining is 82.08.
The dataset "Flickr8K Hodosh2013a" has judgments associated with the images.
The dataset "Abstract Scenes Zitnick2013a" has complete object labels for the images.
The table compares the objective results of the proposed techniques versus the benchmark approach.
The table provides objective results for MCD, BAP, F0 RMSE, and V/UV error rate in Telugu.
The Uni-Grapheme-TTS method achieves an objective result of 4.97 in the combined average.
Uni-Grapheme-TTS has the lowest MCD score compared to Multi-Grapheme-TTS and G2P-TTS in both the first and second sets of values.
The Benchmark approach has the lowest V/UV error score compared to Uni-Grapheme-TTS, Multi-Grapheme-TTS, and G2P-TTS in both the first and second sets of values.
The total number of utterances for each language in the AP16-OL7 dataset is different.
The number of speakers and utterances per speaker in the AP17-OL3 dataset are the same for both the train/dev and test sets.
The system with "+Num&Gen Rescoring" has the highest UN Result in the Large set.
The system with "Baseline" has a UN Result of 55.29 in the Small set.
The table provides F1 scores for the Shared Entity and Shared Relation tasks.
BERT achieves an F1 score of 0.850 on the Shared Entity task.
DrKIT (strong sup.) outperforms DrKIT (e2e) for all hop levels in MetaQA.
DrKIT (strong sup.) performs better than DrKIT (e2e) for the 3-hop sub-task in MetaQA.
The model "DrKIT (e2e)" outperforms other models in all three hop levels.
The performance decreases as the hop level increases in WikiData.
The "DrKIT (Combined)" model has the highest accuracy at all levels (@2, @5, @10, @20).
The model "+Golden Ret" has the highest EM and F1 scores compared to the other models.
The model "+EC IR" has a higher EM and F1 score compared to the baseline model.
The HGN system achieves the highest Joint EM score of 35.63.
The DrKIT + BERT system has the lowest number of calls to BERT with a value of 1.2.
The table shows the labeled attachment score on 13 UD corpora for BERT DepTr+CH and BERT G2GTr+H models for different languages.
The average labeled attachment score for the BERT G2GTr+H model is higher than the average labeled attachment score for the BERT DepTr+CH model.
The models in the table are categorized into Transition-based and Seq2Seq-based.
The BERT G2GTr+H model achieves the highest UAS and LAS scores among all the models.
The Consonant Map Woodward has a lower Confusion Factor than the Vowel Map Jeffers.
The Vowel Map Jeffers has a higher Mean Confusion value than the Consonant Map Woodward.
The ESIM model outperforms the DAM model on all SNLI and MultiNLI datasets.
The DIIN model outperforms the ESIM model on the SNLI Full dataset.
The test loss of RT-L3 is lower than its evaluation loss.
The evaluation loss of the +tanh SDU model is 4.50.
The "+tanh SDU" model has the lowest evaluation loss and evaluation perplexity compared to the other models.
The "+ [ITALIC] σ SDU" model has a higher test loss and test perplexity compared to the "T-L3" model.
The "Ablation study" model with the "+tanh L1-2" setting has the lowest evaluation loss.
The "L12-XL" model has the lowest test bpc.
Table 7 presents the relation prediction results on FB15K dataset.
RPJE has the lowest MR raw and MR filtered values among all the models.
Table 5 presents the results of different entity prediction models on FB15K dataset.
DPTransE model achieves the highest Hits@ 10(%) filtered score among all the entity prediction models.
The RPJE model outperforms all other models in entity prediction on FB15K dataset.
The TransG model performs the best in tail prediction with N-N mapping on FB15K dataset.
Table 8 shows the entity prediction results on FB15K-237 for different models.
The RPJE model achieves the highest MRR and Hits@10(%) values among all the models on FB15K-237.
Table 9 provides entity prediction results on WN18 and NELL-995.
RPJE achieves the highest MRR and Hits@10 scores among all models on both WN18 and NELL-995.
The architecture used for experiments 1.3 and 1.6 is "GRU+VGG19".
The highest value for "image→text R@1" is 38.4.
The correlation scores for the different methods vary.
Different methods are used for the correlations.
Table 3 provides direct assessment scores for the first hundred sentences of the WMT19 German-English test set.
The human assessment score for adequacy in Table 3 is [BOLD] 95.2.
The diversity score for the method "Beam" is consistently lower than the diversity score for the method "Random" across all values of n.
The diversity scores for all methods are higher for n=20 than for n=2.
The method "Beam" achieves a sentenceBleu score of 0.317 on the zh-en dataset.
The method "TreeLstm [ITALIC] s" achieves a sentenceBleu score of 0.281 on the kk-en dataset.
The inter-annotator agreement is highest for the German\rightarrowEnglish language pair.
There are 3 annotations for the Russian\rightarrowEnglish and Chinese\rightarrowEnglish language pairs.
The table shows the results of different experiments on WMT14 dataset using Glove embeddings.
The table presents the results of 8 different experiments on WMT14 dataset.
The table shows the accuracy and AC1 agreement for different language pairs translated into English using the ELMo-based model.
The average accuracy and AC1 agreement for all language pairs is 81.35.
Table 4 presents the results of few-shot experiments with varying numbers of target-language examples.
The mBERT model shows a higher score and a higher improvement (Δ) in performance with the "Longest" sampling method compared to the "Random" sampling method in the DEP task.
Our model achieves a slightly lower recall in entity type prediction compared to the model without multi.
Our model performs better than the model without multi-task learning.
Our model performs better than the model without multi-task learning in answering direct simple questions.
Table 6 shows the results on the PubMed validation set with three different models: +our hierarchical-add+fine-tune-bert+s=1.5, lead, and undirected.
The "hierarchy-add (ours)" model achieves the highest ROUGE-1 score of [BOLD] 43.42.
Table 4 provides human evaluation results based on 20 sampled reference summaries with 281 system summary sentences.
HipoRank performs better than PACSUM in terms of both % of YES and content-coverage.
DAFE outperforms other approaches in terms of translation accuracy in all domains.
The combination of Back-DAFE and DAFE achieves the highest translation accuracy in all domains.
Figure 2 shows the percentage accuracy scores for En→It translation with different training sizes and regularization techniques.
The accuracy scores for En→It translation increase as the training size increases without regularization using the Generalized Cross-validation (GC) technique.
"Hop GCN2 = 3" outperforms all other methods in all language pairs and evaluation metrics.
The "BASELINE" method performs better than the "Hop GCN2 = 1" method in the EN-ZH language pair at rank 1.
The FST size for the LM with 3-gram is the largest.
The WER for the LM with 3-gram and 3-gram, 1e-10 is higher than the WER for the LM with 3-gram, 3e-10.
Our Model has a higher accuracy than TST in the "Neg. to Pos." category.
Our Model has a lower accuracy than TST in the "Pos. to Neg." category.
"Joint-NN++" achieves the highest accuracy scores in the "dev" and "test" sets for all languages except for "fr" in the "test" set.
"Joint-NN" achieves a higher accuracy score than "Binary Logistic Regression" and "HMCNF" in the "dev en" set.
The accuracy obtained from SVM decreases as the step number increases.
The number of texts selected via Active Learning increases as the step number increases.
The table shows the performance of different models on video + text and text only data.
Our model outperforms the baseline on all metrics for the video + text data.
The Input-aware model achieves the highest F1 score for activity recognition.
The Input-agnostic model achieves the highest F1 score for entity recognition.
The "Baseline" row represents the baseline performance.
The "Both" row has the highest F-score.
The similarity score between "man" and "woman" is the highest among all pairs of content words.
The similarity score between "fish" and "needs" is the lowest among all pairs of content words.
Table 3 shows the performance obtained on augmenting word embedding features to features from four prior works, for four word embeddings.
The "Dependency Weights" method achieves the highest F-score among the four word embedding methods.
Table 1 shows the Micro F1 scores of two NER models on two test sets.
The Hybrid NER model has a higher Micro F1 score than the i2b2 NER model on both test sets.
The F1 score for the "Grammatical" category is higher than the F1 score for the "Ungrammatical" category in Task 1.
The Quality score for the "Correct Meaning" category is higher than the Quality score for the "Wrong Meaning" category in Task 2.
Table 2 presents the main results for Task 1 and Task 2, including the performance of different models in each task.
The Coherency Rank model performs the best in Task 1 with a F1 score of [BOLD] 3.00±0.12.
The slot "phone" has the highest accuracy among all the slots in the test set.
The slot "count" has the highest number of errors among all the slots in the test set.
The model "binmt" has a higher accuracy score than the model "mass".
The model "binmt" has a higher fluency score than the model "mass".
The BLEU score tends to increase as the training size increases.
The binmt model performs better than the scratch and mass models in terms of BLEU score.
The "binmt" model has the highest BLEU score among the models in the table.
The "baseline-mass" model has the lowest SER score among the models in the table.
The best-performing method for Tatoeba deu→spa dataset is ABSent.
The precision@5 value for Baseline C on Europarl deu→spa dataset is 34.2.
The ABSent method consistently outperforms all other methods in terms of precision@1 and precision@5 for all language pairs.
The BERT method performs better than the seq2seq NMT, fairseq NMT, and Conditional GAN methods in terms of precision@1 and precision@5 for all language pairs.
The precision@1 values for the Tatoeba dataset are generally higher than the precision@1 values for the Europarl dataset.
The precision@1 values for the "A" setting are generally higher than the precision@1 values for the "U" setting.
Table 1 represents the results of rewriting quality.
The highest rewriting quality score is achieved with CRN + RL.
The addition of CRN and RL to DAM in the single-turn response selection model improves the performance in terms of MAP, MRR, P@1, R10@1, R10@2, and R10@5.
The use of multiple turns in the response selection model (DAM) improves the performance in terms of R10@5 compared to using a single turn (DAM [ITALIC] single).
The table shows the number of stimuli, concepts, and features for different languages.
The number of stimuli varies for different languages.
The "ST-NAT(ours)" model has a lower real-time factor (RTF) compared to the other models.
The "ST-NAT+LM(ours)" model has lower performance (DEV and TEST) compared to the other models.
Table 1 compares the model with different CTC weights α and trigger thresholds β.
As the CTC weight α increases, the CER(%) on the test set decreases.
Table 2 compares the effects of different trigger thresholds on the inference speed.
As the threshold increases, the time spent on decoding the test set decreases.
Table 3 presents a comparison on the CoNLL 2009 English test set, including results of other models, our models, and the best published models.
The Sem.-only model achieves an F1 score of 73.87 on the Brown dataset.
The LAS score of the Joint (this work) model is slightly lower than the LAS score of the Joint (2008) model.
The Sem. F1 score of the Joint (this work) model is lower than the Sem. F1 score of the Joint (2008) model.
The average macro F1 score for all languages on the multilingual CoNLL 2009 test set is 82.64.
The Joint model outperforms the individual models for all languages on the multilingual CoNLL 2009 test set.
The length of the grounded documents varies across different cases.
The performance of DGMN varies across different lengths of grounded documents.
The ConvNet model performs better when selecting a fixed number of sentences compared to when selecting up to a certain percentage of sentences.
The overall accuracy of the model is higher when using the full reviews compared to when selecting up to 50% of sentences.
Table 3 shows the BLEU scores for different models on the WMT data for translation DE↔EN.
The Transformer model achieved a BLEU score of 40.1 for the DE-EN translation in 2018.
Table 5 shows the mean oscillations for SUBNUM variations.
The BLEU score for RNN is higher than the BLEU score for Transformer.
BioBERT achieves the highest Exact F1 score on the i2b2 2006 task.
The Bio+Discharge Summary BERT model achieves the highest Exact F1 score on the i2b2 2010 task.
The accuracy of the IMDB dataset is affected by the different weighted loss values (λ1, λ2, and λ3).
The RMSE of the Yelp 2014 dataset is affected by the different weighted loss values (λ1, λ2, and λ3).
The HUAPA model outperforms all other models in terms of accuracy and RMSE for sentiment classification on the IMDB, Yelp 2013, and Yelp 2014 datasets.
The NSC+LA(BiLSTM) model performs better when user and product information is included compared to when it is not included for sentiment classification on all three datasets (IMDB, Yelp 2013, and Yelp 2014).
Table 3 describes the effect of user attention and product attention on different models.
The HUAPA model achieves the highest accuracy and lowest RMSE scores among all the models.
The performance of SkipNER Feature using all words is 81.7.
SkipNER Feature achieves the highest performance when using POS tags as the feature subset for all entity types.
The table provides an example of BIO tagging for a sample of MUC-7.
In the BIO tagging, the word "acquiring" is tagged as "O".
The "Baseline + SkipNER" system achieves the highest F-score among all the NER methods.
The "Baseline + SkipNER" system outperforms all other NER methods in terms of F-score.
The reranking models show improved performance in reducing the Word Error Rate (WER) as compared to the ASR 1-best and Oracle 1-best models.
Adding p-dRBM to SLP improves the performance of the reranking model in reducing the Word Error Rate (WER).
The "LSTM+TA+KBA" model has the highest accuracy in aspect categorization strict accuracy.
The "Sentic LSTM + TA + SA" model has the highest macro F1 score in aspect categorization.
The table represents the Hardest (AUCμ,w) scores for different words.
The word "it" has a score of 56.2 for Hardest (AUCμ,w).
The AT+Video model shows a statistically significant improvement in ROUGE-L score compared to the other models.
The ASC model has the lowest BLEU-4 score among all the models.
The "Self-B1" score is consistently higher than the "BLEU1" score for all beam sizes.
The "QAcontext" score is higher than the "QAsource" score for all beam sizes.
Table 6 compares the BLEU scores on the test set of the IWSLT14 De-En task for different task windows.
The highest BLEU score on the test set of the IWSLT14 De-En task is [BOLD] 31.79, achieved with a task window of "w=2" in the TCL-NAT (NPD 9) model.
Table 4 shows the BLEU scores of different models on the IWSLT14 De-En, IWSLT16 En-De, WMT14 De-En, and WMT14 En-De tasks.
The ENAT model achieves a speedup of 25.3 times compared to the Autoregressive Models (AT Teachers).
Table 4 shows the performance of different models under different conditions.
The "Dual-Attention" model has the highest accuracy for both SGN and EQA test sets on both Easy and Hard difficulties.
The "Concat" model has higher accuracy for SGN test sets compared to the "GA" model for both Easy and Hard difficulties.
All instructions in the table involve going to a specific object or pillar.
Instructions involving the "red" object or pillar are harder to follow compared to instructions involving the "pillar" alone.
The "Dual-Attention" model achieves the highest accuracy on both the SGN and EQA test sets.
The "GA" and "FiLM" models have the same accuracy on the EQA train set.
Table 4 provides information about the serendipity per attribute.
Table 1 provides the offline evaluation performance of the RecSys system.
The NDCG value of the RecSys system is 0.71.
Table 2 shows the diversity per article attribute.
The diversity for the "Authors" attribute is higher than the diversity for the "RecSys" attribute.
The introduction of the news recommender in August led to a statistically significant difference in the reading behavior in terms of the section division.
The introduction of the news recommender in August led to a statistically significant difference in the reading behavior in terms of coverage.
The "Tri-sibling" models have a higher UASo compared to the "Sibling" models.
The presence of empty elements does not significantly affect the UASo of the models.
With SR decoding, the performance of the BLSTM order3 model is substantially improved for Chunking, English-NER, and Dutch-NER tasks.
The performance of the BLSTM order2 model is improved when using SR decoding for Chunking, English-NER, and Dutch-NER tasks.
The F1 score of "This paper" is higher than all the other models listed in the table.
The F1 score of "This paper" is [BOLD] 95.23.
The English training set contains 38,667 sentences.
The Chinese test set contains 1,520 overt elements.
The table shows the UASo (Unlabeled Attachment Score) of different SR decoding models on test data.
Table 6 presents timing results on varying the orders for different tasks.
The F1 scores for the models improve as we move down the table.
The F1 scores for the models are higher in the "AddOneSent" condition compared to the "AddSent" condition.
Table 3 provides a speed comparison between the QANet model and RNN-based models on the SQuAD dataset, with batch size 32.
QANet shows significant speed improvements compared to RNN-based models on the SQuAD dataset, with speedups of 2.9x, 9.4x, and 13.3x for RNN-1-128, RNN-2-128, and RNN-3-128 respectively.
The adjective "pivotal" is strongly associated with severe threats.
Subjective adjectives in the "Adj. serious" column are more strongly associated with severe threats compared to the subjective adjectives in the "Adj. aware" column.
The severity rankings for vulnerabilities in NVD are divided into four categories: Low, Medium, High, and Critical.
The base scores for each severity level in NVD are different.
Table 7 provides information about the model performance of identifying severe threats with Precision@k and AUC metrics.
Our model performs better than the Volume model in identifying severe threats based on Precision@k metrics.
Our model has a precision of 70.0 in the top 10 predictions.
The volume model has a recall of 14.2 in the top 100 predictions.
The highest grade for the "Persuasive" task is 10.
The range for the "Narrative" task is from 2 to 30.
There are 8 different sets in the table.
The average number of words in each set varies.
The values in the first row of the table are consistently lower than the values in the second row.
The value in the "m 40" cell is the highest value in the column.
There are 8 different sets listed in the table.
The value for m 100 in Set 1 is 0.
Different methods are used for video clip retrieval and language retrieval in the MSR-VTT dataset.
The AVLnet-Text method achieves the highest language retrieval score for both T+A→V and V →T+A in the MSR-VTT dataset.
Different methods are used for video clip retrieval and language retrieval in the MSR-VTT dataset.
The AVLnet-Text method achieves the highest language retrieval score for both T+A→V and V →T+A in the MSR-VTT dataset.
Table 2 provides an analysis of different design choices for AVLnet and AVLnet-Text.
AVLnet-Text performs better than AVLnet in the fine-tuning scenario for both the YouCook2 and MSR-VTT datasets.
Table 8 provides validation BLEU scores for freezing different parts of the mBART model and adding adapters for Xx → En.
The BLEU score for the test set after fine-tuning the encoder-decoder is 37.8 for the Ro-En (608k) dataset.
The BLEU score for the validation set when the encoder is frozen is 39.1 for the Ro-En (200k) dataset.
The "Mix" model achieves the highest UAS and LAS scores on average across all languages.
The "Delex" model achieves the lowest UAS and LAS scores for the German language.
The model on micropost train has the highest F1@MA score and the highest F1@MI score.
The AIDA system performs better on the development set (MicropDev) than on the test set (MicropTest).
AIDA B has the highest F1@MA score among all the models.
The base model with attention and global features has the highest F1@MA score among all the models.
The ED Global model performs better in terms of matching gold entities compared to the ED Base + Att model and the ED Base model.
The percentage of correctly matched gold entities decreases as the number of mentions increases.
The average out-of-domain transfer score for the hotel domain using the baseline model is 0.04.
The policy gradient fine-tuned model achieved a score of 0.13 in the restaurant domain.
The value of α for STS-B is 0.9.
The batch size is 16 for all tasks.
The Equalized EER for Scenario A in the Intelligent Voice speaker recognition system is [BOLD] 14.93.
The Unequalized Cmindet value for PLP in the Intelligent Voice speaker recognition system is 0.6857.
The Conversational dataset has more training examples than the Fisher dataset.
The Fisher dataset has a smaller encoder BLSTM size compared to the Conversational dataset.
The table provides an overview of the number of documents and pairs for the TB-Dense, MATRES, and TCR datasets.
The number of pairs for TB-Dense in the training set is 4032, and the number of pairs for MATRES in the test set is 310.
The Global Model performs better than the Local Model in terms of F1 score.
Table 7 presents the ablation results over global constraints of symmetry and transitivity on the forward test set and both-way test set.
Model M2, which includes the constraints of symmetry and transitivity, achieves higher performance on the forward test set and both-way test set compared to other models.
The results of the ablation study show that additional linguistic features do not lead to significant improvement and can even hurt performance in 2 out of 3 datasets.
The model without additional linguistic features outperforms the model with additional linguistic features in all three datasets.
The table compares the performance of BERT and GloVe word representations in an ablation study.
The proposed framework using global BERT word representation outperforms previous SOTA approaches.
The "one-to-many" approach has a higher average BLEU score than the "many-to-many" approach.
The "one-to-many" approach performs better than the "many-to-many" approach in translating from English to German.
The one-to-many performance is higher than the many-to-many performance for the En-Gl language pair.
The average performance of the one-to-many approach is higher than the average performance of the baselines.
The language with the highest BLEU score in the "many-to-one" translation task is "He" with a score of [BOLD] 36.33.
The average performance decreases as the number of languages involved increases.
The performance on the En-Fr language pair is higher than the performance on the En-Ru language pair.
The average performance of the translation pairs decreases as the number of languages involved increases.
The performance of the Fr-Ar translation pair is significantly better than the other translation pairs in the "25-to-25" row.
Table 2 shows the perplexity scores of the LSTM network on a word level Language Modeling task with and without forward dropout.
Our model outperforms the gal2015dropout model on the validation set with a dropout rate of 0.5 per-step.
Our model outperforms the Ours model on the test set with a dropout rate of 0.25 per-sequence.
The table displays F1 scores on the NER task.
The F1 score for 15-word long sequences with a dropout rate of 0.25 using GRU is 87.10.
Table 5 provides F1 scores for different dropout rates and RNN architectures on the Sentiment Evaluation task.
The F1 scores for dropout rate 0.25 are higher than the F1 scores for dropout rate 0 for all datasets when using the GRU architecture.
The AMR graph for the concept "a / apple" has different properties in the "Collapsed" and "Reified" rows.
The Smatch score for the concept "a / apple" with different modifiers and quantifiers varies in the "Reified" column.
The "Sem+Seq" paradigm consistently outperforms the "Sem" paradigm for all three embeddings (GloVe, ELMo, BERT).
The "ELMo [ITALIC] SSS" embedding achieves the highest scores for all three ROUGE metrics (ROUGE 1, ROUGE 2, ROUGE L).
The SSS(BERT) model achieves the highest scores in BLEU, ROUGE 1, ROUGE 2, and ROUGE L among all the models.
The SSS(BERT) model achieves the highest scores in BLEU, ROUGE 1, ROUGE 2, and ROUGE L among all the models.
Seq-GCN performs better than Str-LSTM and Par-GCN-LSTM in terms of BLEU score for all three embeddings (GloVe, ELMo, BERT).
BERT performs better than GloVe and ELMo in terms of ROUGE score for all three architectures (Seq-GCN, Str-LSTM, Par-GCN-LSTM).
The model "ATTOrderNet" performs the best in both the "Order discrimination Accident" and "Order discrimination Earthquake" tasks.
The model "Varient-LSTM+PtrNet" achieves the highest reordering accuracy among all the models.
The table includes hyperparameters used in coherence experiments for three different tasks: Order Disrcim., [EMPTY], and Reordering.
The number of epochs used in the coherence experiments varies for each task mentioned in the table.
The PPD-based heuristic outperforms the suite of established heuristic summarizers in terms of ROUGE scores.
The RL model has the highest ROUGE-1 score and the second highest ROUGE-L score among all the models.
The R10@1 score for the Multi-view + ECMo (continue-train) model is 0.723.
The MAP score for the SMN + ECMo model is 0.549.
The "Multi-view + ECMo" model performs better than the "Multi-view" model on the Ubuntu Corpus.
The "SMN + ECMo" model outperforms the "SMN" model on the Douban Corpus.
The "Multi-view + ECMo" model performs better than the "SMN + ECMo" model in the Ubuntu Corpus.
The "Multi-view + ECMo" model has a higher R2@1 score than the "SMN + ECMo" model in the Ubuntu Corpus.
Table 2 shows the percentage of reviewers in different levels of expertise to the submissions recommended by the model.
100% of the reviewers predicted by the system have an expertise level of 2 or higher.
Table 1 presents the mean precision of different baselines with optimal hyperparameters on the NIPS dataset.
Table 10 provides the ranking of different models according to human judgments and QA-based evaluation for the Newsroom-Abs dataset.
The model with the highest score according to human judgments and QA-based evaluation is "gold".
Table 4 shows the performance of extractive baselines on CNN, DailyMail, NY Times, Newsroom, and XSum datasets.
The ext-oracle system achieves higher ROUGE scores on Newsroom-Ext and Newsroom-Abs datasets compared to other datasets.
Table 7 shows the proportion of novel n-grams in summaries generated by various models on the XSum test set.
The XSum summaries have a wider range of scores compared to the Newsroom-Abs summaries.
The XSum summaries cover a wider range of topics compared to the Newsroom-Abs summaries.
Table 11 provides information about the informativeness of summaries in the XSum and Newsroom-Abs datasets.
The "P8 P1+NMT 20%" system has higher accuracy than the "P1 Paracrawl baseline" system in all three categories (n2014, d2015, patent).
The "P9 P1+NNLM 20%" system has lower accuracy than the "P1 Paracrawl baseline" system in all three categories (n2014, d2015, patent).
The subsets S80%, S40%, and S20% are nested in a hierarchical order, with S80% being the strictest and S20% being the least strict.
The denoising performance improves as the subset becomes stricter, with higher values for stricter subsets.
The performance of the models varies depending on the type of task sharing used.
The average performance across all models and task sharing types is 85.19.
There are four different approaches for predicting HTER for English-German: SVM, 4×SVM, MLP, and MLP4.
The performance of MLP4 for predicting HTER on the test2017 dataset is 0.412.
For the "fbk" lab, the majority of winner decisions were made by the "neural" system.
The "neural" system had the lowest average TER score for the "rwth" lab.
The "neural" system is the Gold Winner in terms of TER rankings.
The "neural" system has the highest absolute percentage.
The accuracy for the "Scenes" task increases as the number of proposals increases.
The accuracy increases as the number of proposals increases for all tasks.
The accuracy for the "Pair's rel." task in the Easy Task increases as the number of proposals increases.
The accuracy for the "Future" task in the Easy Task increases as the number of proposals increases.
The Embedded CNN+LSTM approach performs better than the plain CNN+LSTM approach for both the Easy Task and the Hard Task.
The plain CNN+LSTM approach performs better than the Embedded CNN+LSTM approach for the objective affection category.
The consistency of the composition in the generated videos is higher for "Ours" compared to "Pixel Generation L1".
The visual quality of the foreground in the generated videos is higher for "Ours" compared to "Pixel Generation L1".
Table 3 shows the classification results of a SVM trained on synthetic samples along with real training samples for different scenarios in cross-corpus experiments.
The UAR (%) for the scenario "Only real openSMILE" is 45.14.
The table shows the classification results of a SVM trained using different setups involving synthetic and real data.
The setup involving the combination of 2D code vectors and synthetic data has the highest UAR percentage.
The table presents classification results for two datasets: "2D code-vectors" and "Improved-conditional".
The UAR for the "2D code-vectors" dataset is 97.09%, while the UAR for the "Improved-conditional" dataset is 35.23%.
Table 1 represents the best setting for DNNs, as found on the dev set.
The best setting for DNNs, as found on the dev set, has a batch size of 16, dropout rate of 0.3, regularization strength of 0.001, 10 intermediate layers, and 125 task-specific layers.
Table 2 provides an ablation study by categories on CoNLL2003e-test, where the F1 scores are reported for four different feature categories: A, Name, KB, and Entity.
The highest F1 score among all the feature categories in the ablation study on CoNLL2003e-test is achieved when all the features (A, Name, KB, and Entity) are combined, with an F1 score of 91.12.
The system "KnowNER gold" achieves the highest F1 score of 91.12 on the CoNLL2003e-test dataset.
The system "Chiu and Nichols" achieves a higher F1 score than the systems "Luo et al." and "Yang et al." on the CoNLL2003e-test dataset.
The Stanford CoreNLP (3.6.0) system is used for Named Entity Recognition (NER) on different entity types.
The F1 performance score for the Stanford CoreNLP (3.6.0) system on the LOC entity type is 89.04 on the CoNLL2003e test dataset.
G-DuHA outperforms all other models in terms of dialogue generation performance across all metrics.
G-DuHA achieves the highest BLEU score among all the models.
The table provides overall macro and micro precision and recall for two new ground truths.
The UUR values are higher for the Young group compared to the Elder group.
There are three different user buckets: High, Mid, and Low.
The correlation coefficient for the Low user bucket is [BOLD] 0.65.
The F1 scores for the target domains ACAD, WEAT, and ADLD are lower than the F1 scores for the other target domains.
The F1 scores for the target domains ACAD-AWLD, IMDB-IWLD, and YELP-YWLD are higher than the F1 scores for the other target domains.
The "Our model" performs better than "User at learned rate" in terms of F1 score.
The "Our model" performs better than "User at learned rate" in terms of NDCG score.
Table 6 provides information about the accuracy, macro-F1, and micro-F1 metrics on the RCV1 dataset.
The best accuracy on the RCV1 dataset is 0.652, the best macro-F1 is 0.678, and the best micro-F1 is 0.874.
The "SOTA+Meta-Training-Prediction" model achieves the highest accuracy on the "OntoNotes" dataset.
The "SOTA+Meta-Training-Prediction" model outperforms the "SOTA" model on the "FIGER" dataset.
Table 3 presents the robustness analysis on the FIGER dataset, including the metrics Accuracy, Macro-F1, and Micro-F1.
The worst performance for all three metrics (Accuracy, Macro-F1, and Micro-F1) is lower than the average performance on the FIGER dataset.
The XLSR-Monolingual model was trained for 1 hour and fine-tuned on 1 hour of Italian supervised data.
The XLSR-Monolingual model was pretrained with 5 hours of Italian CommonVoice data and 50 hours from another language.
Table 2 presents an ablation study of the ReSAN model, testing different variations of the model.
The "600D Gumbel TreeLSTM encoders" model has the highest test accuracy among all the models.
The "300D ReSAN" model has the lowest inference time among all the models.
The table displays the test performance of different models trained on the neutral split of the PGM dataset.
The T-LEN model achieves an accuracy of 70.3% on the neutral split of the PGM dataset.
Table 2 shows the results of different POS tagging methods.
JMTAB achieves the highest F1 score among the listed POS tagging methods.
Ling et al. (2015) achieved the highest accuracy of 97.78 in POS tagging.
JMTall has a higher accuracy than Ma and Hovy (2016) in POS tagging.
The method "Dozat and Manning (2017)" achieves the highest UAS and LAS scores.
The UAS and LAS scores generally increase as we move down the table.
JMTDE achieves the highest accuracy among all the methods in the table.
JMTDE has a higher accuracy than JMTall.
Table 9 shows the effectiveness of the Successive Regularization (SR) and the Vertical Connections (VC) on different tasks.
Table 10 shows the effects of the order of training on different tasks.
The POS task achieves higher accuracy when trained with JMTall compared to training with Random.
The W&C model improves the performance of POS tagging using character embeddings compared to using only word embeddings.
The Single model decreases the performance of dependency parsing using character embeddings compared to using only word embeddings.
Table 14 provides dependency parsing scores on the development set with and without character n-gram embeddings, focusing on UAS and LAS for unknown words.
The UAS score increases as we add modifications to the direct parsing method.
The LAS score increases as we add modifications to the direct parsing method.
The zero-shot parsing model achieves the highest UAS and LAS scores among all the models compared in the table.
The human performance in parsing is better for the English language compared to the Chinese language.
Our model outperforms all other models on the Electricity 3 horizon.
The SETAR model has the second best performance on the Electricity 12 horizon.
The highest autoregressive (AR) value is achieved for the "Solar Energy 24" dataset.
Our model achieves the best performance on the "Solar Energy 6", "Solar Energy 12", "Solar Energy 24", "Traffic 12", and "Traffic 24" datasets.
Our model outperforms all other models on most time horizons in terms of AR, LRidge, LSVR, GP, SETAR, LSTNet-Skip, and LSTNet-Attn.
Our model does not perform the best on Electricity 24.
Our model outperforms all other models on all RSE Horizons.
The performance of the models generally improves as the RSE Horizon increases.
Our model achieves the best performance on the RSE Horizon dataset with the AR metric.
Our model achieves the best performance on the Exchange Rate 6 dataset with the Exchange Rate metric.
Our model outperforms all other models in terms of CORR Horizon.
The SETAR model performs better than other models in terms of Traffic 24.
The "Our Original Model" has the highest EM and F1 scores compared to the other models.
The "Self Matching" model was trained for 10 hours, while all other models were trained for 14 hours.
Stanford IE has the highest number of true positives among the three Open IE baselines on the dev set of QA4IE-SPAN-S.
ClauseIE has the highest number of extracted triples after filtering among the three Open IE baselines on the dev set of QA4IE-SPAN-S.
The "Bottom-Up" system has a lower percentage of faithful summary sentences compared to the "Reference" system.
The "PG" system has a higher percentage of grammatical summary sentences compared to the "DCA" system.
The concatenation merging method has high scores in faithfulness, grammaticality, and coverage.
The merging methods of Replacement and Other have much lower scores in faithfulness, grammaticality, and coverage compared to the concatenation method.
The FLC-F1 score increases when both CRF and Self-train are used compared to when only one of them is used.
The FLC-F1 score increases as the number of self-training iterations increases.
The feature "comma inside" has a count of 119.
The feature "CIA before" has a p-value of <0.001.
The "Span Prediction" model outperforms all other models in terms of Bleu-1, Bleu-4, Meteor, Rouge-L, and MRR scores.
The "Attention Sum Reader" model achieves the highest MRR score among all models.
Table 5 displays the unsupervised cross-lingual word embeddings cosine similarity and L2 distance between source words and their translations, as well as the Pearson correlation on the SemEval'17 cross-lingual word similarity task.
XLM achieves the highest cosine similarity, the lowest L2 distance, and the highest Pearson correlation on the SemEval'17 cross-lingual word similarity task among the three methods.
Table 1 provides results on cross-lingual classification accuracy for machine translation baselines and zero-shot classification approaches based on cross-lingual sentence encoders.
Table 2 presents results on unsupervised MT with BLEU scores on WMT’14 English-French, WMT’16 German-English, and WMT’16 Romanian-English.
The previous state-of-the-art by Lample et al. (2018b) achieved a BLEU score of 24.18 for the fr-en translation task.
There are three different types of pretraining: CLM, MLM, and -.
The model trained on both directions (ro ↔ en) with back-translation (BT) achieves higher BLEU scores compared to the other models.
Table 4 shows the results on language modeling and the Nepali perplexity when using additional data from different languages.
The perplexity of Nepali decreases as more languages are added to the training.
The table shows the character-wise accuracy of different models on the Handwriting Recognition dataset.
The base model using 2-gram achieves an accuracy of 84.93 on the Handwriting Recognition dataset.
According to Table 3, "CRIAGE (Brute Force)" outperforms other methods in terms of both ρ and τ for the "Nations Adding" and "Nations Removing" scenarios.
Table 3 shows that the "Influence Function without Hessian" method performs better than the "Influence Function" method in terms of both ρ and τ for all scenarios.
Table 1 provides the accuracy of inverter functions for different models on the WordNet and YAGO datasets.
The accuracy of the WordNet DistMult model is 93.4 and the accuracy of the YAGO ConvE model is 98.1.
The table shows the performance of different models on the YAGO3-10 and WN18 datasets.
The MRR value decreases when adding random attack or opposite attack to the DistMult model on both YAGO3-10 and WN18 datasets.
The method "CRIAGE" has the highest accuracy for both "Noise Hits@1" and "Noise Hits@2" compared to the other methods.
The method "Lowest" has a lower accuracy for "Noise Hits@1" compared to the method "Random".
The "Bag-of-words" feature set has 1 layer.
The "Bag-of-concepts" feature set has a hidden dimension of 300.
Table 2 provides hyperparameters for feedforward neural network classifiers for four different tasks.
The number of layers in the feedforward neural network classifiers varies for each task.
The table provides results for different machine learning models on the Reuters-21578 Subset dataset using Google News Word Vector.
The CNN model shows a decrease in micro recall compared to the baseline, while the MNB and SVM models show an increase in micro recall compared to the baseline.
The table provides the micro and macro recall scores for the MNB and SVM models on the Reuters-21578 subset dataset using Google News Word Vector.
The WV Enrichment model improves the micro recall performance compared to the baseline model for both MNB and SVM on the Reuters-21578 subset dataset using Google News Word Vector.
The WV Enrichment method improves the recall performance compared to the Baseline method for both MNB and SVM classifiers.
The Error Reduction method improves the macro recall performance compared to the Baseline method for both MNB and SVM classifiers.
The table presents the results for the DRC Dataset with domain-specific word vectors.
The Micro Recall for SVM with WV Enrichment is higher than the Micro Recall for SVM in the Baseline.
The table provides information about the DRC Dataset, which consists of 2,159 instances and 64 classes.
The Micro Recall for WV Enrichment is higher than the Micro Recall for Baseline.
Table 3 provides a performance comparison with other studies on the CHiME2 test set.
The proposed approach has a higher WER than Wang et.al.
The enhancement via fidelity loss results in a lower word error rate than the enhancement via joint loss.
The system with pre-softmax mimic loss achieves a lower word error rate than the system with no enhancement.
There are five different methods used in the experiment.
The "WordNet" method has the highest score in the "Polarity" category.
"WordNet" has the highest score among all the methods.
"Potts 2011" has the lowest score among all the methods.
The model used in the experiments is "Our Model".
The highest BLEU4 score is achieved when the value of λ is 0.2.
"Our Model" outperforms the other models in terms of BLEU4, ROUGE-L, and METEOR scores.
The "ans_loss" modification improves the METEOR score of "Our Model".
The model "Our Modelans_loss" achieves a performance of 75.5 on predicate identification.
The model "Our Modelans_loss" achieves a higher performance on predicate identification compared to "P16-1056" and "N18-1020".
The performance of "Our Modelans_loss" is higher than the performance of "P16-1056" and "N18-1020" on naturalness.
The performance of "Our Modelans_loss" is higher than the performance of "P16-1056" and "N18-1020" on naturalness.
The table provides the performances of two models, "TransE" and "Our Modelans_loss", in terms of BLEU4, ROUGE L, and METEOR scores.
When "N18-1020" is True, the model achieves higher scores in terms of BLEU4, ROUGE L, and METEOR compared to when "N18-1020" is False.
The table shows the performances of generated questions for QA using different types of data.
The performance of the model with "gen_data (Our Modelans_loss)" is higher than the performance of the model with "gen_data (Elsahar et al., 2018)".
The "WARV" method has the highest accuracy of 94.95.
The "NBSVM" method is NB-SVM TriGram (Wang et al.).
The method "WARV, NBSVM" has the highest accuracy for both Ensemble-1 and Ensemble-2.
The accuracy of the method "PARAGRAPH, NBSVM, CNN, WARV" is higher for Ensemble-2 compared to Ensemble-1.
The compactness scores for NIPS-1k, NIPS-5k, and NIPS-10k are lower than the compactness score for News-1k.
The compactness scores for NIPS-1k, NIPS-5k, NIPS-10k, and News-5k are lower than the compactness score for NYT.
Model III performs better on the ATIS dataset compared to Model I and Model II.
Model III performs better on segment-level evaluation on both the ATIS dataset and the LARGE dataset.
Model III achieves the highest F1 score and Segment-F1 score among all the models.
The F1-score of "Model III (Ours)" is higher than all other methods mentioned in the table.
Model III achieves a segment-F1 score of [BOLD] 100.0 on ATIS with chunk length >=3.
The baseline model (Bi-LSTM) achieves a segment-F1 score of 86.25 on LARGE with chunk length 1.
"Model III (Ours)" achieves the highest F1-score of [BOLD] 95.86 on the ATIS data.
"Bi-RNN with ranking loss" outperforms "Bi-RNN" in terms of F1-score on the ATIS data.
Table 3 shows the macro F1 average on the W-TOX and W-ATT datasets for different models.
The fastText(ngrams=2, BERT BPE, PreE) model achieves a macro F1 average of 86.8 on the W-TOX dataset.
The model "MLE" has a world level perplexity of 42.
The model "ScratchGAN" has a world level perplexity of 154.
The "ScratchGAN" model has a FED sensitivity of 0.015.
The "SeqGAN-step (no pretraining)" model has a FED sensitivity of 0.084.
The model "Gal (2015) - Variational LSTM (large, untied)" has a validation perplexity of 77.9±0.3.
The model "Pointer Sentinel-LSTM (medium)" achieves a lower perplexity than the model "Gal (2015) - Variational LSTM (large, untied)".
The Seq2Seq + KerBS model without kernel achieves a BLEU-4 score of 26.79.
The Seq2Seq + KerBS model with only single sense achieves a BLEU-4 score of 26.80.
The table compares the Word Error Rate (WER) (%) of the "2-output PIT" model and the "SOT" model.
Table 1 provides the WER(%) of 512-dim models for 2-speaker-mixed speech.
The WER(%) for single speaker speech by the single-speaker ASR is 5.4% for dev_clean and 5.7% for test_clean.
As the model dimension increases, the number of parameters also increases.
The SAA (Sec 4.2) technique is applied to the model with a dimension of 1024.
The table shows the results of different attack metrics on the image-to-sentence retrieval task with text-domain adversarial attacks.
The mAP performance for the "obj" category using VSE++ is 29.95.
The total mAP performance for all categories using UniVSE is 170.42.
Table 5 shows the performance of various models on WikiQA.
The introduction of word matching features improves the performance of the BiLSTM baseline model on WikiQA.
The probability of the phrase "what sort of part do queen play in concert" for the relation "music.concert_performance.performance_role" is 0.0659.
The probability of the phrase "which team do shaq play out" for the relation "sports.sports_team_roster.team" is 0.0655.
The table reports the performance of two different methods, SimpleGraph and Para4QA, on simple and complex questions.
Both SimpleGraph and Para4QA perform better on complex questions than on simple questions.
Method (3) has the highest average accuracy among all the strategies.
Method (4) has the lowest average rank among all the strategies.
The DS+Logistic method achieves the highest F1 score on both the NYT and Wiki-KBP datasets.
The FCM method achieves the highest Precision and F1 score on the BioInfer dataset.
BioInfer has the highest number of entity types among the three datasets.
BioInfer has the highest percentage of noisy relation mentions among the three datasets.
CoType has the highest F1 scores across all three datasets.
PLE has a higher F1 score compared to DeepWalk.
The "CoType" method outperforms all other methods in terms of relation classification accuracy on the NYT, Wiki-KBP, and BioInfer datasets.
The "CoType-TwoStep" method performs better than the "DS+Kernel" and "DeepWalk" methods in terms of relation classification accuracy on the Wiki-KBP dataset.
"KnowBert-W+W" achieves the highest F1 score among all the systems in Table 2.
"BERTLARGE" outperforms "BERTBASE" in terms of F1 score in Table 2.
KnowBert-W+W has the lowest masked LM perplexity and highest Wikidata probing MRR among all the systems in the table.
KnowBert-Wiki has the highest number of parameters in the KAR and entity embeddings among all the systems in the table.
The table shows the end-to-end entity linking strong match micro-averaged F1 scores for different systems.
The Daiber et al. system achieved a score of 49.9 on the AIDA-A dataset.
Table 7 shows the results (WERs) obtained using mono and multi-lingual acoustic models, with off-diagonal numbers representing WERs on non-native speech.
The WERs for English are higher when using a multi-lingual AM compared to mono-lingual AMs.
Table 8 provides WERs obtained with mono- and multi-lingual acoustic models adapted to non-native speech using three different modalities (m1, m2, m3).
The WER for Italian is lower when using the multi-lingual acoustic model (m2) compared to the mono-lingual acoustic model (m1).
Each model in the table is an extension of the previous model, with additional features added.
The values for RMSE decrease as we move from model (1) to model (6), indicating an improvement in performance.
The table shows the results of different baseline models.
The SVR+O model has an RMSE value of 1.92 and a ρ value of 0.56.
The table shows the results of different baseline models.
The SVR+O model has an RMSE value of 1.92 and a ρ value of 0.56.
Table 8 provides a summary of finetuning results on QA, coreference resolution, and document classification, comparing Longformer-base and RoBERTa-base models.
Longformer-base performs better than RoBERTa-base in all tasks: QA WikiHop, QA TriviaQA, QA HotpotQA, Coref. OntoNotes, Classification IMDB, and Classification Hyperpartisan.
All models (WikiHop, TriviaQA, HotpotQA) use the same hyperparameters.
The batch size is the same for all models (WikiHop, TriviaQA, HotpotQA).
The F1 score for the "hasMember" relation using the CINEX biLSTM-CRF architecture is [BOLD] 56.1.
The F1 score for the "containsAdmin" relation using the CINEX CRF architecture is 57.0.
The "containsWork" relation has the highest F1 score among all the relations in the "Baseline Cardinals" column.
The "hasChild" relation has the highest F1 score among all the relations in the "CINEX-CRF (per type) Numt.+Art." column.
The BERTLARGE model achieves the highest accuracy for both the RACE and DREAM datasets.
The "Human Adult*" model achieves the highest accuracy for the DREAM dataset.
The judge is more accurate on long passages when it answers based on only sentences chosen by competing agents (last 5 rows) instead of the full passage.
The judge's accuracy is highest when it uses the BERTBASE model to predict the answer.
The table shows the performance of a judge trained on Middle School exam questions and tested on High School exam questions.
The judge is more accurate when using evidence agent sentences (last 5 rows) rather than the full passage.
The percentage of times humans select the agent's answer is higher when using the BERTBASE model compared to the Search Agents.
The difference in the percentage of times humans select the agent's answer is higher for the Learned Agents compared to the Search Agents.
Table 3 displays the results of variable fine-tuning in BERT using mean reciprocal rank (MRR).
The MRR for the "Top 4 layers" is higher than the MRR for the "Top 6 layers" in the "Advising" task.
The models listed in Table 1 are ranked in descending order based on their performance on the "R10@1" metric.
The BERT-VFT(DA) model outperforms all other models listed in Table 1 on the "R10@1", "R10@2", and "R10@5" metrics.
The BERT-DPT model achieves the highest R100@10 score among all the models.
The BERT-VFT model outperforms the BERT model in terms of R100@50 score.
The mean error rate for the "SCRF + DNNs trained from scratch" settings is higher than the mean error rate for the "Sig.-indep. SCRF, DNNs trained from scratch" settings.
The mean error rate for the "Sig.-indep. model w/ fine-tuned DNN" settings is lower than the mean error rate for both the "SCRF + DNNs trained from scratch" settings and the "Sig.-indep. SCRF, DNNs trained from scratch" settings.
The "Seq-to-seq paraphrase + RASA" parser performs better than the "RASA" parser across all training dataset sizes.
The "Seq-to-seq paraphrase + BiRNN" parser has a higher accuracy than the "RASA" parser for a training dataset size of 500.
The table shows the results of the binary discrimination task on the WSJ.
The MTL model has significantly higher performance than all the other models on both the PRA and TPRA metrics.
All the "u" rows have a value of 1 in the "m1" column.
All the "u" rows have a value of 1 in the "f3" column.
Table 2 provides results for the Spearman rank correlation (ρ×100).
The average similarity score for the WSJ dataset using the NP-MSSG model is 52.2.
Table 7 shows the results of a question ranking feature ablation study.
The "J2Q-QR-XGB-Pairwise" model achieves the highest precision at 1.
The "J2Q-QR-XGB-Pairwise" model outperforms all other models in terms of Precision @1, Precision @3, Recall @1, Recall @3, NDCG @1, and NDCG @3.
The "J2Q-QR-XGB-Pointwise" model achieves the highest AUROC score among all the models.
The table shows the classification accuracy of different trained models on their respective test sets.
The table compares the performance of the standard Transformer model with the performance of the latent variable model proposed in the current work.
Our method achieves the highest BLEU score when considering HTML tags and when considering only the text enclosed by tags.
The "Match" method achieves the same BLEU score with and without considering HTML tags.
The number of argumentative units is higher than the number of argumentative sentences for each topic.
The topic "school uniforms" has the highest increase in the number of argumentative units compared to argumentative sentences.
Table 7 shows the classification performance for deepgeo with the addition of noise and loss term l.
The addition of noise and loss term l improves the classification performance of deepgeo.
The "Location" feature has a significant impact on the accuracy of the model.
The "UTC Offset" feature has a positive but minimal impact on the accuracy of the model.
The retrieval MAP performance increases as the value of "Bits" increases for all combinations of models.
The "fasthash word2vec" combination has the highest retrieval MAP performance among all combinations.
The "multi-layer" architecture performs better than the "single-layer" architecture for the "de" language.
The "multi-layer" architecture has a higher average Kendall's tau value compared to the "single-layer" architecture.
The Kendall’s \tau score for the metric "cz" in System 4metrics is 15.88.
The system "DiscoTK [discoMT:WMT2014]" achieved the best performance on the WMT12 dataset.
The Kendall's tau score for the combination of BLEUcomp, syntax25, and Wiki-GW25 is 26.15 for French.
The average Kendall's tau score for the combination of BLEUcomp and syntax25 is 23.70.
The semantic vectors trained on CC-300-42B, CC-300-840B, and Composes400 perform better than the semantic vectors trained on Wiki-GW25 when combined with 4 metrics and syntax25.
The semantic vectors trained on CC-300-42B, CC-300-840B, and word2vec300 perform better than the semantic vectors trained on Wiki-GW25 when used alone.
Different cost functions are used for the calculations.
The performance of the "EnhancedKGP (our model)" is better than the performance of the "BaseKGP (our model)".
The "ROP_ARC2 (GRU)" method performs better in both MQ and H@10 compared to the "ROP_ARC2 (ADD)" method.
The models are ranked in ascending order based on their performance.
The MAP score increases as we move down the table.
The performance of the model with shallow sentence representation (Ssr) is lower than the performance of the model without Ssr in terms of Surface SeLen.
The performance of the model with deep sentence representation (Dsr) is higher than the performance of the model with shallow sentence representation (Ssr) in terms of Semantic SoMo and Semantic CoIn.
The table compares the performance of three different models on WMT14 En⇒De and En⇒Fr test sets.
The Transformer models perform better on the En⇒De translation task compared to the En⇒Fr translation task.
The table shows the EER results on a semi text-independent task for different systems.
The i-vector system has EER results of 19.32, 11.09, 8.70, and empty for the PDTR, cosine, LDA, and PLDA systems, respectively.
The table shows the EER results for different feature extraction methods (i-vector and d-vector) and different phrases (P1 and P2) on a text-dependent task.
Table II provides EER results with additional data.
The EER% values decrease as more phrases are included in the training.
The first three models (Transformer-XL 151M, QRNN, LSTM) have higher values for r=0.1 compared to r=0.5 and r=1.0.
The values for r=0.1, r=0.5, and r=1.0 decrease as the parameter budget decreases.
The "Ours - 24L Transformer-XL" model has the lowest bpc value among all the models listed in Table 2.
The "Ours - 18L Transformer-XL" model has 88M parameters.
The model "Ours - 24L Transformer-XL" has the lowest number of parameters among all the models listed in the table.
The model "Ours - 24L Transformer-XL" achieves the lowest bits per character (bpc) among all the models listed in the table.
The Transformer-XL Large model achieves a perplexity of 21.8 on the One Billion Word dataset.
The Mesh Tensorflow model has approximately 4.9 billion parameters.
When using the "Transformer-XL (128M)" model and the "Ours" encoding, the shortest possible attention length during evaluation to achieve the corresponding result (PPL best) is 500.
The table presents an ablation study on the One Billion Word dataset, comparing the performance of three different methods.
The "Ours" method achieves the lowest perplexity (PPL) value of 25.2 on the One Billion Word dataset.
The F1 scores on all subsets of TriviaQA dev increase from H2-P to H3-D.
The size of the subsets of TriviaQA dev increases as we move from \mathcal{Q}^{ss} to \mathcal{Q}^{ll}.
The objective "H3-D" performs the best among all the objectives for TriviaQA and NarrativeQA.
Document-level models perform better than paragraph-level models for TriviaQA and NarrativeQA.
The verb pair "to reply / to respond" has the highest rating among the verb pairs in Table 1.
The verb pair "to stay / to leave" has the lowest rating among the verb pairs in Table 1.
The corpus includes datasets such as SearchSnippets, StackOverflow, Biomedicine, Tweet, GoogleNews, and PascalFlickr.
The average length of the snippets in the corpus varies across different datasets.
The LDA model achieves a purity value of 0.456±0.011 on the Biomedicine dataset.
The GPU-PDMM model achieves an NMI value of 0.607±0.013 on the Stack Overflow dataset.
The GSDMM model has the highest initiation time among all the models.
The GPU-PDMM model has the highest per iteration time among all the models.
The Latent model outperforms all other models in terms of BLEU-4, METEOR, ROUGE-L, and perplexity scores on both the ROCStories and VIST datasets.
The VIST dataset has higher perplexity scores compared to the ROCStories dataset for all models.
The Latent model achieves the highest scores in terms of fluency and the geometric mean of fluency, relevance, and coherence on the ROCStories dataset.
The table presents the results of an ablation study on three different models: Latent, − PM, and − SM.
The Latent model achieves the highest ROUGE-L score compared to the other two models.
Table 4 compares the performance of variational inference (VI) on the ROCStories dataset.
The BLEU score for the Latent model on the ROCStories dataset is 3.70.
The setting "Concatenated (NN3)" achieves the highest F1 score on the MSRPC paraphrase recognition dataset.
The setting "Concatenated (NN3)" achieves the highest precision on the MSRPC paraphrase recognition dataset.
Table 2 provides the overall results on the CoNLL-03 NER task for different settings.
The table shows the results on the CoNLL-03 NER task for different settings.
The "word2vec" setting achieves the highest F1 score for the "Org" entity type.
The accuracy of the Concatenated ([ITALIC] k=5) setting is higher on the development set compared to the test set.
The node2vec [ITALIC] NN setting has a lower accuracy compared to the word2vec and Concatenated settings.
The weights in the table vary for each row.
The values in the "Padó ALL" column for "PPMI 2000" and "Vector Cosine (Baseline)" are equal.
The table shows the entropy and perplexity (Ppl) of the IBM-1 model on the training and development set for the controlled scenario using different synthetic generation methods.
The synthetic generation method without latent space (w/o LS) has the highest entropy and perplexity (Ppl) values on the test2015 set.
The "full tasks" variant performs the best on all metrics in the DailyDialog dataset.
The "word order recovery" variant has the highest PPL score in the Ubuntu dataset.
The accuracy of the acoustic+embs model on the BURNC dataset is 87.5%.
The accuracy of the embs-only model on the LeaP dataset is 80.9%.
The baseline accuracy for BURNC is 98.2% and for BDC is 88.9%.
The accuracy for BDC using GloVe embeddings is 92.7% and using LeaP embeddings is 94.2%.
The "Reverse Model (Fr-Mb) with Alignment Smoothing" has a higher recall score than the "Base Model (Mb-Fr)".
The "Reverse Model (Fr-Mb) with Alignment Smoothing" has a higher F-score than the "Base Model (Mb-Fr) with Alignment Smoothing".
The sentiment scores for the combinations of positive emojis and plain verb messages are higher when multiplied by 3 compared to the control group.
The standard deviations of the sentiment scores for the combinations of positive emojis and plain verb messages are higher when multiplied by 3 compared to the control group.
Table 5 shows the means and standard deviations of willingness scores of each emoji for each intention.
The mean willingness score for Intention1 with emoji is 6.64.
The sentiment scores for the combinations of non-facial emojis and plain verb messages are higher for the "×3" condition compared to the "Control" condition.
The standard deviations of the sentiment scores for the combinations of non-facial emojis and plain verb messages are generally higher for the "×3" condition compared to the "Control" condition.
The "Ours" model has the highest accuracy in both BR (%) and LR (%).
Fine-tuning the GPT-2 model improves its accuracy in both BR (%) and LR (%).
The "Ours" model has the highest coverage among all the models.
The "GPT-2 (Fine-tune)" model has the highest BLEU-2 score among all the models.
The manual evaluation results compare the performance of our model ("Ours") with different baseline models including GPT-2 (Scratch), GPT-2 (Pretrain), GPT-2 (Fine-tune), Fusion, DSRL, Ours w/o Pretrain, Ours w/o Knowledge, and Ours w/o Multi-task.
Our model ("Ours") outperforms all other baseline models in terms of grammaticality.
The F1 score increases from D1 to D4 for all models.
The percentage of predicted story types decreases from D1 to D4 for the "Ours" model.
The winning rate for the "Ours" model is higher than the winning rate for the "GPT-2 (Fine-tune)" model in the comparison between Correct and Wrong templates.
The winning rate for the "Ours" model is higher than the winning rate for the "GPT-2 (Pretrain)" model in the comparison between Training and Wrong templates.
Table 10 shows the distribution of error types for different models.
Our model has the lowest percentage of repetition errors compared to Fusion and GPT-2 (Fine-tune).
The R-GAT+BERT method outperforms all other methods in terms of accuracy and macro-F1 scores on all three datasets.
The CDT method performs the best among all the methods in terms of accuracy and macro-F1 scores on the Restaurant and Laptop datasets.
Table 3 presents the results of R-GAT based on two different parsers, along with their UAS and LAS scores.
The Biaffine parser outperforms the Stanford parser in terms of UAS and LAS scores across all three datasets.
The experiment compares the performance of two types of trees: "Ordinary" and "Reshaped".
The experiment compares the performance of four different methods: "GAT", "R-GAT", "R-GAT−n:con" (without using n:con), and "R-GAT−n:con" (with using n:con).
The table presents the results of error analysis of R-GAT and R-GAT+BERT on 100 misclassified examples from the Restaurant dataset.
The percentage of misclassified examples categorized as "Advice" is 6%.
Table 5 provides the results of error analysis of R-GAT and R-GAT+BERT on 100 misclassified examples from the Restaurant dataset.
The "Double negation" category has 3 occurrences in the error analysis of R-GAT and R-GAT+BERT on 100 misclassified examples from the Restaurant dataset.
Table 2 provides PESQ scores, number of parameters, and inference times.
The PESQ score for BERT with a threshold of 0.3 is 1.971.
As the percentage of training data increases, the performance of BiDAF with constraints and ELMo improves.
The addition of ELMo and constraints (R1 and R2) consistently improves the performance of BiDAF across all percentages of training data.
The accuracy of the BiLSTM+CRF tagger increases as the percentage of training data increases.
The combination of the CRF and C1:5 constraints improves the accuracy of the BiLSTM tagger.
Table 5 presents the IR results with various methods in COLIEE 2016.
The method "With splitting + No references" achieves the highest F1-score among all the methods mentioned in Table 5.
Our model has a higher accuracy than the other models.
The accuracy of the models improves as we move from the Convolutional neural network to our model.
Table 9 presents the results of Task 3 with three different scenarios: "No voting", "Voting without ratio", and "Voting with ratio".
Among the three scenarios, "Voting without ratio" has the highest accuracy in Task 3.
Our model (JNLP3) has the lowest performance in both phase 2 and phase 3.
The methods iLis7 and KIS-1 have the same performance in phase 2.
The number of tweet pairs for each emotion is different.
The average emotion intensity score for NQT tweets is lower than the average emotion intensity score for HQT tweets.
Table 2 provides the split-half reliabilities for the anger, fear, joy, and sadness tweets in the Tweet Emotion Intensity Dataset.
The Spearman correlation values for the anger, fear, joy, and sadness tweets are higher than the corresponding Pearson correlation values in Table 2.
The intensity prediction for anger using word embeddings is 0.48.
The intensity prediction for joy using SentiStrength is 0.61.
The combination of all lexicons (L) shows the highest correlation score for the "anger" emotion.
The combination of word embeds. (WE) and all lexicons (L) shows the highest correlation score for the "joy" emotion.
Table 7 shows the Pearson correlation of emotion intensity transfer on all target tweets.
The emotion intensity transfer Pearson correlation for fear is 0.65.
Table 2 provides the validation and test accuracy of different models on the CNN dataset.
The BrsDNC model achieves a higher test accuracy than the rsDNC model.
Table 1 provides information about the event detection performance on the CG task 2013 test dataset.
TEES has a precision of 61.42% and a recall of 52.93%.
The table compares the nested, overlapping, and flat event detection F1 (%) scores of TEES and SBNN (k = 8).
SBNN (k = 8) achieves higher F1 (%) scores than TEES in nested, overlapping, and flat event detection.
The T+I-Merge model outperforms the Unimodal model in terms of BLEU-1 score.
The T+I-Init model has a higher perplexity score than the Unimodal model.
Table 8 shows the "Translation" precision @10 when seeking to retrieve the true translation of an English sentence from a bag of 200k Italian sentences, or vice versa, averaged over 5k samples.
The method proposed by Dinu et al. achieves higher precision @10 for retrieving the true translation of English sentences to Italian using a word dictionary and Italian sentences to English using a phrase dictionary.
The precision values for English to Italian translation are higher than the precision values for Italian to English translation.
The precision values for English to Italian translation in this work are higher than the precision values in the other methods.
Table 5 shows the translation performance using the Europarl corpus as a phrase dictionary.
The translation performance of "Italian to English" in this work is better than the translation performance of "Italian to English" by Mikolov et al. and Dinu et al.
Table 6 shows the "Translation" precision @1 when retrieving the true translation of an English sentence from a bag of 200k Italian sentences or vice versa, averaged over 5k samples.
The precision @1 is higher when using the Italian to English phrase dictionary compared to the English to Italian phrase dictionary.
Table 7 shows the "Translation" precision @5 when retrieving the true translation of an English sentence from a bag of 200k Italian sentences, or vice versa, averaged over 5k samples.
The precision for Italian to English translation using SVD is 0.774.
The Rnmt system has the highest average performance among all the systems.
The Rnmt system performs the best on the MT04 dataset.
The system "Rnmt" outperforms the system "BiRNN+GCN" on the "test16" dataset.
The system "Rnmt" achieves a performance of 25.4 on the "test16" dataset.
The RNNsearch⋆ system has a BLEU score of 22.40.
The Rnmt system has an AER score of 45.66.
The table shows the results of different similarity functions in the matching pattern-based module on CQADupStack.
The "dot" similarity function has the highest accuracy score among the different similarity functions.
The "Multi30K-MMT" dataset consists of three splits: Training, Validation, and Test.
In the "Multi30K-MMT" dataset, each split has an equal number of images and captions.
Table 8 shows the robustness in BLEU[%] for different operations on the Clean (C), C+Soundex, C+Noise (N), and C+N+Soundex source data.
The C+N+Soundex operation has a BLEU score of 28.83.
The "Random" method has the highest values for both "Sum 1" and "Sum 2" compared to the other methods.
The "K-Means" method has the highest value in the "Max 3" column compared to the other methods.
The baseline method achieves a BLEU score of 31.10 for the EN-FR translation task.
The W+Metaphone method achieves a BLEU score of 33.31 for the FR-EN translation task.
Table 7 shows the translation results in BLEU[%] for ZH-EN using different coding methods.
The coding method "W+Pinyin in letters" achieves the highest translation result in BLEU[%] for ZH-EN.
Table 2 shows the fusion results approaches for two OOV KWS approaches in terms of the MTWV metric.
The MTWV metric for the "Lists" approach is 0.795 for all systems.
The OOV decoder approach outperforms the Proxies approach in terms of MTWV metric for the DNN1 AM.
The OOV decoder approach has a higher RTF than the Proxies approach for the DMN8 AM.
The table shows the effect of different beam widths on the development and test scores of sBLEU and cBLEU.
Increasing the beam width leads to a decrease in the sBLEU score on the development set.
The "Our model" outperforms all other methods in terms of F-1 score, sBLEU, and cBLEU.
The "citecite.angeli-10ALK10" method performs worse than the "Our model" in terms of F-1 score, sBLEU, and cBLEU.
The "With" encoder has higher F-1, sBLEU, and cBLEU scores compared to the "Without" encoder.
The "With" encoder has a higher F-1 score compared to the "Without" encoder.
"Our model" achieves the highest F-1 score among all the methods mentioned in the table.
"citecite.angeli-10ALK10" achieves the highest sBLEUG score among all the methods mentioned in the table.
The table presents results of different models on goal navigation with RL.
The t-RNetAttn model achieves the highest values in the "PW local PQ ↑" and "ISI PQ ↑" columns.
The table shows the results of value map regression for different models.
The t-RNetAttn model outperforms both t-CNN Janner et al. (2017) and t-UVFA Schaul et al. (2015) in terms of mean squared error (MSE) and mean difference (MD) for both local and global value maps.
VL-BERTLARGE achieves the highest performance on both test-dev and test-std among all the models listed in the table.
Pre-training improves the performance of VL-BERTBASE on the VQA dataset.
The validation accuracy for VL-BERTBASE is 73.8.
The QA → R accuracy for R2C is 44.0.
The table compares the performance of four different models on the RefCOCO+ dataset.
VL-BERTBASE performs better on the testA subset of the RefCOCO+ dataset compared to the other models.
The accuracy of the best model decreases as the grade level increases.
The accuracy for grade 1 is 71.43%, for grade 2 is 78.85%, for grade 3 is 67.86%, for grade 4 is 60.26%, and for grade 5 is 60.27%.
The average number of words per text is higher in "Our corpus" compared to the MCTest corpora.
The average number of words per question is the same across all corpora.
The experimental results in Table 4 are grouped based on different model groups.
The "DSW + Elmo" method has the highest dev percentage among all the methods.
Table 5 shows the experimental results of our proposed model on the test set of MCTest.
The model performs better on MC160 Test compared to MC500 Test.
The accuracy of the model decreases as the question length increases.
In the group of questions with a length of 26 or more, the model correctly answered 6 out of 9 questions.
WM has a higher accuracy than SSR.
PP has a higher accuracy than AoI.
The performance of KBC + Path is lower than KBC on both FB15k and Wiki100 datasets.
Table 8 shows the performance of Operator Training over Regex Queries.
The average MRR for the AVG model on the FB15k-Regex dataset is 5.43.
The "UNIT + AVG" model outperforms the "FP + MLP" and "MLP + MLP" models in terms of MRR.
The "MLP + MLP" model performs better than the other models in terms of HITS@1 for the "r+1" and "r1 r+2" metrics.
The inter-annotator agreement is higher for ordinal variables compared to nominal variables.
The Slovene migrants dataset has the highest number of posts compared to the other datasets.
The English LGBT dataset has the highest average number of annotations compared to the other datasets.
The Bleu scores for language pairs with English as the source language are generally higher than the scores for language pairs with English as the target language.
The language pair with Hindi as the source language has the highest Bleu score.
The classification models LR, SVM, DT, and RF perform better in terms of P-Micro when using the "Intuitive" features compared to the "Textual" features.
The classification model DT performs better in terms of F-Macro when using the "Intuitive" features compared to the "Textual" features.
The decision tree (DT) model has the highest performance in terms of precision, recall, and F1-score among all the models.
The decision tree (DT) model performs the best among all the models in terms of precision, recall, and F1-score for the "Textual" classification task.
The DT classifier has the highest precision among all classifiers for the Intuitive category.
The RF classifier has the highest F1 score among all classifiers for the Textual category.
The DT classifier has the highest performance in terms of precision, recall, and F1-score in the "Intuitive" category.
The RF classifier has the lowest performance in terms of precision and recall in the "Intuitive" category.
The precision for Decision Tree (DT) is higher than for the other classifiers in the "Intuitive P-Micro" column.
The F1 score for Random Forest (RF) is lower than for the other classifiers in the "Intuitive F-Macro" column.
The table shows the accuracy of four different methods: UM(SVHN), UM(MNIST), MM, and MFM.
The method MFM achieves the highest accuracy score among all the methods.
The model MFM outperforms all other models in terms of all evaluation metrics.
The factorized generation-discrimination factor contributes more to the performance improvement than the hybrid generation-discrimination factor.
The Multimodal Factorization Model (MFM) performs better than the Purely Generative and Discriminative Baselines in terms of reconstruction MSE and prediction accuracy.
Having all modalities present leads to lower reconstruction MSE and higher prediction accuracy compared to missing one or two modalities.
The ratio for CMU-MOSI in the MFM model is 0.307.
The value of rℓ for CMU-MOSI in the MFM model is 0.030.
The ratios for language, visual, and acoustic features in the POM dataset for personality traits prediction are 1.090, 0.996, and 0.898 respectively.
The values for language, visual, and acoustic features in the POM dataset for personality traits prediction are 0.996, 0.898, and 1.090 respectively.
The default configuration has a width of 96 and a depth of 4.
Increasing the width and depth of the convolutional layers results in a longer time for model pre-training.
The accuracy of the model improves as more training data is used.
The relative improvement in accuracy increases as the fraction of training data increases.
The GSA attack has the highest classification accuracy for the Word-CNN, LSTM, and Bi-LSTM models.
The proposed model, PG-Net + TA, achieves the highest ROUGE-1 F1 score compared to the other models.
The Attn Seq2Seq + TA model achieves a higher ROUGE-L F1 score than the Attn Seq2Seq model.
The table shows the word and character-level sequence labeling of the sentence "Anh rời EU hôm_qua." (UK left EU yesterday.).
The table provides word-level sequence labeling for the words "Anh", "rời", "EU", and "hôm_qua." in the sentence "Anh rời EU hôm_qua." (UK left EU yesterday.).
The model "ME" has the highest performance with a score of 88.78.
The model "Bi-LSTM-CNN-CRF" has a higher performance than the model "CRF".
Table II shows the correlation between metrics and semantic score.
GNMT has a semantic score of 0.692 for the STS metric, 0.734 for the TRS metric, and 0.927 for the GRS metric.
The baseline system performs better on the EN→RO dev set compared to the EN→RO test set.
The system with synthetic data performs better on the RO→EN test set compared to the RO→EN dev set.
The "+ensemble8" system achieves the highest Bleu scores on both the CS→EN dev and test sets.
The "+ensemble8" system does not have a Bleu score reported on the EN→CS dev set.
The "+10M synthetic" system achieves the highest scores in both the "best single dev" and "best single test" cells.
The scores for both the "ensemble4 dev" and "ensemble4 test" cells increase as the amount of synthetic data increases.
The table shows the POS tagging accuracies of different models on the GENIA and CRAFT datasets.
The "+ CNN-char" model has the highest POS tagging accuracy among all the models on both the GENIA and CRAFT datasets.
Table 4 shows parsing results on the GENIA and CRAFT corpora using different parsing systems.
The parsing system "Retrained Stanford-Biaffine-v2" achieves the highest LAS score of 33.87 on the CRAFT corpus without punctuation.
Table 5 shows LAS scores of Stanford-Biaffine on GENIA, categorized by frequent dependency labels in the left dependencies.
The LAS scores of Stanford-Biaffine on GENIA generally increase as the occurrence proportion in each distance bin decreases for all frequent dependency labels in the left dependencies.
The table shows the LAS by the basic Stanford dependency labels on GENIA using different dependency parsers.
The Biaffine parser achieves the highest LAS score for most of the dependency labels.
Table 7 shows the LAS by the CoNLL 2008 dependency labels on CRAFT.
The "Type" column represents different types of dependency labels.
The highest LAS for the POS tag "VB" in the GENIA Biaffine model is 89.68.
The LAS for the POS tag "VBD" is higher than the LAS for the POS tag "VBP" in the GENIA NNdep model.
Table 10 compares the UAS and LAS scores of four different parsing models: Biaffine, jPTDP, NLP4J, and NNdep.
The Biaffine parsing model has the highest UAS score among the four models.
The F1 scores for the systems vary across the datasets.
The precision score for the Stanford-Biaffine-v2 system is higher than the precision score for the jPTDP-v1 system.
SVM has the lowest Hamming Loss compared to other methods across all f-scores.
Naive Bayes has the highest Hamming Loss for f1.
The Hamming Loss decreases as we move from Naive Bayes to SVM to Elman RNN to RaKel.
RaKel has the lowest Hamming Loss among all the methods.
The SVM method has a lower Hamming Loss than the Naive Bayes method.
The RaKel method has a lower Hamming Loss f3 than the Elman RNN method.
The perplexity of GPT-2 small without fine-tuning on PTB is 129.6.
The number of parameters for GPT-2 is 345M.
BERT has the highest accuracy among BERT, GPT-2, and MASS.
BERT has the highest percentage of input tokens among BERT, GPT-2, and MASS.
The LDA method has the highest accuracy among all the methods for both CV and Test.
The "2013 Shared Task Winner" baseline has an accuracy of 84.5% for CV and 83.0% for Test.
The highest F1-score among all the classes is for GER.
The average Precision, Recall, and F1-score for all the classes is 0.87.
Our Model achieves the highest performance in terms of SQL BLEU4 and SQL METEOR.
Our Model achieves the highest performance in terms of Python BLEU4 and Python METEOR.
Our Model outperforms both DCS and CoaCor in terms of SQL MRR and SQL NDCG.
Our Model outperforms both DCS and CoaCor in terms of Python MRR and Python NDCG.
The DCS model performs better on Example(a) than on Example(b) in terms of code retrieval rank.
The CO3 model performs better than any other model in terms of code retrieval rank for Example(a).
The evaluation results in Table 2 are based on different methods for different language pairs.
The GM-EHD-JEA method achieves the highest scores among all methods for all three language pairs (ZH-EN, JA-EN, FR-EN).
The Chinese dataset in the DBP15K ZH− EN dataset has 66,469 entities.
The Japanese dataset in the DBP15K JA− EN dataset has 164,373 triples.
As the value of τ increases, the Max sub-space decreases.
The highest accuracy is achieved at τ = 0.10.
The table lists different values of α.
The table lists different numbers of decoding rounds.
The model "Holistic MTL-CNN (target only)" has an average accuracy of 62.45.
The model with 20 clusters has an average accuracy of 68.11.
The model "ASAP-Clus-FSL" has the highest average accuracy among the models listed in the table.
When the value of "clus" is 5, the models "RobustTC-FSL" and "ASAP-Clus-FSL" have higher average accuracy compared to when the value of "clus" is 10 or when there is no clustering.
The Fine-tuned holistic MTL-CNN model has an average accuracy of 30.36.
The model with clus=20 has the highest average accuracy of 37.59.
The table shows the accuracy of Multi-Task Learning on 1,491 dialog intent classification tasks.
The model with "clus=40" achieves the highest average accuracy in Multi-Task Learning.
The test set BLEU scores increase as the amount of paired data increases for all systems.
The GNMT-Multi-SSL system has the highest test set BLEU score in the EN→FR translation task.
The DKL value for GNMT-Multi-SSL is the highest among all the systems.
The table compares the test set BLEU scores for missing word translation between the VNMT and GNMT systems.
The BLEU score for the VNMT system in translating from English to French is 23.79.
GNMT-Multi-SSL achieves the highest BLEU scores for all unseen pair translations.
GNMT-Multi-SSL outperforms GNMT for unseen EN → FR translations.
The model size of the "Language Embedding" model is different from the other models.
The precision of the "MultiGPT-C (double heads)" model is higher than the precision of the "Baseline" model.
Table 4 provides an inference speed comparison for scenarios with different beam widths and sequence lengths, using different beam search setups.
Table 6 shows thread reconstruction results.
The Conv. Entity Grid method achieves the highest edge-level accuracy in thread reconstruction.
The final value chosen for the "Pooling" parameter is "Last".
The final value chosen for the "Hidden Size" parameter is 300.
The table compares the accuracy of different models, including Hierarchical Bi-LSTM-CRF, on the SwDA dataset.
The Hierarchical Bi-LSTM-CRF model achieves the highest accuracy (79.2%) compared to other models on the SwDA dataset.
The "Hierarchical Bi-LSTM-CRF" model achieves the highest accuracy of 90.9% on the MRDA dataset.
The "Naiive Bayes" model has the lowest accuracy of 82.0% on the MRDA dataset.
The system "PG + RLR" achieves the highest ROUGE scores (R-1, R-2, R-L) among all the systems.
The system "PG + RLC" achieves the highest factual F1 score among all the systems.
The RLR+C method consistently outperforms the PG Baseline method for all variables in terms of factual F1 scores on the Stanford dataset.
On average, the RLR+C method performs better than the PG Baseline method in terms of factual F1 scores on the Stanford dataset.
The table provides information about fluency in documents with a sample size of 50.
The fluency score for document B2 is 9.
The total number of Document MT ratings is 66.
The total number of Sentence Human ratings is 106.
The table shows the fluency scores for different documents with a sample size of 50.
Document B2 has a fluency score of 4 for attribute "a" and 18 for attribute "b".
The table presents data on fluency in document tasks with a sample size of 50.
The value in the "A" cell represents the data for category "a" in the "C" column.
The value in the "D a" column increases from row 1 to row 2.
The value in the "D b" column decreases from row 2 to row 3.
The table shows fluency scores for different documents labeled as "a", "t", and "b".
The count of occurrences for category "a" in the "H" column is 11.
The count of occurrences for category "b" in the "H" column is 13.
The frequency of the letter "a" is higher in the documents compared to the letter "t".
The frequency of the letter "b" is higher in the documents compared to the letter "t".
The values in the "H a" column are smaller than the values in the "H b" column.
The value in the "F t" cell is larger than the values in both the "F a" and "F b" cells.
The count of occurrences of "a" in category G is 1.
The table represents the adequacy of sentences with respect to two factors, labeled as "a" and "b", with a total of 104 sentences.
The adequacy of sentences labeled as "a" is 31 and the adequacy of sentences labeled as "b" is 22.
The size of the shared phrases in the full training data is 9.0M.
The BLEU score for the non-shared phrases in the NMT models is 0.
The models are categorized as Small, Base, and Big.
The shared size for all models is 7.0M.
The token "abafundi" appears 20 times in the Isolezwe articles of August 26 and 27, 2015.
The translation of "umfundi" is "student".
Zulu has the highest percentage of tokens that have a vowel as the final character.
Pedi-Sepedi has the highest number of occurrences of the letter "r" in the document.
The NIC corpus has the highest percentage of consonant-ending tokens among the listed corpora.
The UDHR corpus does not include article numbers in its examples.
The En→Ru Ensemble model achieves higher SacreBLEU scores than the En→Ru Single Model on all four test sets.
The perplexity scores for language models on the target language "En-De" are consistently higher than the scores for the target language "De-En".
The perplexity scores for language models on the target language "En-Ru" are consistently higher than the scores for the target language "Ru-En".
Table 2 shows model ablations on the full Wikipedia development set.
The 3-Level Cascade, Multi-loss (Ours) model achieves a performance of [BOLD] 52.18 on the Wikipedia Dev (EM) dataset.
The PLLM model has the highest accuracy among all the models listed in the table.
The N-gram model has the lowest accuracy among all the models listed in the table.
The model "PLLM" has the highest Spearman's R value among all the models.
The model "PLLM" has the highest correlation score with the Junior student among all the models.
The micro-F1 scores for "ours" are higher than the scores for the other methods on the SemEval, NYT, and WebNLG datasets.
The micro-F1 score for "BERTEM-MTB" on the SemEval dataset is 89.5.
Table 2 provides Bleu-4 evaluation and reconstruction loss for the Sentence Auto-Encoder on the Karpathy test split of MSCOCO 2014 caption dataset.
The Vanilla SAE model achieves a Bleu-4 score of 96.33 on the Karpathy test split of MSCOCO 2014 caption dataset.
The performance of the "Two-Stream Attention" model improves as the number of categories increases from 64 to 256, but then decreases when the number of categories increases to 512.
The "Two-Stream Attention" model with 256 categories achieves the highest score in terms of METEOR.
The method "CCX (best)" outperforms the method "CRX (best)" in terms of F1 score for both Sport x Politics and Sport x Religion.
The method "WE [ITALIC] max" performs better than the method "WE [ITALIC] hung" in terms of F1 score for both @425 and @450.
Table 3 shows the evaluation of concept embeddings for measuring entity semantic relatedness using Spearman rank-order correlation (ρ).
The CCX model gives the best results outperforming all other models in terms of entity semantic relatedness.
The CRX model with bootstrapping achieves the highest accuracy in concept categorization across all datasets.
The "+bootstrap" method consistently improves the accuracy of all models compared to their counterparts without bootstrapping.
avpixlat.info has the highest number of comments and words among all the websites.
nordiskungdom.com has the lowest number of comments and words among all the websites.
Stefan Löfven is the Swedish minister who is mentioned the most in the comments during the time period.
Morgan Johansson is the Swedish minister who is mentioned the second most in the comments during the time period.
"Our method" outperforms all other models in terms of accuracy on both SimpleQuestions and WebQuestions.
Using both words and relations as input improves the accuracy of the models on SimpleQuestions and WebQuestions.
The ABWIM model outperforms other models in both SimpleQA and WebQA datasets.
The Encoding-comparing model with bi-directional attention performs better than other models in the WebQA dataset.
The table provides annotation quality for three different languages: Indonesian, Russian, and Hindi.
The F1 score for non-speaker (NS) annotations is lower than the F1 score for fluent speaker (FS) annotations across all three languages.
Table 7 shows the automatic stylistic evaluation metrics comparing the base vocabulary with the vocabulary with style.
The comparison between the base vocabulary and the vocabulary with style in Table 7 shows a statistically significant difference.
The values of the BLEU, METEOR, CIDEr, and NIST metrics are increasing from row 1 to row 4.
The Average SER metric decreases from row 1 to row 4.
The table represents the results on BioASQ questions.
The MRR value for DIEBOLDS is 0.094.
Table 2 provides an analysis of word embedding strategies and similarity heuristics.
The virtual-sentence embedding strategy achieves the highest F-score of 74.8.
Table 1 shows the performance of different methods for dialogue session segmentation.
The method of "+ embeddings trained by virtual sentences, and heuristic-max similarity" has the highest F-measure score.
The table compares the performance of a retrieval dialogue system using two different methods: fixed context and session segmentation.
The session segmentation method outperforms the fixed context method in terms of precision at rank 1 (p@1).
The "SVMchar+ling+emoji" system has the highest F11 macro score.
The "All IND baseline" system has the highest accuracy.
The "BERTbase-uncased" system has the highest F1 macro score.
The "All NOT baseline" system has a higher F1 macro score compared to the "All OFF baseline" system.
The "BERTbase-uncased" system has the highest F1 macro score among all the systems in Table 2.
The "SVMcharacter-ngram" system has the highest accuracy score among all the systems in Table 2.
The SVM classifier has the highest F1 macro score among all the systems in Table 4.
The SVM classifier has the highest accuracy score among all the systems in Table 4.
Table 5 shows the accuracy on single-sentence binary classification tasks from SentEval, with experiments split between Sorted and Unsorted.
The GenSen model achieves a higher accuracy on all tasks compared to the other models.
The performance difference between the best random architecture and InferSent-3 is 2.3 on the TREC task.
The BOREP model achieves the highest performance on the SICK-E task.
"ST-LN" performs the best on the "TreeDepth" probing task compared to other models.
"BOREP (4096d, class.)" has the highest performance on the "WC" probing task compared to other models.
The number of descriptions whose prompting label agrees with the emotion labeled by at least four Phase-2 annotators is lower than the number of descriptions whose prompting label agrees with the emotion labeled by at least three Phase-2 annotators.
The emotion "Joy" has the highest number of descriptions whose prompting label agrees with the emotion labeled by all Phase-2 annotators in both German and English.
The table provides counts of post-annotation for different dimensions of emotions in German and English languages.
The Guilt dimension has the highest count of post-annotation for both German and English languages.
Table 4 shows the perplexity of real paired knowledge on four datasets.
The perplexity of the RealLM model is lower than the perplexity of the LM model.
M2 Transformer achieves the highest score in the B-1 metric among all the models listed in Table 2.
ORT achieves the highest score in the C metric among all the models listed in Table 2.
The performance of the models improves when using an ensemble/fusion of 4 models compared to an ensemble/fusion of 2 models.
The M2 Transformer model has the highest B-4 score among all the models.
The SCST method achieves a higher BLEU-1 score for c40 compared to c5.
The M2 Transformer method achieves a higher METEOR score for c5 compared to c40.
"Up-Down + CBS" has a higher overall CIDEr score than "NBT + CBS".
"M2 Transformer" has a higher in-domain SPICE score than "Transformer".
The SPICE score for "Obj." (Object) is higher than the SPICE score for "Rel." (Relation) for the Transformer model.
The model used in all rows of the table is Jasper DR 10x4.
The "Conv Mask" row has the lowest value in the "Dev Clean" column.
The Greedy WER for the combination of "Jasper 10x4" model with "Batch Norm" normalization is 6.15.
The Greedy WER for the combination of "Jasper 5x3" model with "Layer Norm" normalization is 11.29.
The presence of both the Sequential Encoder and Speaker-Level Encoder modules leads to the highest F1 score on the IEMOCAP dataset.
The presence of the Sequential Encoder module improves the F1 score on the IEMOCAP dataset.
The fusion of image and LSTM with α=0.6 achieves a top-1 accuracy of 20.6.
The LSTM model with text input achieves an msAP of 22.3.
The Fusion model achieves a top-1 accuracy of 1.5.
The Zero-shot Text model achieves a top-100 accuracy of 20.9.
The CNN (Image) method performs better than other methods in the Twemoji (Images) category.
The Zero-shot Fusion method performs better than other methods in the Twemoji (Images) category.
The number of training, validation, and test examples varies for each row in the table.
As the value in the "V" column increases, the corresponding values in the "Train,Val,Test" column decrease.
The number of training, validation, and test examples varies for different configurations.
As the value of the "V" parameter increases, the performance metrics decrease.
Table 2 provides information about the sort sequence for different configurations.
The number of training, validation, and test samples varies for different configurations.
Table 3 displays the F1 score of KMCNN and other methods.
All methods, including KMCNN, have a higher F1 score in the UniProtKB/ Swiss-Prot dataset compared to the NHGRI-EBI GWAS Catalog dataset.
The "hidden layer dimension" parameter has two different values: 100 and 30.
The "filter sizes" parameter has three different values: 1, 2, and 3.
KMCNN has higher precision than MCNN, KCNN, and Lee et al.(2018) in all datasets.
UniProtKB/Swiss-Prot has higher precision than NHGRI-EBI GWAS Catalog, MycoSet, GWAS Catalog 2019(a), and GWAS Catalog 2019(b) in all methods.
Table 5 represents the recall values of KMCNN and other methods.
The method "G2S dyn+BERT+RL" performs better than "G2S dyn" according to the BLEU-4 score.
The method "G2S sta+BERT" performs better than "G2S sta w/o BiGGNN, w/ Seq2Seq" according to the BLEU-4 score.
The "G2S dyn+BERT+RL" method achieves a BLEU-4 score of 17.55.
The "G2S sta+BERT+RL" method achieves a ROUGE-L score of 46.02.
The table presents the human evaluation results on the SQuAD split-2 test set for three different methods: MPQG+R*, G2S sta+BERT+RL, and Ground-truth.
The KBLSTM-CRF model achieves the highest precision, recall, and F1 score among all the models.
The BiLSTM model has a lower precision, recall, and F1 score compared to the BiLSTM-CRF, BiLSTM-Fea, BiLSTM-Fea-CRF, and KBLSTM models.
The F1 score is higher when using both NELL and WordNet as knowledge bases compared to using only NELL or only WordNet with the BiLSTM-Fea-CRF model.
The F1 score is higher when using both NELL and WordNet as knowledge bases compared to using only NELL or only WordNet with the KBLSTM-CRF model.
"JointEventEntity" achieves the highest F1 score of 68.7 among all the models.
The F1 score of "KBLSTM-CRF" is 69.7∗.
The methods in the table are compared at varying difficulty levels, from short paths consisting of one landmark to long paths consisting of four landmarks.
Our method performs better than the other methods at all difficulty levels.
The method "Ours" outperforms the other methods in terms of SPL, Navigation Error, and Total Steps.
The method "Ours" requires fewer total steps compared to the other methods.
The Arabic language has the highest accuracy for the Hausa Bridge, with a score of 0.75.
The Uzbek language has the highest accuracy for the Uzbek Bridge, with a score of 0.79.
The F1 scores in the table range from 77.3 to 85.6.
As the pattern length increases, the Spearman's rank correlation also increases.
The average Spearman's rank correlation for all pattern lengths is 0.370.
Table 6 provides information about the specific entropy of the Sample texts based on English words.
The entropy of the text normalized to 10000 word length is higher for Sample II compared to Sample I, III, and IV.
The perplexity values for the "baseline" model are higher than the perplexity values for the other models.
The "spelling" model has the lowest perplexity value after handling OOV words.
The "dict, MP, sum, no back-prop (D1)" model performs better than the baseline model on the EM dev score.
The "spelling+lemmas (SL)" model performs slightly worse than the spelling model on the EM test score.
The dictionary method achieves higher accuracy on the SNLI dataset compared to the baseline and spelling methods.
The GloVe method performs better on the MultiNLI matched dataset compared to the baseline and spelling methods.
The table shows the results of different neural machine translation models on the En-De, En-Vi, and De-En test sets.
The "DynamicConv" model achieves the highest score on the De-En test set compared to other models listed in the table.
The "UpDown" model has the highest BLEU-4 score among all the models.
The "Explicit Sparse Transformer" model has the highest CIDEr score among all the models.
The Adaptive-span model has the lowest number of parameters among all the models listed in the table.
The Adaptive-span model has the lowest BPC score among all the models listed in the table.
Table 5 shows the results of an ablation study of sparsification at different phases on the En-Vi test set.
The BLEU score for the "T&P" phase is higher than the BLEU score for the "Base" and "T" phases.
The "Augment directed relations only" variant has the highest F1 score.
The "Augment Other only" variant has a lower F1 score than the "No Augmentation" variant.
The performance of CNN is slightly lower at Depth 2 compared to Depth 1.
The performance of RNN is higher compared to CNN.
The table compares the performance of various models for relation classification.
The DRNNs model with data augmentation achieves the highest F1 score of 86.1.
The average vocabulary size per sentence for the English-German translation is 17.2.
The coverage of the reference for the English-Romanian translation is 89.3%.
Table 5 compares the BLEU scores obtained by a baseline SMT system and an adapted SMT system.
Table 2 provides comparative BLEU scores for English/German systems.
The back-off LM approach has a lower BLEU score on Day 2 compared to Day 1.
The percentage of ADJ decreases as the kernel size increases.
The percentage of words categorized as "rest" is the lowest among all kernel sizes.
The sub-type "Gradable" applies only to adjectives and adverbs.
The sub-type "Reversives" does not have a basis for comparison.
Table 7 shows the pointwise mutual information (PMI) of word pairs.
The average PMI for the word pairs in the high-contrast set is 1.471.
The table shows the distributional similarity of word pairs using a measure proposed in Lin (1998).
The average distributional similarity for the opposites set is 0.064.
The F-score for the "ALL" category is 0.85.
There are 1044 instances of antipodal questions.
The table shows the percentage of contrast questions correctly answered by the automatic method, where different question sets correspond to different parts-of-speech.
The F-score for all parts-of-speech combined is 0.85.
The difference in BLEU score between paraphrases with and without removing repeats is small for both NAT-base and PNAT models.
The PNAT model performs better than the NAT-base model in terms of BLEU score on the test set.
The PNAT model achieves the highest scores for both Paraphrase(BLEU) on the validation set and Paraphrase(BLEU) on the test set.
The Seq-to-seq(GRU) model has a lower Paraphrase(BLEU) score on the validation set compared to the other models.
The table presents the results on the validation set of WMT14 DE-EN with different position strategies for various models.
The PNAT w/ HSP model achieves a speed improvement of 12.5 times compared to the Transformer(beam=4) model.
The "Subsystems TVS-PLDA" has the lowest "Score fusion mean" among all the score fusion methods.
The "Duration-Based" score fusion method has the lowest "Verification EER" among all the score fusion methods.
The NIPS-5k dataset has the highest compactness score in the "HLTA-batch" method.
The compactness scores of the "nHDP-bow" method decrease as the dataset size increases.
The table shows the probabilities of different words given the latent variable Z21.
The word "space" has the highest probability given the latent variable Z21.
Table 6 shows the per-document held-out loglikelihood on binary data.
HLTA-step performs better on "News-1k", "News-5k", and "NYT" than the other models.
The "+superAE (this paper)" model outperforms all other models on the LCSTS test set in terms of ROUGE-1, ROUGE-2, and ROUGE-L scores.
The models with the "RNN-cont" prefix perform better than the models without the "RNN-cont" prefix in terms of ROUGE-1, ROUGE-2, and ROUGE-L scores.
The team "sabir" achieved the highest precision at 5 (P@5) score in the TREC-COVID Round 1.
The team "TU_Vienna" achieved the lowest mean average precision (MAP) score in the TREC-COVID Round 1.
The "DenSPI (unpublished)" model with the "2020-04-10-recent" training data performs the best in terms of Interrogative EMsent@1 on the "NQ" dataset.
The "DenSPI + Sparc" model with the "2020-04-10" training data performs the best in terms of Keyword EMsent@50 on the "SQuAD" dataset.
The event "Ottawa shooting" has the highest variation ratio uncertainty estimate among all events in the table.
The SVM classifier has a higher accuracy than the RF classifier for the "Twitter 15" dataset.
The supervised rejection method has a higher macro F-score than the unsupervised rejection softmax method for the "PHEME" dataset.
The table shows the per-class f1-scores of a branch-LSTM model on different datasets.
The f1-score for the "True" class on the Twitter 16 dataset is 0.88.
The values in the "No calibration S" column are higher than the values in the "Histogram Binning S" column for both "PHEME" and "Twitter 15" datasets.
The values in the "No calibration VR" column are lower than the values in the "Histogram Binning VR" column for both "PHEME" and "Twitter 15" datasets.
Table 9 shows the performance (accuracy) after per-fold unsupervised rejection on the PHEME dataset for all types of uncertainty.
The uncertainty measures "Variation ratio", "Entropy", and "Variance" have a value of 0.413 for the 50% uncertainty level.
Table 10 shows the performance (accuracy) after per-fold unsupervised rejection on the Twitter 15 dataset for all types of uncertainty.
The value of the Entropy for the 50% uncertainty level is 0.756.
The variance decreases as the percentage of uncertainty decreases.
The aleatoric uncertainty increases as the percentage of uncertainty decreases.
The DNN ReLU model has a lower word error rate (WER) on both the dev93 and eval92 datasets compared to the GMM Kaldi tri4b model.
The DNN Kaldi s5 model has a lower word error rate (WER) on both the dev93 and eval92 datasets compared to the GMM Kaldi tri4b model.
Adding more layers to the BLSTM model does not significantly improve the WER for the "dev93" and "eval92" datasets.
Table 3 shows the ablation effects of the TC-DNN-BLSTM-DNN model.
The BLSTM-DNN model performs better than the DNN-BLSTM model in terms of dev93 WER and eval92 WER.
Table 4 describes the effects of distributed optimization for the TC-DNN-BLSTM-DNN model.
The ASGD experiments in Table 4 use 3 independent SGD shards.
Table 5 shows the effect of decoding hyperparameters (bi-directional).
The "me" hyperparameter has values of 2, 6, and 10 in the table.
As the value of "ms" increases, the value of "eval1" decreases.
The average percentage across the three evaluations is 8.4.
The average evaluation score decreases as the value of "me" increases.
As the value of V increases, the average evaluation percentage decreases.
As the value of V increases, the evaluation percentage decreases.
Table 5 shows the effect of decoding hyperparameters (bi-directional).
When the decoding hyperparameter is set to 10 in the bi-directional model, the test accuracy is 18.9%.
When vectorization is performed and GPU is used, the RTF is 0.18.
The performance on the test set is higher than the performance on the dev set.
The table shows the F1-score with different implementations of the sequence modeling layer.
The proposed (CNN) model performs better on OntoNotes compared to the ExSoftword (Transformer) model.
The average F1-score of the method with different implementations is higher for WP compared to MP and SWP.
The F1-score of the method with WP implementation is higher for Weibo compared to the other two implementations.
The proposed (CNN) model has the highest inference speed for all four datasets.
The proposed (LSTM) model has an inference speed of 73.72 on the OntoNotes dataset.
The LR-CNN proposed by Gui et al. achieves the highest F1 score of 93.71 on MSRA.
The Precision, Recall, and F1 scores of Chen et al. 2006 are 91.22, 81.71, and 86.20, respectively.
The "FactorCell" model performs the best in terms of perplexity and classification accuracy across all four word-based datasets.
The "FactorCell" model outperforms the "ConcatCell" model in terms of classification accuracy on the AGNews dataset.
The "FactorCell" model outperforms the other models in terms of perplexity and classification accuracy for both the EuroTwitter and GeoTwitter datasets.
The "FactorCell" model achieves a lower perplexity than the other models for the GeoTwitter dataset.
The top boosted words for the Amalfi hotel are "amalfi, chicago, allegro, burnham, sable, michigan, acme, conrad, talbott, wrigley".
The Four Points Sheraton hotel has a rating of 3.0.
The R2@1 score for the U2R-IMN model is 0.936.
The R10@5 score for the subset with 2 utterances is 0.911.
The table provides the results of ablation tests of the C2R and R2C representations in the U2U-IMN model on four different datasets.
The table presents the results of ablation tests of the distance-based prior for interactive matching on four different datasets.
The BLEU scores for the strategies WM0, WM0+GE, and WM0+GE+TT are increasing.
The PP values for the strategies WM0, WM0+GE, and WM0+GE+TT are decreasing.
WM performs better than all other models in terms of fluency in the Quatrains category.
Human performs better than all other models in terms of meaning in the Iambics category.
The size of the Hindi dataset is 34M and the SG score is 50.3.
The size of the DE ZG dataset is 44M and there are no Out of Vocabulary words encountered while testing.
Table 8 compares the performance of the SG model and the SG + Exp model regarding word pair similarity.
The SG + Exp model performs better than the SG model in capturing the word similarity for the word pair "censorships".
Table 9 compares different systems after the Morph step based on their Spearman ρ scores.
The system SG + Exp + Morph performs better on the RG task compared to the other systems.
The attention weights for the made-up word "petfel" differ in different context sentences.
The highest attention weight for the made-up word "petfel" is 0.31.
fastText has the highest number of occurrences among all the models.
FCM and AM have the same number of occurrences at 49.1.
The model "AM" outperforms all other models in terms of accuracy and F1 score for all frequency ranges.
The model "FCM" performs the worst in terms of accuracy and F1 score for the frequency range [8,16).
The additive model performs better with 2 context sentences compared to 4 and 6 context sentences.
The AM-ctx model performs the best with 6 context sentences compared to 2 and 4 context sentences.
FCM performs significantly better than skipgram for 1, 2, 4, 8, and 16 contexts.
AM performs significantly better than fastText for 1, 2, 4, 8, 16, 32, 64, and 128 contexts.
The match percentage for the frame "Calendric_unit" is 99.5%.
The match percentage for the frame "Locative_relation" is 87.4%.
The "Cyclical" model performs better than the "Baseline (Unsup.)" model in terms of captioning evaluation.
The "Cyclical" model performs better than the "Attention consistency" model in terms of grounding evaluation.
The Cyclical* method outperforms the Baseline* method in terms of F1 scores for both overall and per sentence grounding evaluation.
Removing the self-attention mechanism improves the performance of the GVD method in terms of F1 scores for both overall and location grounding evaluation.
The baseline method performs better than the other methods in the model ablation study on the Flickr30k Entities val set.
The cyclical method performs better than the other methods in the model ablation study on the Flickr30k Entities val set.
The "Cyclical" method achieves the highest F1all score in the Grounding Evaluation.
The "GVD" method outperforms the "GVD (w/o SelfAttn)" method in terms of F1loc score in the Grounding Evaluation.
The "Cyclical (zero-loss)" method outperforms the "Cyclical" method in terms of F1all per sent and F1loc per sent.
The "Cyclical (zero-representation)" method performs better than the "Baseline" method in terms of B@4 score.
The "Cyclical (zero-representation)" method performs better than the "Baseline" method in terms of "F1all per sent".
The "Cyclical (zero-representation)" method performs better than the "Baseline" method in terms of "F1loc".
The F1all per sent score increases as the weightings on decoding and reconstruction losses change from (0.8, 0.2) to (0.2, 0.8).
The F1loc per sent score decreases as the weightings on decoding and reconstruction losses change from (0.8, 0.2) to (0.2, 0.8).
The highest correlation score for the 2-variable thematic-fit evaluation is in the "sync shared" condition.
The correlation score for the KS108 Verb only condition increases as the synchronization method changes from asynchronous to synchronous and from separate to shared.
The evaluation is performed on two different setups: "SV" and "VO".
Our method (Ours SVO) outperforms our method with a different configuration (Ours SV-VO) in terms of Spearman's ρ correlation.
The "Paragraph-level Model + CRF" achieves the highest Macro F1 score among all the models.
The "Paragraph-level Model + CRF" outperforms the "GRU" model in terms of Macro F1 score.
The predicted count for STA in the training set of MASC+Wiki with 10-Fold Cross-Validation is 12558.
The predicted count for GENA in the training set of MASC+Wiki with 10-Fold Cross-Validation is 431.
Table 2 compares different models on insuranceQA using max-pooling or attention as the composition module.
The QA-LSTM with attention model outperforms the global self-attention with attention model on Test1.
The table compares the performance of three different models: local self-attention, local self-attention (+gate), and GGSA.
The GGSA model is the fastest among the three models, with a time of 0.32 seconds per batch.
The "iGGSA" model outperforms other models in terms of test performance on both Test1 and Test2.
The "global self-attention" model performs better than the "group self-attention" and "group self-attention (+offset)" models in terms of test performance on both Test1 and Test2.
The table displays the results of different optimization techniques on the yahooQA dataset.
The iGGSA with attention optimization technique achieves the highest P@1 and MRR scores among all the models listed in the table.
The table compares the performance of GGSA with max-pooling and iGGSA with attention on yahooQA.
iGGSA with attention (pairwise loss) outperforms GGSA with max-pooling (pairwise loss) in terms of P@1 on yahooQA.
The system ♡ Slug-alt has the lowest normalized average score.
The system ♡ Gong has the highest ROUGE-L score.
TGen has a higher mean value than Slug.
ZHAW1 has a lower mean value than Chen.
The KGLM model has the lowest perplexity score of 44.1.
The KGLM model has the highest UPP score of 88.5.
The models "RDGCN", "HGCN-s", "RD", "GCN-s" have the best performance in all three language pairs.
The "BootEA" model has the highest performance in the ZH-EN language pair.
The highest value in Table (a) is 71.
There are missing values in Table (a).
Table 2 provides the accuracy of our model and baselines on word analogy tasks for Czech, German, English, and Italian separately in both semantic and syntactic categories.
Table 3 shows the Spearman’s rank correlation coefficient between human judgement and model scores for different methods using morphology to learn word representations.
The table is titled "De Syntactic". 
The highest values in each row are marked as bold. 
The values in the cells with row and column numbers greater than or equal to 5 are mostly empty, except for the cell at row 5 and column 6 which is [BOLD] 80.
The values in the cells with row numbers greater than or equal to 4 and column numbers greater than or equal to 5 are mostly empty, except for the cells at row 5 and column 6, and row 6 and column 6 which are both [BOLD] 80.
The table contains missing values in some cells.
The highest values in each row are marked in bold.
The sisg model performs better than CLBL, CANLM, LSTM, and sg models in terms of test perplexity for all languages.
The Russian language has the largest vocabulary size among the five languages.
The "No Reading" metric is lower than the "Full Model" metric for both Tencent and Yahoo datasets.
The "W-BLEU-1" metric for the Yahoo dataset is higher than that of the Tencent dataset.
The table includes evaluation results for two different datasets, Tencent and Yahoo.
The DeepCom model outperforms the other models in terms of METEOR, W-METEOR, Rouge_L, W-Rouge_L, CIDEr, W-CIDEr, BLEU-1, and W-BLEU-1 metrics.
The total number of instances for each language is the sum of the positive and negative instances.
Turkish has a higher number of negative instances compared to positive instances.
The F1 Score for Greek is higher than the F1 Score for Turkish.
The Accuracy for Arabic is higher than the Accuracy for English.
The F1 score for our model with zero-shot learning is 0.700.
The accuracy for the ParaschivCercel2019 model is 0.794.
The "USchema+LSTM+Dict" model has the highest F1 score among all the models.
The "USchema+LSTM+Dict" model has a higher recall than the "LSTM" and "LSTM+Dict" models.
The system "Ours (local)" has the highest F1 score of 77.2.
The system "Ours" with 3x ensemble has the highest F1 score of 78.9.
The system "Ours (local)" has the highest F1 score of 77.2.
The system "Ours" with 3x ensemble has the highest F1 score of 78.9.
Increasing the amount of noise in the "noise C" column decreases the F1 score in the "noise E" column.
Increasing the amount of noise in the "noise C" column decreases the F1 score in the "Outer" column.
The fluency scores of the generation models increase from S2S-GRU to Our model.
The coherency scores of the generation models increase from S2S-GRU to Our model.
Our model outperforms S2S-GRU, CopyNet, and Transformer in terms of ROUGE-1 score.
Our model outperforms S2S-GRU, CopyNet, and Transformer in terms of ROUGE-L score.
The "Additive-BLSTM In-Delta" model has the lowest rmse values for both Mandarin Syl Level and Mandarin Utt Level compared to other models.
The "Additive-BLSTM In-Delta" model has the lowest rmse value for Cantonese Syl Level compared to other models.
BERTmulti achieves the highest recall@1 and recall@4 scores among all the methods.
BERTmulti achieves the highest normalized recall@1 and recall@4 scores among all the methods.
UMLS has the smallest number of concepts compared to Wikipedia and WikiUMLS.
WikiUMLS has the highest average number of alias names per concept compared to UMLS and Wikipedia.
The DQI C1 values for the term "Good" are higher than the DQI C1 values for the term "Bad" in all three categories (T1, T2, T3).
The term T2 has the highest DQI C1 value among all three terms.
The values in the "Split" column represent different splits of the dataset.
The values in the "Good" column represent the scores for the "Good" category at different levels of SIML.
The values in the "ISIM=0.3" column are smaller than the values in the "ISIM=0.4" column, and the values in the "ISIM=0.4" column are smaller than the values in the "ISIM=0.5" column, and so on.
The value in the "Good" row and "ISIM=0.6" column is the largest among all the values in the table.
The "T5" split in DQIc5 has a value of 20.3518 for the "Good" term.
The table shows the sample counts for the "Good" and "Bad" splits across the labels.
The "Bad" split has a higher count of samples labeled as "Contradiction" compared to the "Good" split.
The table shows the values of SSMIL for three different splits.
The "Bad" values increase as the SSMIL value increases.
The table shows the success rate of a blackbox attack on BERT-QA after inserting a whitebox generated adversarial sentence to different positions.
The success rate of the blackbox attack is highest when the adversarial sentence is inserted at the front.
The table compares the success rates of blackbox attacks on sentiment analysis using different models.
The success rate for the Concat Attack using adversarial words is higher for BERT than for SAM.
Table 4 shows the whitebox attack results on QA in terms of exact match rates and F1 scores.
The F1 score for the BERT model in the "Answer Targeted Attack Adv(Word)" column is [BOLD] 52.6.
The table represents blackbox attack results on QA in terms of exact match rates and F1 scores.
The BiDAF (w) model performs better than the BERT (w) model in terms of exact match rates.
The Majority method has an accuracy of 0.95 in QA.
The table shows the performance of different methods on a QA task.
The AdvCodec(Word) method has a higher F1 score than the AdvCodec(Sent) method.
Table 11 provides whitebox attack results on BERT-QA in terms of exact match rates and F1 scores.
The success rate of the Position Targeted Attack Adv(Sent) on BERT is 29.3%.
The BERT model has an EM score of 29.3 for the random position targeted attack.
The BERT model has an F1 score of 33.2 for the question-based position targeted attack.
The table shows the success rate of blackbox attacks after inserting whitebox generated adversarial sentences at different positions for BERT-QA.
The success rate of blackbox attacks is highest when the whitebox generated adversarial sentences are inserted at the front position for both "Adv(Word)" and "Adv(Sent)" attacks.
The count of labels with political bias left and factuality low is 243.
The count of labels with center political bias and mixed factuality is 249.
The +ULR method improves the performance of the Multi-lingual NMT baseline on the Ro language.
The + MoLE method improves the performance of the Multi-lingual NMT baseline on the Ko language.
The "Full data (612k) NMT" model achieves the highest BLEU score among all the models.
The addition of "MoLE" and "BT" to the "Multi-NMT + ULR" model improves the BLEU score.
The table presents cross-validation results for ablation tests.
The values for "gout" are higher than the values for "gn" and "gin".
The highest accuracy for the "Who" question type is 50.56 for CNN2.
The question type with the highest number of questions is "What" with 678 questions.
Among all the topics, "Country" has the highest MRR scores in all four models.
The MRR scores for CNN2 and RNN1 are higher than the scores for CNN0 and RNN0.
The table shows accuracies on the SelQA evaluation set for answer triggering with respect to different topics.
The "Country" topic has the highest accuracy for answer triggering on the SelQA evaluation set.
The "EGR" model achieves the highest Egregious F-measure among the three models.
The "Text-based" model outperforms the "Rule-based" model in terms of Non-Egregious recall.
The EGR model has the highest F-score among all the models.
The Text-based model has a higher precision than the Rule-based model.
The IAPR-TC12 dataset has 9k pairs for training, 500 pairs for validation, and 1k pairs for testing.
The IAPR-TC12 and Multi30k datasets have the same number of pairs for training in English.
The performance of our method with Lpivot on Multi30k De-En is 61.3.
The performance of our method with Lpivot is lower than the performance of the 3-way model on both IAPR-TC12 En-De and Multi30k En-De.
The image caption model achieves a higher BLEU4 score on the IAPR-TC12 dataset compared to the Multi30k dataset.
The image-pivoted translation model achieves a higher BLEU4 score on the IAPR-TC12 dataset compared to the Multi30k dataset.
The perplexity for reg-Ranking is lower than the perplexity for reg-Binary.
The perplexity for MLE is higher than the perplexity for NCE.
The Proposed Method performs better in terms of G-score on Yelp than on Amazon.
MDAL performs better in terms of ACC on Yelp than on Amazon.
The accuracies for both NC and WC are not provided for the "Our work" model.
There are three different methods used in the experiment: Vanilla, PGD, and FreeLB.
The maximum increase in loss for the PGD method in the RTE task is 4.9.
The methods listed in the table are ordered in increasing order of accuracy.
The addition of attention improves the F1 scores for the methods.
The method "Attentive (Dependency) Tree-LSTMs" has the lowest MSE value among all the methods.
The table shows the accuracy results of different variations of RNN, CNN, and attention models.
The addition of attention improves the accuracy of the models.
The predicted scores for Sentence 1 and Sentence 2 are different for each row in the "SICK" dataset.
The ground truth and predicted scores are different for each row in the "MSRP" dataset.
The table provides perplexity and Bleu scores for different models on validation data.
The OpenSubtitles F model has a Bleu score of 4.6.
When words exposed to direct visual evidence during training are discarded, there are 97 unseen abstract words out of a total of 127 words.
There are 198 words in total.
MMSkip-gram-A has a strong negative correlation with the word frequency.
Kiela et al. has a negative correlation with the word frequency.
The table shows the accuracy on the Spider development and test sets for two different splits: "RAT-SQL" and "RAT-SQL + BERT".
For the "RAT-SQL" split, the accuracy decreases as the difficulty level increases.
RAT-SQL has a lower logical form accuracy (LF Acc%) on both the development and test sets compared to MQAN and Coarse2Fine.
PT-MAML has the lowest execution accuracy (Ex. Acc%) on both the development and test sets among all the models.
The accuracy of the RAT-SQL model increases when oracle columns and oracle sketch information are provided.
The RAT-SQL + Oracle sketch + Oracle columns model achieves the highest accuracy among all the models.
Table 4 shows relevance scores for the word "minister" in three different test sentences from the CoNLL dataset.
Each row in the table represents a different sentence containing the word "minister".
The scores in the table represent the sentiment or importance of each word.
The words in the table are arranged in descending order based on their scores.
The word "number" has the highest score of 10.148.
The word "sites" has the highest score of 18.487.
"Arc-II" has the highest P@1(%) value among all the models in Table 2.
The P@1(%) value for random guessing is 20.00 in Table 2.
The word2vec model performs better than the Context Encoder model on the analogy task.
The word2vec model performs better than the Context Encoder model on the "capital-common-countries" category of the analogy task.
Table 9 compares the macro and micro average LSTM results to SVM unigram and bigrams and word embeddings NLM WSD set.
The LSTM S500 W50 configuration has a higher micro accuracy compared to the SVM WE S500 W50 configuration.
Table 12 shows the macro average results for ambiguous words grouped by the number of senses for the MSH WSD set.
The method "SVM WE S500 W50+Unigrams" achieves a macro average result of 95.22 for ambiguous words with 3 senses.
The "+STM" model has the highest F1 score of [BOLD] 49.2†.
DSQA+STM outperforms DSQA+DS significantly.
The BERT-HA+STM model outperforms other models in terms of accuracy on most datasets.
The BERT-HA+STM model has the highest macro-average F1 score on most datasets.
Table 5 presents the evaluation results of different models on the CoQA and MultiRC datasets.
The precision of evidence extraction for MultiRC at P@1 using the +STM (iter 3) model is 69.5.
The RoBERTa-HA+STM model has a higher answer prediction accuracy (Ans. Acc) than the RoBERTa-HA model.
The RoBERTa-HA+STM model has a 40% higher evidence extraction accuracy (Evi. Acc) than the RoBERTa-HA model.
The model "UIE-BERT" achieves the highest V-measure F1 score on the "NYT+FB" dataset.
The model "SelfORE" achieves the highest B3 F1 score on the "T-REx SPO" dataset.
The inter-annotator agreement is lower for challenging relations compared to the other two categories.
The inter-annotator agreement is higher for control relations compared to the other two categories.
The table shows the test set F1 scores on TACRED, the revised version, and the scores weighted by difficulty.
SpanBERT performs better on the revised version compared to the original version.
The average BLEU score improves in both iterations for the language pairs Ar→Fi, He→Fi, Nb→Sl, Ar→Nb, He→Nb, and Sk→Nb.
The average BLEU score increases by 1.57 in the second iteration compared to the initial score.
The total number of instances in the dataset is 153,383.
The number of instances in the test set for each emotion is 4,794.
The team "Amobee" achieved the highest rank in the IEST 2018 competition.
The team "Amobee" achieved the highest F1 score in the IEST 2018 competition.
The category "J" has the highest number of gold labels.
The number of predicted labels for category "Su" is 183.
MLing & FineTuning (FT) achieves the lowest WER and PER scores for all languages compared to other models.
MLing + SWBD & FT achieves the lowest WER and PER scores for Haitian compared to other models.
The table compares the word error rate (% WER) on test languages for two different models: MLing + BAB300 and MLing + SWBD.
MLing + SWBD performs better than MLing + BAB300 in terms of word error rate (% WER) for the languages Kazakh, Turkish, Haitian, and Mongolian.
The frame and phoneme inputs result in an improvement in BLEU scores for both the dev and test sets.
Using frame and phoneme inputs reduces the training time for Spanish-English SLT.
The target preprocessing method "1k bpe" has the highest values in the "Frames test" and "Phonemes test" cells.
The target length for the "1k bpe" preprocessing method is the lowest among all the target preprocessing methods.
Table 4 compares the performance of frame vs phoneme input on Spanish ASR, including the average reduction in WER and training time.
Phoneme input achieves lower WER than Frame input on both the development and test sets.
Table 4 provides information about the adjusted hyperparameter ranges for the AV method Unmasking.
The original setting for the hyperparameter U_{1} (Initial feature set sizes) is 250.
The method "COAV" achieves the highest accuracy among all the methods.
The method "NNCD" achieves the highest AUC among all the methods.
The accuracy of the Simulated dataset is 88.0%.
The accuracy of the Excel dataset is higher than the accuracy of the FigureQA dataset.
The accuracy of bar chart prediction is highest for the "Title" object in all datasets.
The accuracy of bar chart prediction is higher for the "Upper value" object compared to the "Lower value" object in the FigQA dataset.
Table 2 shows the development performance with cross-domain concatenation for languages that have multiple treebanks.
The concatenation method performs better than the single method in terms of performance on the "lassysmall" treebank.
Table 2 shows the development performance with cross-domain concatenation for languages that have multiple treebanks.
The concatenation method performs better than the single method for both English and French.
The results in the table are obtained through cross-lingual concatenation.
The highest score for the "ug_udt" dataset is 69.27, and the highest score for the "uk_iu" dataset is 88.84.
Self-Chat ACUTE-Eval (engagingness) shows a small win (not significant) for using persona contexts after fine-tuning on the BST tasks.
The Generative BST 2.7B model with persona context performs better than the Generative BST 2.7B model without context.
The perplexity of the BST RetNRef model is higher than the perplexity of the pushshift.io Reddit Generative model and the BST Generative model on ConvAI2, WoW, ED, and BST datasets.
The perplexity of the pushshift.io Reddit Generative model is higher than the perplexity of the BST Generative model and the BST RetNRef model on ConvAI2, WoW, ED, and BST datasets.
Figure 8 compares the engagingness of different beam-blocking variants using the Self-Chat ACUTE-Eval metric.
Blocking both context and response 3-grams during generation leads to higher scores in terms of engagingness compared to blocking only context 3-grams.
The table shows the counts of the most common 3-grams from the BST Generative 2.7B model (likelihood) from conversation logs, the same model trained with unlikelihood, and human logs.
The count for the 3-gram "Do you have" in the MLE column is 110.
The "actb (extended)" target agent label has more relations than the "actb" target agent label.
The "adsorption (extended)" population has more populations than the "adsorption" population.
The reproduced results for the "EN-DE" language pair are slightly better than the original results.
The "Rand. Compl." ablation performs slightly better than the "Rand. Cut." ablation in terms of average accuracy for the "EN-ES" language pair.
Among the four languages, Persian (FA) has the highest best accuracy.
Among the four languages, Estonian (ET) has a higher average accuracy than Latvian (LV).
The table provides MAPs for in-vocabulary (IV) and out-of-vocabulary (OOV) words of different models.
The SCorP(+TW) model has the highest MAP for in-vocabulary (IV) words.
Table 4 provides the Sememe prediction MAPs for words with different frequency ranges.
Words with a frequency of 5000 or more have a MAP of 62.18 for SCorP(+TW,SE).
Table 1 shows the main results on ACE 2005 using different methods.
Table 2 shows the performance of our model with different prediction modules on various evaluation metrics.
The Linear module outperforms the MLP and Biaff modules on the "bc" evaluation metric.
The "Ours (multiple-per-pass)" method has a higher score in terms of Averaged F1 Macro compared to the "Ours (single-per-pass)" method.
The "Ours (multiple-per-pass)" method has higher scores in both Averaged F1 Micro and Averaged F1 Macro compared to the "Ours (single-per-pass)" method.
The average accuracy for Subject-Verb Agreement is higher in the Multi dataset compared to the Mono dataset.
The accuracy for Across object rel. clause (no that) is higher in the Mono dataset compared to the Multi dataset.
The German Multi model achieves the highest accuracy among all languages.
The French Multi model achieves a perfect score of 1.00 in simple agreement.
The model has higher accuracy on simple agreement in English and French compared to German.
The model has higher accuracy on across prepositional phrase in German compared to English and French.
BERT performs better on the Simple agreement task in English Mono than in English Multi.
BERT performs better on the Across subject rel. clause task in French than in German.
The "our model + TF-IDF + sum" has the highest F1 score among all the models.
The "BiDAF + TF-IDF + sum" has a higher EM score than the "BiDAF" model.
The S-Norm model has the highest EM score among all the models.
The Mnemonic Reader model has a higher F1 score than the baseline Joshi et al. (2017) model.
There are 8 unique utterances with a frequency of 456, resulting in a total of 3648.
There are 2 unique utterances with a frequency of 444, resulting in a total of 888.
The F-score of i-vector increases as the dimensionality increases from 128 to 768.
x-vector consistently outperforms i-vector in terms of F-score at each dimension.
Adding MNLI to RTE improves the RTE score.
Including all tasks from the GLUE benchmark improves the RTE score.
The Single→Single model shows a statistically significant improvement in performance on SST-2 compared to the Single model.
The Single→Multi model achieves a statistically significant improvement in performance on MNLIf compared to the Single model.
The "Snorkel MeTaL ensemble" model achieves the highest GLUE score among all the models in the table.
The "BERT-Large + BAM (ours)" model achieves a GLUE score of 82.3.
Adding single-task fine-tuning to the Multi model improves the average score by 0.3.
The Single→Multi model has an average score of 86.0.
The CIDEr scores for all visual representations are higher when using ground truth annotations compared to using the output of a detector.
Using all three features (Frequency, Obj max size, and Obj min distance) as visual representations achieves the highest CIDEr score.
The CIDEr score for captioning is 0.748 when using the minimum pooling of object size features.
The CIDEr score for captioning is 0.740 when using the mean pooling of object distance features.
The BLEU score for the subset of 2301 samples where all 5 neighbors have 0 distance is 1.000.
The SPICE score is higher for the "Proj." type compared to the "Freq." type.
Table 4 compares the performance of different feature sets with fixed and tuned hyperparameters.
The LSTM model with fixed hyperparameters achieves a CIDEr score of 0.922.
ELMO, BERTBASE (CLS), BERTBASE (averaged), BiLSTMKD, and BiLSTMSRA are different models.
BiLSTMSRA has the highest F1 score among all the models.
There are four different models evaluated in the table.
The accuracy scores generally increase as the proportion of training data increases for all models.
Table 2 provides summary information of the six major topics events studied.
The H1N1 topic event occurred from June 9th to June 26th, 2009.
The table shows the performance of two models - Baseline(Bi-LSTM) and Model-I∗ on three traditional Chinese datasets.
Model-I∗ has a higher average performance across all three datasets compared to the Baseline(Bi-LSTM) model.
The WER for the TTS Model trained on LibriSpeech and augmented with both Isolated-Sentences and LibriSpeech is 29.6.
The WER for the TTS Model trained on no additional data and only using Isolated-Sentences is 32.9.
Table 1 shows the LibriSpeech Word Error Rate (WER) by augmentation with TTS utterances using different d-vector approaches.
The Word Error Rate (WER) for the Original approach is 4.84 for "test-clean" and 14.75 for "test-other".
Table 4 provides evaluation results on BioScope and PK using NegEx and NegBio. The performance is measured by precision (P), recall (R), and F1-score (F) on negations.
The precision for BioScope using NegBio is [BOLD] 96.1 and the recall is [BOLD] 95.7.
Table 2 shows the effect of the order of template, wh word, and question mark on downstream QA performance.
The F1 score for the "Wh + B + A + ?" order is [BOLD] 56.82.
Table 3 shows the effect of query and context matching on downstream QA performance.
The F1 score for the query + context matching procedure is [BOLD] 56.82.
Our BERT-large model achieves a higher SQuAD Test F1 score compared to the BERT-large model proposed by Lewis et al., 2019.
Our BERT-large model performs better in terms of SQuAD NER F1 score compared to the BERT-large model proposed by Lewis et al., 2019.
The system with the combination of Baseline, Gaz, and LexEmb achieves the highest scores on both the Dev and Test sets.
The model "Lex-0.01" has the highest accuracy among the models listed in the table.
The accuracy increases as we move from the "Skip-Gram" model to the "Lex-0.05" model to the "Lex-0.01" model.
The system "Baseline + Gaz + LexEmb" has the highest F1 score on the Test set.
The F1 score of the system "Baseline + Gaz + Skip-gram" on the Dev set is 80.70.
Table 5 shows the results on the generated fixed data using different architectures.
The average score of the "Ours" architecture on the Dev set is 91.93 and on the Test set is 91.82.
The table presents the results of different methods used in the study.
Our model performs better in the "Paragraph" task compared to the "Single-Sentence" task.
The table compares the test accuracy of baselines and the proposed model using an ensemble of three models for each architecture.
The proposed model outperforms both the Language Only and Turn to X baselines in terms of test accuracy.
OmniGraph [ITALIC] NEW achieves higher accuracy than the majority class baseline in all sectors.
Table 2 compares the performance of different models with the baseline and three benchmarks.
Both versions of OmniGraph significantly outperform the baseline and three benchmarks.
Table 6 provides precision, recall, and F1-scores for different language pairs using both BERT and fastText multilingual embeddings.
The F1-scores for all language pairs increase as the training data size increases when using BERT multilingual embeddings.
The table shows the F1 scores of NER systems trained using noisy Twitter data.
The NER systems trained using Twitter clusters perform better than the Baseline systems for all languages.
Table 6 presents the results of quantitative evaluation for different models based on various metrics.
The model ED-ND outperforms all other models in terms of BLEU, ROUGE-L, BERT-P, BERT-R, BERT-F, P, R, F0.5, Gramm., and PPL.
The "Bottleneck Sim." policy has the highest average number of noun phrases per system utterance and the highest average number of overlapping words between the user's utterance and the system's response in both "This Turn" and "Next Turn".
The "Supervised" heuristic has a higher average number of overlapping words between the user's utterance and the system's response compared to the "Q-Function Approx." and "REINFORCE" heuristics.
The table shows different systems used for the attack and support tasks.
ArgRanker [ITALIC] M/ [ITALIC] H achieves the highest precision and recall scores for both attack and support tasks.
The proposed ResNet34-MFB80-AM-TrainData-IV performs better than Thin ResNet34 in terms of EER on the short utterances protocol.
Table 2 compares different approaches for topic-aware NMT.
The table compares different approaches for topic-aware NMT in the context of E-commerce translation from English to French.
The table provides information on translation divergences in different languages.
There are no translation divergences in the Promotional category.
The human performance on the CANARD dataset is consistent for both the development and test sets.
T5-base achieves the highest BLEU score on both the development and test sets of the CANARD dataset.
The learning rate for GPT-2-medium, BERT-large, and UniLM-large is 10−5.
LSTM has the fewest parameters among all the models listed in the table.
LDA performs the best in terms of NS in the Wikipedia dataset.
DCCA performs the best in terms of tfidf in the Wikipedia dataset.
The highest value in the LS column is 17.1.
The table shows the results on an emotion classification task.
The standard convolutional layer outperforms the baseline for both 2 labels and 5 labels.
The "Test set UA Recall [%]" for Emotion 5 classes is higher than the "Test set UA Recall [%]" for Emotion 2 classes.
The "Test set UA Recall [%]" for Gender is higher than the "Test set UA Recall [%]" for Age.
The table shows the accuracy (%) of behavior identification using two different methods: k-Means and k-NN.
The Conversation Model with Online MTL (proposed) has a higher accuracy in behavior identification for the "Acceptance" behavior compared to the Conversation Model without Online MTL (proposed).
The "Universal Sentence Encoder + DNN" method achieves the highest WA (%) score of [BOLD] 64.83.
The "Conversation Model + DNN" method achieves the lowest WA (%) score of 55.82.
The table shows the grounding accuracy of UMD Refexp and INGRESS on the RefCOCO dataset with human-annotated ground-truth (HGT) object proposals and automatically generated MCG object proposals.
The grounding accuracy of UMD Refexp is higher than that of INGRESS.
The perplexity of the RNN-BNLM 100M model is higher than the BNLM model for all n-gram orders.
The perplexities of RNN- BNLM 100M are higher than the perplexities of BNLM for all n-gram orders.
The perplexities decrease as the n-gram order increases for all models except for RNN n-gram.
Table 1 displays the arguments of "was arrested" with their pret's.
The argument "the man4" has a pret of 1.0, the argument "at his home15" has a pret of 0.22, and the argument "Friday17" has a pret of 0.05.
Table 3 shows the results for the baseline and two algorithms.
The F1-score increases from ILP to Top-down to Top-down + NSS.
The dimensions in Table 7 include "general," "loneliness," "lonely," and "solitude."
The highest arousal score in the SOLO corpus is in the "high" category.
The percentage of words associated with an age group decreases as the age group increases in the General Tweets corpus.
The word "loneliness" is most associated with the age group "30+" in the corpus.
The dataset after the first stage of annotation consists of four types of annotations: non-harmful, harmful, I don't know, and equivocal.
The majority of annotations in the dataset after the first stage of annotation are non-harmful.
The percentage of correct labels decreases as the ID number increases.
The number of correct labels decreases as the ID number increases.
The total number of cases for each type of ambiguity is calculated correctly in Table 6.
There is no disambiguation method mentioned for the "has 'I don't know'" type of ambiguity in Table 6.
The highest score for the I2T training methodology is 45.4.
The T2I training methodology performs better than the I2T training methodology in image search.
The performance of the BoW model is consistent for both Image Annotation and Image Search.
The BoW model performs better in terms of Recall@K for Image Annotation compared to Image Search.
Table 6.6 compares the performance of different n-gram models on Flickr30K data.
The bold figures in the "n-gram features comparison Evaluation Metric" column represent the best results in each row.
The "tf-idf" feature performs better than the "binary" feature in the "n-gram features comparison Evaluation Metric" column.
For the "our model: n-gram Image Annotation" column, the values of "rPrecision(5)", "R@K: rnd_txt", "R@K: avg_txt", and "R@K: 1st_txt" are the same for all the different values of "k".
For the "our model: n-gram Image Search" column, the values of "R@K: any_txt" are missing ("-").
The recall at rank K for average text is higher than the recall at rank K for random text for all values of K.
The values for rPrecision(5) and median rank are the same for all values of K in the "our model: shallow Image Annotation" column.
For the SSE textual model with window size = 5, the R@K values for Image Annotation are 6.6, 15.2, 23.9, and 37.1 for K=1, 2, 5, and 10 respectively.
The R@1 value for image annotation is higher than the R@1 value for image search in the "our model: BoW" row.
The R@10 value for image annotation is higher than the R@10 value for image search in the "DeFrag FAO + GO + MIL" row.
The model "+L(adv)rec( [ITALIC] zsyn)" has the highest "word-BLEU (corpus)" and "Average TED (per sentence) Refsyn↓" values, indicating better performance in terms of both word-level similarity and syntactic similarity.
As the KL-Weight decreases, the BLEU score increases.
As the KL-Weight decreases, the Forward PPL decreases.
The "DSS-VAE" model has the highest "BLEU-ref" score among all the models.
The "DSS-VAE" model has the highest "BLEU-ori" score among all the models.
Table 5 provides information about the hyper-parameters used in the PTB dataset.
The batch size used in the PTB dataset is 50.
Entity markers with mention encoding achieve the highest classification score on the ChemProt dataset.
The original encoding with mention achieves a higher classification score on the DDI dataset compared to the entity dummification encoding.
The table provides the F1 entity-level scores for the NER task on BC5-chem, BC5-disease, NCBI-disease, BC2GM, and JNLPBA datasets.
The table provides the accuracy scores for the Question Answering task on PubMedQA and BioASQ datasets.
The performance of PubMedBERT on BC5-chem is higher than on BC5-disease.
The performance of PubMedBERT on NCBI-disease is higher than on BC2GM.
The performance of PubMedBERT with adversarial pretraining is higher than the performance of PubMedBERT without adversarial pretraining in most tasks.
The table provides data for four different languages: English, French, German, and Italian.
The number of neutral, negative, and positive examples in the training set varies for different languages.
L1 has more filters than L2 and L3.
L3 has a larger filter window size than L1 and L2.
The table represents the results of a translation experiment where each source language was translated into each target language and a model trained on the target language was used to predict tweet polarity.
Translating from English to Italian yields the highest performance in predicting tweet polarity.
The BAG model without attention has an accuracy score of 63.1 on the unmasked validation set of the WIKIHOP dataset.
The BAG model with all components performs better than any individual ablation variant on the unmasked validation set of the WIKIHOP dataset.
The BERT model outperforms the ESIM model in terms of Precision, Recall, and F1 scores for both the Dev and Test sets.
The BERT model achieves higher F1 scores than the ESIM model for both the Dev and Test sets.
KGAT (RoBERTa Large) achieves the highest fact verification accuracy in all four categories: Dev LA, Dev FEVER, Test LA, and Test FEVER.
BERT Concat performs better on the Dev set compared to the Test set for both LA and FEVER.
The aspect "Aroma" is more correlated with the aspect "Look" than the aspect "Palate".
The aspect "Look" and the aspect "Taste" are equally correlated with the aspect "Aroma".
The model RR-TF achieves the highest AUC score for the study "Antipsych.".
The study "Estrogens" achieves the highest AUC score among all the studies.
The AUC values for the different representations are consistently lower than the AUC values for the supervised models.
The AUC values for "Look", "Aroma", "Palate", and "Taste" in the "CNN + Triplet" row are all higher than the corresponding values in the "Baseline" column.
The AUC result for the "Look" aspect evaluated against the "Look" embedding is 0.92.
The AUC result for the "Taste" aspect evaluated against the "Aroma" embedding is 0.94.
Table 6 describes the performance of Monolingual, Multi-translation, and Multi-comparable models trained on different datasets.
The Multi-comparable model trained on the downsampled comparable set with caption-to-caption (c2c) alignment performs better on the English dataset.
The table shows the vocabulary overlap between different languages on the translation portion of the Multi30K dataset.
The Jaccard coefficient for the vocabulary overlap between English and German is 0.04.
The VSE model in the symmetric setting achieves a Recall at 5 of 60.4 in the I→T retrieval task.
The OE model in the asymmetric setting achieves a Recall at 1 of 34.8 in the T→I retrieval task.
The VSE model outperforms the other models in terms of recall at 1 and recall at 5 for image-to-text retrieval.
The Parallel-Asym model outperforms the other models in terms of recall at 1 and recall at 10 for text-to-image retrieval.
The retrieval result for English I→T in the Bi-comparable setting is [BOLD] 67.9.
The retrieval result for German T→I in the Bi-translation setting is 44.6.
The retrieval results for the Full Monolingual model are better for English captions compared to German captions.
The retrieval results for the "+ c2c" model are better than the retrieval results for the Bi-overlap and Bi-disjoint models.
The Monolingual model performs better on French compared to Czech.
The model with the "+ c2c" addition performs better on both French and Czech compared to the other models.
The T5-11B + SSM model achieves the highest scores on the NQ, WQ, and TQA tasks.
The T5-Large model outperforms the T5-Base model on the NQ, WQ, and TQA tasks.
The sentence "a woman is slicing potatoes" is ranked highest by all three models (Tree, M, and S).
The sentence "a boy is waving at some young runners from the ocean" has the lowest ranking for all three models (Tree, M, and S).
PeerT performs worse than Peer26 on the ROUGE-1 metric in both 2001 and 2002.
HNet(T) achieves the highest ROUGE-1 score in 2001 and 2002.
The Quartet model has the highest accuracy across question types compared to the other models.
The Quartet model has the highest overall accuracy compared to the other models.
The table shows the accuracy of the explanation structure (i,j,di,de) and the overall explanation accuracy (accexpl).
The Quartet method has higher accuracy in explaining the di and de components compared to the i and j components.
Table 4 provides information on the accuracy of different methods on the QA (end task).
Quartet achieves an accuracy of 82.07 on the QA (end task).
Table 6 shows the confusion matrix for "di" and "de" overall.
The count for "di" and "di" is 1972.
Table 6 represents the confusion matrix for "di" and "de" overall.
The number of correct predictions (represented by "+") is higher than the number of incorrect predictions (represented by "-") for both "di" and "de" overall.
The models in the table are categorized into Extractive Models and Abstractive Models.
The table compares the performance of different models on the image-caption retrieval task.
The Char-GRU model achieves the highest R@1, R@5, and R@10 scores in retrieving captions given an image.
The total number of instances for the "Comparison" label is 87.
The total number of instances for the "Circumstance" label is 294.
The "es" language has the highest score in the "SL+O" category.
The table compares the performance of different models using average word embeddings with our projection method.
The table shows the performance of different models in terms of their cross-lingual and in-language performance.
Table 2 presents the monolingual results of four different models: GloVe (GV), GoogleNews (GN), Morph Specialized (MS), and Attract-Repel (AR).
The power mean [p-values] column in Table 2 shows the different scores obtained by applying different power means to the individual embeddings for each model and metric.
The average scores for additional power means (based on BV ⊕ AR ⊕ FT) are higher for Σ X-Ling compared to Σ In-Language.
The Bounded adaptive-reward method achieves the highest BLEU score on the test set.
The Default method achieves a higher BLEU score on the dev set compared to the test set, and the dev ratio is the same for both sets.
The MAN-L2-SP-MLP model performs the best in the Book domain with a score of 78.45.
The MAN-L2-SP-MLP model has an average performance of 82.24 across all domains.
The SP-MLP model achieves the highest performance in the "Book" domain.
The standard error for the "Ele. (Electronic)" domain in the MAN-L2-SP-MLP model is ±0.04.
The average performance of the ASP-MTL model on the FDU-MTL dataset is 76.7.
The MAN-NLL-SP-CNN model performs well on the camera domain with a performance of 90.9.
The table shows the results of different variations of the RoBERTa model on the validation set.
The RoBERTa\scriptsize OMCS+P2 model achieves the highest accuracy on the test set.
Table 2 shows the results (accuracy) on the validation set.
As the codebook size increases, the bitrate decreases.
A codebook size of 128 achieves the highest R@10 value.
The WaveNet-VQ (ZS) model performs better than the WaveNet-VQ (PA) model in terms of ABX score.
The "∅→{2}" layer has a higher RLE bitrate compared to the "∅→{2,3}" layer.
The ABX score is higher when {2, 3} is the input to the Res3 block compared to when ∅ is the input.
The ABX score is significantly higher when {3} is the input to the Res3 block compared to other inputs.
The table shows the number of codebook vectors at a particular VQ layer that learned to be a detector for any word with an F1 score greater than 0.5.
SVM with Info-Gain-Top 200 achieves a higher overall accuracy than SVM.
SVM achieves a higher overall accuracy than NB.
"Food Inc." has the highest number of 5-star reviews among all the movies.
"Food Inc." has the highest number of reviews among all the movies.
"BattRAE" achieves the highest BLEU scores for both the MT06 and MT08 test sets.
"BattRAE" has the highest average BLEU score across all test sets.
"BattRAE" achieves the highest BLEU scores for both the MT06 and MT08 test sets.
"BattRAE" has the highest average BLEU score across all test sets.
The best system performance difference for the task "Mpqa" is 0.96.
The task "SemTraits" includes the feature "+FreqBin".
The combination of word vectors (WV) and target features (T) achieves the highest F-score.
The combination of word vectors (WV) and target features (T) has higher precision and recall compared to the combination of RV + T.
Table 5 compares the performance of CNN and SVM models with different hyperparameters.
RF and SVR are used as regression models in the experiment.
The MAE values for RF, SVR, and MLP are higher on the test set compared to the validation set.
The combination of Bag of Words and Bag of Embeddings has the highest MAE value.
The overall performance of all features combined is 0.6948.
The RF regressor performs better on the test set compared to the validation set.
The MLP regressor has the highest cosine similarity score on the test set.
Table 5 provides a breakdown of the performance on the test set using different combinations of features.
The combination of Bag-of-Words (BoW) and Lex features performs the best in terms of cosine similarity and mean absolute error (MAE).
The impact of λ on AR is evaluated using different values of λ.
As λ decreases, the BLEU-4 score increases.
The text length in the table is either 20 or 40.
The CatGAN model performs the best in terms of NLLoracle scores for both text lengths.
Table 5 compares the performance of four different methods: CatGAN w/o H, CatGAN w/o T, CatGAN w/o O, and CatGAN.
The CatGAN method achieves the highest BLEU-4 score of [BOLD] 0.867 in the ablation study on AR.
Table 6 provides the NLLoracle scores on general text generation.
The NLLoracle score for RelGAN is 6.680 for a length of 20.
SeqGAN has the lowest values for BLEU-2, BLEU-3, BLEU-4, and BLEU-5 among the methods listed in Table 7.
LeakGAN has the lowest value for NLLgen among the methods listed in Table 7.
Sub-segmentation of negative recordings significantly reduces the FRR(%) and SNIPS (FAH=0.5) values.
Sub-segmentation of negative recordings greatly reduces the False Rejection Rate (FRR).
Without data augmentation, the false rejection rate (FRR) is 0.6% for SNIPS (FAH=0.5) and 5.6% for Mobvoi (FAH=1.5).
With data augmentation, the false rejection rate (FRR) is 0% for both SNIPS (FAH=0.5) and Mobvoi (FAH=1.5).
The alignment-free LF-MMI loss has a lower value than the regular LF-MMI loss.
The Mobvoi system performs better than the SNIPS system.
The regular LF-MMI with alignment-free LF-MMI refinement has a lower FRR (%) compared to the regular LF-MMI without alignment-free LF-MMI refinement.
The regular LF-MMI with alignment-free LF-MMI refinement has a lower error rate (FRR) compared to both SNIPS and Mobvoi systems.
The table provides information about the top news stories from both 2018 and 2019.
VADER has the lowest mean rank among all the methods in the 3-Classes dataset.
NRC Hashtag has the highest coverage percentage among all the methods in the 2-Classes dataset.
The coverage values for each dataset in the table are different.
The F1 scores for positive and negative classes are different for each dataset in the table.
The Mean Rank for the 2-Classes Method is generally higher than the Mean Rank for the 3-Classes Method.
The coverage for the SenticNet method is 75.46%.
The method "SentiStrength" has the lowest mean rank among the 3-Classes dataset.
The method "Emoticons DS" has the highest coverage percentage among the 2-Classes dataset.
The dataset "Sentiment140" has the lowest mean rank value among all the datasets in the table.
The dataset "Emoticons DS" has the highest coverage percentage among all the datasets in the table.
As the depth increases, the number of convolutional layers also increases.
For each depth, the number of convolutional layers in the Convolutional Block 512 is the same.
The accuracy of SVDCNN 9 is lower than the accuracy of SVDCNN 17 and SVDCNN 29 for both the Ag News and Yelp Polarity datasets.
The storage size of VDCNN 29 is larger than the storage size of VDCNN 17 for all datasets.
NeuSum achieves the highest ROUGE-2 score among all the methods.
Refresh achieves the highest ROUGE-L score among all the methods.
The ROUGE-1 F1 score for the abstractive method "PG" is 27.5 on high abstraction samples.
The ROUGE-1 F1 score for the extractive method "Refresh" is 40.9 on high quality samples.
The method "TF-IDF" has the highest precision score.
The method with the highest F1 score is "ROUGE-AVG F1".
Table 1 provides precision, recall, F1 score, and support for all 637 concepts and selected individual concepts.
The precision, recall, and F1 score for the concept "province" are all 0.81.
The 5-gram FFNN models have lower perplexity values compared to the RNN models.
The LSTM models have lower perplexity values compared to the ReLu-RNN models.
The PPL values for the language models are lower when using the [ITALIC] softmax activation function compared to the S-NCE and B-NCE activation functions.
The total number of parameters in the language models is higher for LSTM compared to ReLu-RNN and ReLu-LSTM.
The average percentage of agreement for all aspects in the lexicon annotation results is 87.9%.
The lexicon agreement in the lexicon annotation results has a non-conflicting value agreement of 90.0%.
The table shows the class distributions in the connotation lexicon for fully-labeled words.
The category "Factuality" has a higher percentage of negative connotations compared to positive connotations.
Table 7 provides cluster connotation purity ratios for different aspects including "Social Value," "Polite," "Impact," "Fact," "Sent," and "Emo Avg."
The connotation purity ratio for "Social Value" is negative in the "c" condition.
The number of normalized response candidates increases as we move from SubD1 to SubD5.
SubD3 contains the highest number of normalized response candidates.
The error decreases as the embedding size increases for both SOLS and SOPP.
The number of iterations until convergence decreases as the embedding size increases for both SOLS and SOPP.
The average accuracy of the combined models for Hit@1 is 0.264.
The mean accuracy of the initial models is 0.205 for Hit@10.
Our model outperforms other models in all three categories (CLOTH, CLOTH-M, CLOTH-H).
The "1B-LM" model utilizes external data.
The model "1B-LM" outperforms humans in the "CLOTH" and "CLOTH-M" datasets in the short context.
Humans outperform the model "1B-LM" in the "CLOTH" and "CLOTH-M" datasets in the long context.
BERT BASE performs better on the small corpus compared to BCN+ELMo and ULMFiT Adapted.
The models listed in Table 2 are ranked in descending order based on their performance on the Rakuten Binary dataset.
The models listed in Table 2 show a decreasing trend in error percentages as we go down the table, indicating better performance on the Rakuten Full dataset.
BERT BASE has the lowest error percentage among all the models in the table.
ULMFiT performs better than BCN+ELMo in terms of error percentage.
The BERT BASE model has the lowest error percentage among all the models tested on the Yahoo dataset.
Transfer learning-based methods are trained on a smaller portion of the dataset compared to the other models tested on the Yahoo dataset.
The "Best lear" method has the highest value among all the methods listed in the table.
The "Poincaré (nouns)" method has a higher value than the "sgns (cos)" method.
Table 2 analyzes the importance of synergy in the full lear model on the final performance on wbless, bless, HyperLex-All (hl-a), and HyperLex-Nouns (hl-n).
The full lear variant has the highest performance on wbless, bless, HyperLex-All (hl-a), and HyperLex-Nouns (hl-n).
Table 6 shows the results on CNNDM summarization using ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L).
The model "Msc (36 L)" achieves the highest scores in ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) among all the models in the table.
Table 5 presents the results of an ablation study on the IWSLT14 En→De task, where different modifications were made to the "Msc" model and their BLEU scores are reported.
The baseline method achieves an accuracy of 75.9 on the WinoGrande-L dataset.
The model relabeling method achieves an accuracy of 77.7 on the CSQA dataset.
G-DAug-Combo performs better than G-DAug-Rand on SNLI-3K.
G-DAug-Diversity and G-DAug-Combo have the same accuracy on Codah.
G-DAug-Diversity and G-DAug-Combo achieve the highest average score among all the methods.
The Backtranslation method performs the best on the Hellaswag-2K dataset.
The "G-DAug-Influence" method performs the best on the validation set.
The "G-DAug-Diversity" method is the most perturbed by TextFooler.
Table 5 compares different training strategies on WinoGrande-L.
The "Two Stage Training" method achieves the highest accuracy among the different training strategies on WinoGrande-L.
The "Influence" method achieves the highest accuracy of 74.4%.
The "Whole Pool" method uses the entire synthetic data pool for augmentation.
Table 8 shows the test performance of different methods on WinoGrande.
The G-DAug-Combo method achieves a test AUC of 71.4 on WinoGrande.
The performance of our model with interaction match is better on the development set compared to the test set.
The performance of our model with query attention and sequence editing (with predicted query) is 47.9 on the test set.
Table 6 shows the ATIS results on the dev set and test set.
The performance of "Ours" is slightly lower than "FULL Suhr et al. (2018)" on both the dev set and test set.
The EpiReader model outperforms all other models on both the CNN valid and CNN test datasets.
The performance of all models improves on the CNN test dataset compared to the CNN valid dataset.
EpiReader outperforms AS Reader on both CBT-NE test and CBT-CN test.
EpiReader achieves higher performance than LSTMs and MemNNs on CBT-NE valid.
The cohesion score for the XRAY algorithm is consistently lower than the cohesion score for the FastAchor algorithm.
The similarity count score for the AnchorFree algorithm is consistently higher than the similarity count score for the FastAchor algorithm.
The number of clusters in the experiment results on the Reuters-21578 corpus ranges from 3 to 25.
The table shows the learned hyperparameters for the GP SSTK models in the fr-en dataset, with α fixed at 1.
The hyperparameters λsrc and λtgt correspond to the kernels on the source and target parse trees, respectively.
The SVM BOW method performs better than the SVM SSTK method in terms of Pearson's correlation score for the Emotion Analysis task.
The GP SASSTK S method performs better than the GP SASSTKfull method in terms of Pearson's correlation score for the Emotion Analysis task.
Table 4 shows error scores for the Quality Estimation task.
The RBF SASSTK [ITALIC] S + model achieves the lowest MAE score for the English-Spanish dataset.
Our model outperforms all the baselines from Dodge et al. (2015) in both the QA task and the Recs task.
Our model achieves the highest score in the Recs task compared to all the baselines from Dodge et al. (2015).
The highest accuracy for the English language is achieved with a BPE-size of 300.
The highest accuracy for the Icelandic language is achieved with a BPE-size of 200.
The "Transformer" model achieves the highest accuracy for English.
The "NoAtt-GRU" and "NoAtt-LSTM" models have the same CER value for German.
Except for the "Transformer" model, all other models show improvement in accuracy for the English language.
Table 8 provides information about the average edit distance of changed spellings and incorrectly normalized changed spellings in different languages.
The average edit distance for changed spellings in English is 1.45 and for incorrectly normalized changed spellings is 1.81.
Table 10 provides the accuracy of different models in Swedish with different dataset settings.
The Transformer model achieves the highest accuracy for all three datasets (Training, Development, and Test).
Table 3 shows the NLI accuracy scores with structured attention and mentions the performance of LP-SparseMAP models.
LP-matching achieves higher accuracy scores on SNLI valid and SNLI test compared to softmax and matching.
Table 4 displays the Multilabel classification test F1 scores.
The F1 score for the "bibtex" dataset using the LP-SparseMAP loss is 43.43.
Table 3 reports the accuracy of the classifier for different sets of hyperparameters (ϵ, γ).
The accuracy of the classifier for the "beautiful" category is highest when (ϵ, γ) = (∞, 0).
Table 2 displays the NER MUC out-of-domain results for different embeddings with a dimensionality of 25.
The F1 score for the "None" context type is 71.8.
Speaker S02 has the highest CER among all speakers.
The average CER for all speakers is 24.86%.
The table shows results for different languages.
The best performing neural network perplexity is bolded in the table.
The DSN adapted DNN acoustic models have a lower average WER (%) performance than the GRL adapted DNN acoustic models.
The DSN adapted DNN acoustic models have a lower WER (%) performance on the simulated development set compared to the real development set.
Table 2 shows the ASR WERs (%) for the DSN adapted acoustic models with respect to Nh reversal gradient coefficient α on the real development set of CHiME-3.
As the Reversal Gradient Coefficient α value increases, the ASR WER (%) generally decreases.
The table presents the results of different label merging methods on the CoNLL-2009 English development set.
The "ED" system has the highest F1 score of 88.3.
The "WMD Kusner et al. (2015)" system has the highest precision score of 89.1.
The KET model performs the best on the EC dataset according to Table 5.
The performance of the KET model decreases as we move from the EC dataset to the EmoryNLP dataset according to Table 5.
The highest relatedness-affectiveness tradeoff value on the validation sets is achieved with λk = 0.7 for EC, DailyDialog, MELD, and EmoryNLP datasets.
Increasing the value of λk from 0 to 0.3 improves the relatedness-affectiveness tradeoff for all datasets.
EmpGAN outperforms all other models in terms of all evaluation metrics.
EmpGAN generates more diverse and unique responses compared to the other models.
EmpGAN has the highest average score for Empathy among all the models.
HRED has the highest average score for Content among all the models.
The table shows the per-category performance of the Stanford AR and our system in different question categories.
Our system performs slightly worse than the Stanford AR system in the "Partial Clue" category.
The number of instances for the "News" category is 2,258,581 and for the "Movie" category is 2,073,177.
The number of items for the "News" category is 7,362 and for the "Movie" category is 11,648.
There are three subsets of turns in the Japanese data: V1<3,5, V3<1,5, and V5<1,3.
For the Japanese data, there are no turns for which mse1(v) is less than mse5(v) and mse3(v) is less than mse5(v), where v is an element of V.
Run 5 has the highest accuracy among all the runs.
Run 5 has the highest F1 score (B) among all the runs.
The highest accuracy score in the Japanese data is [BOLD] 0.5476 in "Run 3".
The highest F1 score in the Japanese data is [BOLD] 0.4613 in "Run 2".
The table shows the official results of MSE (NB, PB, B) for Je2.
In the first run, the IRS value for Je2 is 0.0662.
There are three subsets of turns mentioned in the table: V1<3,5, V3<1,5, and V5<1,3, with 866, 958, and 176 turns respectively.
The condition {v | mse1(v) < mse5(v) ∧ mse3(v) < mse5(v), v ∈ V} has a value of 0.
The table provides a breakdown of 15 failure cases in the human experiment, categorizing them based on different reasons for the failure.
Among the 15 failure cases in the human experiment, 3 cases involve sub-questions where the answer can be multiple.
The F1 score for DecompRC in the Full wiki setting All is 43.26.
The F1 score for DecompRC is higher in the Distractor setting All compared to the Full wiki setting All.
DecompRC achieves the highest F1 score on the test set of HotpotQA distractor and full wiki setting.
BERT Plus and Cognitive Graph do not have F1 scores reported on the test set of HotpotQA distractor and full wiki setting.
Table 5 provides F1 scores for two models, DecompRC and BERT.
DecompRC outperforms BERT in terms of the Joint F1 score.
Table 7 shows the ablations in sub-questions and their F1 scores on a sample of 50 bridging questions from the dev set of HotpotQA.
Table 7 shows the F1 scores for span-based and free-form reasoning methods performed by humans on the dev set of HotpotQA.
The F1 scores for the confidence-based, pipeline, and DecompRC decomposition decision methods are lower than the F1 score for the Oracle method.
The F1 score for the DecompRC decomposition scorer method is higher than the F1 scores for the confidence-based and pipeline methods.
RoBERTa Base consistently outperforms BERT Base in terms of accuracy on the symmetric dataset in all fine-tuning scenarios.
Fine-tuning with Elastic Weight Consolidation (FineTune+EWC) improves the accuracy of all models compared to traditional fine-tuning (FineTuned) on the FEVER dataset.
The accuracy for claim-only classification is generally lower than the accuracy for sentence pair classification for fact verification datasets trained on RoBERTa.
For the Liar-Plus dataset, the accuracy for claim-only classification is higher than the accuracy for sentence pair classification.
The system "DC(2)" performs the best among all the NER systems for both "Test (News) All" and "Test (News) Unseen".
The performance of all NER systems decreases when tested on unseen news data compared to the overall test data.
The F1 score for all features combined is higher than the F1 scores for individual features.
The "Self-Training (Standard)" classifier has a higher F1 score than the "Original Classifier".
The "Self-Training (Sampling)" classifier has a smaller train size than the "Self-Training (Standard)" classifier.
The "DC(2)" system achieves the highest score in the "Test (News) All" evaluation for Japanese NER.
The "DC(2)" system achieves the highest score in the "Test (News) Unseen" evaluation for Japanese NER.
The DF(1) system achieves the highest performance for Spanish NER on TestA All.
The DF(1) system achieves the highest performance for Dutch NER on TestB Unseen.
The "RelAwe(DepPath&RelPath)" method achieves the highest F1 score on the English test set.
The "LISA(Dep)" method has higher precision and recall scores compared to the "LISA(Dep&RelPath)" method.
The F1@20 score of the full model is 0.329.
The difference in MRR between the full model and the model without intersection features is 0.118.
Our BM25 implementation performs better than the implementation by Ren et al. (2014) on the DBLP dataset in terms of F@20 score.
Our BM25 implementation performs better than the implementation by Ren et al. (2014) on the PubMed dataset in terms of MRR score.
The "Context" model has the highest correlation with the "AGREE" aggregation operation.
The "NoSup" model has the lowest correlation with the "DISAG" aggregation operation.
The "Context" model performs better than the "NoSup" and "Token" models in terms of all automated metrics.
The "Context" model has a higher BLEU score than the "Token" model, which in turn has a higher BLEU score than the "NoSup" model.
The table shows the average ratings of language aggressiveness and explicitness/pragmaticness in the generated output for different personas.
The persona "DISAG" tends to generate more aggressive language and more explicit/pragmatic language in the generated output compared to the persona "CONSC".
The best single model trained achieves an LA F1 score of 67.10.
The Greedy approach achieves an SA F1 score of 59.71.
As the negative sampling rate for answerable questions increases, the SA F1 score also increases.
The highest SA F1 score of 47.02 is achieved when the negative sampling rate for answerable questions is 1% and the negative sampling rate for un-answerable questions is 4%.
The "Noisy-Or" aggregation strategy achieves the highest F1 score in the "LA F1" metric.
The "Max" aggregation strategy achieves the highest F1 score in the "SA F1" metric.
The BiCVM model achieves a performance of 0.87 on the TOEFL task.
The RNNenc model achieves a performance of 0.49 on the SimLex-333 task.
The table compares the embeddings learned by the original RNN Search and extended-vocabulary RNN Search-LV models for translation from English to French (EN-FR) and English to German (EN-DE).
The RNN Search-LV model performs better than the original RNN Search model in terms of semantic analogies.
Table 6 shows the Pearson correlation between mouse actions / jump backs and first waiting time / jump backs.
The Pearson correlation between mouse actions / jump backs and first waiting time / jump backs for the English-French language pair is 80.75.
The highest accuracy achieved in the En-De language pair is 16.15% and in the En-Fr language pair is 26.09%.
The accuracy of the "Action Seq" model is 84.37% for En-De and 67.07% for En-Fr.
The accuracy for En-De is consistently higher than the accuracy for En-Fr in all ablation studies.
The accuracy for En-Fr is lower than the accuracy for En-De in the ablation study where only the 1st waiting time is considered.
The table provides details of three different corpora: NTCIR, ASPEC, and Tanaka.
The table provides the maximum length of each corpus: 60 for NTCIR, 40 for ASPEC, and 16 for Tanaka.
The table provides details of three different corpora: NTCIR, ASPEC, and Tanaka.
The table provides the maximum length of each corpus: 60 for NTCIR, 40 for ASPEC, and 16 for Tanaka.
The performance of our system is better than the base system in all languages in Track 2.
Our system performs better than the base system in both "de" and "en" languages in Track 1.
The EER results for the "robust" type are consistently higher than the EER results for the "baseline" type across all SNR levels.
The EER results for the "baseline" type are higher than the EER results for the "robust" type at the 20 dB SNR level.
The table shows the accuracy scores of different models on a ten-fold cross-validation.
The Co-Hyp vs Random scores for SVM_CC and SVM_ADD are the same.
The table shows the F1 scores of different methods on the ROOT9 dataset.
The F1 score for Co-Hyp vs Random is 97.8.
Table 7 provides the performance of the baselines and systems on three compression subsets of the development set.
The Pointer-N system achieves the highest scores for all three compression levels.
The systems perform better than the "Lede-3" system in terms of informativeness.
The "TextRank" system performs worse than the "Syntactic COH" system in terms of fluency.
The performance of the "Pointer-N" system is lower on article-summary pairs with low coverage compared to article-summary pairs with medium coverage.
The performance of the "Fragments" system is higher on article-summary pairs with high coverage compared to article-summary pairs with medium coverage.
Table 2 shows the means of RQA measures for various genres extracted from the Gutenberg dataset (gutenbergr).
The algorithm used in Step 1 for all rows is "DT".
The highest BLANC F value in the table is "[BOLD] 73".
Table 4 provides the number of instances for train and test in the AEP and ASP tasks.
The number of instances for the AEP task in the train set in 2013 is 471,209.
The average precision (P) for the entity classes in the year 2009 is 0.663 and the average recall (R) is 0.748.
The table provides human evaluation results of different models and baselines on a sample of SQuAD-dev-test.
The table shows the validation perplexity (PPL) of each model.
QVEC-CCA Multilingual outperforms QVEC-CCA Monolingual in terms of both QVEC and CCA scores.
QVEC-CCA Multilingual achieves a higher QVEC score than QVEC-CCA Monolingual.
QVEC-CCA scores are higher than QVEC scores for the "CorrNet W+N+L" method in both the "3 Languages Monolingual" and "3 Languages Multilingual" settings.
The "MultiCross" method does not have data available for the "3 Languages Monolingual" and "3 Languages Multilingual" settings.
Table 6 describes the name tagging results using monolingual embedding and multilingual embeddings.
Multilingual CorrNet achieves the highest F-score of 55.8% on Amh name tagging.
Table 7 shows the name tagging performance when the tagger was trained on a source language and tested on a target language.
The proposed approach, CorrNet W+N+Ch+L, achieves the highest F-score in all language combinations.
The proposed approach, CorrNet W+N+Ch+L, performs better than MultiCCA in terms of F-score for name tagging.
The proposed approach, CorrNet W+N+Ch+L, improves the F-score for Tig in name tagging when combined with Amh.
The technique "Obfus.,Int. Vagueness,Confusion" has a precision of 0 and a recall of 100.00 for both MGN and LatexPRO.
The F1 score for the technique "Reductio ad hitlerum" is 62.77 for LatexPRO (T+L).
The table shows the BLEU scores for different translation methods.
The En→Ro+Fr translation method achieves the highest BLEU scores among the four methods.
Table 1 provides the number of sentences in the training, dev, and test splits for each language pair used in the experiments.
The number of sentences in the training split for the English-Romanian language pair is 180,484.
The accuracy of "Our best feedforward variant" is higher than the accuracy of "Our best sequential LSTM variant".
"Lin et al., (2009)" has the highest accuracy among all the models mentioned in the table.
The "Our best feedforward variant" model has the highest accuracy for both the English and Chinese datasets.
The "Our best feedforward variant" model has a higher accuracy than the "Our best LSTM variant" model for the English dataset.
The CCM model outperforms other models in all metrics.
The CCM model achieves a higher BLEU-4 score compared to the Baseline model.
The BERT model performs the best on the "Abstract O ⟷ P" category.
Human performance is better on the "Situated O ⟷ A" category compared to the "Situated O ⟷ P" category.
The POS feature set has the highest UAS score of 0.921.
The UAS score for the Spectral (count model) feature set is 0.867.
The "mBART" model performs better on dissimilar pairs in the En-Ne language pair compared to other models.
The "MASS song2019mass" model performs better on similar pairs in the En-Ro language pair compared to other models.
Only the "mBART02" and "mBART25" models have both English and Romanian pre-training data.
The "mBART02" and "mBART25" models achieve the highest performance in the Ro→En fine-tuning task.
The highest transferring score for each language pair is bolded in Table 11.
The testing languages in Table 11 correspond to the fine-tuning languages.
Table 12 presents the best transferring scores for different language pairs.
The combined score of back-translation and language transfer is higher than the individual scores for each method.
The "SVM with ADWSK" classifier has the highest F-score for both the "STRENGTH" and "WEAKNESS" classes.
The Precision for the "WEAKNESS" class is higher than the Precision for the "STRENGTH" class for the "Pattern-based" classifier.
Table 2 shows the results of the TextBlob sentiment analyzer on dataset D1.
The "STRENGTH" category has 544 positive sentences in dataset D1.
Table 9 provides the comparative performance of various summarization algorithms based on their ROUGE unigram F1 scores.
LexRank performs better than LSA, TextRank, and ILP-based summary based on their ROUGE unigram F1 scores.
Both the CNN and RNN-LSTM models have an embedding size of 300.
The learning rate for the CNN model is 0.001 and for the RNN-LSTM model is 0.0025.
The table shows the adversarial accuracies of four different systems: Orig., AddC, AddQ, and AddQA.
After two optimization epochs, the adversarial accuracies of the CNN and RNN-LSTM models are provided.
Table 6 provides the adversarial accuracies on the validation set under the sentence-level white-box attack based on the removal of the plot sentence with the highest attention for two different systems, CNN and RNN-LSTM.
The ensemble adversarial accuracy is higher than the average adversarial accuracy for both the CNN and RNN-LSTM systems.
For all entity types, except City and ComicsCharacter, the Precision@1 in the test is higher than 0.80.
In the cold start scenario, the Precision@5 for all entity types is higher than 0.60.
Table 1 provides information about different types of entities in the WebNLG dataset, including Location, Product, Brand, and Person.
The WebNLG dataset contains structured entity tuples with sparse attribute values, as indicated by the low average tuple density.
The table shows the runtime (in seconds) of different stages in IDEL for the entity type Building.
The load time for creating an index for tuples is 0.04 seconds.
The basic model achieves a BLEU-4 score of 5.04 on the Our Dataset (Image Editing Request).
The -static +dynamic rel-att model achieves a CIDEr score of 46.41 on the NLVR2 dataset.
The basic model wins in 11 out of 100 examples in the "Ours(IEdit)" task.
In 5 out of 100 examples, both the basic and full models are considered good in the "Both Good" task.
Document D1 contains terms T1, T2, T3, T4, and T5.
Document D6 contains terms T13, T14, and T15.
Table XII shows the F-measure on the test set for different weights and classification algorithms.
The F-measure for SVM-Gaussian using TFIDF weights is 0.5825.
The F-measure values for each report are consistently lower than the corresponding values for E∩A/A and E∩A/E.
The mean F-measure for E∩A/A is higher than the mean F-measure for E∩A/E.
BERT performs better than 1DConv in terms of precision, recall, and f1-score.
SVM has a precision, recall, and f1-score of 0.00 for the "Neutral" category.
There are four different models used for the Nepali to English and English to Myanmar translation task.
The ST + BT model performs the best for the English to Myanmar translation task.
Our method outperforms the Pipeline method in both Entity and Relation tasks.
The TransF method with global features performs better than the Pipeline method with additional features in both Entity and Relation tasks.
Our model performs better than the baselines on the Restaurant14 dataset.
Our model outperforms the TransF model on the Restaurant16 dataset.
Table 2 compares different settings for End-to-End RE on TREC.
The model with strict evaluation and with boundary achieves the highest entity and relation scores in Table 2.
The model setting "TransF+SR+RR" achieves the highest scores in all evaluation metrics.
The model setting "TransF+" achieves higher scores than the model setting "TransF" in all evaluation metrics except for the ACE05 Relation.
The F1 scores of the teams are decreasing as the ranking increases.
The recall scores of the teams are generally high.
The team "Martin" is ranked 1st in the performance of the baseline and official submissions on Subtask-2.
Table 7 shows the BLEU scores of different models on the WMT En→Fr dataset.
The RL-refined WPM-32K model performs better than the WPM-32K model on the WMT En→Fr dataset.
Table 1 provides the results of model inference on CPU, GPU, and TPU for a model trained with the ML objective only with quantization constraints.
The BLEU scores for CPU, GPU, and TPU are all similar.
The "RL-refined WPM-32K (8 models)" has a higher BLEU score than the "WPM-32K (8 models)".
Table 8 shows the model ensemble results on WMT En→De (newstest2014) and refers to Table 5 for a comparison against non-ensemble models.
The table provides the relative improvement percentages for different translation directions.
The GNMT model outperforms the PBMT model in terms of translation scores for all translation directions.
The phoneme accuracy of HMM + DTE-LDA + DNN is higher than the phoneme accuracy of HMM + DTE-PCA + DNN (see Table 2).
The phoneme accuracy of HMM + DNN - W is higher than the phoneme accuracy of HMM + GMM (see Table 2).
The table lists different training methods used for tri-phoneme and phoneme classification accuracy.
The HMM + DTE-LDA + DNN method achieves the highest classification accuracy for both tri-phoneme and phoneme.
The performance of our single model with both model derived features (MD) and mis-matching features (MM) is better than the performance of the POSTECH Single model.
Removing the mis-matching features (MM) from our single model results in a decrease in performance in terms of mean absolute error (MAE) and root mean square error (RMSE) on the test set for the de-en language pair.
Table 1 provides the parameters b∗, γ∗, and C∗=C(γ∗,b∗) obtained from ML-fit of Eq. (1) for different languages.
The values of γ∗ for different languages range from 1.62 to 1.94.
The TGT model consistently outperforms the PLuGS model in terms of CIDEr scores for all languages.
The addition of TTG data improves the performance of the PLuGS model, as evidenced by higher CIDEr scores for the PLuGS-TTG model compared to the PLuGS-TGT model.
The table shows the performance of PLuGS models compared to TGT and TTG models across five target languages on OID1k.
The PLuGS models have higher gains in terms of SxS performance compared to both TGT and TTG models for all five languages.
The table displays CIDEr scores on the CC-1.1 validation set for bilingual and multilingual models for different languages.
The TTG model performs better on Fr, It, and Es languages compared to De and Hi languages, based on the CIDEr scores.
The PLuGS Stabilizer outputs are better captions across all three languages.
Table 8 shows the Spearman correlation of Stabilizer vs TGT, TTG, and PLuGS Captions across three languages.
The Spearman correlation between Stabilizer and PLuGS-Fr captions is 0.5982.
The BLEU-4 score for the translation of the stabilizer in French is 93.26.
The BLEU-4 score for the translation of the stabilizer in Spanish is 93.88.
"Hypertension" is the most frequently diagnosed condition in the dataset.
The total number of diagnoses in the dataset is 7284.
The system with the highest frequency is "cardiovascular" with a frequency of 2245.
The total frequency of all systems is 7275.
The model "DN+CBERT" achieves the highest values for "M-AUC", "M-F1", and "m-F1" among all the models.
The models "DN+CBERT" and "UMLS-DN-CBERT" achieve the highest values for "M-AUC" and "m-F1" among all the models.
"Atrial fibrillation" has the highest prevalence rate among all the diseases in the table.
"Asthma" has the highest F1 score among all the diseases in the table.
Table 5 provides the prevalence rate for each system in our best RoS abnormality prediction model.
The contribution to precision-at-1 is highest for the skin system in our best RoS abnormality prediction model.
The "Ext-ED (Wiki)" model has the lowest PPL and the highest BLEU-4 score.
The "Ext-ED (NELL KB)" model has a lower PPL and a higher BLEU-4 score compared to the "Ext-ED Ablation" model.
The model with the best performance and the highest Gain for SimLex is GloVe Basis∗.
The model with the best performance and the highest Gain for Rel-All is GloVe DF∗.
The number of sentences annotated for the "IsA" relation is 44.
The number of sentences annotated for the "MadeOf" relation is 2.
The BiLSTM-CNN model has the highest F1 score among the three models.
The Stacked-BiLSTM model has a lower precision than the BiLSTM model.
The table presents the performance of attention-based NMT and Transformer models on both the purified and redundant datasets.
The Transformer model outperforms the NMT model in terms of BLEU-4 score on both the purified and redundant datasets.
The table compares different methods for incorporating diverse semantic constraints in SynGCN embeddings on all extrinsic tasks.
SemGCN (X,4) outperforms other methods in terms of performance on the SQuAD task.
SynGCN outperforms all existing approaches on all tasks (POS, SQuAD, NER, and Coref) according to Table 2.
SynGCN performs better than other methods for named entity recognition according to Table 2.
SemGCN with SynGCN initialization performs better than other methods across all tasks.
Retro-fit with X initialization improves the performance of initial embeddings X in terms of Word2vec AP and Word2vec MSR.
E+SemGCN(SynGCN, 4) outperforms ELMo (E) on the NER task.
E+SemGCN(SynGCN, 4) outperforms ELMo (E) on the SQuAD task.
The model "2D LSTM with attention" has the highest F1 score among all the models tested on the Reddit dataset.
The model "2D LSTM with attention" has the highest precision score among all the models tested on the Reddit dataset.
The "2D LSTM with attention" model outperforms both the "CNN-LSTM with attention" and "CRF with all features" models in terms of F1 score.
The "CRF with all features" model achieves the highest precision among the three models.
The Seq2Seq Baseline method has the lowest scores for R-1, R-2, and R-L.
D-TRF (Finetuned + nucleus) has the highest scores for R-1, R-2, and R-L.
The table shows the accuracy results of three different models: Victim model, Genetic adv training, and w-MHA adv training.
The w-MHA adv training model achieves the highest accuracy at all three training sizes: 10K, 30K, and 100K.
The table provides information on the success rates, invocation numbers, and perplexity for different approaches on the IMDB and SNLI tasks.
The approaches with b-MHA and w-MHA have the highest acceptance rates for the IMDB and SNLI tasks.
The table presents the robustness test results on IMDB for three different models: Genetic, b-MHA, and w-MHA.
The Genetic model has a high attack success rate in all scenarios.
The "MMDenseNet+" method has the highest average SDR score compared to the other methods.
The "BLEND" method has a longer training time compared to the other methods.
Models with "MMDenseNet" in their names have higher SDR in dB for Drums compared to the other models.
Models with "MMDenseNet" in their names have higher SDR in dB for Acco. compared to the other models.
The table compares the similarity and diagonal attention rate between LN, LW, and Baseline settings.
The similarity value is highest for the LW setting and lowest for the Baseline setting.
The table shows the MOS scores with 95% confidence intervals on VCTK and LibriTTS.
The MOS score for LibriTTS is lower than the MOS score for VCTK.
The table shows the MOS with confidence intervals and diagonal attention rate r of the ablation study on VCTK.
The percentage of wrong predictions decreases as the batch size decreases.
Table 7 shows the percentage of examples that required annotation on Geo880 dataset for different batch sizes.
The test accuracy of the baseline CTRNNs is lower when using phonetic representation compared to word embedding for both EMILv1 data and EMILv1 + Teacher data.
The AMTRNNs outperform the optimised MTRNNs in terms of test accuracy for both phonetic representation and word embedding.
Test accuracy is higher when all three sensory inputs (audio, smile, and video) are used compared to when only audio and smile inputs are used.
Test accuracy is higher when audio and video inputs are used compared to when audio and smile inputs are used.
The degree of controllability for k@1 and k@5 is higher for the "unlimited" category compared to the "limited-3", "limited-6", and "limited-9" categories.
The degree of distinctness for k@1 and k@5 is higher for the "unlimited" category compared to the "limited-3", "limited-6", and "limited-9" categories.
The LSTMC model achieves the highest accuracy among all Shortlisting-HypRank models for both traditional IPDA and large-scale IPDA.
The UPPER model performs better than the NCH model for the large-scale IPDA smxa setting.
The table shows the k-best classification accuracies (%) of Shortlister using different softmax functions in the traditional and large-scale IPDA settings.
Shortlister with the softmax function "smxa" performs better in the traditional IPDA setting compared to the large-scale IPDA setting.
The table shows the cosine similarity of word embeddings on selected words.
The correlation between the SVD method and the short term is 0.590.
The TSVD term has a correlation of 0.590.
The correlation between embedding norm and normalized frequency is higher for the "Methods PPMI" than for the "Methods SVD".
The correlation between embedding norm and normalized frequency is higher for "TSGNS" than for "SGNS".
The correlation value for the "Short" term is higher for PPMI compared to DW2V.
The correlation value for the "Long" term is higher for PPMI compared to DW2V.
The relation "olympic_games/medals_awarded . olympic_medal_honor/medal" has 16 triples in the dataset.
The DistMult model achieves an accuracy of 0.73 for the relation "food/nutrients . nutrition_fact/nutrient".
The table compares the performance of different approaches - "shared" and "separate" - on text-based image retrieval.
The "vis-w2v-coco" approach performs better than other approaches in terms of R@10 (%).
The performance of the "vis-w2v-coco (separate)" approach is better than the "vis-w2v-coco (shared)" approach on the common sense task.
The inclusion of vision in the "vis-w2v-coco (separate) + vision" approach improves the performance on the common sense task compared to the "vis-w2v-coco (separate)" approach.
The performance on the visual paraphrasing task improves as we move from "w2v-wiki" to "vis-w2v-coco".
"vis-w2v-coco" achieves the highest performance on the visual paraphrasing task.
The highest performance scores for the vis-w2v-coco Model are marked as [BOLD] in each row.
The performance scores in the "vis-w2v-coco Descs" column are consistently higher than the corresponding scores in the "vis-w2v-coco Sents" and "vis-w2v-coco Winds" columns.
The performance on the Visual Paraphrase task for vis-w2v-wiki improves as the number of dimensions increases.
The performance on the Visual Paraphrase task is higher for vis-w2v-wiki when using descriptions compared to sentences.
Table 5 shows the performance on the common sense task using 78k real images with text baseline.
The performance on the common sense task is highest when K is 1000 and the model is initialized from vis-w2v80k + vis-w2v80k.
Table 6 shows the performance on the common sense task using 4k real images with a text baseline at 68.1, initialized from w2v-wiki.
The performance on the common sense task increases as the value of K increases.
Table 1 evaluates the generated image quality using different methods.
The human scores for the different methods are provided in Table 1.
The table presents the evaluation of the generated image quality by conditioning on a varying number of paraphrased sentences (NC).
The table presents classification results for three different datasets: Celebrity, US-Election2016, and PoliticalNews.
The FNDetector model achieves different accuracies for the Celebrity, US-Election2016, and PoliticalNews datasets: 0.73, 0.81, and 0.76, respectively.
The "Order-embeddings" model has the highest "Caption Retrieval R@1" and "Caption Retrieval R@5" scores compared to other models.
The "BiLSTM-Max (on AllNLI) + ResNet101 (113k)" model has the highest "Image Retrieval R@1", "Image Retrieval R@5", and "Image Retrieval R@10" scores compared to other models.
BiLSTM-Max achieves the highest scores for all metrics (NLI dev, NLI test, Transfer micro, and Transfer macro) among all the models.
The models LSTM, GRU, BiGRU-last, BiLSTM-Mean, Inner-attention, and HConvNet have the same dimension of 4096.
The best accuracy for the full system on the EN-IT dataset is 48.53%.
The average runtime for the EN-DE dataset using the "Re-weighting" method is 9.1 minutes.
The proposed method achieves the highest accuracy in the ES-EN language pair.
The proposed method has a higher average runtime in the TR-EN language pair.
The table shows the bar chart reconstruction accuracy using Algorithm 1 with PreFIL (Oracle OCR).
The accuracy for shape prediction, label prediction, and value prediction is the same for both familiar and novel tests.
PReFIL (Ours, Improved OCR) performs better on the Test-Novel Overall metric compared to SANDY (No OCR) and PReFIL (No OCR).
PReFIL (Ours, Improved OCR) performs better on the Test-Familiar Reasoning metric compared to SANDY (Tesseract OCR) and PReFIL (Ours, Tesseract OCR).
Table 6 shows the results of ablation studies on the DVQA dataset using different models.
The PReFIL (full model) outperforms the other ablation models on the familiar test set.
The table provides information about the performance of different models on KGA-CGM (Avg).
The performance of the ESA+CI model on KGA-CGM (Avg) is better with Beam > 1 compared to Beam = 1.
The color white has the highest percentage of terms associated with it.
The color black has the highest percentage of terms associated with it when considering votes.
The percentage of terms in the majority class size one is 15.1%.
The percentage of terms in the majority class size ≥ two is 84.9%.
Table 5 provides information about the number of senses of color terms in WordNet.
The color term "red" has 14 senses in WordNet.
Table 2 shows the precision of narratives based on human annotation.
The average precision of narratives is higher for the seed dataset compared to the bootstrapped dataset.
Table 5 provides the results on the TimeBank corpus.
The accuracy of the models increases when the CP score is added.
The "Our Results" method has the highest accuracy among all the methods listed in the table.
The "Our Results" method outperforms the "Chambers and Jurafsky (2008)" method in terms of accuracy.
The "Data Different" + Conv model has the lowest performance on the test data.
The "Transformer Imitator" model has the highest functionality similarity with the victim model.
Table 2 presents English to German imitation results for both in-domain and out-of-domain test data.
The imitation models closely mimic the production systems for both in-domain and out-of-domain test data.
Table 4 shows the effects of different relation types in GraphReader, comparing input graphs containing different sets of edges using GraphRetriever and GraphReader (binary).
The Cross-doc relation type achieves a performance of 34.2.
The "GraphRetriever" model performs better than the "Text-match" model on all datasets.
The "GraphReader (relation)" model achieves the highest performance on the "Natural Questions Test" dataset.
The GraphRetriever method performs better than the Text-match method in both the WebQ and Natural Q datasets.
The GraphRetriever method outperforms the Text-match + Wikidata method in both the WebQ and Natural Q datasets.
Both GraphReader (binary) and GraphReader (relation) have the same performance on the Natural Q dataset.
Using all pairs from the graph in ParReader++ leads to a lower performance compared to using only pairs from the graph.
Table 2 shows the DA classification error rates of baseline and unified models with given segmentation for different search methods.
The "Training with ASR feature" model has a lower error rate than the "Baseline (ASR transcript)" model for the 1-best search with a language error rate (LER) of h=1.
The table presents average human evaluation scores for different types of responses.
The average performance of the models on the SQuAD-dev dataset is 64.41 EM and 74.54 F1.
F-Net has the highest performance on the SpokenS-test dataset with an EM score of 46.51.
The WER(%) increases as the level of noise increases from "No noise" to "Noise V1" to "Noise V2".
The WER(%) for the "Noise V2" testing set is higher than the WER(%) for the "Noise V1" testing set, which in turn is higher than the WER(%) for the "No noise" testing set.
Table 3 compares the performance of different models trained on text documents and ASR transcriptions.
The BiDAF model performs better on the SpokenS-test dataset for both EM and F1 scores compared to the Dr.QA model.
The models in the table are labeled as (a), (b), (c), (d), and (e).
The "WORD+SYLLABLE" model has the highest "No noise EM" value among all the models.
Table 6 provides hyperparameters for training the text classification models.
The kernel size for the CNN model is 3.
The hyperparameter values for the IMDB dataset are different from the hyperparameter values for the AGNEWS dataset.
The step size for adversarial training is 10.
The BERT-wwm-ext model achieves the highest accuracy on the C{}^{3} test set.
Human performance is higher than the performance of any baseline model on the C{}^{3} development set.
The percentage of matching questions in the C{}^{3} dataset is 14.7%.
The percentage of general world questions in the RACE dataset is 57.3%.
Table 4 shows the prediction results of different methods for tobacco, alcohol, and drug classification.
The D-DBOW method performs better than other methods for tobacco, alcohol, and drug classification.
The table shows the prediction results for different methods used in the study.
The User-D-DBOW method shows the highest prediction scores for tobacco, alcohol, and drug categories.
The "wGCCA_balanced" method performs the best on the Tobacco dataset with a score of 0.848.
The "DCCA_balanced" method performs better on the Drug dataset compared to the "DCCA_imbalanced" method.
The similarity values in the "Similarity" column range from 0.00 to 1.00.
Each cluster in the "Cluster" column is labeled with a unique word.
The method TSR achieves the highest F-score in the RL category with a value of [BOLD] 0.257.
The method TSR achieves the highest Recall value in the RL category with a value of [BOLD] 0.301.
Our method (finetuning UniLM in sync with AC) achieves a higher F1 score than both the baseline and UniLM for ROUGE-1.
Our method (finetuning UniLM in sync with AC) achieves a higher F1 score than both the baseline and UniLM for ROUGE-L.
There are three different models evaluated in the paraphrases evaluation on Stubhub skill.
The fixed encoder model has a relative error change of -26.6% over the baseline.
BERT-base achieves the highest accuracy on the CoNLL 2009 dataset.
BERT-base outperforms Shi and Zhang (2017), Roth and Lapata (2016), and He et al. (2018b) on the CoNLL 2009 dataset.
BERT-LSTM-base achieves the highest F1 score of 73.3 on the TACRED test set.
Zhang et al. (2018) (ensemble) achieves the highest precision score of 71.3 on the TACRED test set.
The F1 scores for the "Out-of-domain (Brown)" dataset are lower than the F1 scores for the "CoNLL 09 (In-domain)" dataset for all models.
The "BERT-LSTM-large" model achieves the highest F1 scores for both the "CoNLL 09 (In-domain)" and "Out-of-domain (Brown)" datasets.
"BERT-LSTM-large" achieves the highest F1 scores in both in-domain and out-of-domain tasks.
"BERT-LSTM-large" has a higher recall score than "BERT-LSTM-base" in the in-domain task.
As the degree of parallelism increases, the processing time decreases.
As the degree of parallelism increases, the throughput increases.
The system "CN-ADD" outperforms the system "One sense" in terms of F1 score.
The system "CN-ADD/AVG" outperforms the system "1c1inst" in terms of Fuzzy B-Cubed score.
The table compares the Spearman's rank correlations with human assigned similarity in the WordSim353 dataset for three different approaches: No Lexicon (CBOW), No Filter, and WSD Filter (Proposed).
The WSD Filter (Proposed) approach achieves the highest Spearman's rank correlation in the Polysemous WordSim353 dataset.
The "WSD Filter (Proposed)" method has the highest validation accuracy among the three methods.
The accuracy of the text classification improves when using the "WSD Filter (Proposed)" method compared to the "No Lexicon (CBOW)" and "No Filter" methods.
The "Proposed" method achieves the highest correlation score on the WordSim353 dataset.
The "Retrofitting" method achieves the highest score on the Word Analogy Semantic task.
The multi-step-reasoner (Dr.QA) model achieves the highest F1 score for both Quasar-T and SearchQA datasets.
The DocumentQA∗∗ model achieves the highest F1 score for the Triviaqa-unfiltered dataset.
P8 has the highest score among all the models in the GAR category.
O2 has the highest score among all the models in the BiDAF category.
Table 2 shows the results of 5 different models when trained on the original RACE dataset and tested on a subset of the RACE test set and on the corresponding P5 created using this test set.
The BiDAF model performs better on the P5 dataset compared to the other models.
The values in the GAR column are higher for P6 than P2 for all models.
The values in the MRR column are higher for P6 than P2 for all models.
Table 7 shows the MRR (Mean Reciprocal Rank) for query (MRR(Q)) and answer (MRR(A)) N-grams in the Query-Aware Passage Attention layer.
BiDAF performs poorly on the MRR(Q) metric at P2 in the Query-Aware Passage Attention layer.
Adding both POS tags and words to the dependency path improves the performance on all datasets.
Adding words to the dependency path improves the performance on dataset D1.
The top system in D1 achieved an F1 score of 74.55.
The PoD system with the Sadd variant achieved an F1 score of 73.54 with a p-value less than 0.05.
The "FG fine-grained gate" model achieves the highest scores in both the CN test and NE test categories.
The scores for the "GA scalar gate" model are higher in the test categories (CN test and NE test) compared to the development categories (CN dev and NE dev).
The model "fine-grained gate" achieves the highest precision@1, recall@10, and the lowest mean rank on the Twitter dataset.
The model using character-level representations outperforms the model using word-level representations in terms of precision@1, recall@10, and mean rank on the Twitter dataset.
The GA scalar gate model achieves an F1 score of 0.6850 and an Exact Match score of 0.5620.
The FG fine-grained gate + ensemble model achieves a higher F1 score of 0.7341 compared to the FG fine-grained gate model without ensemble.
HDLTex achieves an accuracy of 91.58 on the WOS-46985 dataset.
The Baseline method for the WOS-5736 dataset is DNN.
The table provides details of three different data sets: WOS-11967, WOS-46985, and WOS-5736.
The data set WOS-46985 has the highest number of testing instances among the three data sets mentioned in the table.
The table compares the VQA antol2015vqa, Visual7W zhu2016visual7w, and MemexQA datasets.
The MemexQA dataset has a higher average question length compared to the VQA antol2015vqa and Visual7W zhu2016visual7w datasets.
The overall performance of the Human (Q+A) method on the MemexQA dataset is 0.573.
The Embedding method performs the best on the "where" question type with a score of 0.719.
The method "MemexNet" has the highest F1 score of 0.767 on the SQuAD development set.
The method with the highest F1 score has an LR value of 0.767.
CDNMT∗ [ITALIC] mo achieves the highest scores in all three translation directions.
CDNMT∗ [ITALIC] m performs better than CDNMT∗ in the En→De and En→Ru translation directions, but worse in the En→Tr translation direction.
The amount of paired data increases from 100 to 500.
The Mean Opinion Score (MOS) increases as the amount of paired data increases.
Our method has a lower MOS (TTS) score compared to the GT and GT (Griffin-Lim) methods.
Our method has a higher PER (ASR) score compared to the GT (Supervised) and Pair-200 methods.
The total number of supporters in Community cluster CC1 is 205.
The total number of supporters in all community clusters is 358.
The "Active Yes" to "Passive Yes" communication channel has the highest proportion of original tweets.
The "Passive Yes" to "Retweet" communication channel has the highest proportion of retweeted tweets.
The fraction of mention tweets that occurred between nodes that are connected in the follower network is higher for original tweets compared to replies and retweets.
The fraction of mention tweets that occurred between nodes that are connected in the follower network is higher for retweets compared to replies when the "From" node is "Active No".
The method "E-SCBA" outperforms the methods "S2S" and "S2S-AW" in terms of consistency, logic, and emotion.
The method "S2S-AW" performs better than the methods "S2S" and "E-SCBA" in terms of angry emotion.
Table 2 shows the results of automatic evaluation using different methods.
The E-SCBA method outperforms all other methods in terms of Greedy Matching, Embedding Average, Vector Extrema, Distinct-1, and Distinct-2.
Table 2 compares the WER for N-best rescoring between a 12-layer Transformer and a 12-layer SRU.
The WER for the 12-layer Transformer on the Dev clean, Dev other, Test clean, and Test other datasets are 1.62, 4.41, 1.96, and 4.70 respectively.
The SRU model has two entries with different layer numbers (12 and 24).
Table 3 shows the WER (in %) comparison among different setups.
The +24-layer SRU setup achieves the lowest WER on clean test data among all the setups.
The ASAPP-ASR system has the lowest WER values among all the systems in the Dev clean and Test clean datasets.
The Park, et al. system has the highest WER values among all the systems in the Test clean and Test other datasets.
Table 2 shows Italy's Spearman correlation results with total and daily case count prediction for mBERT and LASER (Embed) using different time settings.
The highest correlation for total case count prediction using mBERT is 0.880.
The difference in accuracy between the full model and the filtered model decreases as the model complexity increases.
The accuracy of the models trained on AFLite-filtered instances is lower than the accuracy of the models trained on a random 40% subsample of ImageNet.
The size of the training set increases from "Train Data D" to "Train Data D(ϕRoBERTa)".
The performance decreases as we move from "Train Data D" to "Train Data D(ϕRoBERTa)".
Table 4 shows the dev accuracy (%) on original and AFLite-filtered MNLI-matched and QNLI.
The difference in performance between RoBERTa and RoBERTa -PartialInput is 15.9.
EfficientNet-B7 achieves a higher top-1 accuracy on ImageNet-A compared to EfficientNet-B5.
Training on AFLite-filtered data leads to an improvement in top-1 accuracy on ImageNet-A.
The size of the support of the expectation in Eq. (4) is larger for the "Synthetic" dataset compared to the other datasets.
The precision of the B-LSTM & CRF classifier for the IOB labels is 66.67%.
The F1 score for the IOB labels using the SemEval classifier is not provided.
Our method outperforms Seq2Seq, LAS, and C-Seq2Seq on TIMIT dataset in terms of mean opinion score (MOS).
Our method outperforms Seq2Seq, LAS, and C-Seq2Seq on NSynth dataset in terms of mel-cepstral distortion (MCD).
The F1 performance for trigger identification is higher than the F1 performance for trigger classification in the "Ours w/ partial data" model.
The precision for argument identification is higher than the precision for argument classification in the "JointFeature" model.
The "Attack" frame has the highest "Trigger Id." and "Trigger Cls." scores.
The "Trigger Cls." score is lower than the "Trigger Id." score.
The performance of RGB + depth + semantics decreases as the number of processes increases from 1 to 5 for the "GPU→GPU" strategy.
The performance of Habitat-Sim decreases as the frame resolution increases.
The performance of Habitat-Sim for RGB sensors at a resolution of 128 is 4@093 frames per second.
The error rate for "Hrsn" is lower than the error rate for "AdHominem" for all values of labels.
The error rate for [ITALIC] a=0, [ITALIC] c=0 is the lowest among all combinations of [ITALIC] a and [ITALIC] c.
The error rates for the different labels vary across the Imposters, AvEeer, Glad, Hrsn, and AdHominem models.
The error rates decrease as the values of "a" and "c" increase.
The average precision of all subjects is 0.46, the average recall is 0.51, and the average phoneme error rate is 73.32.
Moses et al. achieved a phoneme error rate of 87.56.
The adversarial model outperforms the baseline model in terms of accuracy for words with a frequency of 0.
The total number of tokens in the dataset is 259,306.
The model "wang:2015" achieved the highest accuracy of 97.78% on the PTB-WSJ test set.
The model "Ours - Adversarial" outperformed the model "Ours - Baseline (BiLSTM-CRF)" in terms of accuracy on the PTB-WSJ test set.
The accuracy for the word frequency range "0" is 95.37%.
The total number of tokens is 14,418.
The scores for the POS clusters increase from the initial (GloVe) to the baseline to the adversarial training.
The average score for the word embeddings increases from the initial (GloVe) to the baseline to the adversarial training.
The cluster tightness evaluation scores for word embeddings improve after baseline training and further improve after adversarial training.
The average cluster tightness scores for all parts of speech (POS) are higher after adversarial training compared to the initial and baseline stages.
As the perturbation scale increases, the average cluster tightness of word embeddings also increases.
The perturbation scale of 0.05 results in the highest average cluster tightness for word embeddings.
Table 12 provides information about the peak memory consumption of GPU systems.
The peak memory consumption for the "newstest2014" dataset using the "Marian gpu_base4bit" system is 783.05 MiB.
The "Meeting" component has the highest duration of 205.5 hours.
The "Command and Control" component has the lowest percentage of domain at 2.4%.
The Word Error Rate (WER) for the baseline model with all data is 29.4.
The Word Error Rate (WER) for the baseline model with all data in the close-talk scenario is 20.9.
The number of epochs used for training the proposed model is 15.
The number of distinct words in the NCBI dataset is 8147.
The table shows the effect of predicting different numbers of randomly selected voxels for different types of word embeddings.
As the number of voxels increases, the effect of predicting randomly selected voxels decreases for the "glove-300" word embedding.
The "Glove" embedding is evaluated with dimensions of 50, 100, 200, and 300.
The "ELMo" embedding has a search space of [600, 200] for the number of units in the hidden layer.
The Compressive Oracle score for ROUGE-1 is 57.12.
The Extractive Oracle score for ROUGE-2 is 30.37.
Table 5 provides the summary state ablation results for the CNN dataset.
The Compressive method achieves a ROUGE R1 score of 32.5.
The table presents the results of different speaker verification methods on the VoxCeleb1 dataset.
The xvector-AAM method with MoCo pretraining and cosine similarity achieves the lowest EER, minDCF0.01, and minDCF0.001 scores among all the speaker verification methods.
Among the baseline systems, the xvector-AAM (cosine) method has the lowest EER value.
Among the baseline systems, the xvector-AAM (cosine) method has the lowest minDCF0.01 value.
The table presents the results of AAM trained models using different pretraining strategies.
The xvector-AAM (MoCo-extra) method achieves the lowest minDCF0.001 score among all the methods.
The FST method has higher FRR values compared to the S-DTW method for all keyword combinations at different SNR levels.
The average FRR values for the Hey Snips keyword are higher than the average FRR values for the Hey Snapdragon keyword for both the S-DTW and FST methods.
The highest ROUGE score is achieved when the "approach" is "phrase-wise" and the "modality" is "I + M".
Table 2 shows the standard deviation as a measure of robustness for different models.
The F1z value decreases as the standard deviation increases.
The "All Features(A)" feature set has a higher F-score compared to the individual feature sets.
The Precision, Recall, and F-score values are equal for the "Random" feature set.
The Precision, Recall, and F-score values vary for each method in the comparison results of methods identifying ambiguous headlines.
The CSR Features method has the highest Precision among the methods in the comparison results of methods identifying ambiguous headlines.
The F-score of the Body-dependent method is 0.630.
The Precision of the All Features method is 0.646.
Our model outperforms all other models in terms of precision, recall, and F1-score for both the English and Chinese datasets.
Our model achieves a higher F1-score on the Chinese dataset compared to the Huang et al. model.
The table shows the EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.
The EER(%) result for the i-vector system trained on VoxCeleb and evaluated on the CN-Celeb(E) evaluation set is 19.05.
As the retention rate increases, the BLEU score decreases.
The highest BLEU score is achieved at a retention rate of 0.2.
The method "PG-BLEU-4" achieves the highest BLEU-4 score among all the methods listed in the table.
The method "PG-BCMR" achieves the highest CIDEr-D score among all the methods listed in the table.
Table 2 shows the accuracy on the test set and on script/text-based questions for different models on MCScript2.0.
TriAN achieves the highest accuracy on the test set, script-based questions, and text-based questions on MCScript2.0.
Table 1 represents the distribution of question labels before validation.
The total number of answerable questions is the same for the text-based, script-based, and text-or-script categories.
The maximum accuracy for each model is printed in bold in the "acc" column.
The accuracy on the test set and on script-based questions (acc scr) is the same for the TriAN model.
Different subsets of adapted parameters of the Diff-Lp model are tested on TED, AMI, and SWBD test sets.
The WER(%) results decrease as more adaptations are applied to the Diff-Lp model.
The table provides WER(%) results for different subsets of adapted parameters of the Diff-Gauss model on the TED, AMI, and SWBD test-sets.
The WER(%) results for the Diff-Gauss model with LHUC adaptation are lower than the results for the Diff-Gauss model without LHUC adaptation on the TED, AMI, and SWBD test-sets.
The MAN model achieves the highest accuracy and macro-F1 scores for both the Restaurant and Hotel datasets.
Removing positional attention from the MAN model results in a lower macro-F1 score for the Restaurant dataset.
MAN-BiLSTM achieves the highest ACC and Macro-F1 scores for both the Restaurant and Hotel datasets.
BERT-base achieves the highest ACC and Macro-F1 scores for the Hotel dataset.
LSA and LexRank have the highest scores in the "R-2" column.
The average number of words in the original documents is 5864.
The average number of words in the documentary subtitles is 5864.
The maximum number of sentences in the documentary subtitles is 656.
The ROUGE scores for MMR with Support Sets are higher when using Subtitles compared to using Script.
The ROUGE scores for Original Docs are higher when using Script + Subtitles compared to using Subtitles or Script alone.
The GRASSH. model achieves the highest ROUGE scores on the Script dataset compared to other datasets.
The Original Docs model performs better on the Script + Subtitles dataset compared to the Subtitles and Script datasets.
Our Model outperforms the Direct baseline in terms of precision, recall, and F1 score.
Our Model achieves a higher F1 score compared to the Direct baseline.
The dataset in Table 1 consists of three categories: Verb, Noun, and Adjective.
The total number of instances in the dataset for the Verb category is 3624, for the Noun category is 4062, and for the Adjective category is 7946.
The count for ICD-10 I10 is significantly higher than the count for ICD-10 M81.0.
The AUC value for ICD-10 I73.9 is higher than the AUC value for ICD-10 I25.10.
The AUC Micro score for Logistic Regression with a maximum length of 512 is 0.815.
The AUC Macro score for EHR BERT Big + XML with a maximum length of 1024 is 0.927.
The "Multi-head Att" model has a Macro AUC value of 0.825.
The "Big EHR BERT + XML" model has a Macro AUC value of 0.933.
The table provides results of different models, including MLP and FAKTA.
The FAKTA model with G/2lbl setting has the highest F1 (Macro) score among all the models mentioned in the table.
The models are ranked based on their performance in terms of R@1, R@5, R@10, and R@20.
The models in the "using Query Generation" row are using a query generation technique.
The table compares the WER and training speedup of DNNs trained using ASGD, BMUF, and Minibatch-SGD algorithms on 4 GPUs with various synchronization periods.
As the minibatch size increases, the training speedup also increases.
The WER(%) for the test-clean dataset is lowest when using a minibatch size of 1024.
The LSTM RNN with MLE has a lower test perplexity (PPL) compared to the LSTM RNN with seqGAN and the Memory Network LM.
The Memory Network LM has the lowest number of lines of model code compared to the LSTM RNN with MLE and the LSTM RNN with seqGAN.
The table compares the performance of Transformer decoder and GRU RNN decoder in the HERD conversation model for response generation on the Switchboard dataset.
The precision score for BLEU-3 is higher for HERD-Transformer compared to HERD-GRU.
The table shows the results of text style transfer on the Yelp data using three different models.
Table 2 presents the results of different similarity calculation techniques.
The Role Factor Tensor technique achieves the highest accuracy for word pairs.
The "Role Factor Tensor" system has a Spearman's ρ score of 0.71 for ρ WP and 0.64 for ρ EV.
The "Comp. Neural Net" system has a Spearman's ρ score of 0.68 for ρ WP and 0.63 for ρ EV.
The dataset used in Table 3 is curated manually for coherence by removing noisy instances and context events.
The Role Factor Tensor system has a higher performance on MCNC WP compared to CMCNC WP.
The table shows the average annotator scores for generated schemas for three different systems.
The average scores for Role Factor (EV) and Comp. Neural Net (EV) are higher when considering 0's in the calculation.
Table 4 describes the impact of different sub-sampling methods on the word-level perplexity for two datasets, PTB and WT-2.
The Silhouette score for TASA with a dimension of 15 is higher than the Silhouette scores for TASA with dimensions of 5 and 10.
The Correlation for ukWaC with a dimension of 15 is higher than the Correlations for ukWaC with dimensions of 5 and 10.
The table presents F1-scores on the TED corpus document classification task for different settings.
Bridge CorrNet achieves the highest F1-scores for multiple languages in the TED corpus document classification task.
The table shows the label counts for different categories of news titles.
There are 31,414 news titles in the finance category.
The count for the "Finance" label is 133480.
The count for the "Olympics" label is 34767 and the count for the "Health" label is 2340.
The average score for the "Graph VQA (full model)" method is 74.94.
The average accuracy for the "with attention-based image features" method is 34.73.
Graph processing improves the accuracy of the method when enabled for both question and scene.
Performing only 1 iteration of graph processing improves the accuracy of the method.
The "Graph VQA (full model)" achieves the highest overall score in the "Multiple choice" method.
The "LSTM blind" has the lowest score in the "Yes/no" category of the "Open-ended" method.
The accuracy of our model on the text data is 82.5%.
The accuracy of our model on the combined speech and text data is 88.7%.
Models with greater input context achieve higher accuracy.
Models with CNN+LSTM architecture achieve higher accuracy compared to models with CNN only architecture.
The textual embedding dropout for CMU-MOSEI is 0.3.
The optimizer used for IEMOCAP is Adam.
The MulT model achieves the highest accuracy, F1 score, MAE, and correlation values compared to all other models in the ablation study.
The model that only considers language and visual modalities and predicts audio performs worse in terms of accuracy, F1 score, MAE, and correlation compared to the models that consider language and audio or language, audio, and visual modalities.
The Word Classifier outperforms the Alignment-based approaches without cross-modal supervision (our approach) in terms of accuracy for all corpora.
The Word Classifier outperforms the Alignment-based approaches without cross-modal supervision (our approach) in terms of accuracy for all corpora.
Table 1 displays the accuracy of different models on the CoNLL dataset.
The model "Our (confidence-order)" achieves an accuracy of 95.04±0.24 on the CoNLL dataset when trained using the training set.
The table compares the word similarity and analogy results for the languages pa, hi, gu, mr, te, and ta.
The table shows the performance of different models on NLVR2.
VisualBERT without Grounded Pre-training achieves a performance of 63.9 on the development set.
The F1 scores for agreement between annotators a1-a2 and a1-a3 are lower than the F1 score for agreement between annotators a2-a3.
The F1 scores for agreement between annotators a1-a2 and a1-a3 are lower than the F1 score for agreement between annotators a2-a3.
Table 8 shows the performances of models on RW-FG.
The NCP model achieves a performance of 43.31 on the Dev CS task.
Table 9 shows the error types of manual evaluation for two different models, NCP and NCP+TR.
The NCP+TR model has a 8.94% rate of wrong claims (WC).
The "AT Enc-6 Dec-6" and "AT Deep-Shallow (12-1)" models have the same scores for both "Orig." and "Reorder" columns.
The "DisCo Enc-12 Dec-1" model has the highest score difference between the "Orig." and "Reorder" columns.
The AT Enc6-Dec6 model achieves the highest BLEU score in the WMT14 EN−DE →DE task.
The Lev. Transformer, DisCo, and SMART models outperform the CMLM model in the WMT16 EN−RO →RO task.
Table 5 shows the WMT14 EN→DE test results in BLEU that analyze the effects of distillation in fast translation methods.
The "AT Enc-6 Dec-6" model achieves the highest BLEU score in the WMT14 EN→DE test results.
The seq2seq model has only 1 layer.
The decay factor for the seq2seq model is 0.99.
The Ensemble setting achieves a higher Smatch score than the seq2seq setting.
The Ensemble setting achieves a higher score for Concepts than the seq2seq setting.
HRAN has the lowest validation perplexity among all the models.
HRAN has the lowest test perplexity among all the models.
The table compares the performance of HRAN model against S2SA, HRED, and VHRED models.
The HRAN model outperforms the S2SA, HRED, and VHRED models in terms of win percentage.
The proposed baseline performs better than the baseline for all speakers.
The proposed baseline achieves higher average speech quality compared to the baseline.
The MLP model performs better than the CNN model for the "Randomly initialized character embedding" architecture.
The MLP model performs better than the CNN model for the "Pre-trained fasttext word embedding" architecture.
The accuracy of the classification decreases as the number of classes grouped together increases.
The F1 scores for both classes increase as the number of classes grouped together increases.
The proportion of datasets for VQA-Other, VQA-Number, VQA-YesNo, Comp, and Supp are all 12.66% in the "VQA + Comp + Supp" row.
The number of training samples is 443,754 for all three rows.
The "YesNo" training dataset consists of 100% samples from the "VQA-YesNo" dataset.
The "YesNo + Comp + Supp" training dataset contains a total of 505,879 samples.
The predictors "f" and "DL" are statistically significant in predicting word death.
The predictor "DU" has a positive relationship with word death.
The mIoU values for the "Image+Text" modality are consistently higher than the mIoU values for the "Image" and "Text" modalities.
The precision values for the "Image+Text" modality are consistently higher than the precision values for the "Image" and "Text" modalities.
Table 6 presents the results for four different experiments with two modalities each.
The ExCL-clf 1-c model achieves a clip localization accuracy of 23.3% at IoU = 0.7 on the Charades-STA dataset.
The ExCL-reg 2-b model achieves a clip localization accuracy of 45.5% at IoU = 0.3 on the TACoS dataset.
The CNN-SC (ours) model outperforms other baseline models that do not use external data in all three datasets (Yelp, DBPedia, Yahoo).
The ULMFiT model performs better than other models that use pre-training and fine-tuning with external data in the Yelp and DBPedia datasets.
The table represents the average system performance of different systems.
The "LsCosFs" system has the highest latency-penalized F1 score.
The "Proposed simultaneous MT" system outperforms the "Baseline wait-k (k=9)" system in terms of Bleu and Ter scores.
The "Proposed simultaneous MT" system has a lower delay value compared to the "unidirectional" and "(6 enc. 2 dec.)" systems.
The proposed simultaneous NMT system outperforms the offline baseline and the unidirectional and bidirectional systems in terms of performance.
The proposed simultaneous NMT system achieves higher Bleu scores compared to the offline baseline, unidirectional, and bidirectional systems.
The average ELAS score is higher for udpipe compared to stanza.
The ELAS score for gold tokenization is higher than stanza for all languages.
Table 1 shows the LAS on the combined dev set for udpipe models trained on different languages' combined training treebanks and the models trained on the language's largest treebank.
The udpipe model trained on the largest Dutch treebank performs worse than the model trained on the largest Polish treebank.
Our fixed system performs better than the official submission on average.
Our official submission performs better than Baselines B1 for the Italian language.
Table 2.3 shows the Spearman correlation of the metrics p@n and j@n for 1000 target words for different values of n for word2vec, GloVe, and fastText.
The value of p@5 is higher than the value of p@2.
The number of tokens for Hindi is 48×106, for Finnish is 155×106, for Chinese is 215×106, for Czech is 225×106, for Polish is 469×106, for Portuguese is 489×106, and for English is 4501×106.
The models in this work outperform the models published by Bojanowski (2016) and Grave (2018) on the word analogy tasks for most languages.
The table compares the scores of word2vec models on word analogy tasks for different languages.
The table shows the highest score achieved by the word2vec models in this work.
The table compares the coverage of our models on word analogy tasks for different languages to the results published by Grave et al. in 2018.
Our model achieves a coverage of 96.6% on word analogy tasks for the Chinese language.
The relative difference between the lowest and highest scores for the "Hi" language is 13.5%.
The lowest score observed on the word analogy tasks for the "Pl" language is 55.44.
Table 2.3 shows the Spearman correlation of the metrics p@n and j@n for 1000 target words for word2vec, GloVe, and fastText for different values of n.
The value of p@5 is higher than the value of p@2 in Table 2.3.
The correlation between the same metric at different values of n is always 1.
The correlation between different metrics at the same value of n is always less than 1.
Table 2.3 shows the Spearman correlation of the metrics p@n and j@n for 1000 target words for different values of n∈{2,5,10,25,50} for word2vec, GloVe, and fastText.
The metric j@10 has a perfect correlation with itself.
Table 2.3 shows the Spearman correlation of the metrics p@n and j@n for 1000 target words for different values of n∈{2,5,10,25,50} for word2vec, GloVe, and fastText.
The metric j@5 has a perfect correlation with itself.
The target word "inertia" has a mean value of 9.24⋅10−3.
The target word "massless" has a probability value of <10−10.
The table shows the Spearman correlation of the metrics dPIP for 1000 target words for different values of |V'| for word2vec, GloVe, and fastText.
The Spearman correlation increases as the value of |V'| increases from 103 to 105.
The table shows the Spearman correlation of the metrics dPIP for 1000 target words for different values of |V'| for word2vec, GloVe, and fastText.
The Spearman correlation between the metrics dPIP for 1000 target words and the value of |V'| being 103 is 0.996 for GloVe.
The table shows the Spearman correlation of the metrics dPIP for 1000 target words for different values of |V'|∈{103,104,105,|V|} for word2vec, GloVe, and fastText.
The Spearman correlation between |V'|=103 and |V'|=104 for word2vec is 0.949.
The mean analogy score for the English language is 71.89.
The GloVe analogy score for the Polish language is 16.50.
The table shows the performance of different models on the validation and test sets for the Penn Treebank language modeling task.
The char-level 4-layer LSTM model has a train time of 4.2 hours per epoch and a BLEU score of 16.53.
The word similarity scores for the "7.2B-word collection" are higher than the scores for both the "17M-word (text8)" and "1B-word benchmark" corpora.
The vocabulary size for the "1B-word benchmark" and "7.2B-word collection" corpora is the same.
The performance of Distributed w2v on the Word Similarity task is 64.0 for N=1.
The performance of BDW on the Word Analogy task is 32.4 for N=1.
The system "Ours + Inde.Train." achieves a BLEU score of 46.15 on the MT06 dataset.
The average BLEU score of our NMT systems is 45.85.
The "Ours" system achieves the highest score of 27.86 on the WMT 2014 English-to-German (EN-DE) task.
Our implementation of the Transformer model achieves a higher score of 27.54 compared to the original Transformer model's score of 27.30 on the WMT 2014 English-to-German (EN-DE) task.
The "Full" model performs better than the "Single" and "Fair" models in all language pairs.
The translation performance from German to English is better than the translation performance from English to German.
Table 1 shows the Test BLEU scores for each model and dataset, including the best performing (unaligned) model for each dataset.
The BLEU score for COCO & STAIR JA-EN is [BOLD] 35.25 when using the pretrained, spk & enc fixed model.
The accuracy on the test set decreases as the dimension of the model increases.
The Vec2seq+FFNN model performs better than the FFNN model on the development set for all dimensions.
Table 2 displays the human performance on the DMC task with the MCIC-COCO dataset.
The accuracy for "at least 2 out of 3" is [BOLD] 82.8%.
The accuracy on the development set increases as the value of λgen increases from 0.0 to 16.0.
The CIDEr score on the test set is highest when λgen is set to 4.0.
The model "NSM – our model" achieves a higher average F1 score compared to the model "STAGG".
The model "NSM – our model" achieves a higher average precision compared to the model "STAGG".
The table provides scores for UPOS, UAS, and LAS for different treebanks in five different languages.
The ELMo-enhanced UDPipe 2.0 models with ELMoOSCAR achieve the highest scores for UPOS, UAS, and LAS in all five languages.
Adding ELMoOSCAR(3) improves the UPOS score for the Catalan-AnCora treebank.
Adding ELMoOSCAR(10) improves the LAS score for the Bulgarian BTB treebank.
The UAS score for the Finnish-FTB treebank with +ELMoOSCAR(10) is 98.13.
The LAS score for the Indonesian-GSD treebank with +ELMoOSCAR(5) is 94.23.
The total emissions for the OSCAR-Based ELMos model is 216.78 kg.
The power draw for the OSCAR-Based ELMos model is higher than the power draw for the Wikipedia-Based ELMos model.
The macro F1-score improves with the increase in the number of epochs.
The macro F1-score at 4 epochs is 0.797.
The macro F1-score achieved after 1 epoch is 0.900.
The macro F1-score achieved after 13-956 epochs is 0.903.
The method "HeLI 2.0 + iterative LM-adapt." achieves the highest weighted F1 score on the 2017 GDI test set.
The method "Perplexity (Citius_Ixa_Imaxin)" uses "ch. 7-grams" as features.
The Macro F1-score increases when the value of k changes from 1 to 52, 54, or 55.
The Macro F1-score is consistent at 0.774 when the value of k is 52, 54, or 55.
The F1 score for the method "HeLI 2.0 with LM adapt." is 0.707.
The features used for the method "CNN with GRU (safina)" are characters.
The table shows the macro F1-scores gained with different values of k when tested on the ILI 2018 development set.
The highest macro F1-score is achieved when k is either 32 or 48.
Table 13 provides information about the Macro F1-scores with iterative LM adaptation on the ILI 2018 development set.
The Macro F1-score achieved after 1 epoch is 0.964.
The method "HeLI 2.0 with iter. LM adapt." achieves the highest F1 score of 0.958 on the 2018 ILI test set.
The method "HeLI 2.0 with LM adapt." achieves an F1 score of 0.955 on the 2018 ILI test set using the same features as the method "HeLI 2.0."
Table 15 provides the macro F1-scores for the second part of the test set using different training data combinations.
The highest macro F1-score is achieved when using only the 1st part of the test set as the training data.
The model with hierarchical training and mutual information maximization achieves the highest performance compared to other combinations.
Increasing the dropout rate negatively affects the performance of the model.
The VHDA model outperforms the RNN model in terms of accuracy on all datasets in the WoZ2.0 Goal column.
The GCE+ model performs better than the GLAD+ model in terms of accuracy on the MWoZ-H dataset in the MWoZ-H Inf column.
The VHCRa model has a lower WoZ2.0 ROUGE score compared to the VHDAb model.
The VHDAb model has a higher DSTC2 Ent score compared to the VHCRa model.
The user simulation performance of VHDAb is better than VHUSa in terms of accuracy and entropy in the WoZ2.0 dataset.
The user simulation performance of VHDAb is better than VHUSa in terms of accuracy and entropy in the DSTC2 dataset.
The table shows the F1 (%) scores for different methods on the CoNLL-2003 dataset.
The F1 (%) score for the method described as "This work" is 91.14±0.04.
The word dropout rate used in the experiments is 0.05.
The LSTM dropout rate used in the experiments without BERT is 0.2.
The precision of "This work" is higher compared to "- L" and "- L&D" for both ACE-2005 and GENIA datasets.
The recall of "- L&D" is lower compared to "This work" and "- L" for the GENIA dataset.
The method "This work" achieves the highest F1 score among all the methods listed in the table.
The addition of BERT+FLAIR improves the F1 score of the method "This work".
The GPT2-small rand. model has the lowest Total effects (TE) on gender bias in the Winobias dataset.
The GPT2-xl model has the highest total effect on Winobias Dev.
The GPT2-medium model has a higher total effect on Winogender BLS compared to the GPT2-small model.
The table shows the Total effects (TE) of gender bias in various GPT2 variants evaluated on the professions dataset, when separating by gender-stereotypicality.
The accuracy of document matching using "Our approach Eq. ( 3 )" is higher than using "en-fr" and "en-es".
The "Random Negatives" approach performs better than the "(5) Hard Negatives" and "(10) Hard Negatives" approaches in terms of precision at N (P@N) results for the en-fr language pair.
The "Random Negatives (Augmented)" approach performs better than the "Random Negatives" approach in terms of precision at N (P@N) results for the en-es language pair.
The "Negative Selection Approach" with "(20) Hard Negative" has the highest precision at all N values for both en-fr and en-es translations compared to the other approaches.
The precision at N values generally increases when using the "Random Negative (Augmented)" approach compared to the "Random Negative" approach for both en-fr and en-es translations.
Table 5 shows the BLEU scores on WMT datasets of NMT models trained on original UN pairs and two versions of mined UN corpora.
The BLEU score for the en-fr dataset at the mined sentence level is 29.63.
The table shows the BLEU scores on WMT datasets of NMT models trained on filtered ParaCrawl data.
The NMT model trained on filtered ParaCrawl data achieved a BLEU score of 39.81 for en-fr translation and 33.75 for en-es translation.
Table 7 shows the GOOD translation rate (%) annotated by translation professionals for the en-fr and en-es language pairs.
The translation rate for "our model" is higher than the translation rate for "zipporah" for both the en-fr and en-es language pairs.
The values in the "r4r → r4r sr↑" and "r4r → others sr↑" cells are consistently higher than the corresponding values in the "r4r → r4r cls↑" and "r4r → others cls↑" cells.
The values in the "r4r → r4r cls↑" and "r4r → others cls↑" cells are consistently higher than the corresponding values in the "r4r → r4r sdtw↑" and "r4r → others sdtw↑" cells.
The "fast+⋆" setting generally achieves higher success rates and completion length scores compared to other settings.
The "BabyWalk +" setting generally achieves higher average metrics compared to the "BabyWalk" setting.
Table 4 provides the performances of BabyWalk with different settings of curriculum-based reinforcement learning (crl).
BabyWalk's performances with curriculum-based reinforcement learning (crl) and lecture improve the metrics of "r4r → others sr", "r4r → others cls", and "r4r → others sdtw" compared to the performances without lecture.
The performance measures for each model and each data split are reported in Table 6.
The re-implemented versions of the models generally have lower performance compared to the reported results.
The Neural Network (NN) method has an accuracy of 48.94% on the SNLI dataset.
The Logistic Regression + Features method has an accuracy of 73.18% on the Clinical-QE dataset.
Table 2 shows the accuracy (%) of Deep Learning and Machine Learning methods on four datasets: SNLI, MultiNLI, Quora, and Clinical-QE.
The Logistic Regression + Features method achieves the highest accuracy of 98.60% on the Clinical-QE dataset.
The average IAA P (%) for A vs. B and A vs. C is 79.36.
The average Partial IAA F1 (%) for A vs. B and A vs. C is 94.33.
The "IR+RQE System" performs better than the "IR-based System" in all measures.
The "IR+RQE System" achieves higher scores than the "IR-based System" in both the best and median results.
The table provides p-values from ANOVA for different tasks and metrics.
The p-values indicate that there is evidence to reject the null hypothesis for the performance of the best and worst WIs and DOs.
Table 3 provides the TED capitalization results on test-set-2.
The Single-BiRNN model achieves a higher UPPERCASE F1 score than the Corr-BiRNN model for both the Ref. and ASR tasks.
The system "Our submission" has the highest Avg-BLEU score.
The system "RWTH Neural Redund." has a higher Avg-BLEU score than "RWTH Neural Indep.".
Table 2 shows the results on development data using different filters.
The highest performance on development data is achieved with a dominance value of 0.25 in the de-en⋅adq⋅dom filter.
The "Our submission" system has the highest average BLEU score.
The "Our submission" system outperforms all other systems in terms of BLEU score.
Our submission has the highest BLEU score of 28.62.
Our submission has a higher BLEU score than RWTH Neural Redund. and RWTH Neural Indep.
"Our submission" has the highest average BLEU score of 32.05.
"AliMT Mix-div" has the second-highest average BLEU score of 31.92.
Our submission has the highest Avg-BLEU score among all the systems.
Our submission performs better than RWTH Neural Redundancy in terms of Avg-BLEU score.
The CodeBERT (rtd+mlm) model has the highest overall score of 17.83.
The Seq2Seq model has a score of 9.64 for Ruby.
The "CodeBERT (MLM+RTD, init=RoBERTa)" model achieves the highest scores for all programming languages.
The "CodeBERT (MLM+RTD, init=RoBERTa)" model has the highest average score among all models.
"code2seq" achieves the highest BLEU score of [BOLD] 23.04 among all the models in Table 6.
"CodeBERT (MLM+RTD)" achieves a higher BLEU score of [BOLD] 22.36 compared to both "CodeBERT (MLM)" and "CodeBERT (RTD)" in Table 6.
The UAS values for all policies in the German dataset are above 90.
The roll-in policy for all cases in the German dataset is a uniform oracle.
The "Ours" method has been tested with encoding dimensions of 1024, 512, 256, 128, 64, 32, 16, 8, 7, 6, and 5.
The Character Error Rate (CER) decreases as the encoding dimension decreases for the "Ours" method.
"bow-CNN" has the lowest error rate on RCV1.
"seq2-bow n-CNN" has the lowest error rate for sentiment classification on IMDB.
The label for Table 2 is "single" and the label for Table 4 is "multi".
The number of test samples for Table 2 is 49,838 and the number of test samples for Table 4 is 781,265.
The method "seq2-bow n-CNN [Ours]" has the lowest error rate among all the methods compared in Table 3.
The method "NB+SVM bow2 [WM12]" uses an ensemble approach to achieve an error rate of 8.78%.
The cosine similarity for BERT embedding is 1.21.
The accuracy for Word2vec embedding using supervised Naïve Bayes is 0.89.
Table 4 provides the results of different systems on the whole test data.
The mean rank decreases as the system number increases.
The average quality scores for well-formed questions are higher than the average quality scores for ill-formed questions.
The percentage of question pairs in which the ill-formed and well-formed questions are semantically equivalent is higher for well-formed questions.
The Transformer model outperforms all other models in terms of BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR scores.
The Grammatical error correction method performs the best among the methods built from other resources in terms of ROUGE-L score.
Table 7 shows the results of how additional training data affects the performance of the transformer model.
The addition of ⟨well-formed, well-formed⟩ pairs to the MQR TRAIN dataset improves the ROUGE-1 and ROUGE-2 scores.
The combination of "Transformer (MQR + Quora) → GEC" performs better than "Transformer (MQR + Quora)" alone on all evaluation metrics.
The combination of "Transformer (MQR + Quora) → GEC" improves the BLEU-4 score compared to "GEC" alone.
Table 9 presents the results of human evaluation of three models on 75 test examples.
The Transformer (MQR + Quora) model performs better than the Ill formed model in terms of spelling, grammar, explicitness, semantics wrt. ill-formed, and semantics wrt. well-formed.
The Combined algorithm with TADW structure performs worse with the Content D2V algorithm compared to other structures.
The performance of the CENE BiRNN algorithm improves as the percentage of data used for training increases.
The DW algorithm performs better than the Structure algorithm for all percentages of data used for training.
The performance of the algorithm DW improves as the percentage of data used for training increases.
The algorithm NC outperforms the algorithm TADW in terms of performance on Zhihu-Gender.
The TADW algorithm performs better than the Algorithm DW in terms of performance on Zhihu-Location.
The performance of DLNE with RNN is higher than the performance of DLNE with 90% data weight.
Table 1 provides a summary of the corpora used in the paper.
CMU-MOSI and SEMAINE have higher emotional content compared to Fisher and DAIC.
The performance of "Sub-part Alignment" is higher on addOneSent overall compared to addSent overall.
The performance gap between the normal SQuAD and the overall performance on adversarial set is larger for "Sub-part Alignment" compared to "Our BERT".
The table compares the performance of a model on different types of questions with sub-part alignment and normal alignment, as well as with triggers and without triggers.
The model's performance is lower when using triggers compared to when not using triggers.
The Cosine-Brier's method achieves the highest score on the MSR dataset.
The LSTM model achieves the highest accuracy for the NER task.
The LSTM model achieves the highest accuracy for the UDPOS task.
The Hits@10 and Hits@100 values are higher for the Vector used in cosine similarity [BOLD] Session-32 compared to the Vector used in cosine similarity [BOLD] Enriched-32.
The Click ([BOLD] V [ITALIC] c) value is higher for the Vector used in cosine similarity [BOLD] Enriched ( [BOLD] V [ITALIC] e) compared to the Vector used in cosine similarity [BOLD] Click ( [BOLD] V [ITALIC] c).
The table represents the prediction results of the most likely hotel the user will click next among all possible hotels.
The enriched model achieves a Hits@1000 score of 90.1.
The "Enriched-32" model has a higher Hits@1000 score compared to the "Session-32" model.
The "Enriched-32" model has a higher Hits@10 score compared to the "Session-32" model.
"BERT-base" and "BERT-large" are two different methods used for text classification.
The "BERT-base" method achieves a higher accuracy on the IMDB dataset compared to the "Multi-Task DNN" method.
The "EWT+TLE [ITALIC] orig" row has a higher value for both "Train Set" and "UPOS" compared to the "EWT" row.
The "Grammatical" rows have higher values compared to the "Ungrammatical" rows for both "EWT" and "TLE [ITALIC] orig" in terms of "LAS".
The performance decreases as the number of layers increases.
The table compares the performance of two different systems, LSTMP and HLSTMP.
The performance of the systems without overlap is lower than the performance with overlap.
The table compares the performance of AspeRa using GloVe embeddings and AspeRa using SGNS embeddings.
The table compares the performance of AspeRa using GloVe embeddings and AspeRa using SGNS embeddings.
The NARRE model does not have a MSE score for the Instant Videos category.
The AspeRa (SGNS) model has the lowest MSE score for the Toys & Games category.
The SAE-large(ours) model outperforms the Baseline model in terms of Joint EM score on the Test set.
The SAE(ours) model performs better than the Baseline model in terms of Answer F1 score on the Dev set.
The EMS score for BERT only is 70.65 and the joint EM score is 31.87.
The RecallS score for +PR(0,1,2) is 95.61 and the joint F1 score is 66.45.
The table shows the ablation study results on the HotpotQA dev set for different variations of the model.
The full model achieves a joint EM score of 39.89 and a joint F1 score of 66.45 on the HotpotQA dev set.
Table 4 compares the performance of different reasoning types in terms of joint EM and F1 scores.
DFGN performs better in terms of joint EM score under the Comparison reasoning type compared to the Baseline.
The CVAE model with reconstruction achieves a uniqueness rate of 0.68.
The VAE+DISC model with reconstruction achieves a match rate of 1.00.
The "All" domain has 5,651 samples in the training subset.
The "Movies" domain has 353 words.
The table shows the performance of different services in terms of Seq2Seq BLEU, Seq2Seq SER, CVAE BLEU, CVAE SER, GPT2 BLEU, and GPT2 SER.
The GPT2 service has a BLEU score of 0.0000 for all services except for "music_1" which has a BLEU score of 0.7224.
The table provides data on the performance of different models (Seq2Seq, CVAE, and GPT2) on various partially-unseen services.
The GPT2 model has the lowest Slot Error Rate (SER) for the "restaurants_2" and "banks_2" services.
Table 6 provides the manual sentence evaluation for English-to-Hindi translation.
The percentage of sentences evaluated as "Helpful" is 55.8%, "Doubtful" is 33.8%, and "Misleading" is 10.4% in the manual sentence evaluation for English-to-Hindi translation.
The "Test" score for the "Bu,h" system is 34.79.
The "Tune" score for the "Bu,hTg" system is 37.65.
The class accuracies for the EmotionPush Dataset WA and EmotionPush Dataset UWA are higher than the class accuracies for the Friends Dataset WA and Friends Dataset UWA.
The F+E accuracy for the EmotionPush Dataset UWA is higher than the F+E accuracy for the Friends Dataset UWA.
The difference in F1 score between the WSJ system and the J & N system is 1.61.
The F1 score for the Our system on the WSJ dataset is 89.20.
Our system outperforms the "he2018jointly he2018jointly" system on the Brown out-of-domain test set in terms of precision and F1-score.
Our system achieves higher precision and recall on the CoNLL-2012 (OntoNotes) dataset compared to the "Ours (Single)" system.
Table 6 shows the effectiveness of ELMo representations and biaffine scorer on the CoNLL 2005 and 2008 WSJ sets.
Our system achieved an accuracy of 86.3% on the CoNLL-2005 set and 85.3% on the CoNLL-2008 set.
The table compares pairs of observable lists.
The table provides the Spearman correlation coefficient for each pair of observable lists.
The F-score for Span (τ=0.5) in the BIO category is higher than the F-score for Span (τ=τ∗) in the BIO category.
The precision for Span (τ=τ∗) in the BIO category is higher than the precision for Span (τ=0.5) in the BIO category.
The Exact Match score is higher for the Expanded data compared to the Original data.
The Precision and Recall scores are higher for the Expanded data compared to the Original data.
The expanded question generation model achieves a slightly higher exact match (EM) score compared to the original question generation model.
Both the original and expanded question generation models achieve the same partial match (PM) score and weighted average (WA) score.
The table shows the results of joint span detection and question generation with a threshold of 0.5.
The precision, recall, and F-score for the "Expanded" row are lower than the "Original" row.
The system "Templ" has the highest rank among all the systems, while the system "ED" has the lowest rank.
The system "RevAbs" is ranked the highest among all the systems.
The model "CAFE (Tay et al., 2018)" performs better on the "In" task compared to the "Cross" task.
The model "DIIN * (Gong et al., 2017)" performs better on the "Cross" task compared to the "In" task.
The KIM model achieves an accuracy of 88.6 on the SNLI dataset.
The KIM model outperforms the LSTM Att. model on the SNLI dataset.
Table 5 provides the number of instances and accuracy per category achieved by ESIM and KIM on the (Glockner et al., 2018) test set.
"TaxoExpan-FWFS" has the highest F1 score among all the methods listed in the table.
"ETF" has the lowest Recall score among all the methods listed in the table.
PGAT consistently outperforms other models in terms of graph propagation.
Model variants 4 and 7 have the highest performance in terms of matching.
BERT-Large achieves the highest performance in the SentEval SSS task.
ELMo achieves the highest performance in the DiscoEval DC task.
Table 3 provides the average layer number for the best layers in SentEval and DiscoEval.
ELMo has a lower average layer number than BERT-Base in SentEval and DiscoEval.
The accuracy of the baseline encoder without a hidden layer is 52.0.
The accuracy of the baseline encoder with a hidden layer is 61.0.
Table 6 shows the accuracies (%) for a baseline encoder on the Sentence Position task when using a downstream classifier with or without context.
The accuracy of the baseline encoder with context is 47.3% on the Sentence Position task.
The average number of words in the content for the topic of Sport is higher than the average number of words in the content for the topic of Ent.
The average number of characters in the content for the topic of Sport is higher than the average number of characters in the content for the topic of Ent.
Table 5 provides information about the discrimination error rate on the development set and a sample of 200 examples, evaluated by the discriminative model and human evaluator.
The discrimination error rate for the VaeEncoderDecoder model on the sample is 2.0.
The accuracy of the models increases as we move from the EncoderDecoder model to the AttEmbedDecoder model.
The AttEmbedDecoder model improves the performance of the original dataset in terms of both loss and accuracy.
The "insert-lead" setting has the highest mean score compared to other settings.
The "insert-lead3" setting has a higher score compared to the "original" setting.
Table 4 shows the average ROUGE-1, -2, and -L F1 scores on three different sets: Dearly, Dmed, and Dlate, each containing 100 documents.
The Oracle model has the highest average ROUGE-1, -2, and -L F1 scores on the Dearly, Dmed, and Dlate sets.
The table shows the accuracies of different object detection methods on RefCOCO, RefCOCO+, and RefCOCOg datasets.
MMnasNet (1×) performs better than other methods on RefCOCOg, RefCOCO+ Val, and RefCOCOg Test datasets.
The table shows the per-type accuracies of MMnasNet with different decoder operation pools.
The overall accuracy is highest when the decoder operation pool includes {SA, RSA, GA, FFN}.
The method "MMnasNet (1×)" achieves the highest accuracy scores in all categories.
The accuracy scores for the "Dev Y/N" category are higher than the scores for the "Dev All" category.
Table 5 compares the recall@{1, 5, 10} on Flickr30K for different methods.
MMnasNet (1×) achieves the highest recall@1 score on the Text⟶Image task.
The "Joint" model performs the best on the STS-B (Pears.) task compared to the Vanilla and MEG models.
The "MEG" model performs the best on the QNLI task compared to the Vanilla and Joint models.
Table 12 shows the comparison of our approach K-nom to a baseline that does not make use of background knowledge for different relation types.
Our approach K-nom achieves higher precision values compared to the baseline that does not make use of background knowledge.
The PPAD score for WKP is higher than the PPAD- score for WKP.
The Coll- score for NYTC is higher than the Coll- score for WKP.
The table compares the number of API mappings mined by StaMiner and DeepAM.
Both StaMiner and DeepAM have high percentages of correct API mappings.
Table 5 provides information about the average loss between attention generated by input-feeding and non-recurrent systems and the manual alignment over RWTH German-English data.
The average loss between attention generated by input-feeding systems is lower than the average loss for non-recurrent systems over RWTH German-English data.
The table shows the classification accuracy using formal features after 1000 epochs.
The mean accuracy (1K) using raw features is 0.6757 with a standard deviation of 0.0230.
The "Joint SAN" model performs the best on the SQuAD 2.0 development dataset.
The "R.M-Reader2" model performs the best on the SQuAD 2.0 test dataset.
The Joint SAN + Classifier model outperforms the SAN model in terms of EM score.
The Joint SAN + Classifier model has a higher F1 score than the SAN model.
Table 5 shows the result of baseline linear SVM model when combining both text and network features for models (A) and (B).
Model (B) performs better than model (A) when trained on both text and network features.
The VDCNN model with fMLLR + ivec as the auxiliary feature has the lowest WER among all the models.
The VDCNN model with fMLLR + ivec as the auxiliary feature has a lower WER than the VDCNN model with MFCC as the auxiliary feature.
Table 1 shows the WER (%) of baseline DNN and CNN on Aurora 4.
The average WER (%) for DNN is 11.11% and for CNN is 10.64%.
The model "vd10" has the lowest average WER compared to other models.
The model "vd10" has a WER of 6.26.
The "vd10-fpad-tpad" model with 1 input feature map has an average WER (%) of 8.81.
The "vd10-fpad-tpad" model with 3 input feature maps has both frequency axis pooling and time axis padding.
The model VDCNN has the lowest average WER (%) among the three models.
The state-level weighted log likelihood score combination in joint decoding (1⃝ ⊗ 2⃝) has the lowest WER (%) among all the combinations.
The F1-score improves gradually as we move from stage 2 to stage 3 in XtremeDistil.
The F1-score is highest for the Softmax layer at stage 3 in XtremeDistil.
Table 7 shows the impact of using various word embeddings for initialization on multilingual distillation.
The SVD + mBERT (fine-tuned) combination achieves the highest F1-score among all the word embeddings used for initialization.
Table 10 presents the results of distillation with BERT Large on 500 labeled samples per class.
The distillation with BERT Large achieves the highest accuracy on the Elec dataset.
The HQ(P) score is consistently higher than the other scores in the "WMT.p" column.
The scores for all submissions are very low in the "human" column.
Table 2 provides human adequacy assessments for different kinds of references.
The human adequacy assessment for the combination of all four reference translations is 95.3.
The values for "WMT.p", "HQ(R)", "HQ(P)", and "HQ(all 4)" in the "Full Set (22)" column are higher than the values in the other rows of the same column.
The performance of "AR+WMT" and "all 4" is the same based on the values in the "Reference" column.
Among the alternative metrics, Yisi-1 has the highest correlation with the translation quality in the WMT 2019 English→German task.
The BLEU scores for the "human" and "WMT.p" references are higher than the BLEU scores for the "WMT" and "HQ(R)" references.
The model ranking agrees with human judgments for the "human" and "HQ(p)" references, but not for the "WMT" and "HQ(all 4)" references.
The translation "de.orig" has a higher lexical variety compared to the other translations.
The translation "de.tr1.p" has a higher lexical density compared to the other translations.
The perplexity values for all methods are lower in the "Valid" column compared to the "Test" column.
The match rate for the "mixed training" with "GRU" in the "Sequicity" framework is 0.6367.
The accuracy for the "Belief tracker" in the "Multi-domain" setting is 0.8253.
Table 6 shows error rates for different RL initialization strategies.
The error rate for the "Yahoo Answer" dataset with the "Ext (random Init)" RL initialization strategy is 28.0.
The "Description (Abs.)" model has a test error rate of [BOLD] 10.0 on the Reuters dataset.
The "Description (Abs.)" model has a lower test error rate of [BOLD] 25.7 on the AAPD dataset compared to other models.
The table shows test error rates on the BeerAdvocate and TripAdvisor datasets for multi-aspect sentiment classification using LSTMs, Hi-Attention, Label-Emb, and BERT models.
The Description (Abs.) model has a test error rate of 15.6 on the BeerAdvocate dataset.
The RL+MLE model performs better than the RL and MLE models in terms of style, content, and fluency.
The RL+MLE model has a higher success rate than the RL and MLE models.
The DualRL method achieves the best overall scores on both the Yelp and Gyafc datasets.
The Human references achieve perfect accuracy and content similarity scores on both the Yelp and Gyafc datasets.
The "DualRL" method has the highest average ratings compared to all other methods on the Yelp dataset.
The "DualRL" method has the highest success rate compared to all other methods on the Gyafc dataset.
The table compares the F1 scores of different models for subject anaphora resolution on modified NTC.
The model WBP-Shwartz (ens) −{word, path} achieves an F1 score of [BOLD] 56.47 for zero anaphora resolution with NOM dependencies.
Table 2 shows the impact of each feature representation on different models.
The model WBP-Roth performs the best on the task with a dependency distance greater than or equal to 5.
The highest accuracy is achieved when using the "Replace tf-idf Step" reconciliation scheme.
The highest accuracy is achieved when using the "Replace tf-idf" context-vectors.
The RL-Extract system performs the best in terms of accuracy in the "Shootings NumKilled", "Shootings NumWounded", "Adulteration Food", "Adulteration Adulterant", and "Adulteration Location" categories.
The RL-Query system performs better than the Majority Agg. system in terms of accuracy in all categories except for "Adulteration Adulterant".
Iu Mien is a language belonging to the Hmong-Mien family and is spoken in Laos.
Lahu has 1452 tokens.
The instrument F-Statistic is 3,338.7.
The model includes opinion fixed-effects.
The approach "Lchi++ [ITALIC] α(convex softmax)" with the addition of "LWA-MISI-5" achieves the highest SI-SDR performance on wsj0-2mix.
The SI-SDR performance of the approach "Lchi++ [ITALIC] α(doubled sigmoid)" is consistently lower than the SI-SDR performance of the approach "Lchi++ [ITALIC] α(convex softmax)".
Table 5 presents the results for cross-lingual transfer using four different methods: "Both", "Only ML", "Only AE", and "Neither (vanilla)".
The full back-translation method performs better for the "eng–est" language pair compared to the "eng–dan" language pair.
The back-translation method without using an autoencoder performs better for the "eng–slo" language pair compared to other language pairs.
The "Attention-CFA" model has the highest Conflict F1 score and Life F1 score.
The "Concat-CFA" model has a higher precision score than recall score for both Conflict and Life tasks.
W-METEOR has a higher correlation with human judgments compared to METEOR.
BLEU-2 has a positive correlation with human judgments.
Table 1 provides information about the likelihood of human dialogues using different models.
The Hierarchical model has a lower perplexity compared to the RNN model.
The "Full" model has a higher score compared to both the RNN and Hierarchical models.
The "Baseline Clusters" model has a slightly higher score compared to the Hierarchical model.
Table 3 compares different rollout strategies for the Full dataset and shows that diverse rollouts significantly improve performance.
The Baseline rollout strategy performs better than the No Rollouts strategy.
Table 3 shows the accuracy on CLEVR validation data at different budgets B as a function of the bootstrap set size, |Binit|.
As the budget increases, the accuracy on CLEVR validation data also increases.
The accuracy of the model increases as the budget increases.
The accuracy of the model is higher when the generator is conditioned on both the image and the question type compared to when it is conditioned only on the image.
The models are tested on both NLP and Programs.
The FiLM model has higher accuracy than the CNN+LSTM+SA model for both NLP and Programs.
The Shakespeare chatbot model has the lowest training loss among the four chatbot models.
The DeltaAir chatbot model has a higher training loss than the Shakespeare chatbot model on the 20NewsGroups dataset.
The average parsing F1 score with bi-LSTM Treebank is 81.39.
The joint decoding F1 score for English with bi-LSTM Treebank is 93.49±0.43.
The F1 scores for parsing and tagging consistently increase when using MTL compared to single-task learning.
The F1 score for joint decoding using BERT is higher than the F1 score for parsing and tagging using BERT.
Our parsing models outperform the best-performing systems from the CoNLL 2018 shared task for most treebanks.
Our parsing models achieve higher labeled attachment scores than the best-performing systems from the CoNLL 2018 shared task.
The performance of the parsing model is lower when using the bi-LSTM feature extractor compared to using the MTL parsing model.
The complexity ratio decreases when using the bi-LSTM feature extractor compared to using the joint decoding model.
Table 2 compares the performance of the RPN initialized by random values and the RPN initialized by the CMBP coefficients on dstc2eval and dstc3eval.
The RPN initialized by CMBP coefficients achieves higher accuracy on dstc2eval compared to the RPN initialized by random values.
Table 4 compares different shortcut block combinations.
Dense compositions (Type 2 and 5) perform better than sparse ones.
Table 2 compares the performance of shortcut topologies in two different cases: Case 1 and Case 2.
The shortcut topology that performs the best on the test set is when both clt and hlt are updated in Case 1 with a gate.
The table compares the performance of different hyper-parameters for two different cases: "window size" and "character-level".
Increasing the hyper-parameter values improves the performance for both "window size" and "character-level" cases.
Table 5 describes the training task settings over a shrunken training set.
As the percentage of the full training set decreases, the number of instances per class also decreases.
The human baseline achieves an EM score higher than 90 for all data types.
The Extended NumNet+v2 + Knowledge model achieves an F1 score higher than 60 for all data types.
The table describes the loss of plug and play models with or without KL cost annealing under different epochs on the Chinese CECG dataset.
The total loss decreases as the number of epochs increases for both plug and play models with and without KL cost annealing on the Chinese CECG dataset.
The Word-based CNN architecture achieves the highest accuracy score of 0.927 on the Chinese CECG dataset.
The Word-based GRU-last architecture achieves an AUC score of 0.980 on the Chinese CECG dataset.
The values in the "Metrics" column change as the sentiment score increases from 0.0 to 1.0.
The value in the "COH1" cell is smaller than the value in the "COH2" cell.
The table shows the evaluation of reinforcement learning models with different reward combinations on the Chinese CECG dataset.
The sentences in the "Original input" column are translations of the corresponding sentences in Chinese.
Increasing the vocabulary truncation percentage results in a decrease in the quality of the translation.
The Word-based GRU-last-2 architecture achieves the highest accuracy score of 0.994.
The Word-based GRU-avg architecture achieves the highest AUC score of 0.998.
The neural segmentation method has higher precision than the dpseg method for the grapheme boundary scores.
The merged neural segmentation method has higher recall than the proportional segmentation method for the pseudo-phones boundary scores.
MT1 has the highest number of error categories compared to HA and HB.
For the input space dimension of 80, the test set correlation coefficient is 0.475.
The original input space dimension is 160.
Among all the feature combinations at the feature level, "BF + CF + pitch + i-vector" has the highest correlation coefficient.
Among all the feature combinations at the score level, "BF + CF + ACF_pitch + Feature fusion" has the highest correlation coefficient.
The table presents scores for different languages.
Table 2 provides information about the percentage of scores for Knowledge Relevance and Context Coherence for different models.
The ITE+CKAD model has a higher percentage of scores for Relevance and Coherence compared to the Wizard model.
The F1@5 and F1@10 scores for Krapivin are consistently higher than the scores for Inspec.
The Tf-Idf method achieves an F1@10 score of 0.304 on the Inspec benchmark among all the unsupervised methods.
The F1 scores for both Inspec and Krapivin datasets generally increase as the number of layers in the GCN model increases.
The SeqPointer model performs better in terms of F1@10 score compared to F1@5 score for both Inspec and Krapivin datasets.
The Tr-DQN agent outperforms the Tr-DRQN agent in all difficulty levels for both 20 Training Games and 100 Training Games.
The GATA-GTF input performs better than the GATA-GTP input in all difficulty levels for both 20 Training Games and 100 Training Games.
There are two models with the Graph Encoder: OG w/ Graph Encoder and COC w/ Graph Encoder.
The Ground-truth model achieves the highest F1 score on the test set.
The ROUGE scores for the "w/o r_i" condition are lower than the scores for the "w/o c" condition.
The ROUGE scores for the "w/o z" condition are lower than the scores for the "w/o c" condition and the "w/o r_i" condition.
Table 1 provides the perplexity of trained models for different tasks.
The perplexity of the models without attention is generally higher than the perplexity of the models with attention.
Table 6 provides the performance breakdown per relation for CNN and BIDIRECT on the development set.
Table 5 provides the performance of different relation classification systems along with their F-scores.
The CR-CNN classifier achieves an F-score of 84.1 with special treatment to the Other class.
The F1 score is highest when the "+WordNet" feature is added.
The precision is highest when the "+POS" feature is added.
The table compares the F1 scores of different neural network models with various sets of features.
The "depLCNN" and "SDP-LSTM" models have the highest F1 scores among all the approaches.
The F1 score increases as more features are added.
The WordNet feature contributes the most to the F1 score.
The table shows the translation results of different models when reconstruction is used in training but not in testing.
The model with both encoder reconstruction and decoder reconstruction achieves the highest Test score and the highest improvement compared to the baseline model.
Both SMT and NMT systems show an improvement in translation performance when using manually labelled DPs compared to the baseline.
The SMT system has a lower baseline translation performance compared to the NMT system.
The model "+ enc-rec + dec-rec" achieves the highest test score.
The model "+ enc-rec + dec-rec" improves the test score by +1.45 compared to the baseline.
The translation performance gap between automatic and manual labeling DPs for input sentences in testing is +4.06.
The translation performance gap between automatic and manual labeling DPs for input sentences in testing is +3.91.
The Semantic IR system has a higher F-Measure Snippets value than the aueb-nlp-5 system.
The GMAP Snippets value for the ustb_prir4 system is 0.0205.
Table 5.4 shows the BioASQ 7B Phase A Snippet Ranking results.
The F-Measure Snippets value for Semantic IR is 0.2568.
The table shows the results of the BioASQ 6B Phase A Batch 2 Document Ranking task for different systems.
"ustb_prir3" has the highest F-Measure Docs score.
"Semantic IR" has the lowest GMAP Docs score.
The system "aueb-nlp-3" has the highest F-Measure Docs score.
The system "ustb_prir2" has the highest MPrec Docs score.
The "aueb-nlp-5" system has the highest precision and F-measure scores among all the systems.
The "aueb-nlp-4" system has the highest recall and GMAP scores among all the systems.
The system "aueb-nlp-1" has the lowest precision, recall, F-measure, MAP, and GMAP scores among all the systems.
The system "aueb-nlp-3" has the highest precision, recall, F-measure, MAP, and GMAP scores among all the systems.
The "aueb-nlp-5" system has the highest F-Measure Docs score.
The "ustb_prir4" system has the lowest GMAP Docs score.
The "Semantic IR" system has the highest F-Measure Docs score.
The "MindLab Red Lions++" system has the lowest GMAP Docs score.
"aueb-nlp-5" has the highest F-Measure Docs score.
"lh_sys1" and "lalala" have the same MRec Docs score.
The system "aueb-nlp-5" has the highest F-Measure Docs score.
The MindLab QA System has a higher MAP Docs score than F-Measure Docs score.
"aueb-nlp-5" has the highest F-Measure Docs score.
"MindLab QA System ++" has the lowest MAP Docs score.
The table presents the results of snippet ranking for two different systems: "Semantic IR" and "Deep ML methods…" in BioASQ 7B Phase A Batch 1.
"aueb-nlp-2" has the highest F-Measure Docs score among all the systems.
"MindLab QA System" has the lowest GMAP Docs score among all the systems.
Table 2 shows the case-sensitive BLEU scores for En→De translation using different variations of the Transformer model with different sizes and routing methods.
The BLEU scores for En→De translation increase as the model size increases and with the inclusion of routing methods.
The Transformer-Base model outperforms the baseline model on MT03, MT04, MT05, and MT06 translations.
The addition of dynamic routing and EM routing improves the average BLEU score of the Transformer-Base model.
The x-vector clustering method does not have any DER values for Sim1spk, Sim2spk, Sim3spk, and Sim4spk.
The estimated number of speakers increases as the number of speakers in the simulated mixtures increases.
The estimated number of speakers increases as the number of speakers increases, except for the case of 6 speakers, where it is higher than the case of 5 speakers.
The oracle number of speakers generally increases as the number of speakers increases, except for the case of 4 speakers, where it is lower than the case of 3 speakers.
The "Best post-is2019-deadline" method achieves the lowest DER and JER values among all the methods in Table 7.
The "SA-EEND + EDA (Estimated #Speakers)" method outperforms the "DIHARD-2 baseline" method in terms of DER and JER.
Table 1 presents the results on IMDB in terms of accuracy (%).
The "Discrete softmax" has a higher accuracy (90.78) compared to "Discrete sparsemax" (90.58).
The highest accuracy on IMDB is achieved with N=32, N=64, and N=128 for Disc. + Cont. sparsemax.
The accuracy on IMDB increases with N from 32 to 64 for Continuous softmax, but then decreases when N is further increased to 128.
The experiments in Table 4 used different types of transition systems for discontinuous parsing.
The performance scores for the different transition systems vary in Table 4.
The values in the "Attribute" column represent different morphological attributes.
The values in the "Cov." column represent the coverage percentages of the morphological analysis.
The GRU model with k-max-max-pooling outperforms the full-pooling model on WikiQA.
The Addition method achieves a higher MAP score than the CNN-Cnt method on WikiQA.
The category "rejet" has the highest number of cases in the "First-word ruling (6-class setup)" column.
The category "cassation" has the highest number of cases in the "Full ruling (8-class setup)" column.
The pattern "co-conspirator" appears 443 times in the dataset.
The last column in the dataset is labeled as "50,000".
The patterns listed in the table are arranged in descending order based on their count.
The patterns listed in the table are related to different concepts or entities.
The pattern "X is" has the highest count among all the listed patterns.
Table 5 provides a summary of three different spaces: "domain", "function", and "mono".
The "mono" space has the highest density (1.91%) among the three spaces mentioned in Table 5.
The "verb:verb" category has the highest accuracy among all the parts of speech in the dual-space model.
The total number of instances for all parts of speech categories in the dual-space model is 374.
The correlation values in the table are calculated based on leave-one-out correlation between subjects.
The average correlation value for each phrase type is provided in the table.
The correlations in the table are calculated using leave-one-out correlation between subjects.
Different algorithms are used for different phrase types in the table.
The number of "false new" (FN) errors is lower than the number of "false anaphor" (FA) errors in the proper noun category.
Table 1 shows the results (F1) on the CoNLL 2012 test set, with the average of MUC, B3, and CEAFe scores.
The F1 score for the baseline model with the LEA metric is 64.39.
The CRF model has a higher F1 score than Lemma and Word2Vec.
The CRF model has a higher precision than Lemma, Word2Vec, and CRF without sequential features.
Table 1 provides information about the identification of script-relevant verbs within a scenario and independent of the scenario.
The precision and F1 score of the "Our model (scen. indep.)" for identifying script-relevant verbs independent of the scenario are 0.513 and 0.645, respectively.
The table shows the results of full text-to-script mapping.
The Ident. model+CRF model has the highest F1 score among the three models.
Table 3 shows the Pearson's correlations between word similarity/qvec/qvec-cca scores and the downstream text classification tasks.
The "qvec-cca" score is the highest among all the tasks in Table 3.
Table 2 shows the BLEU scores obtained from the SMT systems for newstest2011 and newstest2012.
The baseline SMT system achieved a BLEU score of 11.64 for newstest2012.
The accuracy of the ABSA English model is lower when applied to MT outputs instead of human translations.
The accuracy of the ABSA model is higher when using UGC ⊕ BT ⊕ PE + tags.
Table 4 shows the results of the model's robustness to capital letters on the Foursquare-test dataset.
The model performs better in terms of case insensitive BLEU lower and title when the source side of Foursquare-test is set to "noised case".
The table shows different models and their corresponding performance on the "news" and "test" datasets.
The combination of UGC, back-translation, and PE with tags achieves the highest performance on the "test" dataset.
Table 8 displays the ROUGE scores on the Amazon test set for alternative summary adaptation strategies.
The "Ours" model performs better than the "Gold" model in terms of the 2nd characteristic.
The "USL" model performs worse than the "USL+R" model in terms of the 1st characteristic.
Table 3 describes the differences between more and less successful counselors in their responses to nearly identical situation setters.
More successful counselors have a higher percentage of responses with hedges compared to less successful counselors.
Table 5 provides information about the proportion of novel n-grams in summaries generated by various models on the XSum test set.
The percentage of novel bigrams in summaries generated by ConvS2S is [BOLD] 79.50.
Table 2 provides information about the corpus bias towards extractive methods in the CNN, DailyMail, NY Times, and XSum datasets.
XSum has a higher percentage of novel n-grams in gold summaries compared to the CNN, DailyMail, and NY Times datasets.
The table provides ROUGE results on the XSum test set for different models.
The T-ConvS2S (enc( t′, tD), dec tD) model achieves the highest F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L.
The "ELMo + MD QA PGNet(PT)" model achieves a performance of 92.29 without any dosage.
The models achieve a performance of 34.58 with NN frequency.
BERT + QA PGNet(PT) achieves the highest dosage F1 score among all the models.
Lookup table + MD QA PGNet (B) achieves the lowest frequency F1 score among all the models.
Table 4 shows the ROUGE-1 scores of the best performing models on the ASR and human written transcripts for the MR extraction task.
The model "BERT + QA PGNet(PT)" has the highest F1 score for the "HW" transcripts.
The EoG (Full) method performs better on overall sentence pairs than on intra- and inter-sentence pairs.
The EoG (NoInf) method performs better on intra-sentence pairs than on overall and inter-sentence pairs.
EoG (Full) performs better than EoG (NoInf) on both the development and test sets.
EoG (Sent) performs better than both EoG (Full) and EoG (NoInf) on the intra-domain task.
EoG (PubMed) performs better overall compared to EoG (GloVe) and EoG (random).
EoG (PubMed) performs better in intra-class classification compared to EoG (GloVe) and EoG (random).
Table 4 provides an ablation analysis for different edge and node types on the CDR development set.
The edge type "-SS" has the lowest F1 score for the overall performance.
The F1 score for the "EoG" model on the CDR development set is 63.57%.
The ablation analysis that removes both node types, MM context, and distances ("−T,C,D") has an F1 score of 43.48% for inter-class relations.
Table 9 provides the hyper-parameter values used in the reported experiments.
The batch size used in the reported experiments is [2,3].
The "End-to-end+Visual" model achieves the highest F1 score for the "Discussed" category.
The "Viscoref" model achieves the highest F1 score overall.
Table 8 shows the results for UL vs FT vs GN for four different features: ALL, SENT, READ, NE, and TIMEX.
The standard deviation of the F1 scores for the features ALL, SENT, READ, NE, and TIMEX are 0.070, 0.106, 0.100, 0.100, and 0.069, respectively.
Table 4 provides information on the average sentiment and emotions.
The average sentiment for all sentences in the FT category is 5.03.
The F1 scores for UL vs. GN are higher than the F1 scores for UL vs. FT for all features.
The NE feature has the highest F1 score among all features.
Table 7 shows the classification results on different single classes.
BERT-large (ArgKP) has the highest accuracy among all the methods mentioned in the table.
BERT-large (MNLI) has a higher accuracy than BERT-large (SNLI).
The argument "Mainstream schools are essential to develop social skills" has the highest number of arguments.
The argument "Homeschooling is often the best option for catering for the needs of exceptional/religious/ill/disabled students" has the second highest number of arguments.
The average percentage of votes for the emotion "AFRAID" is 0.04, and the average percentage of votes for the emotion "DONT_CARE" is 0.05.
The percentage of votes for the emotion "AMUSED" is 0.10.
Table 6 presents regression results using Pearson's correlation.
The highest correlation value for the "SURPRISE" emotion is 0.25.
Table 7 represents classification results in terms of F1 measures.
The F1 measure for SADNESS is highest in the "DepecheMood nf" column.
Table 2 shows the language model perplexity across templated datasets.
The perplexity is lowest for the "Full" template.
The "Base" model has a slightly higher BLEU score on the "dev" set compared to the "test" set.
The addition of abbreviation expansion (ae) to the s2s model improves the BLEU scores on both the "dev" and "test" sets.
Table 6 provides the fact-set content selection results of different systems on the dev set.
The "s2s+ae" system has higher precision, recall, and F1 scores compared to the "base" and "s2s" systems.
Table 7 presents the hallucination results of systems with respect to the Wikidata input on the dev dataset.
The precision, recall, and F1 scores for the "s2s" system are 0.96, 0.55, and 0.70, respectively.
Our proposed model performs better than the Captioning Model in terms of BLEU-4, METEOR, ROUGE, CIDEr, and SPICE scores.
Finetuning our proposed model improves the performance in terms of BLEU-4, METEOR, ROUGE, CIDEr, and SPICE scores.
Table 1 provides information about the corpora genres and the number of named entities (NEs) of different classes in each corpus.
In the ACE NW corpus, the number of named entities is 894 for the PER class, 2238 for the LOC class, and 703 for the ORG class.
The F1 score for Memorisation is lower than the F1 scores for CRFSuite, Stanf, and SENNA across all datasets.
The F1 score for CRFSuite is higher than the F1 scores for Memorisation, Stanf, and SENNA.
The p-value for the concept pairs "small table" and "big table" is 0.007.
The average bounding box size for "big cat" is larger than the average bounding box size for "small cat".
The CIDEr score for the "butr + rr" model is 92.7.
The METEOR score for the "full" model is 27.4.
The scores for Color A, Color I, Size A, Size I, Verb T, and Verb I are different for each category.
The scores for the + rr category are higher than the scores for other categories.
The table provides scores for diversity metrics for different models.
The concept pair "hold child" has the highest number of occurrences in the training set.
The concept pair "black cat" has the highest number of occurrences in the validation set.
The table shows BLEU scores on news-test2014 calculated with multi-bleu.perl for different setups.
The NMT-LV setup has a BLEU score of 32.86 with the Hiero system.
Table 2 shows BLEU scores on news-test2014 for different NMT setups.
The NMT-Hiero setup achieves a higher BLEU score than the NMT-LV setup.
Table 2 shows the number of candidate birth senses obtained for different time periods.
The number of candidate birth senses obtained for time period T1 is 4220.
The success rate for the "Split" category is higher than the success rate for the "Join" category.
The category with the highest number of candidate words is "Birth".
The Bi-GRNN model has the highest accuracy among all the deep learning models in Table 4.
The RNN model was trained for 24 epochs.
The DeepEmo model achieves the highest F1 Avg. score among all the models.
The char_ngramTF-IDF feature extraction method achieves the highest F1 Avg. score among all the feature extraction methods.
The F1-score of "DeepEmo (Ours)" is higher than the F1-scores of "Roberts (2012)", "Qadir (2013)", and "Mohammad (2015)".
The data size of "DeepEmo (Ours)" is larger than the data sizes of "Roberts (2012)", "Qadir (2013)", "Mohammad (2015)", and "Volvoka (2016)".
Table 10 shows the before-after results on the word similarity task on seven datasets.
The RAND-WALK processed results have a higher score than the RAND-WALK original results on the RG65 dataset.
The "WORD2VEC" embedding is trained on the "Google News" corpus.
The "GLOVE" embedding has an average vector norm of 8.30.
Table 3 shows the before-after results on the word similarity task on seven datasets.
The processed GLOVE embeddings outperform the original GLOVE embeddings on all datasets.
Table 5 shows the before-after results on the word analogy task for different embeddings.
The processed version of WORD2VEC performs better than the original version on the word analogy task.
The original CNN WORD2VEC model achieved an accuracy of 86.68 on the IMDb dataset.
The processed LSTM-RNN GLOVE model achieved an accuracy of 41.90 on the SST dataset.
The table shows the before-after results (x100) on the categorization task.
The processed results of the RAND-WALK method have a higher performance compared to the original results.
Table 12 shows the before-after results on the word analogy task.
The processed version of the RAND-WALK method performs slightly better than the original version on the word analogy task.
Table 15 shows the before-after results on word similarity task on seven datasets.
The processed data with 300 dimensions has a higher score than the original data on the RG65 dataset.
The processed data consistently outperforms the original data in the categorization task.
The performance of the categorization task improves with the increase in dimension.
Table 17 presents the before-after results (x100) on the word analogy task.
The processed data consistently outperforms the original data for all dimensions in the word analogy task.
GLOVE processed performs better than WORD2VEC processed on the "city-in-state" task.
The "+pg" model performs better than the other models on the ctb and spmrl dev sets, except for Hebrew.
The "+pg" model performs better than the "Our baseline" model on the French language.
The table provides information about three different summarization datasets: Gigaword, CNN/DailyMail, and WikiSum.
The table provides the ROUGE-1 recall scores for each dataset: 78.7 for Gigaword, 76.1 for CNN/DailyMail, and 59.2 for WikiSum.
The 40th percentile of the number of citations is 2.
The 80th percentile of the search results size is 135,533.
The "tf-idf" extractor has the highest test log-perplexity among all the extractors.
The "combined" corpus has the highest ROUGE-L score among all the corpora.
The models' performance improves as we move down the table.
The models' performance improves as we move down the table.
Table 5 shows the linguistic quality human evaluation scores for different models.
The "tf-idf-only" model performs worse than the "T-DMCA (best)" model in terms of grammar, non-redundancy, and referential clarity.
Table 6 provides a side-by-side comparison of two models with large automatic metric gaps.
Model A outperforms Model B in terms of ROUGE-L.
The ROUGE-1 R score for "T-DMCA (Ours)" is higher than the ROUGE-1 R score for "Sauper & Barzilay" on "All Wikipedia".
The ROUGE-1 P score for "T-DMCA (Ours), n=1322" is higher than the ROUGE-1 P score for "Sauper & Barzilay" on "American Actors".
S2S-Temp is compared against Seq2Seq, CVAE, HTD, and ECM models.
The Kappa values for S2S-Temp vs. Seq2Seq, S2S-Temp vs. CVAE, and S2S-Temp vs. HTD are 0.68, 0.69, and 0.64 respectively.
The table shows the experimental results of different models on the NarrativeQA reading comprehension challenge using summaries.
The BiAttention (150 d Hybrid MRU-LSTM) model outperforms other models in terms of Bleu-1, Bleu-4, Meteor, and Rouge-L scores.
The model "BiAttention (250d Sim. MRU)" achieves the highest scores in both RACE-M and RACE categories.
The model "BiAttention (No Encoder)" has the shortest training time per epoch.
The Bi-Attention (200 d Hybrid MRU-LSTM) model achieves the highest accuracy on both the dev and test sets.
The Bi-Attention (200 d Hybrid MRU-LSTM) model achieves the highest F1 score on both the dev and test sets.
Table 1 describes the character error rate for the CHiME-4 corpus.
The character error rate for the "mask_net (ref)" model is 28.8 in the real evaluation.
Table 6 provides average classification results on 2018 data, spoken and reported, both on training and test data, in terms of Correct Classification (CC), Weighted Kappa (WK), and Correlation (Corr).
The average classification results for the English and German datasets on the 2018 training data are 0.712, 0.840, 0.684, 0.763, 0.866, and 0.763, respectively.
Students at grade 5 in primary school, aged 9-10, were evaluated at CEFR level A1.
Students in high school at grade 11 are typically aged 15-16.
The table presents the results of an ablation study using different measures.
The maximum value for Precision is 0.90.
As the percentage of compromised tweets decreases, the LM accuracy also decreases.
The recall values for Doc2Vec remain consistent across different percentages of compromised tweets.
The categories in the table include News, Advertisement/Spam, Re-tweet Bot, Compromised, Regular User, and Unknown.
There are 5 accounts assigned to the News category, 4 accounts assigned to the Advertisement/Spam category, 2 accounts assigned to the Re-tweet Bot category, 1 account assigned to the Compromised category, 7 accounts assigned to the Regular User category, and 1 account assigned to the Unknown category.
The table presents precision, recall, and F1 results on the Biocreative V CDR Dataset for different models.
The highest F1 score in the table is 68.4, achieved by the model with additional data.
The table presents precision, recall, and F1 results for different variations of the BRAN model on the Biocreative V CDR Dataset.
Table 2 shows F1 scores on the CoNLL 2002/2003 NER test sets for different models.
The F1 score for the Spanish language in the BERT En-labels + Adv. model is 77.6.
The table presents classification accuracy on the MLDoc test sets for different models.
The model finetuned on labeled English data with language-adversarial training achieves higher accuracy than the other models for all languages except German.
The table provides results for different annotation sets, including CHEBI, CL, GO_BP, GO_CC, GO_MF, MOP, NCBITaxon, PR, SO, and UBERON.
The table compares the performance of the baseline system with the shared-task system for different annotation sets.
The "first-span keep-longer" simplification strategy generally performs better than the "first-span keep-shorter" strategy across all entities.
The F1-scores for the "GO_MF" entity are the same for all the simplification strategies.
Table 4 provides precision and recall values for different annotation sets.
The precision value for the GO_BP annotation set with the "spans-only/-first" harmonization is 0.6170.
The Seq2Seq(10%) model achieves a BLEU score of 86.26 on the Newsela dataset.
The DRESS-LS model has a SARI score of 41.97.
The Seq2Seq(full) model achieves a BLEU score of 87.75.
The Supervised Model achieves a SARI score of 74.48.
The BTTS(+10%) model performs better than the BTTS(+full) model in terms of adequacy, but both models perform worse than the Reference model.
The BTTS(+full) model outperforms our model in terms of fluency.
The Seq2Seq model performs better than our model in terms of adequacy.
There are three different types of noise: original (drop & shuffle), + additive, and + substitution.
The SARI scores for the + substitution noise type are higher than the scores for the original (drop & shuffle) and + additive noise types.
Table 12 compares query formulation methods and amounts of retrieved evidence on the ARC dataset in terms of percentage accuracy.
The table compares different values of Top K in terms of percentage accuracy on the ARC dataset.
The ET-RR model achieves the highest accuracy on ARC, RACE-Open, and MCScript-Open tests.
The ET-RR model outperforms other models in terms of accuracy on ARC, RACE-Open, and MCScript-Open tests.
The model "ARC+RACE" achieves the highest accuracy on multiple-choice selection.
The "Reading Strategies" model achieves higher accuracy on multiple-choice selection compared to the "ET-RR" model.
There are four different models in the table: IR solver, Moqa, ET-RR (Concat), and ET-RR.
The ET-RR model achieves the highest accuracy among all three product categories.
The models "Reading Strategies" and "OpenAI GPT" were used as pre-trained models.
The model "ET-RR (reader)" achieved a score of 52.3 on the RACE dataset.
The highest test score for the "ET-RR" model is 36.61.
The "ET-RR" model has the highest test score.
The models used in the experiments are Vanilla RNN, GRU, LSTM, LSTM (2 layers), and CFN (2 layers).
The test perplexity decreases as the number of layers increases for LSTM and CFN models.
The SOC method achieves the highest correlation values for both the SST-2 BERT and SST-2 LSTM datasets.
The Yelp Polarity LSTM method achieves the highest correlation value on the SST-2 human annotations.
The "All lowercase" processing results in a higher number of headwords compared to the "No processing" category.
A higher percentage of headwords come from the Wiktionary source compared to the UD source in the "Mixed" category.
The "All lowercase" processing results in a higher number of headwords compared to the "No processing" category.
The majority of headwords come from the "Only UD" source.
In the "top ranked" and "second ranked" groups, the proportion of "meaning" is higher than "opinion" in the "Opinion or meaning?" column.
In both the "No proper nouns" and "Proper nouns" rows, the proportion of "No proper nouns" is higher than "Proper nouns" in the "Opinion or meaning?" column.
SpanOIE has the highest AUC score for Re-OIE2016 among all the Open IE systems.
Stanford has the lowest AUC score for OIE2016 among all the Open IE systems.
Table 7 evaluates the diversity of generated captions.
Table 7 compares the performance of different methods in terms of diversity of generated captions.
rAIRL(Up-Down) has the highest correlation values for both RP S and RP D in both the Standard Split and Robust Split.
RL(Up-Down) has a negative correlation value in the Standard Split.
"GAN(Up-Down)" has the lowest values for all correlation metrics compared to other methods.
"rAIRL+SPICE(Up-Down)" has higher values for all correlation metrics compared to other methods.
Table 1 presents the accuracy of three different models: MST Parser, Locked Para-Perc, and Lock-free Para-Perc.
The Lock-free Para-Perc 10-thread model achieves higher accuracy than the MST Parser model.
The "bandit information DuelingBandit" method achieves the highest BLEU score on the test set.
The "full information in-domain SMT" method performs better than the "full information out-domain SMT" method in terms of BLEU score.
The table provides information about the tagging accuracies for different languages.
The table compares the tagging accuracies for the original (O) and universal (U) tagset.
The F-Score of ROUGE for Model "seq2seq (Our impl.)" increases when "+CGU" is added.
The F-Score of ROUGE for Model "DRGD" is higher than the F-Score for Model "CopyNet".
The "Replies without outlier user" algorithm has a higher number of tweets compared to the "Dynamic Templates" algorithm.
The "Replies" algorithm has a lower average number of interactions compared to the "Interpolated Markov Model" algorithm.
Table 4 shows the single system results on the En-Fr task.
The method of "Ours" with +UNK replace achieves a higher score on the test set compared to the method of "Ours" with voc. manipulation.
The Theano model achieved a validation accuracy of 36.81% on the atomic dataset.
The atomic dataset achieved a BLEU score of 5.2 using the Moses metric for the elasticsearch model.
The AUC values for KBP's increased AUC by minimum connectivity group are higher for larger minimum connectivity groups.
The NELL-[ITALIC] Cat column has missing data for the [1,2) and [8,16) minimum connectivity groups.
Table 2 displays the results of the Area Under Precision/Recall Curve (AUC) on KBP datasets.
The approach that combines Information Extraction (IE), Knowledge Base Verification (KBV), and IE with KBV (KBVIE) achieves the highest AUC values on all three KBP datasets.
The SVM-Linear model achieves the highest F1 score of [BOLD] 0.771 for the detection task.
The Audio Span Features + baselines features + Standard Sampler + DNN model achieves the highest precision of [BOLD] 0.990 for the identification task.
The table shows the performance of different models on the RTE dataset.
SpEx-BERTLARGE performs better on the RTE dataset after being fine-tuned on MNLI compared to BERTLARGE.
The Leaderboard Score for BERTLARGE on CoLA is 62.1.
The Test Set Score for SpEx-BERTLARGE on STS is 79.8.
The table shows the exact match scores on the development set for a set of question answering tasks.
The highest exact match score for the SQuAD task is [BOLD] 82.5.
The accuracy score for BERTLARGE on SST is 91.1 and on RTE is 69.0.
The accuracy score for SpEx-BERTLARGE on MRPC is 82.5.
CSAN outperforms both Without Spatial Attention Block and Without Graph Attention Mechanism in terms of HR@10 score.
HGAN outperforms both Without Spatial Attention Block and Without Graph Attention Mechanism in terms of NDCG@10 score.
Table 5 presents the performance of two submodules (CSAN and HGAN), and AMRAN.
AMRAN outperforms both CSAN and HGAN in terms of HR@10 and NDCG@10.
The majority class is not applicable in the given dataset.
The weighted F1-score for success classification using Emotion Flow in the single task setting with 1000 sentences is 0.643.
Simple Margin using an RBF kernel is consistently the best expansion method for all term sets.
Centroid expansion has a higher average number of positive examples per iteration compared to simple margin linear.
The table shows the average number of positive examples found per iteration of the term set expansion method.
The "simple margin rbf" method has the highest average number of positive examples found per iteration among all term sets.
Table 1 provides the average number of positive examples found per iteration of the term set expansion method.
The Simple Margin method using an RBF kernel is consistently the best expansion method for all term sets.
The simple margin rbf expansion method has a higher average number of positive examples found per iteration compared to the centroid expansion method.
The PPMI method has a higher average number of positive examples found per iteration compared to the AFINN POS CBOW method.
The table shows the average number of positive examples found per iteration of the term set expansion method.
Simple Margin using an RBF kernel is the best expansion method for the "Elements CBOW" term set.
The Lex. Overlap heuristic has a higher number of premise-hypothesis pairs compared to the Subsequence and Constituent heuristics.
The Lex. Overlap heuristic has a higher number of premise-hypothesis pairs compared to the Subsequence heuristic, but a lower number compared to the Constituent heuristic.
John Elway was the quarterback who was 38 in Super Bowl XXXIII.
The model predicted Jeff Dean as the name of the quarterback who was 38 in Super Bowl XXXIII after inserting a distracting sentence.
Table 4 shows EM scores after training on regular data or on adversarial data and evaluating on regular dev set or adv-dev set.
Our NMN model outperforms the BiDAF baseline model in terms of EM scores for all evaluation scenarios.
The DeepAtt model outperforms the He et al. model in correctly identifying spans.
The DeepAtt model outperforms the He et al. model in correctly classifying semantic roles.
The "He et al. (Ensemble)" model outperforms all other models in terms of precision, recall, F1 score, and percentage of completely correct predicates on the CoNLL-2005 dataset.
The ensemble model of the DeepAtt architecture performs better than the individual models (RNN, CNN, and FFN) in terms of precision, recall, F1 score, and percentage of completely correct predicates on the CoNLL-2005 dataset.
The DeepAtt (FFN, Ensemble) model achieves an F1 score of 83.9 on the test set.
The DeepAtt (RNN) model achieves a precision of 81.0 on the development set.
The width of the CoNLL-2005 dataset ranges from 4 to 12.
The highest F1 score achieved on the CoNLL-2005 dataset is 83.4.
The DeepAtt model outperforms the previous state-of-the-art model (He et al.) on the development set of the CoNLL-2005 dataset in terms of F1 score.
The DeepAtt model achieves higher precision than the previous state-of-the-art model (He et al.) for the A0 label on the development set of the CoNLL-2005 dataset.
The percentage of predicted A1 labels for the gold A0 label is 76%.
The percentage of predicted MNR labels for the gold A2 label is 22%.
The "MT + project" models trained on English and machine translated data with automatically projected slot labels using fast-align have higher intent accuracy than the models trained only on English data for most languages.
The "MT + soft-align" models trained on English and machine translated data using the soft-alignment method have higher slot F1 scores than the models trained only on English data for most languages.
The intent accuracy for the Target only LSTM model is lower than the intent accuracy for the Cross-lingual LSTM model in all languages.
The Slot F1 score for the Target only BERT model is higher than the Slot F1 score for the Target only LSTM model in all languages.
The "MT + soft-align" method achieves the highest accuracy for intent classification, with a score of 94.87.
The "Source" method has the lowest F1 score for slot filling, with a score of 71.62.
The table compares the performance of "Teacher a" and "NLP experts" on conflict demonstration.
"Teacher a" has higher precision than "NLP experts" in both "Exp. 1" and "Exp. 2".
DocReader Chen et al. (2017) achieves an EM score of 30.25 on the Medication questions.
ClinicalBERT Alsentzer et al. (2019) achieves an F1 score of 93.06 on the Relation questions.
Table 6 presents the values of different evaluation metrics for TGen on the development set.
The BLEU score for TGen on the development set is 0.6925.
The "E2E" dataset has the highest number of tokens and the highest LS score among the three datasets.
The "Bagel" dataset has the lowest TTR score among the three datasets.
As the distance to root increases, the precision and recall decrease for both MST and Malt parsers.
The precision of Malt parser is consistently higher than that of MST parser for all distances.
The table displays the accuracy relative to sentence length.
The difference between the MST and Malt accuracy is shown in the "Δ" column.
The precision decreases as the number of modifier siblings (NMS) increases.
The recall decreases as the number of modifier siblings (NMS) increases.
The MST system generally has higher precision/recall compared to the Malt system for most POS categories.
The Determiner category has a high precision/recall for both the MST and Malt systems.
The table shows the accuracy of root relation.
The MST algorithm has an accuracy of 56.82% for root relation with nouns.
Table 6 provides an analysis of the amortized inference for iterations t=2,3,4 on WMT En-De test BLEU, comparing the performance of models using different training data in the M-step.
Table 7 provides an analysis of the ODD decoding on the WMT En-De test BLEU for different models trained with their EM algorithm.
The ODD decoding achieves a BLEU score of 24.54 on the WMT En-De test for the WMT En-De model.
The highest value in the "Ct" column is 24.7, which corresponds to the "history size" of 8∗.
The highest value in the "Quo" column is 17.7, which corresponds to the "history size" of 8∗.
The table presents the main results on forecasting client codes, including F1 scores for St, Ct on the dev set, macro F1, and F1 scores for each client code on the test set.
The HGRU method achieves the highest F1 score for the macro on the test set.
The table provides the main results on forecasting client codes, including F1 scores for St and Ct on the dev set, macro F1, and F1 scores for each client code on the test set.
The GMGRU [ITALIC] H method achieves the highest macro F1 and F1 scores for each client code on the test set among all the methods in the table.
The label "Res" has the lowest F1 score in the categorizing task.
The precision is higher than the recall in the forecasting task for all labels.
The table presents the results of an ablation study for a proposed model with different embeddings trained on the psychotherapy corpus.
The Glove embedding achieves the highest macro and recall scores among all the embeddings tested.
Table 14 presents the results of an ablation study comparing the performance of different loss functions on a categorizing and forecasting task.
The Cce loss function performs better for the therapist in terms of F1 score compared to the other loss functions.
Table 4 compares the BLEU scores of cascaded results and the best end-to-end results of a certain model.
The cascaded+re-seg model achieves a BLEU score of 17.77 on tst2013.
The TCEN-Transformer model achieves the highest BLEU score of 17.11 on the tst2013 set.
The Vanilla model outperforms the +enc pretrain, +enc dec pretrain, and pretrain+MTL models in terms of BLEU score on the tst2013 set.
The models "berard2018end" and "ESPnet*" have higher BLEU scores compared to the other models.
The TCEN-LSTM model has a BLEU score of 17.05.
The table presents the results of an ablation study for subword-level experiments.
DV-ngram+Unlab'd (our model) outperforms DV-ngram and bag-of-ngram for all n-gram types.
DV-ngram+Unlab'd (our model) performs better than bag-of-ngram for unigrams.
The NBSVM-tri model has the highest accuracy among the bag-of-ngram based models.
The DV-tri+Unlab'd model has the highest accuracy among the deep learning models.
The model "DV-tri+Unlab’d (our model)" achieves the highest performance in all the different ensemble settings.
The model "DV-tri+Unlab’d (our model)" performs better alone compared to the model "PV (Mesnil et al., 2014)".
The multilingual model outperforms the single language pair model on the WMT French→English dataset without oversampling.
The multilingual model consistently improves the performance compared to the single language pair model across all language pairs.
Table 2 compares the BLEU scores of WMT and Prod models on various datasets for single language pair and multilingual models.
The models in the "WMT" category are trained on the English→German, English→French, German→English, and French→English language pairs.
The "Multi" model performs better than the "Single" model for the English→Spanish language pair.
The CMPM model outperforms the GloVe and word2vec models in terms of both Silhouette Coefficient (SK) and Variance Ratio Criterion (VRC).
The CMPM model achieves better clustering performance than the GloVe and word2vec models according to the HDBSCAN algorithm.
The model with features uses a bidirectional recurrent neural network (brnn) as the encoder type.
The model with features uses the Adam optimizer with a learning rate of 0.001.
Table 5 shows the performance on five non-function part-of-speech tags, sorted by performance within content words, and compares it to the performance of the random baseline on content words.
The performance on proper nouns is higher in the "all" column compared to the "content" column, and lower in the "random" column.
Table 6 shows the ROUGE scores (F1) on the test dataset.
The ROUGE-1 score is higher for the "with features" compared to "w/o features".
The ROIBase method performs better on the news dataset than on the Weibo dataset.
The NER+pattern method performs better on the Weibo dataset than on the news dataset.
The highest score on the TED Talks (Zh-En) dataset is achieved when selecting the previous N sentence(s) as context, with a score of 18.65 for N=1.
Increasing the context sentence size N leads to a decrease in the score on the TED Talks (Zh-En) dataset for all three context selection methods.
"Our model" outperforms all other models in terms of BLEU scores on all four datasets.
The BLEU score for "Our model" on the Subtitles Es-En dataset is 24.38.
The LRL aze language has a smaller training data size compared to the HRL tur language.
The bel language has a smaller test data size compared to the rus language.
The table provides the performance of different models (Textual baseline, Visual baseline, MTL ensemble) at different hours.
The MTL ensemble model performs better than the Textual baseline and Visual baseline models.
The system correctly identified the presence of internet access at the hotel mentioned in the first row.
The system output in the first row has a low word overlap with the human reference, but high semantic similarity.
Table 5 shows the performance of different models in predicting depressive symptoms.
The baseline model has an f1 score of 76.0.
The table shows the transition probabilities of observations.
The transition probabilities of observations are higher when the symptom is high compared to when the symptom is low or 0.
Table 6 shows the prediction results of depressive symptoms using different combinations of features.
The combination of features "B + [ITALIC] Mμ+ Δ [ITALIC] m + B + [ITALIC] Tr" achieves the highest F1 score among all the combinations.
The Bi-LSTM model achieves the highest accuracy of 93.7% on the New WOZ (multi-domain) dataset.
The CNN model achieves the highest F1 score of 0.878 on the New WOZ (multi-domain) dataset.
Table 1 provides test set accuracies for different slots in the WOZ 2.0 dataset and the new dataset.
The Bi-LSTM model performs better on the "Joint goals" slot in both the WOZ 2.0 dataset and the new dataset.
As the percentage of labeled data increases, the evaluation metrics (precision, recall, F1-score, and accuracy) remain relatively consistent.
The class labels for all percentages of labeled data are consistently "Related" and "Unrelated".
The data distribution in Table 1 shows that there are two classes: "Related" and "Unrelated".
Table 1 shows that there are 5414 tweets labeled as "Related" and 4619 tweets labeled as "Unrelated".
The "Combined" target data has the highest accuracy among "Queensland" and "Alberta" target data.
The "Queensland" target data has the lowest test loss among "Alberta" and "Combined" target data.
The "Pointer-Gen-Coverage + ROUGE" model outperforms the "Pointer-Gen-Coverage" model in terms of ROUGE-1 score.
The "Pointer-Gen-Coverage + ROUGE-RP-ADV" model outperforms the "Pointer-Gen-Coverage" model in terms of ROUGE-2 score.
Table 2 shows the performance of various models on the entire Daily Mail test set using the limited length recall variants of Rouge at 275 bytes.
The SummaRuNNer model is statistically indistinguishable from the model of Cheng et al, '16 at 95% C.I. on Rouge-1 and Rouge-2.
SummaRuNNer achieves the highest Rouge-1 score among all models in the table.
SummaRuNNer achieves the highest Rouge-2 score among all models in the table.
URANK achieves the highest Rouge-1 score among all the models in Table 4.
The average Rouge-2 score of SummaRuNNer is 23.1 ±0.9.
As the claim number increases, the F-score decreases.
The precision values are higher than the recall values for all claims.
Claim 4 has the highest F-score among all the claims.
Claim 5 has the highest recall among all the claims.
DKN has the lowest accuracy in the category classification task, while KRED BERT has the highest accuracy.
All models perform well in the local news detection task, with accuracy values above 0.9.
Removing any of the layers in KRED leads to a significant performance drop.
Removing any of the layers in KRED leads to a significant decrease in NDCG@10.
Table 5 presents the results of a comparative evaluation of relevance on the human-AI image guessing dialogs.
The RL-Q-IG model has the highest preferred percentage among the models evaluated in the human-AI image guessing dialogs.
RL-Q-IG-NA has the highest PMR score among all the models.
RL-Q-IG has the lowest perplexity score among all the models.
The experimental conditions include the use of a BLSTMP encoder.
The experimental conditions specify a maximum of 15 epochs for training.
The table provides the results of automatic evaluation on the VIST dataset for different methods.
The AREL-t-100 method achieves the highest score for the B-4 metric.
Table 4 provides pairwise human comparisons of different models.
The AREL model outperforms other models in terms of expressiveness.
Table 2 shows F1 scores for ROUGE-1, 2, and L and METEOR (M) for randomly sampled 500 stories from the test set.
SLDS-25 has the highest F1 score among all the models in the "Table 2" table.
Table 5 provides information on the number and type of entries in different resources.
The total number of entries in all resources listed in Table 5 is 81,292.
The table compares the coherence of topics between specific, general, and LDA topics.
The LSTM model and the NIG model have the same values for both MCD and BAP.
The S-LSTM model has the same value for both V/UV and F0 RMSE.
There is a significant difference between the two systems for pairs 3, 9, and 10.
The LSTM system has the highest subjective preference score for pair 3.
Table 4 provides the mean N and standard deviation σN of the number of articles per domain for the WikiTailor model and the IR-based model.
For the WikiTailor model, the mean N for 100-IR10 is 64,239 and the mean N for 100-IR100 is 1,119,637.
Table 5 shows the performance on nuclearity determination for three different approaches: NN, NS, and SN.
The performance of the NN approach decreases from 67.0 to 33.7.
The F1 score for the predicate "sale" is the highest among all the predicates in 2017.
The F1 score for the predicate "price" is the highest among all the predicates in 2016.
The "+Copy +AM" model outperforms the "+Copy" model in terms of BLEU score in the "Navigate" domain.
The "Entity" model achieves a higher entity score than the "Oracle" model in the "Weather" domain.
The baseline system performs worse with a vocabulary size of 4K.
The initialized system performs better with a vocabulary size of 10K.
The performance of the CTC Baseline decreases as the size of the vocabulary increases.
AGWE outperforms CTC Initialized and CTC Regularized in terms of cross-view word discrimination performance.
The "Moses" model achieves the highest Meteor score on the En→De translation task.
The "Calixto2017b" model achieves the highest BLEU score on the En→De translation task.
The table shows bias levels for gender, race, and religion before and after debiasing with different methods.
The bias level for religion decreases after debiasing with SoftWEAT (λ=0.8).
As the value of lambda (λ) increases, the scores for each task in the CPT column decrease.
The MTurk task has the highest score among all the tasks.
The highest performance is achieved when the test set is Rapid and the context originates from Rapid.
The performance is consistently lower when the test set is Europarl compared to other test sets.
Table 2 presents results on zero-resource domain adaptation for PatTR and TED.
The maximum domain embedding value is 17.1 for PatTR and 33.9 for TED.
The accuracy for the method of Kim:2007 using all types of words in iteration 3 is 75.47%.
In iteration 1, the method of Kim:2007 extracted 35 noun compounds using hypernyms with an accuracy of 77.14%.
TDNN-BNF-10lang-SPN has the highest AUC and EER values among all the models.
SAE has the lowest AUC and EER values among all the models.
The table provides information about the South African Broadcast News (SABN) dataset.
The SABN dataset contains a total of 13,445 utterances.
The team with Submission ID 504429 achieved the highest success rate.
The team with Submission ID 504651 achieved the lowest return value.
The team with Submission ID 504430 achieved the highest success rate.
The team with Submission ID 504430 achieved the highest Language Understanding Score.
Table 2 presents the accuracy of four different models on 9-class job occupation prediction from social representations.
The model "All" achieves the highest accuracy among the four models on 9-class job occupation prediction from social representations.
Table 1 shows the accuracy for different models on friend relationship prediction from social representations.
The "Network+Text" model has a higher accuracy than the "Only Network" and "Network+Attribute" models.
The table shows the accuracy of different models on 9-class job occupation prediction from social representations.
The model "Attribute+Text" achieves the highest accuracy among all the models for job occupation prediction.
The precision, recall, and F1 score for tweet-model 1 at iteration 1 are 0.86, 0.40, and 0.55 respectively.
The precision, recall, and F1 score for tweet-model 2 at iteration 3 are 0.87, 0.86, and 0.86 respectively.
The table provides the performances of different models on the extraction of user preferences toward entities.
The "rule" model has the lowest recall value among all the models.
The performance of RoBERTa improves as the training size increases.
The test accuracy is slightly lower than the dev accuracy for all training sizes in RoBERTa.
The accuracy of the proposed method (Model Averaging(D + E + F)) is higher than the accuracy of the baseline.
The precision of the proposed method (Model Averaging(D + E + F)) is higher than the precision of the baseline.
Our Models perform better than the other systems in the TRAC 2018 Dataset.
Our proposed method Model Averaging(D + E + F) achieves the highest F1 score in the TRAC 2018 Dataset.
The method "QuesNet" outperforms the other methods in terms of knowledge mapping accuracy, precision, recall, and F-1 score.
The method "H-BERT" has the best performance in terms of difficulty estimation according to the degree of agreement (DOA).
The Difficulty estimation DOA score for QN-H is 0.6291.
The Student performance prediction ACC score for QN (no pre) is 0.7488.
When the target word dropout is set to 0.1, the combined F0.5 score is 37.89.
When the MLE weight is set to 2, the combined F0.5 score is 34.56.
The table shows the F0.5 scores for three different tracks: Restricted, Unrestricted, and Low Resource.
The table displays the best rank achieved for each track: 10/21 for Restricted, 3/07 for Unrestricted, and 5/09 for Low Resource.
The F0.5 scores of the system increase with each incremental improvement.
Iterative decoding results in the highest F0.5 score among all incremental improvements.
Table 1 displays retrieval results on COCO, including R@1, R@5, R@10, MEDR, and MR for different models.
The R@1 value for COCO5K Caption Retrieval using the DVSA model is 11.8.
The F1 score for the EESE task is higher when using the localized context compared to when not using it.
The precision for the ECSE task is lower when not using the localized context.
The table shows the performance on the ECE.
ETC-Clause has the highest F1 score among the models.
The table shows the performance on the ECE.
The table compares the performance of different models on the ECE.
The model with the highest F1 score is ETC-Clause.
The Precision, Recall, and F1 scores for the Indep model are 69.02, 56.73, and 62.05, respectively.
The F1-score for the "Inspec" dataset is higher for the BiLSTM-CRF method compared to the SE-2010 and SE-2017 datasets.
The SGRank method achieves a higher F1-score on the SE-2010 dataset compared to the Inspec and SE-2017 datasets.
The correlation (ρ) and MAE explained (R1) on the test split for the "ARGUMENT" protocol and the "Is.Particular" feature set is [BOLD] 15.6.
The correlation (ρ) on the test split for the "PREDICATE" protocol and the "Is.Hypothetical" feature set is [BOLD] 43.5.
Table 1 provides the number of questions for each dataset used in the paper.
The number of questions in the training set without distantly supervised training data is provided in the "Train Plain" column.
Table 4 shows the impact of the number of hops in the memory network final step of Weaver on bAbI Task 3, WikiHop, and SQuAD.
As the number of hops in the memory network final step of Weaver increases, the exact match accuracy (EM) on bAbI Task 3 and WikiHop also increases.
Table 5 shows document-scale results with training and testing on SQuAD using the dev set for evaluation.
The F1 score for the "full doc." testing setting in the DrQA⋆ model is higher than the F1 score for the "paragraph" testing setting.
Table 6 shows the results of answering questions using the full English Wikipedia.
The Reinf. reader-ranker model achieves a top-1 EM accuracy of 17.1 when answering questions from the WebQuestions dataset.
Removing the RNN layers and replacing them with a linear projection results in a significant decrease in the model's performance.
The "Weaver" model achieves the highest F1 score among all the models.
The improvement from char-lstm to oracle for the German language is +2.0 LAS.
The char-lstm model performs the same as the word model for the Hindi language.
The table includes LAS improvements for non-OOV and OOV words on the development set for 11 different languages.
The experiment used different types of features, including baseline, embedding char, embedding oracle, encoder char, and encoder oracle.
The accuracy for the "Case" and "Gender" features using the "embedding oracle" is [BOLD] 100.
The table shows BLEU scores between pairs of three translations (MT, Ref, and Human) for German→English at the news translation task of WMT 2019.
The BLEU score between MT and Human translations is higher than the BLEU score between Ref and Human translations.
The table provides inter-annotator agreement scores for three different translation directions: English→German, English→Russian, and German→English.
The inter-annotator agreement score for German→English is 0.320.
Table 2 displays the NER results on the test dataset for SI and SAT models.
The F-measure for NER is 69.5 for the SI model.
Table 4 compares the number of hops needed for obtaining premises.
SciBERT has a higher MAP score than BERT.
SciBERT was tested before BERT.
The RMSE values for embeddings created from the same dataset are all "-".
The RMSE values for aligning embeddings created by different embeddings are symmetric across the diagonal.
The RMSE after alignment for embeddings from the RAW dataset and GloVe embeddings is 4.12.
The RMSE after alignment for embeddings scaled from GloVe to word2vec is 3.
The table shows Spearman coefficient scores for synonym and analogy tests between the aligned word2vec to GloVe embeddings, GloVe transformed to word2vec, and between GloVe embeddings of Wikipedia and CC42 dataset.
The GloVe to word2vec normalized transformation method consistently yields lower Spearman coefficient scores for all test sets.
The table presents Spearman coefficient scores for synonym and analogy tests between different embeddings.
The normalization transformation method leads to lower Spearman coefficient scores compared to other transformation methods.
Table 6 shows the results of similarity and analogy tests before and after alignment and combining embeddings derived from different techniques and datasets by AO+Scaling.
The highest score in the "G(W)⊙W(GN)" column is [BOLD] 0.757 in the "SYN" row.
The table compares the usages of the retrieval set in two different settings: Single and Multiple.
The human score is higher in the Single setting compared to the Multiple setting.
Table 4 shows the results of experiments with differently constructed vocabularies with and without stemming.
The best captioning performance in CIDEr is obtained at Vocab7 in both (a) and (b).
Table III compares the performance of word2vec and BERT.
BERT with contextual information (LDA) achieves higher accuracy, precision, recall, and F1-score compared to word2vec.
The accuracy of the BERT+ LDA model is 0.642.
The F1-score of the METAMAP model is 0.5441.
The language with the highest UAS with full tree transfer is English.
The average UAS with predicted chunks across all languages is 56.8.
As the training set size increases, the average chunking accuracy and average accuracy (English as source) also increase.
The average chunking accuracy is higher when using the gold chunk compared to when using the full tree.
The average chunk head identification accuracy for different languages is 98.3.
The language with the highest chunk head identification accuracy is "ko" with a value of 99.5.
The full tree transfer method performs better than the predicted chunk transfer method on average for the chunk-level transfer parser.
The gold chunk transfer method outperforms the predicted chunk transfer method for all languages in the chunk-level transfer parser.
The model "NTOM" has the highest accuracy and the lowest mean squared error on both the Brexit and Election datasets.
The model "NTOM" has a higher accuracy than the model "LSTM" on the Brexit dataset.
The "Deep-Att" system achieves the highest BLEU score among all the single neural models.
The "Enc-Dec Luong et al. (2014)" system outperforms the "RNNsearch Bahdanau et al. (2014)" system in terms of BLEU score.
Table 8 shows the BLEU scores of Spanish to English speech to speech translation, combining multi-task training in different ways.
The table compares the BLEU scores of different methods for Spanish to English speech to speech translation.
The table shows the BLEU scores of Spanish to English speech to speech translation using three different methods: Direct Translation, VQ-VAE, and UWSpeech.
The UWSpeech method achieves the highest BLEU score among the three methods for Spanish to English speech to speech translation.
Table 2 shows the BLEU scores of English to Spanish speech to speech translation using three different methods: Direct Translation, VQ-VAE, and UWSpeech.
The UWSpeech method achieves the highest BLEU score among the three methods for English to Spanish speech to speech translation.
The Fusion method achieves the lowest Diarization Error Rate and Jaccard Error Rate among all the methods.
The TS-VAD-MC method performs better in terms of Diarization Error Rate and Jaccard Error Rate compared to the TS-VAD-1C (it2) method.
Table 4 shows the performance of the individual components on PTB section 22 (dev).
The F-score for the "Sym" component is 67.09.
The embedding dimensionality for relative positions is within the experiment range of 10-50.
The number of LSTM units (RNN) is within the experiment range of 0-2400.
Table 5 shows the performance (hits@1) comparison over different models and datasets.
The Vocab Model performs better on the WikiMovies-WE R2 dataset.
There are three different retrieval methods used in the experiment.
The retrieval performance increases as we go from the "Entity Matching Baseline (r0)" to "Entity Matching + Rule (r1)" to "Entity Matching + WLA (R2)".
The "Sum of hidden state" model has lower values for R@1, R@10, R@30, and R@100 compared to the "Word Level Attention" model.
The "Query Free Attention" model has higher values for P@1, P@10, P@30, and P@100 compared to the "Sum of hidden state" model.
The r1+AsV model performs better than the KV model in terms of Hits@1 score.
The table shows Hits@1 scores for different question types related to movies.
The ratio of the gate being open varies for different question types.
On average, the "r1+A" model has a higher ratio of the gate being open compared to the "r1+V" model.
The table compares the results of synthetic data generated with a TTS system using GST or i-vector embeddings against the oracle data on LibriSpeech-100.
The table includes results for experiments with and without spectral augmentation.
The WER for our work with i-vector embeddings is lower than the WER for our work with GST embeddings.
Retraining our work and using i-vector embeddings leads to a decrease in WER compared to not retraining and using no additional embeddings.
The table represents a suffix array constructed from a toy sentence.
The suffix array contains the word "the" at positions 6, 7, and 8.
The "Current work" uses C++ for implementation.
The "Lopez2007" implementation using cython takes more time compared to the other implementations.
The "Current work" uses C++ programming language.
The "Lopez2007" implementation using C++ requires 6.4 GB of memory.
Table 3.3 provides results for parallel extraction using 8 processes/threads.
The implementation using C++ in the current work has a memory usage of 6.1 GB.
Table 4.4 provides BLEU scores for common clustering strategies on fr→en data.
The "Class Factored" clustering strategy has a higher BLEU score compared to the "Brown clustering" strategy.
Table 4.6 compares stochastic gradient descent (SGD) and noise contrastive estimation (NCE) for class factored models on the fr→en data.
SGD has a higher perplexity value compared to NCE.
Table 4.8 compares class factored models with and without diagonal contexts trained with noise contrastive estimation on the fr→en data.
The perplexity for the "Diagonal" model is higher than the perplexity for the "Full" model.
The table shows the BLEU scores for the language pairs fr→en, en→cs, and en→de.
The table compares the BLEU scores for source-only and source + target conditioned neural language models.
The EaE model achieves a score of 45.8 in the Full setting of FreebaseQA.
The FaE (ours) model achieves a score of 39.2 in the Inject Facts setting of WebQuestionsSP.
When no additional text is added, the annotator agreement is 92.9% and the average confidence is 2.71.
When multiple languages are identified, the annotator agreement is 100.0% and the average confidence is 3.00.
The Mayo dataset has the highest correlation score for all embeddings except for Flair and SciBERT.
The Bio PPW subset has the highest subset size.
The dataset "same-as" has a size of 20,324.
The dataset "poss.-equiv.-to" has a positive rate of 33.92%.
The "same-as" category has the highest recall score.
The precision score is lower than the recall score in the "poss.-equiv.-to" category.
The best performing similarity metric for LTL win2 is "fJ/pair_τ" with a value of 8.6%/20.1%.
The average similarity score between DP and TP for GPT is 0.77/0.76.
The table shows the precision, recall, and F1-score on the RFP dataset for line classification and section classification.
The average precision, recall, and F1-score for line classification and section classification on the RFP dataset are 0.90, 0.91, and 0.91 respectively.
Our model achieves the highest F1-score of [BOLD] 86.9 among all the models compared in Table 1.
"Our model" is included in the comparison of models in Table 1.
The table shows the SLU performance (F1) with 5% training data on two categories of labels: seen and unseen.
The highest SLU performance (F1) with 5% training data is achieved in the HD category.
There are 10 different models used for entity alignment in Table 2.
The model with the highest Hits@1 score in the ZH-EN language pair is HGCN-JE.
The table provides a summary of the DBP15K datasets.
For the ZH-EN language pair, the DBP15K dataset contains 66,469 entities and 2,830 relations.
GCN-PR outperforms MTransE-R and BootEA-R in terms of Hits@1 performance for both ZH-EN and JA-EN relation alignment.
BootEA-PR performs better than GCN-PR in terms of Hits@10 performance for FR-EN relation alignment.
The alignment between G1 and G2 in terms of relation frequency is higher when both precision and joint alignment are considered compared to when only joint alignment is considered.
G2 has a higher occurrence frequency compared to G1 in the relation alignment.
The BERT-base model has higher accuracy than the RoBERTa-wwm-ext-large model for most categories in the CLUE diagnostics dataset.
The gold label for the "Anaphora" category is contradiction.
The "graph2vec" method has the highest ARI (as %) score among all the methods.
The "Deep WL kernel" method has a higher ARI (as %) score than the "WL kernel" method.
The AURC value for SogouRC using PROBE-CNN with P(T) is 14.60.
The percentage improvement for SQuAD using PROBE-CNN is 4.0%.
Table 5 provides comparison results on the answerable datasets.
The BERT model with PROBA achieves a ROC score of 5.24 on the SQuAD 1.0 dataset.
As the value of β increases, the accuracy of the long text generation task decreases.
As the value of β increases, the distinctiveness of the generated long texts also increases.
In all datasets, there are more similar pairs than dissimilar pairs.
The training dataset has more pairs than the validation and testing datasets.
The evaluation metrics (AC, Precision, Recall, F-measure) have the same value for the feature group [ITALIC] FT.
The feature group [ITALIC] FB has the highest F-measure value.
The F-measure is equal to the Precision and Recall for all feature groups.
The [ITALIC] FB feature group has a higher Accuracy than the other feature groups.
Table 4 represents an ablation study to explore the importance of different feature families.
The reduced feature set performs better in terms of error and accuracy compared to the other feature sets.
The diagonal cells in the table have a value of zero.
The value in cell C2 is 13.
Table 3 shows the stratified 3-fold cross-validation scores for different feature combinations.
The combination of numerical features achieves the highest stratified 3-fold cross-validation score.
The accuracy for the English language is higher than the accuracy for the Vietnamese and Tamil languages.
The Spearman correlation for the Vietnamese language is higher than the Spearman correlation for the Tamil language.
ELMo achieves the highest accuracy and F1 score among all the classification models.
The chance performance for all the classification models is the same.
The BiLSTM + MLP model outperforms the Concatenation + MLP and BoW + MLP models on the binary classification task.
ELMo outperforms w2v, Glove, and fastText word embeddings on the binary classification task.
The table shows the 10-fold cross-validation results on the gradient argumenthood prediction task for different models.
The ELMo model with dimension 1024 performs worse than the w2v-wiki model with dimension 1000 in terms of adjusted R2.
The baseline BLEU score for English→German is 31.87.
The BLEU score for German→English with HAN (k = +1) is 36.64.
Table 2 provides the percentage of different pronoun types.
The percentage of anaphora pronouns is higher in the Subtitles dataset compared to the Europarl and TED Talks datasets.
The baseline model is the "Baseline (Transformer)".
The model "+HAN(k = -2,-1,+1,+2)" achieves the highest BLEU score.
Table 4 provides the results of different methods on the test-set for machine translation.
The "Domain Sensitive + Ensemble" method achieves the highest scores on the test-set for both the En-Fr and Fr-En language pairs.
Table 3 shows the results of noisy data generation for En2Fr and Fr2En translations.
The models perform better in the "Domain Sensitive" setting compared to the "Domain Insensitive" setting.
Table 4 provides the results of the adversarial evaluation of Span-BERT on different tasks.
The stability of the NLI (ORG) F1 score is 89.9%.
The table presents the results of the adversarial evaluation of BERT on different tasks comparing the accuracy on the original model against an algorithm with a canonical dictionary of size (M) 1 or 3.
The original model achieves the highest F1 scores for NLI(ORG), GEC(PER), and CoRef(PER) tasks.
The method "Yang et al." achieves the highest validation accuracy among the four different methods.
The method "This work SVM-Poly" achieves the highest F1 score among the four different methods.
The performance metrics (accuracy, precision, recall, F1 score) improve as the number of features/feature-pairs increases.
The performance metrics (accuracy, precision, recall, F1 score) are higher for Test 5 compared to Test 1.
The table shows the ED and FED scores for simplifications of simple sentences for different datasets.
The ED scores for the simplification of simple sentences are generally lower than the ED scores for the simplification of complex sentences.
The domains with higher F1 scores for classification tend to have lower TER scores for explanation.
The domain "Psychology" has a high F1 score for classification but a relatively high TER score for explanation.
The BLEU score for translating from English to Spanish is higher than the BLEU score for translating from English to French.
The BLEU score for translating from German to Spanish is higher than the BLEU score for translating from German to English.
The table shows the ASR results in terms of WER on the test sets for different languages (De, En, Es, Fr).
The WER for translating from French to German is 27.3.
Method GAP outperforms Method DNB on all three tasks (RTE, MRPC, CoLA).
Method GAP and Method DNB have similar performance on the CoLA task.
The method that perturbates special tokens (ST) achieves a score of 75.0 on the RTE task.
The method that perturbates normal tokens (NT) achieves a score of 86.5 on the MRPC task.
The "Multimodal Routing" method outperforms the "GAM Hastie (2017)" method in both the CMU-MOSEI sentiment prediction and IEMOCAP emotion recognition tasks.
The "Interpretable Methods" perform better than the "Non-Interpretable Methods" in both the CMU-MOSEI sentiment prediction and IEMOCAP emotion recognition tasks.
The "GAM Hastie (2017)" model performs better than the "Multimodal Routing" model in terms of accuracy and F1 score.
The "Multimodal Routing" model performs better than the "Non-Interpretable Methods" model in terms of F1 score for the "CMU-MOSEI Emotion Happy" category.
The average number of words per sentence in the Unreliable News Data 2017 is 23.
The 99 percentile of the number of sentences per document in the Unreliable News Data 2017 is 84.
The table shows precision, recall, and F1 scores for different variations of the proposed model on four different datasets: NCBI-disease, BC5CDR, BC2GM, and JNLPBA.
The BiLM pretraining method achieves the highest F1 score for all four datasets: NCBI-disease, BC5CDR, BC2GM, and JNLPBA.
The BiLSTM model achieves the highest F1 score on the BC5CDR dataset.
The BiLSTM model achieves the highest Precision score on the BC2GM dataset.
The F1 measure for all articles in the test set is below 0.5.
The error rate for the "Religion" article is lower than the error rate for the "European Union" article.
The EER for GPLDA (XV3) on SRE19 Eval is 5.65%.
The CMin for NPLDA (XV3) on SRE19 Eval is 0.39.
Table 2 compares the performance of two different models - GPLDA and NPLDA.
The addition of SRE18 Eval dataset improves the performance of the GPLDA model.
The highest combined score in the table is 107.55, achieved by using the oracle belief state.
The highest BLEU score in the table is 18.9, achieved by using the oracle belief state.
The joint accuracy is highest when both the "End token" and "User/System token" are present.
The inform score is highest when both the "End token" and "User/System token" are present.
KNN has the highest precision macro value among all classifiers.
MLP has the highest F1 micro value among all classifiers.
Table 4 shows the F1 scores of the Positive and Negative class when trained on Positive and Negative Samples from Wikipedia under three different settings and tested on the same.
The F1 score for the Positive class when trained on both Back-translated Negative and Positive samples is 92.56.
The table compares the binary accuracy of different models: Random, LM (XLNet), Weak CDI, and CDI.
CDI performs better than LM (XLNet) and Weak CDI in terms of binary accuracy for both English and Chinese.
The "BERT + Small Training" model outperforms all other models in terms of MRR@10 percentage on both the development and evaluation sets.
The "BM25 (ours)" model performs better than the "BM25" model in terms of MRR@10 percentage on the development set.
Table 2 shows the average MRR and the number of queries for different ranges of FQT.
The MRR values decrease as the FQT range increases for the BERT model.
The MUR values increase as the cut-off values (i) increase.
The MUR values at cut-off values (i) 1, 2, and 3 are less than 1.
Table 6 provides MRR values for different answer types.
The MRR values for BERT are lower than the corresponding BM25 values for all answer types.
Table 9 provides information about the choice of λ's for different consistency and corresponding unlabeled datasets.
For the 1% dataset, the choice of λ's is consistent.
Table 2 shows inconsistencies (%) of models on the evaluation dataset.
The inconsistency (%) of the BERT w/ SNLI model on the evaluation dataset when trained with 100% of the train set and considering symmetry consistency violations (ρS) is 18.6.
The table shows the coverage (%) of unlabeled training sentences during the first epoch of training for two different types of data.
The coverage (%) of unlabeled training sentences decreases as the percentage of "M" increases.
The accuracy for the SNLI dataset is consistently higher than the accuracy for the MultiNLI dataset.
As the amount of training data increases, the symmetry/transitivity inconsistencies decrease.
The "Graph LSTM - FULL" model has the highest accuracy among all the models in both Single-Sent. and Cross-Sent. categories.
The "Feature-Based" model has a lower accuracy compared to the other models.
The Tree LSTM model has the highest accuracy among all the models in both the Single-Sent. and Cross-Sent. tasks.
The Graph LSTM-FULL model has a higher accuracy than the Graph LSTM-EMBED model in the Cross-Sent. task.
The multi-task learning approach improves the accuracy of the BiLSTM model on the Drug-Gene-Mut. dataset.
The multi-task learning approach improves the accuracy of the Graph LSTM model on the Drug-Mut. dataset.
The number of candidates for Single-Sent. is 10,873 and for Cross-Sent. is 57,033.
The number of unique drug-gene-mutation interactions extracted from PubMed Central articles using GDKD + CIVIC is 59.
The "Graph LSTM (GOLD)" model has the highest F1 score among all the models.
The "Graph LSTM (GOLD)" model has the highest precision score among all the models.
Table 3 provides homogeneity results for DI-VAE and DI-VST in terms of SW Act, Topic, DD Act, and Emotion.
The homogeneity result for DI-VAE in terms of SW Act is 0.48.
The CoNLL'05 dataset is labeled for Semantic Role Labeling (SRL).
The MPQA (4-CV) dataset has a training size of 3141.25.
The "ER-AE (ours)" model performs the best in terms of the "Yelp (100-author) USE" and "Conferences' Dataset USE" metrics.
The "AE-DP" model performs the worst in terms of the "Yelp (100-author) Authorship" and "Conferences' Dataset Stylometric" metrics.
The F1 scores for the "Oracle abstract" row are higher than the F1 scores for the "Open" row in the Abstract-level Label-Only results.
The precision scores for the "Oracle rationale" row are higher than the precision scores for the "Zero-shot" row in the Abstract-level Label+Rationale results.
The Rational-Select. F1 score is higher for the combination of Fever + SciFact compared to Fever alone.
The F1 score is higher for the Abstract-only model compared to the Claim-only model.
The F1 score is higher in the "Abstract-level Label-Only" column compared to the "Sentence-level Selection-Only" column.
The precision, recall, and F1 score are higher in the "Oracle abstract" row compared to the "Open" column.
The table shows sentence-level results.
The F1 score for the "Oracle abstract" task using the "Selection-Only" model is 43.82.
The F1 score for the "Open" row is consistently lower than the F1 score for the "Oracle abstract" row in the Abstract-level Label-Only setting.
The precision for the "Oracle rationale" row is consistently higher than the precision for the "Zero-shot" row in the Abstract-level Label+Rationale setting.
The F1 scores for the "Sentence-level Selection-Only" column are higher than the F1 scores for the "Sentence-level Selection+Label" column.
The "Open" category has lower F1 scores than the "Oracle abstract" and "Oracle rationale" categories.
The number of extracted causal tuples from the Annotated Gigaword corpus is higher than the number of extracted causal tuples from the Simple English Wikipedia corpus.
The total number of extracted causal tuples is equal to the sum of the number of extracted causal tuples from the Annotated Gigaword and Simple English Wikipedia corpora.
DeFINE has a lower perplexity value than HGT.
DeFINE has a lower perplexity value than HGT.
The perplexity of the LSTM-based language model is lower on the test set than on the validation set.
The parameter distribution of DeFINE is 0.41 million and the training time is 298 ms/batch.
AWD-LSTM + Finetune performs better than AWD-LSTM (merity2018regularizing) on the test set.
AWD-LSTM-MoS + Finetune has the lowest number of parameters among all the models.
Table 1 shows the performance of different methods for essential terms classification.
NCRF++ and CompleX have the highest accuracy among all the methods.
The LoRRA model outperforms the VQA SoTA (Pythia, BAN) and other baselines.
The accuracy for the "Human" model is 86.79%.
The model "Pythia v0.3 + LoRRA (Ours)" achieves the highest accuracy on the VQA 2.0 test-dev dataset.
The model "Pythia v0.3 (Ours)" achieves an accuracy of 54.72 on the VizWiz dataset.
The accuracy of BAN (I+Q) on the validation set is 12.30%.
The accuracy of Pythia (I+Q) on the test set is 14.0%.
The "Holistic verb phrase vectors" model has the highest Spearman’s ρ score.
The disambiguated version of the linear regression model has a higher Spearman’s ρ score compared to the ambiguous version.
Table 2 provides results for a supervised task with different models.
The condition logit values increase as the training size increases.
The "true/false" condition has a higher condition logit value compared to the "entailment/contradiction" condition.
The F1 score is higher for the "Adaptive" category compared to the "Refining" category in the RumourEval -All part of GAIN.
In the PHEME -Refining part of GAIN, the precision, recall, and F1 score are higher for the "Adaptive" category compared to the "Conflicting" category.
The table compares the performance of different models on the RumourEval and PHEME datasets.
Our proposed model outperforms the baselines on all evaluation measures for both the RumourEval and PHEME datasets.
The performance of the models improves as more components are added to the architecture.
The Glove+BiLSTM+SFSN+GAIN model achieves the highest F1 score among all the models.
The F1 score for RumourEval using Our Model is 82.19.
The precision percentage for PHEME using SFSN-comment-emotion is 78.68%.
The percentage of blank labels is the highest among all punctuation classes.
The count of question mark labels is the lowest among all punctuation classes.
The F1 score for the CNN+T model with a "?" class label is higher than the F1 score for the BLSTM+T model with a "?" class label.
The precision for the BLSTM model with a "," class label is higher than the precision for the CNN model with a "," class label.
Human performance is higher than all other methods in all categories.
The "Ours (18K prog.)" method has the highest accuracy for all four conditions.
The accuracy of the "Ours (18K prog.)" method is higher than the accuracy of the "CNN+LSTM" method for the "Finetune B A" condition.
The "CharNN" model does not have a value for the "Char. embedding dimensions" parameter.
The "CharWNN" model uses both word embeddings and character embeddings.
The "CharWNN" model has the highest F1 score on the test set.
The "CharWNN" model uses both word embeddings and character embeddings as features.
The "WNN" model with word embeddings, suffix, and capitalization features has the highest F1 score in the Total Scenario.
The Naive Bayes classifier has a score of 0.657 on the test set.
The RNN classifier has a score of 0.666 on 2-grams.
The table provides the average SNR values for different speech enhancement models.
The average SNR value for the combination of all sources is lower than the average SNR values for the individual sources.
The "Natural" source has the highest quality score among all the sources.
The "Copy-synthesis" source has the highest similarity score among all the sources.
UDPipe 1.2 performs better than Stanford in terms of tokenization.
TRL performs better than FBAML in terms of part-of-speech tagging.
The "BERT Language Model" has the highest F1 score among all the models.
The "Majority Class" model has a Precision of 45.0% and a Recall of 50.0%.
"Country" has the lowest F1 score among all the genres listed in the table.
"Country" has the lowest F1 score among all the genres listed in the table.
The RoBERT model outperforms the Most frequent and Average models on the CSAT dataset.
The ToBERT model outperforms the Most frequent and Average models on the 20newsgroups dataset.
The sentiment "Positive" has the highest Pearson correlation A and Precision@5 A values.
The sentiment "Neutral" has a higher Pearson correlation B value than the sentiment "Negative".
The proposed model achieves the highest performance in terms of accuracy, Cohen-Kappa, macro precision, macro recall, and macro F1-score compared to the baseline, LSTM, and CNN-LSTM models.
The baseline model achieves the lowest performance in terms of accuracy, Cohen-Kappa, macro precision, macro recall, and macro F1-score compared to the LSTM, CNN-LSTM, and proposed models.
The "+ syn-zh" setting improves the BLEU and METEOR scores for both zh→vi and vi→zh translations compared to the baseline and synthetic settings.
The "100k + syn-vi" setting achieves the highest METEOR score for the vi→zh translation.
The table presents the results of character-based SMT for different settings of synthetic data and different amounts of data.
The highest METEOR score is achieved when using 100k synthetic Vietnamese data.
The "vi→zh" translation direction has lower BLEU scores compared to the "zh→vi" translation direction.
Increasing the amount of synthetic Vietnamese data improves the METEOR scores for the "vi→zh" translation direction.
The "+ syn-zh" setting improves the BLEU scores for both zh→vi and vi→zh translations compared to the "Synthetic" setting.
Increasing the training data size from 60k to 100k improves the BLEU score for vi→zh translation.
The baseline model has an overall performance of 0.776 ± 0.020.
The "+ Bernoulli Adv. Noise" method shows consistently better or equal performances compared with the original model.
RF has the lowest test accuracy on the Facebook Mall dataset.
The top three algorithms based on the AdaBoost scores are SVM, LR, and NB.
The accuracy scores for SVM, LR, and NB on the Facebook Mall test set are 69.1, 69.8, and 65.7, respectively.
The Our self-supervised method achieves the highest F1 scores on both the Full Dev and Full Test datasets.
The Transition-based method achieves a Precision (P) score of 82.2% on the 1000 sents Dev dataset.
The "Our self-supervised (Full)" method achieves the highest F1 score of [BOLD] 90.2.
The "LSTM-NCM" method does not have precision (P) and recall (R) values reported.
The Multi-Task method outperforms all other methods in terms of Full P, Full R, Full F1, 1000 sents P, 1000 sents R, and 1000 sents F1.
The Multi-Task method performs better than the Tagging method and the Classification method in terms of the 1000 sents F1 metric.
The F1-score for "Transition-based" is higher than the F1-score for "Transformer-based".
The F1-score for "Our self-supervised" is higher than the F1-score for both "Transition-based" and "Transformer-based" in the "Either" category.
The "Combine" method achieves the highest F1 scores in both the "F1 (Full)" and "F1 (1000 sents)" columns.
The "BERT-fine-tune" method achieves a higher F1 score than the "Random-Initial" method in the "F1 (Full)" column.
The models used for post-processing in the sequence discriminative training systems are HMM, (LF-bMMI), [EMPTY], and CTC.
The EER values for the post-processing models in the sequence discriminative training systems are 9.8, 2.9, 11.4, and 3.2.
The table provides the performances of the Sub-word Level CTC with or without the wb unit.
The EER is lower for the "long" keyword length compared to the "short" keyword length.
Ensembling via averaging and stack generalization does not have a significant impact on the performance of the "char+char3" model on the Czech dataset.
Table 6 reports the effect of layer size on model performances for different languages.
The "Normal" combining method achieves the highest accuracy for the Wiki65K dataset.
The "Attention" combining method achieves the highest recall for the AAN104K dataset.
SMITH-WP+SP outperforms SMASH on both Wiki65K and AAN104K datasets in terms of accuracy, precision, recall, and F1 score.
SMITH-Short achieves the highest performance in terms of accuracy, precision, recall, and F1 score among all the compared models on both Wiki65K and AAN104K datasets.
The table presents the document matching performance on two datasets: Wiki65K and AAN104K.
Increasing the number of layers in the sentence level Transformers generally improves the accuracy of document matching.
The table compares the performance of four different models for a foreign place name recognition task and Google general English traffic.
The use of biasing units improves the performance of the models in recognizing foreign place names and Google general English traffic.
The table compares the performance of ConvE and CDC on multiple datasets including WN18, FB15k, YAGO3-10, and FB15k-237.
ConvE outperforms CDC in terms of mean rank (MR) on the WN18 dataset.
Table 6 provides the full task performance of localization models trained on human and random trajectories.
The performance of the localization models decreases as the beam size increases.
The performance of the model trained on human trajectories is 38.21.
The performance of the model trained on random trajectories is 50.00.
The table shows the localization performance given different numbers of dialogue utterances.
MASC outperforms no-MASC in all cases in terms of localization performance.
"ResNet (256 dim)" has the highest "Valid F1" score.
"Textrecog" has a higher precision than "Fasttext (100 dim)".
The total number of success in the Talk The Walk dataset is 3124.
The word frequency of "Before" in the TACoS dataset increased from 50 to 62.
The word frequency of "Before" in the TEMPO - HL dataset decreased from 6610 to 5495, and then further decreased to 5478.
MLLC (SS + conTEF) consistently outperforms other models in terms of R@1 score.
MCN and MLLC (Global) perform better than other models on the TEMPO - TL Before task.
Table 7 compares different methods to localize context fragments using the MLLC model and considering the latent context when localizing the whole query.
The MLLC model with the latent context considered when localizing the whole query improves the performance in terms of R@1 and mIoU compared to localizing fragments without considering the latent context.
The table displays the number of utterances in each dataset.
The total number of utterances in the TRN dataset is 9,934, in the DEV dataset is 1,344, and in the TST dataset is 1,328.
The F1 score for the combination of text and condition (c2) is consistently higher than the F1 score for the combination of audio and condition (c2).
The LSTM model with batch normalization (BN) consistently outperforms the LSTM model without batch normalization (BN) in all combinations.
The table shows the test F1 score of the models trained on MSR-Skype and tested on variable-length sequences.
The F1 score for the combination condition on short sequences is 78.3.
The model "DeepDN (1layer)" has the highest test perplexity per word for the dataset "20 News".
The model "Replicated Softmax" performs better than the model "fDARN" on the dataset "RCV1-v2" in terms of test perplexity per word.
The word BLEU score of the w-system improves from T1 to T4.
The morpheme m-BLEU score of the m-system is highest for T4.
The table compares the BLEU scores on the T1 dataset and the full training dataset for two different primary phrase tables: PTw→m and PTm.
The BLEU score is higher for PTw→m compared to PTm on both the T1 (40K) and Full (714K) datasets.
Table 5 shows the results of trying different values for the interpolate parameter on the dev dataset.
As the value of α increases from 0.3 to 0.6, the BLEU score increases, but then slightly decreases at α = 0.7.
Table 6 shows the merging methods of m+phr+lm and w-system on the test dataset, with BLEU scores for the full training dataset.
The BLEU score for "ourMethod" in the merging methods is 14.82+0.74.
The BLEU-4 score for the IU X-Ray dataset using the Ours-CoAttention method is [BOLD] 0.247.
The BLEU-3 score for the IU X-Ray dataset using the Ours-Visual-only method is 0.297.
Table 5 shows the F1 score (%) of the BERT-Large model with different training/prediction stride sizes on the CoQA and QuAC datasets.
The F1 score for the BERT-Large model improves when the training stride size is set to 64 and the prediction stride size is set to 61.4 on the CoQA dataset.
The maximum sequence length for CoQA is 512, while for QuAC it is 384.
The F1 scores for BERT-RCM are not provided in the table.
The F1 score decreases as the number of tokens in the document increases for BERT-Large.
The majority of documents in the dataset have less than or equal to 300 tokens.
Table 4 provides the F1 score (%) of different algorithms on the TriviaQA dataset.
The F1 score of the BERT-Large algorithm on the TriviaQA dataset is 61.3.
The table describes the AUC scores for distinguishing antonyms from synonyms in ViSim-400.
The dLCE model achieves the highest AUC score for distinguishing antonyms from synonyms in the Verb category in ViSim-400.
The inter-annotator agreement measured by Spearman's ρ for all word classes is 0.86.
The "weightSA + SVD" method performs better than the "PLMI + SVD" method for all categories (ADJ, NOUN, VERB).
The "weightSA" method performs better than the "PLMI" method for the category of ADJ.
The proposed method "UMGR" outperforms other methods in terms of "Offline Evaluation Act Acc.", "Offline Evaluation Act F1", and "Offline Evaluation EMR".
The proposed method "UMGR" achieves the highest success rate in the online evaluation.
Table V provides the average precision (AP) gains for temporal and redundancy factors over a random baseline.
The average precision (AP) gain for the temporal factor feature 2 in the Director domain is 85.37.
The factor increase for each category after extension using Wordnet synsets is close to 2.
The number of questions for each category is different.
The table compares the phoneme error rate (%) of three different methods on different amounts of paired data.
The phoneme error rate (%) decreases as the amount of paired data decreases.
The "lights clean" category has lower word error rates compared to the "lights noisy" category for both the "5x96 (quantized)" and "TDNN-LSTM" acoustic models.
The "washing clean" category has lower word error rates compared to the "washing noisy" category for both the "5x96 (quantized)" and "TDNN-LSTM" acoustic models.
The "Concept pointer" method has the highest scores for both abstraction and overall quality.
The "Pointer generator" method has a higher score for overall quality compared to the "seq2seq+att" method.
"Concept pointer+RL" achieves the highest ROUGE F1 score on Gigaword RG-1.
"Concept pointer+DS" achieves a higher ROUGE recall score on DUC-2004 test set than "Concept pointer+RL".
Table 3 provides information about the percentage of novel n-grams on the Gigaword dataset.
The percentage of novel 3-grams for the "Concept pointer" model is 68.5%.
TP-Full + labeler, delta encoding achieves the highest UAS, LAS, and F1 scores among all the dependency parsers in the table.
TP-Full + labeler, H&N encoding achieves the lowest UAS score among all the dependency parsers in the table.
The "Carreras2008" parser achieves the highest F1 score among all the parsers in Table 2.
The F1 score of the "This work" parser is higher than the F1 scores of both the "Charniak2000" and "Charniak2005∗" parsers.
The BERT model outperforms all other models in terms of accuracy on both the Wisesight and Wongnai datasets.
The BERT model performs better than the ULMFiT model in terms of accuracy on the Wisesight public dataset.
The table shows the training loss and perplexity after pretraining for ULMFiT, GPT, BERT MLM, and biLM(ELMo) (Character-Level).
The biLM(ELMo) (Character-Level) model has the lowest perplexity among the listed models.
The model "BERT seqmax=128 [thaikeras]" has an accuracy of 0.56612 on the "Wongnai (Acc.) Private" dataset.
The baseline model has an accuracy of 0.4785 on both the "Wisesight (Acc.) Public" and "Wisesight (Acc.) Private" datasets.
Table 6 provides the complexity analysis of different models based on the amount of training data used and their performance on the ACE 2005 development set.
The performance of the C-GCN model decreases as the percentage of training data decreases.
Table 5 presents the performance of different systems on the ACE 2005 development set.
The DRPC system achieves the highest precision, recall, and F1 scores on the ACE 2005 development set.
The total counts for z1 and z2 are 4,459 and 6,287 respectively.
The count for z1 is 1,668 and the count for z2 is 2,860 for the word "trump".
Table 6 shows the BLEU scores of different methods for the Chinese-to-English task.
The "+Adv" method achieves the highest BLEU score of 16.99 for the "Speech" domain.
The Encoder method performs better than other methods in the English-to-German translation task.
The MTL method performs better than other embedding-based methods in the English-to-German translation task.
The "Our Domain Mixing Methods" outperform the "Embedding based Methods" in both the TED and Medical domains.
The combination of the "Encoder" method with "WL" achieves the highest performance in both the TED and Medical domains.
The Direct Training method performs best on the Laws dataset.
The Our Domain Mixing Methods perform best on the Encoder + WL dataset.
The table presents the BLEU scores for different language translation tasks using three different methods: Direct Training, w/o DL, and with DL.
The BLEU score for the English-to-Germany translation task using the "with DL (Ours)" method is [BOLD] 27.78.
The UNET architecture has the lowest test cross entropy and the highest BLEU score among the different baselines in the domain of machine translation.
The TRANSFORMER architecture has a higher BLEU score than the S2SA architecture in the domain of machine translation.
The UNET architecture performs the best among all the baselines in terms of validation cross entropy and test perplexity.
The UNET architecture achieves the lowest test perplexity among all the baselines.
The UNET architecture has the lowest test perplexity among the different architectures.
The ablation study on the U-Net architecture was conducted on the Cornell Movie Dialogues corpus.
The "C&W" word embedding method achieves the highest test score among all the word embedding methods.
The "SCODE" word embedding method achieves a development score of 0.9430.
Table 8 shows the NER results for different word embeddings.
The baseline word embedding performs the worst in all three categories.
The "HLBL" word embedding method achieves the highest test score in the table.
The accuracy of the full model in the test results is 58.4.
The proportion of instructions followed for the gold plan is 63.4%.
Table 2 compares the performance of data ordering patterns based on sorted perplexity and BLEU score from two different pre-trained models.
Both the Small Model and the Base Model required 31 epochs for convergence in terms of Desc PPL.
The table shows the effect of different data ordering patterns on the performance on the IWSLT Vietnamese test set.
Table 2 shows the results on a character level Language Modelling task, reporting prediction quality in bits-per-character and sparsity levels for Wx, Wh, and Wy.
The sparsity levels for Wx, Wh, and Wy in the "SparseVD (random)" method are reported as 79.7%, 88.0%, and 71.7% respectively.
The "SparseVD (VBD)" method has the lowest test MSE among all the methods.
The "SparseVD (no dropout)" method has the highest sparsity level for both Wx and Wh among all the methods.
The table shows the performances with different model strides on the WSJ dataset.
The WER 2 value for the "CTC, uni-gram" model stride is 16.91 and the WER 4 value is 23.76.
The Gram-CTC, auto-refinement method has the lowest loss value among all the gram selection methods.
The Gram-CTC, auto-refinement method has the lowest word error rate (WER) among all the gram selection methods.
Table 2 shows the F1-score (micro) of models using CNN, LSTM, and GCN, with and without additional training signal.
The F1-score (micro) of the models with additional training signal is higher than the F1-score (micro) of the models without additional training signal for both 5+1-way 5-shot and 10+1-way 10-shot scenarios.
Table 3 presents the results of an ablation study on the F1-score of Prototypical-based models.
The "+Intra+Inter" modification consistently improves the F1-scores of the Prototypical-based models.
The "TLM-I+E (G,M)" model has the highest ROUGE 1 score.
The "Oracle" models are of type "Oracle".
The "Our Models" row generally achieves higher ROUGE 3 scores compared to the "Previous Work" row, except for the "Discourse" model.
The "TLM-I+E (G,M)" model achieves the highest ROUGE 1, ROUGE 2, and ROUGE L scores among all models.
The models in the table can be categorized into three types: extractive, abstractive, and mixed.
The Oracle model outperforms all other models in terms of ROUGE scores.
The stability of the learning-based methods is positively correlated with the size of |Ntrain|.
The accuracy of the learning-based methods is higher when using Ref+PM compared to Ref.
Table 7 shows the RSP performance on BioChemDev.
The "BioChemDev correct constituents %" for the combination of WsjTrain + BioChemTrain is 46.7%.
The average F1 score for the MSP + Stanford POS tags is 86.06.
The F1 score for the RSP on biographies is 86.14.
The performance of Cont-MemN2N Task 8: Triple is better than vanilla-MemN2N Task 8: Triple in both TestQA and TestAQ.
The performance of TrainQA is better than TrainAQ in both vanilla-MemN2N Task 4: K. Verification and Cont-MemN2N Task 4: K. Verification.
The performance of Cont-MemN2N Task 8: Triple is better than vanilla-MemN2N Task 8: Triple in both TestQA and TestAQ.
The performance of TrainQA is better than TrainAQ in both vanilla-MemN2N Task 4: K. Verification and Cont-MemN2N Task 4: K. Verification.
The perplexity for the Speaker Model is lower than the perplexity for the Standard LSTM model.
The perplexity for the Speaker Model is 42.2, which is a 10.6% decrease compared to the Standard LSTM model.
The table presents the results of RL gains over the mutual information system based on pairwise human judgments in three different settings: RL-win, RL-lose, and Tie.
In terms of multi-turn general quality, RL-win setting outperforms both RL-lose and Tie settings.
Table 6.2 shows the AdverSuc and machine-vs-random scores achieved by different training/decoding strategies.
The AdverSuc score achieved by the MMI+ p(t|s) training/decoding strategy is higher than the score achieved by the Sampling strategy.
Table 6.3 presents the gain from the proposed adversarial model over the mutual information system based on pairwise human judgments.
The adversarial model outperforms the mutual information system in both the single-turn and multi-turn settings.
Asking Questions (AQ) performs better than only answering questions without asking (QA) in the Mechanical Turk Task Results.
The performance of answering questions without asking (QA) is lower than the performance of asking questions (AQ) in the Mechanical Turk Task Results.
The table shows the performance of the models on both the TestQA and TestAQ tasks.
The table presents the results of Mechanical Turk Task using real data for training and testing.
The performance of Cont-MemN2N Task 8: Triple is better than the performance of vanilla-MemN2N Task 8: Triple.
The combination of Forward Prediction and Reward Based Imitation (RBI+FP) performs the best among all models.
Increasing the reward fraction improves the performance of all models.
XGBoost has the highest Micro F-score Test and Macro F-score Test among all classifiers.
SVM-L has the highest Micro F-score Train among all classifiers.
The total number of sentences in the training dataset is 7923.
The number of negative sentences in the test dataset is 515.
Table 2 shows the Spearman's rank correlation performance for the Word Similarity task on SCWS.
The MSSG model achieves the highest performance with dimension 600.
Table 3 compares the performance of different models on LS-SE07 and LS-CIC sets.
The HTLE model with expansion achieves a GAP score of [BOLD] 43.4▲ on LS-SE07 when the dimension is set to 600.
Table 4 provides GAP scores on the candidate ranking task on LS-SE07 for different part-of-speech categories.
The model HTLE has the highest scores for all dimensions and part-of-speech categories.
The F1 score is higher for the "Dev set + H8" combination compared to the "Test set + H8" combination.
The data size is larger for the "Dev set + H10" combination compared to the "Test set + Beam10 + H8" combination.
As the value of ϵ increases, the data size decreases.
As the value of ϵ increases, the EM and F1 scores increase.
Table 6 compares the performance of different methods for semi-supervised QA on the full development set of SQuAD.
The "+H8" method uses 705k new data for semi-supervised QA.
Suppressing the baselines does not affect the accuracy of the model.
The performance of frozen BERT is consistently lower than unfrozen BERT across all tasks.
The performance of all tasks decreases when the data regime is limited compared to the full data regime.
Table 4 shows the fine-tuning results for classification/regression tasks.
The accuracy for MNLI in the full data regime is 40.9.
Table 6 presents the fine-tuning results for sequence labeling tasks.
The models perform better in the "Data regime full" condition compared to the "Data regime limited" condition.
The highest score for MNLI is in the SNLI column.
The table includes the results of out-of-class transfer from question answering tasks to classification/regression tasks.
The baseline performance scores for the tasks range from 32.4 to 94.9.
The table shows the exact match on the development set of all small datasets as the number of examples taken from the five large datasets increases in the zero-shot setup.
The exact match on the ComplexQuestions dataset increases as we increase the number of examples taken from the five large datasets.
The table shows the test accuracy (%) on COCOQA for different methods, including MLP-A, MLP-IA, MLP-QA, MLP-IQA, and Random.
MLP-IQA achieves the highest test accuracy (%) on COCOQA among all the mentioned methods.
As more information is used, the accuracy of the machine in selecting the right answers on the Visual QA task increases.
The accuracy of the machine is highest when I + Q + A information is used in selecting the right answers on the Visual QA task.
Table 6 shows the results of using models trained on qaVG to improve Visual7W and VQA.
The models trained on qaVG achieve higher accuracy than the models trained without using qaVG.
The table shows the test accuracy (%) on VQA2-2017val for different methods.
The table displays the different methods used for VQA2-2017val.
The table shows the test accuracy (%) on VQA2−-2017val for different methods.
MLP-IQA has a higher IoU value compared to MLP-A, MLP-IA, MLP-QA, and Random.
Table 11 compares the test accuracy (%) on Visual7W using different embeddings for questions and answers.
The classification accuracy of BERT on SST-1 is 77.6.
The classification accuracy of CNN on SST-2 is 92.8.
The table provides the performance of different models on the testset of FB15k-237 and WN18RR datasets.
The model "DeCom" performs better than the model "RESCAL" in terms of MRR on the FB15k-237 dataset.
The model "RESCAL" has an MRR of 0.255 and an H@1 of 0.185 on the FB15k-237 dataset.
The model "+ F-DeCom-Rel" has a dimension of 200 for entity features before the decompressing layer and a dimension of 2002 for entity features after the decompressing layer.
Table 3 shows the performance of different models with and without decompressing on the test set of the WN18RR dataset.
The MRR (Mean Reciprocal Rank) for the vanilla expansion model is 0.436.
The MRR value for RESCAL with an embedding size of 100 is 0.258.
The H@10 value for DistMult + C-DeCom is 0.484.
The table compares the robustness of DeCom and original models on the FB15k-237 validation set.
The DeCom models (C-DeCom and F-DeCom) outperform the original models (DistMult and ComplEx) in terms of MRR and H@10 on the FB15k-237 validation set.
Table 2 presents the inter-annotator agreement for entities and relations.
The inter-annotator agreement for entities is higher than the inter-annotator agreement for relations.
Table 7 presents the performance of a dependency pattern based RE model on the presented dataset.
The "DARE (Gold NE)" model has a higher precision, recall, and F1 score compared to the "DARE (CRF NE)" model.
The method "TP-Ensemble" has the highest DA score of 61.19.
The method "TP-Sampling" has the highest r value of 0.533.
The BiLSTM+ATT model has the highest average performance among all models.
Table 5 presents the results of crisis tweet classification for different events.
Multi-Task models outperform Single-Task models in terms of F1 score for both Priority and Factoid tasks in crisis tweet classification.
The Multi-Task (MTL) approach improves the performance (accuracy and F1 score) compared to the Single Task approach for all target events.
Adding additional Amazon data improves the performance (accuracy and F1 score) compared to the Multi-Task (MTL) approach.
The table provides information about the number of English OOV words replaced by their synonyms in the dataset.
The number of words in the training data is 1015, while the number of words in the development data (dev2010) is 36 and the number of words in the test data (tst2010) is 25.
Table 2 displays the number of Japanese OOV words that have been replaced by their synonyms.
The SAA algorithm detached affixes from 84 words in the tst2012 dataset and 93 words in the tst2013 dataset.
The accuracy of the EmbeddingAll model in sentiment classification is 0.765.
The F1 score of the DSE c model in sentiment classification is 0.781.
The lexicon term sentiment classification results show that the SSWE method performs better on the B & D and B & E datasets in terms of high-level sentiment classification than on the B & K and D & E datasets.
The lexicon term sentiment classification results show that the EmbeddingQ method performs worse on the B & K dataset than on the D & E dataset in terms of both high-level and MPQA sentiment classification.
Table 8 shows the performance of two annotators (A1, A2) and an automatic system (Sys) on the fake news datasets.
The automatic system (Sys) performs better than both annotator A1 and annotator A2 on the fake news datasets.
The readability accuracy for legitimate articles in the Technology domain is 0.90.
The readability accuracy for legitimate articles in the Business domain is 0.53.
The agreement among two human annotators on the Celebrity dataset is 73%.
The Kappa coefficient for agreement among two human annotators on the FakeNewsAMT dataset is 0.38.
Table 6 provides the entropy of the latent/hidden distribution for sMIM and AE models on different datasets.
The entropy of the latent/hidden distribution for sMIM (S) is 11.54, for N(S) is 22.7, and for AE (S) is 35.95 on the PTB dataset.
The difference between the similarity scores of the top 10 and less similar documents for the "idf_max" representation in the "Clustering" with "#Cluster 100" is 0.1714.
The similarity score of the top 10 documents for the "PMC" dataset using the "Top_concept, k=200" representation in the "Baselines" is 0.9861.
As more features are added to the model, the Spearman's rho and Kendall's tau values increase.
The model with "+ Topic & Sentiment Features" has the lowest MAE value.
The performance of the models improves as we move down the table.
There is a negative correlation between the BLEU score and the AER score.
The end-to-end NMT systems developed by the authors of the paper are represented in the "this work" row.
Table 5 provides word level perplexity scores and model sizes for different model configurations.
The model configuration that includes both Bayesian dropout and weight tying has a size of 51M and achieves a train perplexity of 28.2, val perplexity of 75.8, and test perplexity of 73.2.
The LSTM model with LC (Tweet + Article) + 2,048-dim image vectors input has the lowest MSE.
The LSTM model generally has lower MSE values compared to the CNN model.
There are three datasets provided: 2k Labelled, 20k Labelled, and Unlabelled.
The number of posts in the 2k Labelled dataset is 2,495, in the 20k Labelled dataset is 19,538, and in the Unlabelled dataset is 80,012.
The CNN model trained on tweets only has the lowest MSE compared to other input combinations and number of epochs.
The LSTM model trained on tweets and LC (tweet+article) has a lower RMSE compared to the LSTM model trained on tweets only.
MixText achieves the highest accuracy among all the models.
The accuracy of the models decreases as the amount of unlabeled data decreases.
Table 2 provides a performance comparison of different models on two datasets, AG News and DBpedia, with varying amounts of labeled data per class.
The MixText model achieves higher test accuracy on AG News compared to the other models.
The numbers 1-10 correspond to the KFTT-JE+ALT-JE dataset, numbers 11-20 correspond to the IWSLT-JE+ALT-JE dataset, and numbers 21-30 correspond to the IWSLT-CE+ALT-JE dataset.
The proposed methods achieve significantly better results than the existing black-box methods for the ALT-JE dataset.
Vanilla NMT achieves the highest BLEU-4 score for ALT-EJ in the KFTT-EJ dataset.
The system with the number 8 achieves the highest BLEU-4 score for ALT-EJ using IWSLT-EJ, IWSLT-EC, and KFTT-EJ datasets.
The table shows the out-of-domain results (BLEU-4 scores) after domain adaptation with one out-of-domain corpus for ALT-JE using KFTT-JE, IWSLT-JE, and IWSLT-CE.
The domain adaptation method "domextr+MFT" achieves a BLEU-4 score of [BOLD] 27.98†‡ for ALT-JE using KFTT-JE.
The "domspec" method achieves the highest BLEU-4 score of 17.42 for ALT-JE using KFTT-JE.
The "domextr+MFT" method achieves the highest BLEU-4 score of 18.44 for ALT-JE using KFTT-JE, IWSLT-JE, and IWSLT-CE.
The train set contains 960.000 instances and the test set contains 480.000 instances.
The train set contains 1.923 instances of the "disgust" emotion.
The total number of instances for the "negation" modifier is 638.
The number of instances used for training with the "amplifier" modifier is 497.
The train accuracy without fine-tuning is 40.19 for our model trained on the train dataset.
The validation accuracy for our model trained on the train+val dataset is 49.88, which is the highest accuracy achieved in the table.
Table 5 provides the performance of two QA only models on the TVQA dataset.
The TVQA baseline model with Random weights has a higher validation accuracy (39.61) compared to the WikiWord embedding model (32.76).
The table compares the performance of different QA only models on different splits of the MovieQA dataset.
Table 4 shows the experimental results for the NIST condition, including different language pairs and different features.
The scores for mixed-case translations are consistently lower than the scores for AR-EN and ZH-EN translations.
The table presents the MT results of various model combinations in BLEU and TER.
The table presents experimental results to investigate the effects of new features, DTN, and MTL on BOLT and NIST datasets.
Adding HypEn and SrcEn features to the baseline (R1) improves the performance on both AR-EN and ZH-EN language pairs.
The performance of the AoA Reader improves with the addition of each feature.
The AoA Reader performs better than the +Global LM, +Local LM, and +Word-class LM on the CBTest NE Valid dataset.
The embedding dimension for the CNN News task is 384.
The hidden layer dimension for the CBTest CN task is 256.
The method "RE-Net w. GT (s,r)" outperforms all other methods in ICEWS18 dataset.
The methods "RE-Net (mean pool)", "RE-Net", and "RE-Net w. GT (s,r)" perform better than the methods in the "Static" row in ICEWS18 dataset.
The class "Helpful Rejection" has the highest number of instances among all the classes.
The class "Sluice" has the lowest number of instances among all the classes.
The total count in the "Total" column represents the sum of all class counts in the corpus.
The classes in the "NSU class" column are arranged in descending order based on their counts.
The precision for the "FactMod" class in the final approach is 1.00.
The recall for the "HelpReject" class in the baseline is 0.14.
Table 2 presents the perplexity results for different variants of the attentional model on the BTEC zh→en dataset.
The model variant with "+align+sym+glofer-pre" has the highest perplexity score on the test set.
Table 2 shows the classification accuracy of different methods on the CUAVE database.
The Visemic AAM + HMM method achieves the highest classification accuracy on the CUAVE database.
The "End-to-End (Raw + Diff Images, Fig. 1)" method has the highest accuracy among all the methods in the table.
The "End-to-End (Raw Image)" method has a higher accuracy than the "End-to-End (Diff Image)" method.
The table shows accuracy results of the extrinsic evaluation on SemEval (SE) Twitter SA datasets.
The "ours" model achieves the highest accuracy results among all models for each of the SemEval Twitter SA datasets.
Table 8 provides experiment results with different training data combinations, testing on SMS as the target genre.
The training data combination "MergePT(CAT(SMS, CHT), CTS)" achieves the highest test performance in Table 8.
The table shows the results of experiments in adapting Egyptian to look like MSA.
The adaptation of Egyptian to MSA without segmentation improves the performance on the SMS Test and CHT Test datasets.
Table 4 presents experiments with interpolated language models compared to the baseline language model.
The interpolated language model performs better than the baseline language model in all test genres.
Table 6 describes the experiments with class-based language models.
The class-based models outperform the baseline on the SMS TestG dataset.
The table presents the results of tuning on two different genres: SMS and CTS.
The results show that using a length-filtered dataset for tuning improves performance compared to using an unfiltered dataset for both genres.
Table 2 compares the CER (%) between the RPE and APE methods for a transformer model.
The APE method with a range of 20 (k=20) and encoding type "enc" achieves a CER (%) of 9.17, with a SU value of 9.17 and a LU value of 33.56.
The models used in the experiments are "transformer", "PSS-CM", "PSS-offline", and "PSS-online".
The Pmin values for the models "transformer", "PSS-CM", "PSS-offline", and "PSS-online" are 0.8, 1.0, 0.5, and 0.8 respectively.
Table 3 provides a performance comparison in CER (%) when combining PSS and RPE.
The CER (%) for B2 with SU is 10.52.
ResNet152 achieves the highest BLEU-4 score among all the CNN models.
DenseNet121 achieves the lowest METEOR score among all the CNN models.
The table shows the image captioning performance on the Sydney-Captions dataset using different CNN models for the proposed SD-RSIC approach.
The ResNet101 CNN model achieves the highest BLEU-4 score among all the CNN models.
The table presents the classification results for three types of reference: Minimal, Oversp., and Undersp.
The overall precision, recall, and F1-score for the reference type classification are 0.80, 0.75, and 0.75, respectively.
Table 2 provides the referential overspecification accuracy for two different methods: Speaker and Profile.
The accuracy of predicting named entities for the LOC/ORG/FAC train set is higher than the accuracy for the LOC/ORG/FAC test set.
The number of predicted named entities is lower than the number of annotated named entities for the FAC train and FAC test sets.
Table 2 provides information about the hyper-parameters used in the experiments.
The dimension of word embeddings used in the experiments is either 300 (cc) or 600 (cc/mimic).
Pythia + SQuINT achieves the highest consistency metric and consistency percentage among all the methods.
Pythia + SQuINT achieves higher overall VQA accuracy and reasoning accuracy than Pythia.
Table 2 provides evaluation results of the IMN model and previous methods on the PERSONA-CHAT dataset without using personas.
The MRR for the IMN model on the PERSONA-CHAT dataset without using personas is 75.8.
The table provides the results of ablation tests on the DIM model by removing either persona-response matching or context-response matching.
Removing persona-response matching has a greater impact on the performance of the DIM model in terms of "hits@1" and "MRR" compared to removing context-response matching.
Table 5 shows the hits@1 results of transfer tests on the DIM model.
The hits@1 result for the Original train-test on the DIM model is 78.8.
The BLEU scores for each method increase as we move down the table.
The method "SQD w/ PG, LMP" achieves the highest BLEU scores and has the longest time (ms) compared to the other methods.
The number of tags is consistent across different train sizes.
The accuracy of word emission improves with larger train sizes.
The F1 score achieved in morpheme tagging on the test set is 93.70% for a train set size of 5000 and a test set size of 720332.
There are 88 different tags used in morpheme tagging on the test set for a train set size of 5000 and a test set size of 720332.
The CRF model for morpheme tagging achieves a higher accuracy than Chipmunk.
As the training size increases, the PoS tagging accuracy scores also increase.
The accuracy scores for morpheme tagging are consistently higher than the accuracy scores for word emission and suffix.
Table 8 compares the performance of our model, a suffix-based tagger, and the perceptron algorithm on datasets obtained from Metu Sabancı Turkish Treebank.
The HMM model with the last morpheme tag achieves an accuracy of 88.98% on the test set.
The ensemble system has a performance of 56.14 on the KBP 2016 evaluation dataset.
The ensemble system has a performance of 33.0 for the Realis span type on the KBP 2016 evaluation dataset.
Table 6 provides the parameters for span detection and realis status classifiers and type classifiers.
The dropout rates for the span detection and realis status classifiers and type classifiers range from 0 to 0.5.
The "Joint span + realis classifier" system outperforms the "Separate realis and span classifiers" system on the KBP 2016 evaluation dataset.
The performance of both the "Joint span + realis classifier" system and the "Separate realis and span classifiers" system on the KBP 2016 evaluation dataset is comparable in terms of span classification.
The "MNLI+HELP" model performs the best among all the models on the training set.
The "MNLI+HELP" model performs the best among all the models in the downward monotonicity reasoning.
The majority of the models in Table 6 are trained on the SNLI dataset.
The BERT model trained on the MNLI dataset has the highest accuracy among all the models in Table 6.
The performance of Tag Up has improved on Lexical problems.
The performance of Tag Up has improved on non-Lexical problems.
The Top-10 Advanced score for the combination of all dictionaries is higher than the Top-10 Simple score for the same combination.
The combination of DicEx and DicPi performs better on the Top-1 Simple and Top-10 Simple tasks compared to the Top-1 Advanced and Top-10 Advanced tasks.
The experiments used three different dictionaries: DicEx, DicPi, and DicPa.
DicPi has the highest number of mutual words with DicEx among the three dictionaries.
The accuracy of the extracted dictionary increases as the weight increases.
The accuracy of the extracted dictionary is lower for the top-10 results compared to the top-1 result.
In row 2, the source language is English and the target language is German.
The English PU score in row 1 is 76.23 and the German PU score is 81.79.
The training dataset used for all models is CoNLL.
The "2×EP" model performs better than the "EP" model in terms of F1 score for German.
The "Xavier" weight initialization method is used for all models.
The "Adam" optimizer is used for all models.
The weight initialization method for the "Same captions" and "Different captions" rows is Xavier.
The post-image size for the "Different captions" row is 511.
The table shows the performance of different binary classifiers used to find the optimum level of combination.
The BayesNet classifier achieves a balanced accuracy of 65.5 for IWSLT2012 and 72.0 for DT05.
The best WER for CHiME-3 (L5) is lower than the best WER for CHiME-3 (L1).
The best WER for SegO on IWSLT is lower than the best WER for SegO on CHiME-3.
Table 7 shows the WER scores of the enhanced channels for different systems.
The WER of the enhanced channels for mvdr-dnn is 17.6 for DT05.
The rankings in Table 8 are listed in descending order.
The average improvement values in Table 8 are negative for most methods.
MLR+BW has the lowest average improvement compared to the other ranking methods.
The WER for L7 and L8 in RR2+BW is the same as the WER for L6.
The ranking methods are listed in descending order from L3 to L14.
The MLR+BW ranking method has the lowest average WER.
The table shows the results of evaluating the influence of different embeddings on the model HNN4ORT.
The model with (word, POS embeddings) [ITALIC] train has the highest F-score.
Table 4 shows the performances of the model HNN4ORT on sub-datasets containing only overlapping triples of each dataset.
The F1-scores for Wikipedia, NYT, and Reverb are 0.736, 0.681, and 0.714 respectively.
The table shows the results for different CNN models.
CNN-B model has the highest accuracy and precision for both NO_SEG and SEG classes.
CNN-B achieves the lowest cross entropy during training.
CNN-B has a lower cross entropy value compared to CNN-A and CNN-C.
The average score for the top 50% of the data points is 95 or lower.
The average score for all data points is 14.
Table 6 shows the classification accuracies before and after applying FGWS and DISP to the clean test sets for different datasets and classifiers.
The accuracy of the BERT base classifier on the IMDb dataset after applying FGWS and DISP is 90.12.
The table includes attack success rates for the IMDb and SST-2 datasets.
The table includes attack success rates for the CNN, LSTM, and BERT base classifiers.
Table 4 shows the performance results of FGWS on different datasets and attacks.
Our approach outperforms Calibration, CVAE-GAN, and Clustering on all metrics.
Clustering performs better than Calibration and CVAE-GAN on the Intra-3 and Ent-1 metrics.
The SEQ2SEQ model performs worse than the SEQ2SEQ (★) model in terms of Dist-1, Dist-2, and Dist-3.
The Transformer model performs worse than the Transformer (★) model in terms of Ent-1, Ent-2, and Ent-3.
Word-level augmentation improves the BLEU score compared to the baseline.
Sentence-level augmentation increases the Dist-1 score compared to word-level augmentation.
The model's performance decreases as the distance between words increases.
The model performs better according to the BLEU metric compared to the average, extractive, and greedy metrics.
The "Top 1" feature has the highest accuracy and average precision.
The "Top 2" feature has a higher accuracy and average precision than the "Baseline" feature.
The table shows accuracy results for different datasets using a RNN-based approach compared to majority baseline and lexicon-based baseline.
The RNN-based approach outperforms the majority baseline and lexicon-based baseline in terms of accuracy for all datasets.
There is no significant difference in means between the "lexicon" group and the "majority" group.
The mean difference between the "lexicon" group and the "RNN" group is 14.0.
Table 4 provides the performance comparison of DTCA under different claims with different numbers of comments.
The precision of DTCA claims with less than 3 comments is 68.21% on the RumourEval dataset.
The table compares the performance of DTCA against the baselines on two datasets: RumourEval and PHEME.
In the RumourEval dataset, the highest performance for DTCA is achieved in terms of A (%), P (%), R (%), and F1 (%).
Table 3 provides the performance comparison of models on different numbers of comments.
The F1 score for RumourEval with CaSa on all (100%) comments is 79.24.
AdaNorm performs better than LayerNorm on seven datasets.
LayerNorm-simple achieves a score of 76.66 on the Language Model Enwiki8 dataset.
The "LayerNorm-simple" model performs the best on the "Machine Translation En-De(+)" dataset.
The BLEU scores for the 36 individual vanilla models are lower than the scores for our single N×M model.
The BLEU scores generally increase as the model number increases for both the individual vanilla models and our single N×M model.
The model AE+Att+Copy has the highest METEOR score of 8.56.
The model AE+Att+Copy+Salient has the highest scores for ROUGE-SU4, ROUGE-1, ROUGE-2, and ROUGE-L.
Table 3 shows the proportion of summaries which mention a specific aspect/sentiment.
The AE+Att+Copy system variant performs better than the Customized system variant in terms of the proportion of summaries mentioning a specific aspect/sentiment.
The tagging accuracy for the "Baseline + all features" model is 92.1% on the validation set and 92.2% on the test set.
The tagging accuracy for the "Owoputi et al." model is 91.6% on the validation set and 92.8% on the test set.
Table 7 provides the attachment F1 (%) on the validation set using different models and window sizes.
The best result in the "DNN TE" column and the "seq2seq TE" column is in boldface.
Table 2 compares different methods with and without dropout in the PT step and always applying dropout in the FT step in an ablation study on WMT100K data.
The Noisy ST method with beam search and dropout achieves the highest PT score and the highest FT score among all the methods in the ablation study.
The table provides test tokenized BLEU scores for three different methods: baseline, ST (scratch), and ST (baseline).
The baseline method does not mention the PT value, while the ST (scratch) method has a PT value of 16.8.
The "noisy ST" method has the lowest values in the "smoothness" and "symmetric" columns.
The "baseline" method has the highest value in the "error" column.
The "noisy ST" method outperforms the "baseline" and "BT" methods on all translation datasets.
The "FloRes English-Nepali Overall" dataset performs better than the "FloRes English-Nepali En-Origin" and "FloRes English-Nepali Ne-Origin" datasets.
BT performs better than the baseline in all three dataset sizes (100K, 640K, and 3.8M).
The Rouge scores generally increase as the dataset size increases for each method.
The table presents the results of an ablation analysis on the WMT100K dataset using different methods.
The "noisy ST (separate training, all data)" method outperforms the baseline method in terms of BLEU score.
Table 4 provides F1 scores for different lengths of entities in three languages: Spanish, Dutch, and German.
The F1 score for entities with not less than three tokens in Spanish is 47.57.
The table shows the F1 scores of different ablation analyses for three different models: Full Model, Shuffle, Word-only, and Char-only.
The Full Model has an F1 score of 64.48 in Spanish, the Shuffle model has an F1 score of 49.44 in Spanish, the Word-only model has an F1 score of 53.00 in Spanish, and the Char-only model has an F1 score of 18.88 in Spanish.
The OOV rate (percentage) is provided for four different languages: Spanish, Dutch, German, and Bengali.
The OOV rate (percentage) varies across different languages.
ParsBERT outperforms all other models mentioned in DeepSentiPers in terms of F1 score in both the Multi-Class and Binary tasks.
ParsBERT performs better than CNN + FastText in the Binary task according to the F1 score.
ParsBERT performs better on the Digikala dataset compared to the SnappFood dataset.
multilingualBERT performs better on the SnappFood dataset compared to the Digikala dataset.
Table 6 compares the performance of ParsBERT and multilingualBERT on a text classification task.
ParsBERT performs better on the Persian News dataset compared to the Digikala Magazine dataset.
ParsBERT achieves an F1 score of 93.10 on the PEYMA dataset.
Beheshti-NER achieves an F1 score of 84.03 on the ARMAN dataset.
The LMM model outperforms other models in terms of BLEU scores in Arabic, Czech, and Turkish languages.
The Hierarch. model outperforms other models in terms of chrF3 scores in Arabic, Czech, and Turkish languages.
The LMM model outperforms the other models in terms of BLEU, t-BLEU, and chrF3 metrics.
The LMM model performs the best in terms of BLEU, t-BLEU, and chrF3 metrics for the Turkish (TR) language.
Table 2 shows the scoring model evaluation on the Amazon Mechanical Turk test set.
The supervised AMT model performs better than the average predictor model in terms of Pearson correlation coefficient, Spearman's rank correlation coefficient, and mean squared error.
The "Q-learning AMT" policy performs the best among all the policies on both the full test set and the difficult test set.
The "Supervised AMT" policy performs better than the "Off-policy REINFORCE" policy on the difficult test set.
Table 4 represents the off-policy evaluation with respect to expected Alexa user score and number of time steps on the test set.
The learned reward performs better than the supervised AMT in terms of Alexa user score and time steps.
The Q-learning AMT* policy has the highest user score among all the policies.
The Supervised AMT policy has the highest percentage of positive utterances among all the policies.
The "Supervised AMT" policy has the highest average number of overlapping words between the user's utterance and the system's response in the same turn.
The average number of overlapping words between the user's utterance and the system's response in the same turn is generally higher than the average number of overlapping words between the user's utterance and the system's response in the next turn.
The "Our Joint Method" achieves the highest F1 score for "entails" among all the systems.
The precision score for "entails" is higher than the recall score in the system proposed by Ng et al. (2010).
Our Joint Method achieves the highest accuracy among all the systems in most categories.
The average accuracy of all systems is above 50%.
The similarity score between the reachability-based embeddings using WordNet 1 and the WS353S dataset is 0.343.
The no distinction score between the reachability-based embeddings using PPDB 1 and the MEN dataset is 0.465.
Table 3 shows the results on word similarity datasets using different embedding methods, including SPPMI, SynGCN, SS-PPMI, and DSS-PPMI.
The SS-PPMI embedding method with WordNet as the lexical knowledge source and 2 hops has the highest similarity score on the SimLex999 dataset.
The SynGCN method achieves the highest similarity score on the SimLex999 dataset.
The W-retrofit(jcn) method with Wordnet and 2 similarity hops achieves a higher relatedness score on the TR9856 dataset compared to the Retrofit-baseline method with Wordnet and 1 similarity hop.
The highest similarity score for Google is 0.717.
The highest similarity score for SynGCN is 0.234.
The Sprinkling model outperforms all other models in the SimLex999, WS353S, and RG65 similarity tasks.
The W-Retrofitting model performs the best in the TR9856 relatedness task.
Table 7 compares the accuracies of GloVe representation, DT embedding, and the combination of both using PCA.
DT embedding obtained using node2vec (D2V-N) outperforms Word2vec (W2V) in terms of enhancing the performance of GloVe.
The performance of GloVe combined with Word2vec (W2V) and GloVe combined with DT embedding obtained using node2vec (D2V-N) is the same for the M287 dataset.
The BiLSTM model with Synonym word substitution method and PSO search algorithm has the lowest modification rate of 3.71%.
The BERT model with Sememe word substitution method and Greedy search algorithm has a perplexity of 207.71.
The PSO search algorithm consistently achieves higher attack success rates compared to other search algorithms.
The Sememe word substitution method achieves higher attack success rates compared to other word substitution methods in the BiLSTM SNLI task.
The transfer accuracy of the BiLSTM model on the IMDB dataset is 81.93.
The transfer accuracy of the BERT model with Sememe+PSO attack on the SNLI dataset is 59.54.
The "WEC + CNN" approach outperforms all other approaches in terms of Travel DCG@1, Travel DCG@6, Relationships DCG@1, Relationships DCG@6, Finance DCG@1, and Finance DCG@6.
The "Okapi" approach performs worse than the other approaches in terms of Travel DCG@1.
Table 3 shows the performance of different approaches on the Baidu Zhidao dataset.
The WEC+CNN approach achieves the highest DCG@1 and DCG@6 scores in the Computers & Internet category on the Baidu Zhidao dataset.
The recall for the micro-average of all classes is 0.863.
The precision for the macro-average of the positive class is 0.012.
Table 1 provides information about different Transformer setups.
Fine-tuning was not performed for rows 1-4, but it was performed with Elastic Weight Consolidation for rows 5-6.
Checkpoint averaging was performed for rows 2, 4, and 6, but it was not performed for rows 1, 3, and 5.
The combination of DBM, CKG, and VFM has the highest number of positives.
The combination of DBM and CKG has a higher average than the combinations of DBM and VFM, and CKG and VFM.
The "DBM ∧ CKG" category has the highest overlap value compared to the other categories.
The overlap value for the "CKG ∧ VFM" category is not provided in the table.
The Smatch F1 score for LDC2015E86 is higher than that of BioAMRTest in the "ours" row.
The Smatch LDC2015E86 P score is higher than the Smatch BioAMRTest P score in the "upper-bound" column.
The table provides percentiles of Smatch F1 predictions for gold graphs for two datasets: LDC15E86 and BioAMRTest.
The Smatch F1 predictions for gold graphs in the LDC15E86 dataset have a higher percentile 99 value compared to the BioAMRTest dataset.
Table 6 shows the results of different parse-ranking systems with respect to sentence-level parse rankings.
Our parse-ranking system achieves a Smatch score of 0.54 and a percentage of positive ρ of 77.0% on the LDC2015E86 dataset.
CU-NLP has a rank of 3 in LDC2015R36.
JAMR-base has a rank of 12 in BioAMRTest.
As the depth of the conversation increases, the total number of conversations decreases.
As the depth of the conversation increases, the percentage of incorrect conversations also increases.
The Kappa values for the Logistic Regression (wLDA features + online dialogue) model and the CNN + wLDA + online dialogue model are both higher than the Kappa value for the Majority baseline model.
The F-score values for the LSTM + wLDA + online dialogue model and the CNN + wLDA + online dialogue model are both higher than the F-score value for the Majority baseline model.
In the iterative review process, crowd workers filtered a total of 895 visual questions that contained PII.
In-house experts filtered a total of 725 visual questions that contained suspicious complex scenes.
There are three different methods used in the VQA experiments: Q+I, Q+I+A, and Q+I+BUA.
The FT method has the highest BLEU score among all the methods.
The "Q+C" model has an average precision score of 0.306 and an average F1 score of 0.383.
The "Q+I" model has the highest average F1 score of 0.648.
The table presents the performance of different methods on the SemEval 2016 dataset.
The "Contrs" method achieves the highest MAP score on the SemEval 2016 dataset.
The filter value for the "Score ≤0.9985 (score-cutoff)" row is lower than the filter value for the "None" row.
The precision value for the "Score ≤0.9985 (score-cutoff)" row is higher than the precision value for the "None" row.
The table shows the number of distinct Wholes and Parts for entries within a 5th Grade Vocab.
The "hasPartKB (ours)" knowledge base has 3294 distinct Wholes and 3304 distinct Parts.
MAGE (shared) has the lowest average error rate among the three models.
MAGE (16) has the lowest error rate on Task 3 among the three models.
The N2N model has the highest overall average error rate among all models.
The + MAGE (16) model has an error rate of 0.2 on Task 9.
The BiGRU model performs better on the test set than on the validation set.
The GA model performs equally well on both the validation and test sets.
The EOP DIY method was used for the highest performing row in the ParaType column.
The T PPL(-) method achieved the highest BLEU1 score.
The models are tested with different paragraph types, including "None", "SEP \n", "SEP DIY", "EOP \n", and "EOP DIY".
The highest BLEU1 score is [BOLD] 58.87 and the highest BLEU2 score is [BOLD] 89.78.
The "char+word" model has a higher F1-Score than the "word-only" model.
The results of the test set confirm the usefulness of character-level word embeddings.
Table 9 presents the correlation of aspect sentiments with the final recommendation.
The aspect "Impact of Ideas" has a correlation coefficient of 0.273 with the final recommendation.
Table 8 shows the correlation of recommendation scores with the final recommendation decision of the chair.
The correlation coefficients for Pearson's, Spearman's Rank, and Kendall's τ are all 0.67 for the "Majority Voting" recommendation.
The BANNER model achieves a precision of 81.9% on Task A in the validation set.
The table shows the performance of various models on disease name recognition and disease classification tasks.
Increasing the value of n in the uniform sampling approach improves the BLEU score.
The aligned training approach performs better than the mixed training approach in terms of BLEU score.
The baseline performance is consistently lower than the performance of the other methods across all n values.
The values in the "Average" column are calculated by taking the average of the values in the corresponding rows.
The values in the "n=2" and "n=3" columns are higher than the values in the "n=1" column in the "ωn=1" row.
The values in the "Average" row represent the average values of each column.
The values in the "n=5" column are higher than the values in the "Uniform" column.
The Split-Seq2Seq model achieves a BLEU score of 78.77 on the original data split.
The Copy256 model achieves a BLEU score of 23.78 on the new data split.
The "Split-Seq2Seq*" model has the highest BLEU score of 78.77.
The "shashi2017" model has missing values in the BLEU, #S/C, and #T/S columns.
The "Split-Seq2Seq*" model has the highest BLEU score of 78.77.
The "shashi2017" model has missing values in the BLEU, #S/C, and #T/S columns.
The MVAN model outperforms the ReDAN Gan and Tohoku models in all metrics on the VisDial v1.0 dataset.
The MVAN model has the lowest mean score among all models on the VisDial v1.0 dataset.
The experiment involves different models.
The NDCG score is different for different models and configurations.
The "cohesive markers" feature has the lowest performance in all four categories.
The "FW+POS+pos. tok" feature has the highest performance in all four categories.
Table 4 provides information about the perplexity of Germanic and Romance translationese language models on Germanic and Romance neural network test sets.
The Germanic language model has a lower perplexity on the Germanic neural network test set compared to the Romance language model on the Romance neural network test set.
The word embedding-only model does not recognize any entities for the "GENDER" entity.
The concatenation model performs better in recognizing the "PERSON" entity compared to the word embedding-only model.
The BLEU score for the Transformer-big+BT+RT system on the Chinese-English newstest2017 set is [BOLD] 27.21.
The BLEU score for the Edinburgh’s NMT System (ensemble) on the English-German newstest2016 set is 36.20.
Table 1 provides results on NER, POS tagging, and chunking tasks.
DA LSTM outperforms other models in terms of precision, recall, and F1 score for the NER task.
The table compares the performance of different models on the CosmosQA dataset.
Humans outperform all the models on the CosmosQA dataset.
The table shows the accuracy (%) of different models on the CommonsenseQA development set.
The RoBERTa+(T+F) model achieves the highest accuracy (%) among all the models on the CommonsenseQA development set.
The table shows the accuracy (%) of different models on the Cosmos development set.
The Capsule model improves the accuracy on the Cosmos development set.
Table 4 shows the clustering performance on Reddit episodes using embeddings obtained with different methods.
The values for NMI, H, and C in the IUR method are all marked as bold.
The method "IUR" achieves the highest scores in terms of MRR, MR, and R@k for all feature groups.
The method "IUR" performs better than other methods in terms of MRR, MR, and R@k for the "subreddit only" feature group.
BERT (cased) has a higher validation accuracy than BERT (uncased).
The total count of data points for all five lexical items is 3205.
The IPA transcription for the word "water" is [”wOR].
The total number of data points in the table is 6441.
All the words in the table are content lexical items from the TIMIT database used for training.
Table 1 presents the dev accuracy results for the MT-DNN model with different training set sampling strategies.
The AVI strategy consistently outperforms other strategies in terms of dev accuracy for all three percentages of training data.
The difficulty ranking for the item text "Two dogs playing in snow." in the SNLI task, according to humans, is 168.
The label for the item text "Only two words will tell you what you know when deciding to see it: Anthony. Hopkins." in the SSTB task is Positive.
The table includes evaluation results of different methods, such as CNN, LSTM, CNN+LSTM, and LSTM+CNN.
The FMain scores are higher than the FSimple scores for all categories.
The difference between the FMain and FSimple scores is consistent across all categories.
The readability of the Dickens corpus is 8.6±0.1.
The readability of the Simple corpus is 10.8±0.2.
The statistical similarity between different samples is the lowest for n=2.
Table 5 compares the readability of articles from different topics in Main and Simple English Wikipedias.
Table 7 provides information about controversy and readability.
The controversy score of FArticle is 4.8 higher than the controversy score of FTalk.
The BERT model achieves 100% accuracy on all training sets.
The TreeLSTM model outperforms the LSTM model on all datasets.
The "Train" column represents different training conditions or scenarios.
The "BERT" column represents the performance of a model using the BERT architecture.
The accuracy of the BERT model on the MNLI test set is 84.6±0.2.
The accuracy of the LSTM model on the 0pt1 dataset is 47.2±1.1.
The BERT model outperforms the LSTM and TreeLSTM models in all training and testing conditions.
Increasing the training set size improves the performance of the models.
The dataset "Purdue RVL-SLLL ASL" has more signers than the dataset "RWTH Boston 104".
The dataset "MS-ASL" has more videos than the dataset "Video-Based CSL".
The SA+CE+SA+MM (full) method achieves the highest iBLEU and BLEU scores.
The SA+CE method has the lowest inference time.
The methods in Table 2 are categorized into three groups: Supervised, Distant supervised, and Unsupervised.
The SA w/ PLM (Ours) method achieves an iBLEU score of 14.52 and a BLEU score of 21.08.
Table 2 presents the automatic evaluation results on formality transfer using various methods.
The method "tgls" achieves the highest BLEU score among all the listed methods.
The table presents the accuracy of the NER system with different settings of PoS and chunking tags.
The NER system achieves the highest F1 score when PoS and chunking tags are not used.
The system "Our System" has the highest precision, recall, and F1 scores compared to other systems.
The F1 score for the system "Our System" is higher than the F1 scores for the other systems.
Table 5 compares the performance of different PoS tagging systems in terms of precision, recall, and F1 score.
The proposed NER system without PoS and chunking tag-based features achieves the highest F1 score among all the systems.
The word-based model with ws generated by RDRsegmenter has a higher F1 score than the syllable-based model.
The word-based model with gold ws has a higher precision than the word-based model with ws generated by RDRsegmenter.
Removing the "cluster" feature from the feature set leads to a decrease in the F1 score.
The feature set (5) which includes word, word shapes, cluster, and w2v features, has a higher F1 score compared to other feature sets.
Table 7 presents the coverage of various dictionaries in terms of vocabulary percentage, number of comments, and number of racist comments.
The expanded dictionary has a vocabulary coverage of 0.035, 212 comments, and 82 racist comments.
The "Original" method has an "F" score of 0.44 and an "AUC" score of 0.63.
The "Cleaned" method has an "R" score of 0.43, an "F" score of 0.46, and an "AUC" score of 0.63.
The Bérard model is used for ASR, MT, and AST tasks.
The AST performance with the Bérard model is 19.3 on the En–Fr task.
The table shows the effect of pretraining (PT) when added to the baseline or the fine-tuned (FT) baseline.
Table 6 shows the effect of fine-tuning (FT) on different combinations of AST, TTS, and MT.
Fine-tuning (FT) has a positive effect on the performance of AST + TTS and AST + MT + TTS combinations.
The "Paths" column represents different numbers of paths.
The "Cross-Sent." accuracy is higher than the "Single-Sent." accuracy.
Table 6 compares the performance of single sentence and cross-sentence extraction models at different thresholds.
The precision of the cross-sentence extraction model at a threshold of p≥0.9 is 61.
The LSTM cell has the lowest performance on the RTE task.
The Multi-Task cell has the highest performance on the RTE task.
The table provides test results on GLUE tasks for various models, including BiLSTM+ELMo, BiLSTM+ELMo+Attn, Baseline (with ELMo), ENAS (Architecture Search), and CAS Results.
The CAS Step-2 model achieves a score of 54.1 on the RTE task.
The accuracy of the model improves when additional conditions are added.
The model performs better when at least one condition is added compared to when no condition is used.
The CAS Step-1 model achieved an average CIDEr score of 44.5 on the MSVD dataset.
The CAS Step-2 model achieved an average CIDEr score of 61.7 on the MSVD dataset.
The Multi-Task cell performs the best on the DiDeMo M task.
The DiDeMo R task has better performance than the DiDeMo B task for all cells.
Table 5 shows the performance of c+gat+dref+mg on different graph sizes.
The model "c+gat+mg+dref" has the highest F1-score among the three models.
The "c+gcn+mg+ctef" model achieves the highest F1-score among all the models in the "multiple graphs (mg)" category.
The "gat+mg+ctef" model achieves a Precision of 84.98 and a Recall of 84.33.
The proposed models are named "c-gcn-mg-dref" and "c-gat-mg-dref".
The proposed models have F1-Scores of 85.9 and 86.3.
The "direct+pretraining+adaptor" method achieves the highest scores in both the "En→De" and "En→Fr" test sets.
The table does not provide any information about the results of other works.
Table 5 presents the results of different architectures for En→De and En→Fr translation tasks.
Table 6 presents different pre-training schemes for En→De translation and includes metrics such as Bleu and Ter for both the development and test sets.
The addition of the adopter improves the performance in terms of Bleu and Ter scores for all pre-training schemes.
The values for EM and F1 are the same for the pairs "K1-K2 1-1" and "K1-K2 1-1".
The values for EM and F1 are different for the pairs "1-3" and "3-1".
The F1 score is higher for all types of aggregation functions compared to the EM score.
The HAS-QA (MAX Ans. Span) has the highest EM and F1 scores among all types of aggregation functions.
Technology-Z has a lower F-Score compared to Technology-Y.
Technology-Z has a lower Precision compared to Technology-Y.
The average number of tokens per turn is 13.13 for the MultiWOZ dataset.
The M2M dataset has 14,139 slot values.
The domain "www.lagardere.com" has the highest number of missed pairs with 20.
The domain "www.toucherdubois.ca" has the third lowest number of missed pairs with 8.
The performance of the "stricta" features used is higher than the performance of the "cosine (cos)" features used on the training data.
The performance of the "url" feature is higher than the performance of the "cos" and "lcos" features on the test data.
The performance for variable context sizes decreases as the value of k increases for all translation tasks.
The performance for TED Talks translation is higher than the performance for Subtitles translation.
The lexical cohesion scores are higher for the "Noun Translation TED Talks" and "Noun Translation Subtitles" datasets compared to the "Noun Translation News" dataset.
The NMT Transformer model has lower accuracy compared to the human reference for all translation datasets.
Table 5 provides the evaluation results of spatial semantic lifting on DBGeo for both validation and testing sets.
The AUC value for spatial semantic lifting on the validation set is 82.74.
Table 6 provides the evaluation results of \spexkgessl and \spexkge′space on DBGeo for a few selected relation r using APR (%) as the evaluation metric.
The hidden layer size of the Encoder BiLSTM is 600.
The dimensions of the embeddings are 100 for [ITALIC] p and 128 for [ITALIC] q.
The table presents the evaluation scores of three different models on in-domain test datasets.
The model "IPS +ML +RL -Lemma" outperforms the other two models in terms of average scores.
The metrics used in the table are "sentBLEU" and "chrF++".
The table provides the average scores for each language pair.
The HMM-Hybrid model performs better than the E2E model on both the Whispered Dev and Whispered Test partitions for the Original corpus.
The Ours (400/25/25) corpus partition performs better than the Original corpus partition for both the HMM-Hybrid and E2E models on both the Whispered Dev and Whispered Test partitions.
Table 2 presents the performance results (PERs) on wTIMIT with different methods and variations.
The table shows the SVM classification results for different datasets with varying percentages of data used for training and testing.
The "TRB True" column represents the SVM classification results using true labels and consistently has higher values compared to other columns in the same row.
The table includes results for three different datasets: [EMPTY], 50K, and 100K.
The table presents results for four different models: Unsupervised VAE, VAE-S, VAE-SVG, and VAE-SVG-eq.
The EDD FULL model outperforms the ED(Base Line) and EDD models on all metrics.
The EDGAN model outperforms the ED(Base Line) and EDD models on all metrics.
The F1 score for Mensemble is higher than the F1 scores for M1, M2, M3, M4, and M5.
The Precision, Recall, and F1 scores for M3 are higher than the scores for M1, M2, M4, and M5.
Table 8 shows the robustness of RoBERTaBASE trained on a subset of adversaries to unseen adversaries.
The F1 score for AddKSentDiverse is higher when RoBERTaBASE is trained on SQuAD compared to when it is trained on SQ+ASD/PQ/PA.
The table includes three different models: Random, VGG16, and Ours FCC6.
The accuracy of Ours FCC6 in classifying figures with non-trainable captions is 58.57%.
The F-Score for Hebrew Multi-tagging using the "LSTM-CRF +Char+FT" model with raw lattices is [BOLD] 95.1.
The F-Score for Hebrew Multi-tagging using the "LSTM-CRF +FT" model with oracle segmentation is 94.6.
Table 3 provides a quantitative comparison of the CUNI Transformer system + fine-tuning (this work) with other submitted systems.
The Naver Labs Europe system performs better in fr-en translation compared to en-fr translation.
The CUNI Transformer model performs better on French-English translation than on English-French translation.
The baseline MTNT model performs worse on English-French translation than on French-English translation.
Bertdef outperforms Bert on all datasets and part-of-speech types.
GAS ext(Concatenation) performs the best on the Noun part-of-speech type.
The F1-score decreases as the word count increases in the training dataset.
Words with a count of 0 have a higher F1-score compared to words with a count of 1-10 in the training dataset.
The values for "Mean" and "Max" in the "SE2" column are higher than the values for "MeanConcat" and "MaxConcat".
The values for "Max" are higher than the values for "Mean" in the "All" row, and the values for "MaxConcat" are higher than the values for "MeanConcat" in the "All" row.
There are 733 tweets related to the SemEval stance Atheism (A) topic.
There are 564 tweets related to the Climate Change is Concern (CC) topic.
The inter-rater agreement for Hyp. is 0.646 and for Cohyp. is 0.659.
The w2v score for all pairs is 0.864 and for positive pairs is 0.967.
The strategy "NPAD" with 50 parallel decoding processes has the lowest "Valid NLL" and the highest "Valid BLEU" scores.
The strategy "NPAD" with 10 parallel decoding processes has a lower "Test-1 NLL" score compared to the strategy "NPAD" with 5 parallel decoding processes.
Table 1 shows the effect of noise injection on different decoding strategies.
Increasing the value of σ0 in NPAD leads to a decrease in Valid NLL and an increase in Valid BLEU.
The table provides different strategies for NPAD with beam search.
The table shows the validation negative log-likelihood (NLL) values for each strategy.
The table compares the performance of three different decoding strategies: Beam, NPAD+B, and Diverse.
The NPAD+B strategy with a beam width of 5 achieves the highest BLEU score on the test set.
Table 1 represents encoder identification accuracies for different community sizes.
The encoder identification accuracy for MNIST decreases as the community size increases.
The "DDA-Shallow" method achieves the highest translation accuracy in the "De-En LAW" and "De-En MED" domains.
The table shows the performance of DDA-Deep when fusing different parts of models on the law and medical datasets.
The "LM-in + LM-out" strategy achieves the highest performance on both the LAW-MED and MED-LAW datasets.
The "two LMs-out" strategy achieves the lowest performance on both the LAW-MED and MED-LAW datasets.
Table 5 shows the BLEU points of models after continued training on the IT development dataset with different values of coverage penalty β.
The method "ws + auth" achieves the highest F1 score in the sexism class.
The precision of the method "lr + auth" is higher than the precision of the method "hs + auth" in the sexism class.
Table 1 compares the performance of Baselines and Our methods on the Twitter dataset.
The Our methods with lr + auth combination achieves the highest f1 score on the Twitter dataset.
The table shows the performance of different methods for classifying the racism class.
The method "lr + auth" achieves the highest F1 score for classifying the racism class.
There are 42,220 papers in the COVID-19 Knowledge Graph dataset.
There are 240,624 relations of type "authored_by" in the COVID-19 Knowledge Graph dataset.
The "Learned-Mixin +H" method achieves the highest accuracy of 52.05% on the VQA-CP v2.0 test set.
The "Learned-Mixin +H" method has a higher accuracy than the "Reweight" method on the VQA-CP v2.0 test set.
Table 4 presents accuracy results on the adversarial MNLI dataset, HANS, and the MNLI matched dev set.
The Co-Attention model achieves higher accuracy on the HANS dataset compared to the MNLI dataset.
The F1 scores for the "Bias Product" debiasing method are higher than the scores for the "None" method in both the "TF-IDF Filtered Dev" and "TF-IDF Dev" cells.
The F1 scores for the "TF-IDF AddSentOne" method are higher than the scores for the "TF-IDF AddSent" method within each debiasing method.
The "Bias Product" method achieves the highest scores in the "Location CP" and "Location Dev" columns.
The "Learned-Mixin +H" method achieves the highest score in the "Person Dev" column.
The table presents the accuracy and per-class scores for different question classification methods used when building TriviaQA-CP.
The accuracy of the question classification methods improves from the Patterns method to the Supervised Model method.
Table 6 shows the error detection results using auxiliary objectives, trained on additional data.
The F0.5 score is higher for FCE TEST and CoNLL TEST2 compared to FCE DEV and CoNLL TEST1, respectively.
The "Main system" performs better than the "R&Y (2016)" system in terms of correct predictions on the CoNLL-14 TEST1 dataset.
The "+ errors" system has a higher recall than the "Main system" on the CoNLL-14 TEST2 dataset.
Pre-training the model on PTB-POS results in the highest performance on the FCE TEST dataset.
The model achieves higher performance on CoNLL-14 TEST2 compared to CoNLL-14 TEST1.
Table 5 shows the results on error detection when training is alternated between the two tasks (error detection and POS tagging) and datasets.
The "None" dataset achieves a bold value of 43.4 on the FCE TEST.
SoftSVM and DotBiLSTM are two different models.
DotBiLSTM has a higher F-micro score than SoftSVM.
The micro-average f-score for the SoftSVM model is 0.59.
The f-score for the direct class is higher for the DotBiLSTM model compared to the SoftSVM model.
The table compares the performance of BERT base and BERT large models on the ConceptNet relations.
Both BERT base and BERT large models achieve hits@K 100 scores above 30.
The "+ C2T" model has a higher overall exact match accuracy than the "BERT base" model.
The "BERT large" model has a higher F1 score for questions with answers than the "+ C2T" model.
The table compares the performance of three different systems: DNN∗, ResNet, and HDNN.
The table shows the WER values for different systems and configurations.
The table provides results of three different systems: DNN∗, HDNN.
HDNN with transform and carry gate has a lower WER than DNN∗.
The HDNN system performs better when the carry gate is multiplied (×) compared to when it is squared (√).
As the dimension decreases, the WER increases for both the DNN∗ and HDNN systems.
The modifier "will not be" has the highest average difference of -1.066.
The modifier "will not" has a positive impact on negative words with an average difference of 0.878.
Table 2 shows the impact of different modifier groups on sentiment, including negators, modals, and degree adverbs.
Negators have the highest negative impact on sentiment, followed by modals, while degree adverbs have a positive impact on sentiment.
The modals "could", "might", and "may" have a negative impact on sentiment.
The modals "could", "could be", "might be", "can be", and "would be" have a positive impact on sentiment.
The modals "could", "might", and "may" have a negative impact on sentiment.
The modals "could", "could be", "might be", "can be", and "would be" have a positive impact on sentiment.
The F1 score for using all features in the RandomForest model on DBpedia with k=5 is 0.399.
The difference in F1 score from using all features to removing each individual feature in the RandomForest model on LinkedMDB with k=10 is -0.015.
The table provides performance results for three different evaluation sets: WOS+LEXIS, WOS, and LEXIS.
The table shows the performance of the model based on Bio for different evaluation sets.
The number of features increases when cues, bigrams, and trigrams are added to the feature source.
Adding cues, bigrams, and trigrams increases the number of features in the Bio feature source.
The best performance after parameter tuning is achieved with the WOS + LEXIS evaluation set.
The F-score for the WOS evaluation set is higher than the F-score for the LEXIS evaluation set.
Table 7 provides the percentage of uncertain sentences in WOS and LEXIS according to the trained classifiers.
The "Tuned" model has a lower percentage of uncertain sentences compared to the "Bio" model for both WOS and LEXIS datasets.
The method "TGVAE (K=10, T=50)" achieves the highest score in the RF-1 metric.
The method "TGVAE (K=10, T=50)" achieves the highest score in the RR-L metric.
The table shows the results of different methods on three corpora: APNEWS, IMDB, and BNC.
NERD-ML has the highest F1 scores for all entity types in the Ritter dataset.
Table 4 provides information on different error types made by BERT, along with the number of occurrences and examples for each error type.
Table 4 presents the number of occurrences for each error type made by BERT.
GPT-3 Few-Shot outperforms previous unsupervised NMT work by 5 BLEU when translating into English.
En→De has the highest SOTA (Supervised) score of 41.2.
The fine-tuned SOTA model performs better than the fine-tuned BERT-Large model in terms of BoolQ accuracy.
The fine-tuned SOTA model performs better than the GPT-3 Few-Shot model in terms of CB accuracy.
The fine-tuned SOTA model achieves an accuracy of 76.1% on the WiC task and 93.8% on the WSC task.
The fine-tuned BERT-Large model achieves an accuracy of 71.3% and an F1 score of 72.0 on the ReCoRD task.
The results become progressively stronger moving from the zero-shot to one-shot to few-shot setting.
The accuracy of GPT-3 Few-shot on 5-digit addition is 29.2% and on 5-digit subtraction is 21.3%.
The performance of GPT-3 on the "A1" task improves as the number of shots increases.
GPT-3 performs better on the "Random insertion in word" task compared to the "Reversed words" task in the few-shot setting.
Table 10 shows the results of distillation with BERT Large on 500 labeled samples per class.
BERT Large achieves the highest scores among all models in the table.
Table 6 presents the results of distillation using different teacher models on four datasets.
BERT Large performs better than Distil with BERT Large on the IMDB dataset.
Table 8 presents the results of soft and hard distillation with BERT Large on different datasets.
Soft distillation achieves higher accuracy than hard distillation in all datasets.
Table 9 provides information about the variance in prediction probabilities of the teacher.
The maximum variance in prediction probabilities is obtained when the teacher spits hard labels as opposed to soft predictions.
Table 11 shows the results of distillation with BERT Large on 100 labeled samples per class.
The student model with hard distillation achieves higher accuracy than BERT Large on the IMDB dataset.
Table 12 provides different training schedules for distillation with BERT, including different stages and joint optimization.
The performance of the "Ag News" dataset improves with each training schedule.
The average WER is higher for the "Quantized inputs, quantized targets" condition compared to the "Continuous inputs, continuous targets" condition.
The standard deviation is higher for the "Quantized inputs, quantized targets" condition compared to the "Continuous inputs, continuous targets" condition.
The average WER for the "Baseline" setting is 7.97.
Increasing the mask length from 8 to 12 decreases the average WER, but further increasing it to 15 increases the average WER.
The accuracy of the "Ours" model on Yelp increases as the number of labeled examples during training increases.
The "Ours" model consistently achieves the highest accuracy on Yelp compared to other models at all levels of labeled examples during training.
The table provides results on the PTB test set for various baselines using different methods.
The "+ pretrain + anneal" method has the lowest PPL↓ value compared to the other methods.
The "+ pretrain + anneal" method has a non-zero KL value.
The method "Ours (λ=6)" achieves the lowest PPL↓ and Recon↓ values for both PTB and Yahoo datasets.
The method "Ours (λ=8)" achieves the highest AU↑ value for both PTB and Yahoo datasets.
The method "Ours" with λ=4 achieves a BLEU score of 8.62.
The method "Ours" with λ=4 achieves a higher BLEU score compared to all other methods listed in the table.
The method "Ours (λ=4)" has the highest PCC value among all the methods listed in the table.
The method "AE" has the lowest PCC value among all the methods listed in the table.
In Table 6, the values in the "2" and "3" columns of the "Ours (λ=4)" row are the highest in their respective columns.
In Table 6, the values in the "1", "2", and "3" columns of the "VAE" row are all the same.
The ARL-Adv model performs the best among all models in terms of all metrics on the GoogleNews dataset.
The ARL-Adv model performs the best among all models in terms of NMI, ARI, and ACC on the TREC dataset.
The table provides the default settings for ARL-Adv.
The value of K in the default settings for ARL-Adv is 300.
The word "noite" has the highest cosine similarity with the input "@maria_sanchez".
The word "couldn't" has a cosine similarity of 0.84.
C2V2L outperforms all other models in terms of F1 score for most languages in the TweetLID language ID task.
The n-gram LM model performs better than langid.py and C2L for the "cat" language in the TweetLID language ID task.
The F1 score for the "5-gram LM" model is higher than the F1 scores for both "langid.py" and "C2V2L (ours)".
The F1 score for the "C2V2L (ours)" model is higher than the F1 score for "langid.py".
The baseline encoder achieves a Bleu 2016 score of 28.4.
The alternating encoder with a depth of 4 has 2150 parameters.
The table shows the performance of joint relation and supporting evidence prediction using two different methods: Heuristic predictor and Neural predictor.
The Neural predictor outperforms the Heuristic predictor in terms of F1 measurement on both the development and test sets.
The BiLSTM model outperforms the LSTM model in terms of Dev Ign F1, Dev F1, Test Ign F1, and Test F1.
The Context-Aware model outperforms the LSTM model in terms of Dev Ign F1, Dev F1, Test Ign F1, and Test F1.
The "Human" method outperforms the "Model" method in all metrics.
The "Human" method has a higher F1 score than the "Model" method.
The BLEU score for the "Random 1000 samples" set is higher than the BLEU scores for the other sets.
The BLEU score is lower when the blacklist is triggered compared to when the blacklist is not triggered.
The "Both" bias condition has the highest proportion of good runs.
The "Both" bias condition has the highest average final reward.
The "Both" bias leads to the highest proportion of good runs, followed by the "Positive signalling" bias and the "Positive listening" bias.
The performance of the "Positive listening" and "Positive signalling" biases is significantly different from the performance of the "No bias" and "Social influence" biases.
Table 2 provides information on the proportion and average reward of good runs.
The mean visit time for the modified runs is 36.1±3.3.
The Baseline model has the lowest BLEU score for both BiRNN and CNN.
The +Syn (2L) + Sem (2L) model has the highest BLEU score for both BiRNN and CNN.
The "VecMap" model performs better than the "MUSE" model on the Wikipedia EN-ES cross-lingual natural language inference task.
The "MeemiVM" model performs better than the "VecMap" model on the 8K cross-lingual natural language inference task.
The supervised model performs better on the Wikipedia EN-IT task.
The MeemiMS model performs better on the Social media EN-DE task.
Table 2 compares the results achieved with different models in comparison with baseline approaches.
The "Convolutional-IC" model achieves the highest Cross-Entropy Loss ROUGE-L score among all the models mentioned in Table 2.
The "Adv-bs" model has a higher vocab size compared to the "Base-bs" model.
The "Adv-bs" model has a higher number of novel captions compared to the "Base-IC" model.
The table provides the number of annotators for different languages.
The total number of annotators for all languages combined is 282.
Table 9 shows the translation results (BLEU) of 1/0 reward.
Our model achieves higher BLEU scores than NMT for all three language pairs: en-de, en-fr, and en-es.
The "Ours" system outperforms the "NMT" system in terms of translation count for all language pairs.
The "Ours" system outperforms the "NMT" system in terms of translation count for the en-de language pair.
The reported score for the en-fr translation using NMT is 58.95.
Our model achieved a score of 57.49 for en-es translation in the test set.
The perplexity decreases as the memory size increases.
The usage percentage decreases as the memory size increases.
Models with 262k memory slots and without product keys have a perplexity of 22.1477.
Models with 590k memory slots and with product keys have an inference speed of 36.2k words per second.
The table shows the results for entity linking on the validation set, given the underlying entity detection model.
The scores for entity linking increase as the value of "R@ N" increases.
The BiGRU model has the highest R@1 and R@5 scores among all the models.
The table provides the performance of the statistical model with L2-regularized LR for different types of events.
The recall values for the event classes range from 8.28% to 61.90%.
The F1 score for all events combined is 52.08.
The recall for all regulation events combined is 11.41.
The recall rate of all events in the rule-based model before expert intervention is 38.04.
The F1 score for protein catabolism in the rule-based model before expert intervention is 68.57.
The F1 scores for the "Gene_expression" and "Protein_catabolism" events are both above 65%.
The recall, precision, and F1 scores for the "Regulation" events are all below 20%.
Table 1 provides the development results for spinal models, including labeled precision, recall, F1 for constituents, and unlabeled attachment score against Stanford dependencies.
The spinal model with dummy spines achieves an unlabeled attachment score of 93.30.
"Our method" outperforms all other models in terms of BLEU and METEOR scores for both beam size 1 and beam size 10.
Increasing the beam size improves the BLEU and METEOR scores for all models.
Our method outperforms all other models in terms of BLEU and METEOR scores for both beam sizes 1 and 10.
Increasing the beam size improves the performance of our method, as indicated by higher BLEU and METEOR scores.
The "Fuzzy Logic" article has the highest number of total edits among the representative scientific Wikipedia articles.
The "Javascript" article has the highest number of reference edits among the representative scientific Wikipedia articles.
The highest precision scores are achieved with k=10 and k=5.
The highest recall scores are achieved with k=10 and k=5.
The survival probability for Ecstasy is higher than the survival probability for fentanyl.
The survival probability for LSD is higher than the survival probability for cocaine.
The model "Doc2Vec + LIWC + drugs + keywords" has the highest accuracy and F1 score among all the models.
The model "LIWC + drugs + keywords" has a higher accuracy than the model "Doc2Vec + drugs + keywords".
The model using drugs, keywords, and LIWC features has the highest C-Index score.
The model using drugs, keywords, and LIWC features has a test-set C-Index of 0.820.
The "Sixltr" feature has the highest mean value among all the LIWC features.
The "WPS" feature has the highest value in the 75th percentile among all the LIWC features.
The number of training examples increases as the speaker ID increases.
The number of examples for each speaker ID in the ensemble is the same as the "Speaker-Dependent" row.
SentiBERT has the highest accuracy among all the models evaluated in the table.
BERT with GCN has a higher accuracy than both BERT with Tree-LSTM and BERT with Mean pooling.
The table presents the percentage increase in error of selector-predictor highlight methods compared to an equivalent architecture model.
The values reported for SST-2 and SST-3 in the "lei16" row are from their own experiments, while the values for SST-5 and AG News are from previous work.
The table shows the performance of four different models on different datasets.
The "lei16" model outperforms the "Full text baseline" model on the AGNews dataset.
The "Ours" method outperforms all other methods in terms of ROUGE-1 and ROUGE-2 scores.
The "Ours" model outperforms both the "Regression" model and the "Wang et al." model in terms of ROUGE-1 score.
The "Ours" model outperforms both the "Regression" model and the "Wang et al." model in terms of ROUGE-S score.
Our method achieves the highest nDCG scores compared to all the baselines.
BiPACRR achieves the highest precision at rank 1 and the highest R-Precision score among all the models.
Table 2 presents an ablation study of the method, where different variations of the model are tested.
The full model performs better than the variations without term importance, ontology similarity, and only overlap score.
The accuracy for Fine-tuned BERT at distance 4 is 55.88.
The accuracy for Fine-tuned BERT with path (hierarchical) at distance 3 is 62.51.
The accuracy of the "Fine-tuned BERT" model increases as the distance ("d") value increases.
The "Fine-tuned BERT" model outperforms the "BOW + Feature based LR" model in terms of accuracy for all "d" values.
Table 3 provides information about the vocabulary length.
The vocabulary length for m=1 is 733,392.
Table 6 provides information about the number of errors solved for different parametrizations.
The parametrization with dim = 500 and n = 10 solved 873 errors.
The "Our work" system achieves higher test accuracy than all the previous works mentioned.
The "atis2007" system achieved the highest test accuracy among the previous works mentioned.
The "one-to-one" model architecture has a parameter size of 28 million.
The "one-to-many" model architecture has a step time of 0.66.
The system built on a concatenation of the data performs better on "Arabic-English ALL" compared to "Arabic-English TED".
The system built on a concatenation of the data performs better on "German-English OD→TED" compared to "German-English ALL".
Arabic-English OD→TED performs better on average than Arabic-English ALL.
German-English EP→CC→TED performs better on average than German-English ALL.
Table 5 provides the results of systems trained on a concatenation of selected data and on a concatenation of all available data.
Table 6 compares the results of balanced ensemble (ENSb) and weighted ensemble (ENSw) with the best individual model and the concatenated model.
The average performance of the Arabic-English ALL dataset is 28.9.
Table 5 compares the performance of different methods in abstractive summarization and image captioning tasks.
The "Transformer (re-implementation)" method achieves a ROUGE-1 score of 39.3 in abstractive summarization.
Table 2 shows the experimental results of machine translation in terms of BLEU.
The addition of Cross-View Decoding improves the BLEU scores for all language pairs.
The GCA method achieves the highest scores in both the Transformer and DynamicConv models.
The GCA method performs better in the Transformer model than in the DynamicConv model.
The table shows the results of different cross-view strategies on the IWSLT DE-EN dataset in terms of BLEU.
The Full Model outperforms all other strategies in terms of BLEU on the IWSLT DE-EN dataset.
The table presents the results of human evaluation on the IWSLT DE-EN dataset in terms of faithfulness and fluency.
The table shows the classification results obtained with automatic English translations by three models and gold-standard English and untranslated German/Italian tweets.
MO-Reinforce achieves higher F1 scores than Generic and Reinforce in all translation scenarios.
The SemBias dataset is divided into two subsets: SemBias (subset) and SemBias.
The GN-GloVe embeddings have the lowest percentage for the stereotype category in both the SemBias and SemBias (subset) datasets.
Both GloVe and Hard-GloVe have the same accuracy scores for the analogy tasks.
GN-GloVe-L1 has the highest scores for both similarity tasks.
Table 1 shows the precision at different ranks (P@1, P@5, P@10) for the L2-Distance Baseline and the IESG Model (proposal) in the image to word vector matching task.
The IESG Model (proposal) achieves a higher precision at rank 1 compared to the L2-Distance Baseline in the image to word vector matching task.
The table shows the results of word-image vector matching.
The baseline model achieves a P@1 of 28.3, a P@5 of 46.6, and a P@10 of 56.9.
Table 3 provides word similarity evaluation on WordSim353 using different models.
The similarity and relatedness scores for the IESG model in WordSim353 are 0.680 and 0.593 respectively.
Table 5 presents the macro F1-scores of four baseline methods on the CLI dataset.
The effects of both "Scenario" and "Strategy" are statistically significant.
The number of degrees of freedom for "Scenario" is 2, for "Strategy" is 3, and for "Scenario*Strategy" is 6.
Table 1 is a stimulus template for the footbridge scenario involving a runaway trolley and a large man.
The persuader's message in Table 1 proposes pushing the large man down onto the tracks to save the five people.
The table shows the correlations between the measurement variables (PANAS POS, PANAS NEG, PANAS, CES-D) and the model score.
The highest correlation value in the table is 0.255, which represents the correlation between the model score and the CES-D variable.
The table shows the results of intent classification using different models on three different domains.
The "bert-large" model outperforms the "use-large" and "ConveRT" models in classifying intents in the shopping domain.
The R100@1×100% score on the Reddit test set for the ConveRT (single-context) network is 68.2.
The R100@1×100% score on the AmazonQA dataset for the ConveRT (multi-context) network is 71.8.
The Bi-encoder Humeau et al. (2020) system achieves the highest scores for both R100@1 and MRR on dstc7-ubuntu.
The ConveRT (multi-context) system achieves the highest scores for both R100@1 and MRR on dstc7-ubuntu.
CEV has the highest value among all the confidence measures in Table 1.
CEV shows the largest increase in value compared to the other confidence measures in Table 1.
Table I displays the results of unlabeled data methods for stopping AL.
The SC 2000 dataset has the highest average number of annotations at the automatically determined stopping points.
The dataset 20NewsGroups has the highest difference between SP and the Threshold among all datasets.
The dataset WebKB-student has the highest SP value among all datasets.
The "Entity-Destination" relation category has the highest precision, recall, and F1-score compared to other relation categories.
The use of the indicator sequence (IS) improves the precision scores for all relation categories.
"Indicator-aware BERT (Ours)" achieves the highest F1 score among all the models listed in the table.
Adding additional features such as POS, WN, WAN, NER, DEP, and LET improves the F1 scores of the models.
The Transformer-small model performs better on the translation from German to English than on the translation from English to German.
The best layer for the Transformer-big model is 2.
The table presents different existing statistical and neural methods for alignment.
The "align-att-mtl-fullc" method modifies both the training loss and the Transformer architecture, and it uses full-context to induce alignment at test time. The AER for this method is 24.3.
The table shows the validation AER for layer selection.
The AER for layer 2 in the translation from English to German and back to English is 47.0.
The "None" approach has a higher accuracy than both the BERT and ESIM + ELMO approaches for all sample sizes.
The accuracy of both the BERT and ESIM + ELMO approaches increases as the number of samples increases.
The accuracy of the models improves as we move from "fastText" to "BERT" in the "Test" column.
"BERT" achieves the highest accuracy among all the models in both the "Val." and "Test" columns.
Table 2 displays the performance on the HellaSWAG dataset using different supervision approaches.
The accuracy of the BERT model on the HellaSWAG test set is [BOLD] 47.3.
The "Ours PT (Full Model)" approach has the highest accuracy of 38.4.
The "PT + BERT" approach has the highest accuracy of 39.7.
Our method has a higher clustering performance compared to the Var. Autoencoder, Rel-LDA, and HAC methods.
The Var. Autoencoder method has a higher clustering performance compared to the Rel-LDA and HAC methods, but lower than our method.
Table 1 compares the performance of different features for clustering.
The F1 score for Dependency Re-Weighted Emb. is 19.5.
The "Adv. + ref. + CSLS" method outperforms the "Adv. + ref. + NN" method in all language pairs.
The translation performance for the "en-es" language pair is better than the "en-zh" language pair.
Table 7 compares different functions in CSLS on four language pairs.
The scores for the "en-es" and "es-en" language pairs are the same using the Linear function in CSLS.
Table 8 compares the performance of different alignment methods on publicly available aligned vectors for 28 language pairs.
The "Full RCSLS" method outperforms the "Original RCSLS" method in terms of performance.
The models used for prediction are LR + ℓ1, RFC, SVM + ℓ1, SVM + ℓ2, MLP, and RL(T=1), RL(T=3), RL(T=5), RL(T=10), RL(T=15), RL(T=20), RL(T=25), RL(T=30), RL(T=35).
The highest AUC value is [BOLD] 0.818±0.102 in the row with RL(T=35).
Table 8 presents an ablation study on the importance of translating OOV words.
The model "BPE-MoS" with OOV translation achieves a higher BLEU score (30.19) compared to the same model without OOV translation (30.14) and the "Hybrid-LightRNN-MoS" model without OOV translation (30.07).
The model "Transformer-MoS (Big)" outperforms all other models in the "EN-DE" task.
The model "ott2018scaling" performs the best among all models in the "EN-FR" task.
The "DualTrain + SharedProjUp" model consistently outperforms the other models in terms of accuracy for all hidden dimensions.
Increasing the hidden dimension improves the accuracy of the "DualTrain + SharedProjUp" model.
The DualTrain + SharedProjUp model achieves an F1 score of 84.9 and an accuracy of 78.5 on the MNLI-mm test set.
The teacher BERTBASE model has a vocabulary size of 30522, no compression, and achieves an F1 score of 88.5 and an accuracy of 84.3 on the MRPC test set.
The top 3 recommendations in the "Recommended" category are 0.78, 0.57, and 0.55.
The top 3 not-recommended items in the "Not-recommended" category are 0.17, 0.09, and 0.10.
The M-RCNN model shows better performance than the F-RCNN model on both the development and testing sets.
Table 4 presents different settings for the SST dataset and their corresponding performance scores.
The BLEU score for the Authentic model after transfer learning on the EN→GU dataset is 9.1.
The BLEU score for the model \⃝raisebox{-0.9pt}{4} after generating synthetic data on the EN→KK dataset is 16.2.
The accuracy of word embeddings on the Skip-gram All model is 0.0% for the Our testset.
The accuracy of word embeddings on the SubGram All model is 0.0% for the Original semantic testset.
The table shows the BLEU scores and the difference between the baseline and direct transfer models for different translation directions.
The Transformed Vocab model outperforms the Baseline and Direct Transfer models in translating English to Russian.
The Transformed Vocab model outperforms the Baseline and Direct Transfer models in translating Estonian to English.
The table compares different approaches to replacing tokens in parent vocabulary for the language pairs EN→Estonian and Estonian→EN.
The approach of replacing tokens using unmatched random selection achieves the highest performance in the Estonian→EN language pair.
The child model based on the "Merged Vocabulary" method performs better than the child model based on the "Direct Transfer" method for translating from English to Estonian.
The child model based on the "Baseline" method performs better than the child model based on the "Child-specific voc." method for translating from Basque to English.
The "No-Mixing" approach yields the same BLEU score for both English→Estonian and Estonian→English translations.
The "Mix without tag" approach yields a higher BLEU score for Estonian→English translation compared to the "Mix with tag" approach.
The highest score for each language pair is achieved using the "Balanced Voc." method.
The "Transformed Voc." method performs better than the "Direct Transfer" method for the Russian→EN language pair.
The performance of the child model trained from the parent model improves as the size of the parent training data increases.
The performance of the child model trained from the parent model decreases rapidly during child training for models with a star.
The "None" frozen part configuration achieves the highest BLEU score in the "EN→Estonian" translation.
The decoder performs better in translating from Estonian to English than in translating from English to Estonian.
The Child BLEU score for the non-frozen parts is 20.07 ‡ when trained with EN→Estonian translation.
The Child BLEU score for the non-frozen parts is 7.87 when trained with Estonian→EN translation.
The table shows the results of transfer learning with modified parent word order, comparing the performance of the parent and the child.
Shuffling the sentences in the child results in a significantly lower performance compared to other modifications.
Table 5.11 provides various automatic scores on the English→Estonian testset.
The parent length for the baseline is 35326.
The BLEU n-gram precisions for the English→Czech translation are 51.7/24.6/13.7/8.1.
The Parent BLEU score decreases as the number of words in a sentence increases.
The Parent model generates more words on average than the Child model.
Table 5.15 compares different stages of the parent model and learning rate.
The learning rate at 800k is 5.45.
AutoVOT, DeepVOT, and DeepSeg. are three different models.
DeepSeg. has a proportion of differences between automatic and manual measures falling at or below 2 msec of 78.2%.
The AutoVOT model has a proportion of differences between automatic and manual measures falling at or below a tolerance value of 10 msec of 89.9%.
The DeepVOT model has a proportion of differences between automatic and manual measures falling at or below a tolerance value of 50 msec of 98.7%.
The dataset "PubMedDS" has the highest number of categories compared to the other datasets.
The dataset "PubMedDS" has the highest number of instances in the "NCBI" category compared to the other datasets.
Table 1 provides details of the medical entity linking datasets used in the experiments.
The NCBI dataset has 792 documents, 7,645 sentences, 6,817 mentions, and 1,638 unique concepts.
Table 3 evaluates the effect of using WikiMed and PubMedDS for entity disambiguation.
Using both WikiMed and PubMedDS for entity disambiguation achieves the highest performance across all datasets.
The overall performance of the model is better than the performance on individual fields.
The seller field has the highest performance among all the fields.
The ML+RL model with intra-attn has the highest ROUGE recall scores for both R-1 and R-2.
The ROUGE recall scores increase as more words are included in the summary.
The table shows the classification accuracy of different models on the TripAdvisor and Yelp∗ datasets.
The combination of N-gram, Consistency, and Activity+ features achieves the highest classification accuracy on the Yelp∗ dataset.
In the CE domain, the MYelp (C-SVM) model performs better than the Baseline model in terms of Kendall-Tau-B and Kendall-Tau-M scores.
In the Sports domain, the MYelp (C-SVM) model performs slightly better than the Baseline model in terms of Kendall-Tau-B and Kendall-Tau-M scores.
Table 5 displays the variation of Kendall-Tau-M (τm) correlation with the number of reviews per item for different domains.
The Kendall-Tau-M correlation generally increases as the number of reviews per item increases.
The BLSTM 8 states + ILP model has the highest F-score among all the models.
The CRF 17 states model has a precision of 92.9 and a recall of 76.1.
The BLSTM 8 states + ILP model outperforms other state-of-the-art methods in terms of F-score.
The BLSTM 8 states + ILP model achieves an F-score of 85.9.
Table 6 compares the performance of CRF, LSTM, and BLSTM edit detection models with different feature sets on the dev set.
Table 7 displays the F scores of different types of edits for the CRF and BLSTM models on the dev set.
The CRF model achieves an F score of 94.9.
Table 7 presents the differences in means of psycho-demographic variables per gender and classification outcome.
The difference in means between "Female ✓" and "Male ✓" for the variable T/F is -0.17.
The average drop in performance for different sizes is -0.8, -1.7, and -3.5.
The performance on the 32KiB dataset for Amazon pol. is 92.1.
Table 1 shows the best ranked words based on entropy and norm on the Amazon full review dataset.
The word "mediocre" has an entropy value of 1399 and a norm value of 1.
The "Entropy pruning 2M" method uses the fewest number of embeddings.
The "Max-Cover pruning 2M" method achieves the highest coverage.
The average F1 value for the Bi-LSTM method is 94.14.
The F1 value for the Unified BERT method on the PKU dataset is 97.20.
"Unified BERT" achieves the highest average score among all methods.
"Multi-Task BERT" achieves the highest scores on "CITYU" and "NCC" datasets.
Table 5 displays the navigation accuracy for all models on the development set of TriviaQA-NoP.
The ensemble model with the answer as the threshold achieves the highest accuracy in both the development and test sets.
The table shows the navigation accuracy for different models on the development set of TriviaQA-NoP.
The navigation accuracy for the ensemble model with a threshold and l=5 is 35.0 on the development set of TriviaQA-NoP.
The training dataset FVC70 used SVM-RBF, Random Forests, Logistic Regression, and Decision Tree classifiers.
Random Forests classifier achieved the highest F1 score among all classifiers in the training dataset FVC70.
The average performance of CNN models is higher than that of Naive Bayes (NB) models.
The frequency of the "Adv. Heart Disease" condition is the highest among all the conditions.
The best performing model for data augmentation experiments is "real all + gen" with a score of 0.6217.
The model trained using "gen" data only has a score of 0.5769.
The LexMTurk F1 score for Biran is 0.119.
The BenchLS RE score for Paetzold-NE is 0.209.
Yamamoto has higher Precision scores than LexMTurk and BenchLS.
LSBert has a higher Precision score than BenchLS.
LSBert has a higher FRES score than REC-LS.
Access (2019) has a higher SARI score than PBMT (2020).
LexMTurk Large has a higher F1 score than LexMTurk Base.
The precision score is higher for the WWM model compared to the Base model in the SR task.
As the batch size increases, the inference time also increases for all models.
The inference time for the LaserTaggerFF model is the lowest for batch size 1 and highest for batch size 32.
The model "LaserTaggerAR" achieves the highest Exact score of 53.8.
The model "LaserTaggerAR" achieves the highest SARI score of 85.5.
The model "LaserTaggerAR" achieves the highest F0.5 score among all the models in the table.
The model "grundkiewicz2019neural" achieves the highest precision and recall scores among all the models in the table.
EmbN performs better on the English dataset than on the German dataset.
M3P with fine-tuning on each language performs better on the German dataset than on the English dataset.
The average result for all settings is 11.3.
The sentiment classification accuracy is higher for Chinese than for Japanese.
The sentiment classification accuracy is lower for Segmentation-free than for Korean.
The table shows the Spearman rank correlations of the word similarity task on two different Chinese corpora.
The "scne" score is the best score among all the scores in the "scne" column.
The table shows the Spearman rank correlations of the word similarity task on two different Japanese corpora.
The word similarity task performance is higher for the "Wiki." corpus compared to the "SNS" corpus.
Each method in Table 6 is evaluated on different genres.
Adding text information to the acoustic features improves the genre/show classification accuracy.
The table shows the genre/show classification accuracy using text-based LDA models.
As the number of domains increases, the genre classification accuracy using ASR output also increases.
Table 5 shows the genre/show classification accuracy using meta-data.
The accuracy for genre/show classification using only channel and time meta-data is 46.7%.
The model "BiLSTM-Attn + both scaffolds" achieves a macro F1 score of 63.1.
The model "BiLSTM-Attn w/ ELMo + both scaffolds" achieves a macro F1 score of 67.9.
The model "BiLSTM-Attn w/ ELMo + both scaffolds" achieves a macro F1 score of 84.0.
The previous state-of-the-art model by Jurgens et al. (2018) achieves a macro F1 score of 79.6.
The "STE-Diff" model has the highest accuracy, precision, recall, and F-measure scores among all the models.
The "LFTM" model has an F-measure score of 76.8.
Table 2 provides the Spearman correlation ρ×100 for different models on the SCWS dataset.
The table presents the topic coherence evaluation using the PMI metric with different numbers of top words.
The topic coherence value for the STE-Same model with T = 10 is 0.110.
The F1 score for the "Currency" field is higher than the F1 score for the "Number" field.
The precision for the "Total" field is higher than the precision for the "Order ID" field.
The LSTM model performs the best on the "Currency" field.
The LSTM model has a higher precision than the baseline model for the "Tax Total" field.
The model "Att. Encoder-Decoder" achieved the highest F1 score for the Intent task.
The model "Slot-Gated (Full)" achieved the highest F1 score for the Slot task.
The CMRC EM score for NEZHA base with News, PAPE, SL:128 configuration is 37.96.
The PD-NER Test score for News+Wiki+Baike, FRPE, WWM, SL:512 configuration is 98.10.
The BERT base model achieves a CMRC EM score of 64.06 and an XNLI Dev score of 78.75.
The NEZHA base-WWM model achieves an PD-NER Dev score of 81.37.
The table shows the BPC values for SRILM, BERT, and BigBird models.
The BigBird model has the lowest BPC value among the three models.
BERT-base achieves the highest scores in all three tasks (MLM, SQuAD, MNLI).
Combining the Random, Window, and R + W models into the Global + R + W model improves the scores for MLM, SQuAD, and MNLI.
The F1 micro-averaged score for the Arxiv dataset using the RoBERTa model is 87.42.
The excess fraction for the Patents dataset is 0.90.
"BigBird" has the highest F1 score among all the models.
"DeePromoter" has a higher F1 score than "CNNProm".
Table 11 provides information about chromatin-profile prediction using different models.
The BigBird model has the highest prediction accuracy among the models listed in Table 11.
The table provides the GLUE Dev results on base sized models, including BERT, XLNet, RoBERTa, and BigBird.
The models achieve accuracy scores ranging from 84.6 to 87.6 on the MNLI task.
SciBert performs better than BioBert on the BC5CDR dataset for the NER task.
SciBert performs better than BioBert on the ChemProt dataset for the 2-4 REL task.
The SOTA result for the JNLPBA dataset in the Bio NER task is 78.58.
The F1 score for the EBM-NLP dataset in the PICO task using Bert-Base Finetune is 72.28.
The table evaluates the English proficiency of learners and Reddit users.
The mean TU length is higher for Reddit users compared to learners.
Table 1 shows pairwise accuracies for the six relations using different models and representations.
The accuracy using the bag-of-words representation for the Noun-Adj relation is 78.09.
Table 2 presents the results of the ABX task using two different feature representations: Mel-Spec and MFCC.
The questions in the ASAP short answer scoring data set are numbered from 1 to 10.
The ASAP short answer scoring data set contains questions from the subject areas of Science, ELA, and Biology.
The ensemble model type has the highest average correlation among all the model types.
The gradient boosting model type has the highest weighted kappa score among all the model types.
The "ALL" feature set has the highest values for both "Average Correl." and "Average Weighted ASAP".
The "BASE+DEPS" feature set has the highest value for "Kappa".
The table shows classification accuracies on the Opusparcus test sets for models trained on 1 million positive sentence pairs.
The GRAN model outperforms the WA model on the English test set.
There are more sentences in English that have been mistakenly removed by the annotators compared to sentences in Finnish.
The number of sentences that are actually correct is less than the number of sentences that are not grammatical.
Table 6 shows the results on Opusparcus test sets for models trained on PPDB.
The model trained on PPDB achieves a score of 83.4 on the Opusparcus test set for English.
The table shows the vocabulary sizes for 2 million phrase pairs from Opusparcus.
The vocabulary sizes without any segmentation are 65437 for "de", 56571 for "en", 130879 for "fi", 69920 for "fr", 137942 for "ru", and 81407 for "sv".
Table 9 shows the results on Opusparcus test sets for GRAN models trained on 1 million positive sentence pairs.
The classification accuracy for the "de" language in the GRAN-M model is 78.2.
The "T2G2" model is used for both BLEU and SER metrics.
The BLEU score increases as the number of shots increases.
MultiWOZ dataset has fewer domains compared to the SGD-NLG dataset.
SGD-NLG dataset has a significantly larger number of slots compared to the MultiWOZ dataset.
The SC-GPT model has the highest BLEU score among all the models in Table 2.
The Copy model has the lowest SER among all the models in Table 2.
The approach "T2G2" achieves the highest BLEU score for both seen and unseen domains in the SGD-NLG dataset.
The "Naive" approach has a higher SER than the "SlotDesc" approach in both seen and unseen domains in the SGD-NLG dataset.
The Joint NLG performs better in terms of BLEU score compared to domain-specific (separate) NLG.
The Joint NLG has a lower Slot Error Rate (SER) compared to domain-specific (separate) NLG for all domains.
As the value of k increases, the Naive BLEU score and Naive SER score also increase.
As the value of k increases, the T2G2 BLEU score also increases.
SennaEmbeddings and Random have the same value of 50 for the "Embeddings" column.
SennaEmbeddings has the highest F1 score of 74.74.
The models used in the experiments are JRNN, LSTM, BiLSTM, RNN, and BiRNN.
The LSTM model has higher F1 scores compared to the other models.
SennaEmbeddings achieved the highest F1 score among all the embeddings.
Random embeddings with |d| = 50, |cw| = 3, and |h| = 100 achieved an F1 score of 78.79.
Table 8 shows the results for the WEIGHTED technique.
The "WDeps" embedding achieves the highest F1 score in the WEIGHTED technique.
The GoogleNews embeddings achieve the highest F1 score among the SennaEmbeddings, WikiDeps, and GoogleNews embeddings.
The GoogleNews embeddings have the highest F1 score among the SennaEmbeddings, WikiDeps, and GoogleNews embeddings.
The ARNN model achieves the highest scores in "Classification F1 +", "Classification F1 -", and "Classification F1 0" among all the models in Table 11.
The ARNN model achieves higher scores in all metrics with the Google embedding compared to the Senna and Wiki embeddings.
ARNN with Senna embeddings achieves the highest Tagging F1 single score and Tagging F1 joint score among all the models.
ARNN with Wiki embeddings achieves the highest Classification F1 + score among all the models.
The ARNN model outperforms the LSTM and RNN models in terms of F1 scores for both Tagging and Classification tasks.
The ARNN model with the Wiki embedding performs the best in terms of F1 score for the Classification task with label 0.
The "Our Approach + F *" method achieves the highest F-score, Precision, and Recall values among all the methods.
The performance of the "Our Approach" method improves when additional hand-crafted features are used.
Our Approach with the additional hand-crafted features achieves the highest F-score of 70.6.
BERT outperforms the Decom. Atten. w/ Glove and ESIM w/ Glove methods in terms of precision, recall, and F-score.
The highest correlation score on the Word Similarity Datasets is 51.10.
The correlation score for the "w2g" model on the YP-130 dataset is 45.07.
Table 4 includes word similarity evaluation on foreign languages.
The "de" language has the highest word similarity score in the "pft-gm" evaluation.
Table 1 provides information about the accuracy of the question classifier on questions from the OK-VQA/VQA v2 datasets and from Artpedia.
The question classifier has an accuracy of 0.868 on the OK-VQA/VQA v2 dataset and an accuracy of 0.938 on Artpedia.
The table evaluates different models including LM, LM∗, LM∗∗, LM∗∗-C, LM∗∗+PM+RM, Stress-BL, Rhyme-BL, and Rhyme-EM.
The Rhyme-EM model has an average F1 score of 0.71.
The accuracy of the model decreases as we add PM and RM to LM.
LM and LM∗∗ have higher accuracy compared to LM∗∗+PM+RM and LM∗∗+RM.
The F1 score for the Random-reduced subset is 56.85.
The Recall score for the disease_has_normal_cell_origin relation is 83.86.
The PRA-reduced method performs better than the Random-reduced method in terms of F1 score for the "may_prevent" relation.
The PRA-reduced method has a higher precision score than the Random-reduced method for the "may_treat" relation.
The table shows the performance (%) of different models on the test set.
The "RuEHR-BERT 265" model performs better than the "RuBERT 265" and "RuPool-BERT 265" models in terms of F1macro, F1weighted, MRR, Hit@1, Hit@3, Hit@5, and Hit@10.
The BLEU score increases and the TER score decreases when using ASR outputs with post-editing compared to using ASR outputs alone.
The length of the ASR outputs (+PE) is higher than the length of the ASR outputs.
The table provides intent classification performance by machine learning models and human intent analysts (IAs) at different rejection rates for various configurations.
Table 3 provides baseline scores in the cross-lingual setting for the "de", "fr", and "it" languages.
The fastText model achieves an F1-score of 69.9 for "favor" and 71.2 for "against" in the cross-lingual setting.
The majority class (global) baseline score is higher than the Cross-question de baseline score.
The M-Bert baseline has a lower Cross-topic Mean px score compared to the fastText baseline.
The performance of the models improves as the number of French training examples increases for the Spanish to French dependency parsing task.
The GAN framework outperforms the WGAN framework for the Spanish to French dependency parsing task.
The accuracy of the model improves as the amount of training data increases.
Adding 1k MT-FR training data leads to a larger improvement in accuracy compared to adding 1k FR training data.
The performance of the models improves as the number of French training data increases.
The GAN adversarial training method performs worse than other methods.
The OS+ metric has the highest value among all the evaluated metrics.
The Cos Sim AUC metric has the highest value among all the evaluated metrics.
There are multiple models with a perplexity score of 18.25.
The model with the lowest perplexity score is 18.12.
The kNN-LM model has the lowest test score.
The baseline model trained over 5 random seeds has a slightly higher test score than the Transformer XL model.
The "Baseline (Adaptive Span; 5 Runs)" model has performance results reported for both the text8 and enwik8 datasets.
The "Compressive" model has a performance result of 0.97 on the enwik8 dataset.
There are three different model pairs in the table.
The average attention distance for the Baseline - Sandwich model pair is higher than that of the Baseline - Baseline and Sandwich - Sandwich model pairs.
The "GloVe-100" vectors have a higher correlation with Simlex-999, MEN, WS353, WS353-SIM, WS353-REL, and ESL compared to the "SG-100" vectors.
The "GloVe-300" vectors have a higher accuracy on TOEFL and NP compared to the "SG-300" vectors.
The accuracy of response selection (RES) is higher when there are 10 candidate responses compared to when there are only 2 candidate responses.
When the context length (T) is not specified, the accuracy of pair selection (ADR-RES) is 50%.
The category "OWN" has the highest percentage among all the rhetorical categories.
The percentages of all the rhetorical categories add up to 1.
The table presents the results on Wiki-One (not filtered).
The GMatching(Best) model achieves a Hits@5 score of 27.1.
Table 2 provides information about the inference time of CogKR and GMatching for different candidate numbers.
The inference time for CogKR is higher than GMatching for both truncated (5,000) and full (4,838,244) candidate numbers.
MARN outperforms all other multimodal methods in emotion recognition on the IEMOCAP test set.
BC-LSTM performs the best in recognizing dominance among all the multimodal methods on the IEMOCAP test set.
The "Our base + graph (final)" model has the highest full repair rate of 66.4%.
The "Our base + self-attention" model has a full repair rate of 66.0%.
The table shows translation results on the IKEA dataset.
VAG-NMT performs better in English to German translation compared to English to French translation on the IKEA dataset.
The table represents human evaluation results for different models on MSCOCO, Multi30K, and IKEA datasets.
VAG-NMT outperforms Text-Only NMT on MSCOCO and IKEA datasets.
The table presents an ablation analysis on the visual-text attention mechanism in the Multi30K German dataset.
The VAG-NMT method achieves the highest BLEU score in the English → German translation task.
The "Ours" system outperforms the other systems in terms of Rouge-1, Rouge-2, and Rouge-SU4 scores.
The "Ours" system performs better than the "Lead" system in terms of Rouge-1 score.
Table 2 compares the performance of teacher and student models with different training strategies.
The "Teachers (avg)" model outperforms the other models in terms of MAP, P@20, and nDCG@20.
The student model with noisy aggregation performs better than the noisy aggregated teacher model in terms of MAP, P@20, and nDCG@20.
The "Multi-Granularity (5)" model achieves the highest F-1 score in the downstream task of dialog act prediction.
The F-1 score of the "Random Init" model in the downstream task of dialog act prediction is 28.75.
The "Highest Granularity" model has the highest F-1 score for BoW.
The "Highest Abstraction" model has the lowest F-1 score for DA.
The "Multi-Granularity (5)" model achieves the highest F-1 scores for both BoW and DA.
The "Fine-tuned" model performs better on the BoW task compared to the DA task.
The method "BCNN two-conv" has a higher accuracy than the method "BCNN one-conv".
The method "ABCNN-3 two-conv" has a higher accuracy than the method "ABCNN-2 two-conv".
The ABCNN-3 model with one convolutional layer achieves an accuracy of 84.8%.
The majority voting method achieves an F1 score of 79.9.
The number of words annotated in the morphological m-layer is larger than the number of words annotated in the analytical a-layer in the PDT 3.5 train set.
The number of sentences annotated in the morphological m-layer is larger than the number of sentences annotated in the analytical a-layer in the PDT 3.5 development set.
The table presents the results for POS tagging, lemmatization, and dependency parsing on the Czech PDT UD 2.2 treebank.
The CMR+f model has the highest score in terms of Appropriateness Nist.
The Seq2Seq model has the lowest score in terms of Grounding F1.
The precision for the "Neur." syndrome using the Naive Bayes model is 98.2.
The precision for the "Con." syndrome using the SVM model with a radial basis function kernel is 100.
The precision of dpseg (monoling.) using MFCC features is 27.9.
The recall of true phones using SVAE model is 83.5.
The average number of phones per sentence is higher for MFCC HMM compared to MFCC SVAE, MBN HMM, and MBN SVAE.
The average number of tokens per sentence is higher for MFCC SVAE compared to true phones, MFCC HMM, and MBN SVAE.
"BERT + Diff-Net" achieves the highest test accuracy of 90.1 on the Story Cloze Test v1.0 dataset.
"BERT" outperforms "Transformer LM" in terms of test accuracy on the Story Cloze Test v1.0 dataset.
The "PEE" model achieves the highest BLEU4 score among all the models.
The "PEE" model achieves the highest average F1 score among all the models.
The precision@1 for Procrustes (R) on the en-de language pair is 73.5.
The precision@1 for CSLS-D (w/ R) on the es-en language pair is 84.9, which is the best overall unsupervised number.
The precision of BLI for the language pair English to Estonian using CSLS-D (with R) is 37.0.
The precision of BLI for the language pair English to Finnish using 5k+Procrustes (R) is 47.3.
Removing the Lxy & Lyx components from DeMa-BME results in lower scores for all language pairs.
Removing the Lsup component from DeMa-BME results in significantly lower scores for all language pairs.
Table 2 shows the results on the test dataset with variations of model initialization and fine-tuning in the decoder.
The highest BLEU score is achieved when the encoder and decoder are both initialized with fasttext embeddings and the decoder is fixed.
The "Ours" model achieves the highest BLEU scores for both the validation and test sets.
The "Ours" model has a lower METEOR score compared to the other models.
Table 3 shows the Spearman's correlation for HyperLex nouns.
The correlation coefficient for the word pair "DOE-A" is 0.590.
The method "DOE (KL)" achieves the highest test accuracy of 92.3% among all the methods listed in the table.
The method "VOE" achieves a higher test accuracy than the method "VOE (symmetric)".
The correlation coefficient (ρ) increases as the number of negative samples increases for both S1 and S1 + S2.
The correlation coefficient is higher for S1 + S2 when the number of negative samples is 1x and 2x, but it becomes lower for S1 + S2 when the number of negative samples is 3x, 5x, and 10x.
The en-cs translation has more errors than the en-de translation.
The de-en translation has the highest average number of errors per document segment.
Table 3 presents the results of an ablation study for the AE-binary-SP model with different choices of λsp, evaluated with test accuracy on the MR dataset.
The AE-binary-SP model achieves the highest test accuracy on the MR dataset when λsp is set to 0.8.
The table shows the system performance with different dimensions of word embeddings, using either randomly initialized or pre-trained word embeddings.
The PCC on the validation set is higher for the WI word embeddings compared to the RI word embeddings.
The loss function "LPCC" achieves the highest PCC value of 0.8174.
The loss function "LKLD" achieves a PCC value of 0.6839.
Table 1 shows the perplexity scores of various models on the Penn Treebank and WikiText datasets.
The perplexity score of the Transformer-XL model on the WT103 dataset is 18.3.
The canvas (mask only) method has a higher accuracy than the mask-and-infill (MLM) and BLM (ours) methods.
The BLM (ours) method has the highest BLEU score among all the methods.
The table compares the performance of three different models: Base, Shallow Fusion, and Deep Fusion.
Deep Fusion outperforms both Base and Shallow Fusion in terms of test performance.
Table 2 compares the performance of three different models: CNN-baseline, BiLSTM-baseline, and CNN-50k.
The F1 score of CNN-50k-mittens model for the class "ϵ" is 69.7%.
The table compares the performance of two models, LguidedLearn-1 and LguidedLearn, on different datasets.
LguidedLearn outperforms LguidedLearn-1 in terms of accuracy on all datasets.
The table presents the results of an ablation analysis of the LguidedLearn framework, comparing it with a version without the label-guided encoding layer.
The LguidedLearn model outperforms the Label-guided (w/o) model in all four evaluation datasets: 20NG, R8, R52, and Ohsumed.
The BiLSTM model performs better on the Arxiv abstracts lexicon dataset than on the Arxiv abstracts random dataset.
The Adaptive Dropout model performs better on the IMDB reviews lexicon dataset than on the IMDB reviews random dataset.
Swivel performs the best among SGNS, GloVe, and Swivel vectors in the WordSim Similarity task.
The percentage of new entities in the validation set decreases as the number of hops increases.
The percentage of new entity pairs is higher in the validation set compared to the test set.
The table shows the accuracies of defensively distilled networks trained with different temperatures against the test set of the specified dataset.
The "AG" dataset is tested with different values of T (10, 20, 30, 40).
The F1 score is higher than the accuracy score in the Google Analogy test set.
Precision is generally higher than recall in the Google Analogy test set.
The highest accuracy score on the DiffVec test set is [BOLD] 31.3.
The highest F1 score on the DiffVec test set is [BOLD] 24.2.
Table 3 presents the results without position weighting.
Rik iteration R2 has the highest accuracy score in the Google Acc column.
The Google model performs better than the DiffVec model in terms of accuracy.
The Google model performs better than the DiffVec model in terms of F1 score.
The "DiffVec" model outperforms the "Google" model in terms of accuracy for the relation induction task.
The "CBOW DiffVec" model achieves a higher F1 score than the "CBOW Google" model for the relation induction task.
The table presents the WERs and OOV rates for LSTM RNN CTC word acoustic models.
The in-vocabulary WER for the 25k Word model is 14.5%.
Table 1 provides the Word Error Rates (WERs) for conventional and CTC initialization of LSTM RNN acoustic models.
The WERs for CD state, CI phone, and CD phone are provided in Table 1.
The table shows the Word Error Rates (WERs) for sequence-trained LSTM RNN models using different labels and initialization methods.
The EmbDrop rate used in the experiments is the same as in the Merity et al. (2017) paper.
The PPL on PTB (EmbDrop=0.4) Test for the AWD-LSTM-ANI-GRU-1/10 variant is 56.3.
Table 2 provides information about the training, validation, and test PPL of LSTM language models before and after using ANI in each group on the WT103 corpus.
The training PPL is lower in Group III with ANI compared to Group III without ANI.
The Adam optimizer is used for both PeerRead classification and S2ORC regression.
The dropout probability is 0.5 for PeerRead classification and 0.2 for S2ORC regression.
Table 5 describes the effect of the length cutoff policy on the number of words distribution.
The median number of words per example is 5514 when using a length cutoff of 360 sentences.
Table 2 shows the parsing results (EVALB) on PTB Sections 22 (DEV) and 23 (TEST).
Table 3 provides the parser Recall score in recovering ACC conjunct spans on the Regents dataset, comparing the results of ACCPTB and ACCOUR.
The parser Recall score for ACCOUR in recovering ACC conjunct spans on the Regents dataset is 64.8.
The E-BP_4 model has the highest average acceptance score among all the models.
The E-BP_4 model has the highest positivity score among all the models.
The average accuracy of the context-dependent behavior recognition model is 60.43%.
The accuracy of recognizing positive behavior by the context-dependent behavior recognition model is 63.21%.
Table 5 shows the behavior binary classification accuracy in percentage for reduced context-dependent behavior recognition from emotion-informed embeddings.
HBMP outperforms InferSent in terms of accuracy for all three categories: Entailment, Contradiction, and Neutral.
HBMP performs better than InferSent in terms of accuracy for the Contradiction category.
Table 5 compares the model performance of HBMP and InferSent on various datasets.
HBMP performs better than InferSent on the "SciTail" dataset.
Table 6 presents the SNLI confusion matrices for HBMP and InferSent.
The recall for the "entail" class in HBMP is 90.5%.
The recall for the "entail" class in the HBMP model is 80.3%.
The precision for the "neutral" class in the InferSent model is 65.8%.
The recall for the predicted entailment labels using the HBMP model is 82.0%.
The number of instances predicted as neutral using the InferSent model is 2,008.
Table 11 provides the breaking NLI scores for different categories.
The HBMP model achieves the highest scores in most categories compared to the other models.
The 1200D HBMP model outperforms the 600D HBMP model in both the InferSent and SkipThought tasks.
The 1200D HBMP model performs better than the InferSent model in the MR task.
The "1200D HBMP" model performs best on the "TreeDepth" task.
The "600D HBMP" model performs better than the "InferSent" model on the "CoordInv" task.
The NeuralEGrid model has the highest AC1 Agreement score among all the models.
Summarization Systems S-26 has a higher average ranking than Summarization Systems S-13.
The UnifiedModel outperforms all other models in the Local Discrimination task, achieving the highest accuracy in all four cases.
The EGrid model shows an improvement in accuracy as the window size increases in the Local Discrimination task.
The "UnifiedModel" has the highest accuracy and AC1 agreement among all the models.
The "LexNeuralEGrid" model has a higher accuracy and AC1 agreement compared to the "NeuralEGrid" model.
Table 6 provides information about AC1 agreement and average rankings of different summarization systems assigned by the models.
LexNeuralEGrid has the highest AC1 agreement among all models.
The best performing coherence model on the Ubuntu dataset has an R@1 score of 0.761.
The UnifiedModel has the highest MRR score of 0.13 on the Advising dataset.
The system described as "this work" achieves the highest F1 scores for both WSJ20test and CTB20 datasets.
The "DB-PCFG" system does not have F1 scores reported for the NEGRA20 dataset.
Table 9 compares stochastic gradient descent (SGD) and noise contrastive estimation (NCE) for class factored models on the fr→en data.
The BLEU score for SGD is 31.75 and the BLEU score for NCE is 31.55.
Table 7 provides a qualitative analysis of clustering strategies on fr→en data.
The combination of "Class Factored" and "Brown clustering" achieves the highest BLEU score.
Table 11 provides a comparison of class factored models with and without diagonal contexts trained with noise contrastive estimation on the fr→en data.
The perplexity for the "Diagonal" model is 115.119.
The MRR value for the combined correct list set is higher than the MRR value for the intersection set.
The MRR value achieved by our method is higher than the lower bound MRR achieved by the BiLSTM model.
The "+SpellCheck" system outperforms the "MLConvembed" system on the JFLEG Dev F0.5 and JFLEG Dev GLEU metrics.
BiLSTM has the highest precision and F0.5 score among the three architectures.
MLConv has the highest recall among the three architectures.
Table 4 provides the results of different embedding initializations on the CoNLL-2013 test set.
The percentage of original constraints satisfied increases as the number of ReLUs increases.
The table provides privacy comparisons based on different values of Nw (worst-case).
As the value of ε increases, the expected value Nw hyp-100 also increases.
The table represents correct author predictions and lower values indicate better performance.
As the threshold increases, the number of correct author predictions increases for Pan-12 set-D.
The table shows accuracy scores on different classification tasks.
The accuracy score for the original fastText-BoV model is 78.20.
The table shows the correlation between various factors and article credibility rating.
The user expertise positively influences the user-user rating.
Alprazolam, niravam, and xanax are used to relieve symptoms of anxiety, depression, and panic disorder.
Metformin, glucophage, glumetza, and sulfonylurea have 779 users.
Metformin has a high sensitivity score of 79.82% and a high specificity score of 91.17%.
Levothyroxine has the highest Rare SE Recall score of 86.08% among all the drugs.
The Kappa score for Alprazolam (Xanax) is higher than the Kappa score for Levothyroxine (Tirosint).
The NDCG score for Levothyroxine (Tirosint) using the CRF model is higher than the NDCG score for Alprazolam (Xanax) using the CRF model.
Our model, User SVR, has the lowest MSE among all the models listed in the table.
Our model, User SVR, outperforms the Latent Factor Models (LFM) in terms of MSE.
The "Our Model: CCRF+SVR" performs the best among all the models in predicting aggregated article credibility rating.
Adding the "Users" feature to the model improves the prediction accuracy of the aggregated article credibility rating.
The table shows the mean squared error (MSE) for rating prediction, and the model described in the table performs better than competing methods.
The MSE for rating prediction using the Continuous experience model on the BeerAdvocate dataset is 0.247.
Table V.3 compares the performance of "Our model" with baselines in a prediction task.
The "Our model" outperforms the baselines in terms of the Squared Correlation Coefficient (R2) for all categories.
Our model performs worse in ranking reviews for the "Elect." category compared to the other categories.
Our model performs better in ranking reviews for the "Movies" category compared to the other categories.
The table shows the variation of Kendall-Tau-M (τm) correlation with the number of reviews per item for different domains.
The overall Kendall-Tau-M (τm) correlation increases as the number of reviews per item increases.
The "Supervised" model has the lowest CER percentage and bitrate values compared to other models for both English and Indonesian languages.
The "VQ-VAE-x8" model has the highest ABX percentage values for both English and Indonesian languages.
The VQ-VAE model performs the worst on decoder output with no speaker conditioning and the best on latent.
The Filterbanks model has the highest bitrate.
The BLEU score increases as the number of lines in the dataset increases.
Adding 1 million authentic sentences to the base set improves the performance of the models.
The table shows the results for language models trained on the Wikipedia dataset.
The perplexity score for the language model trained on Stack with ut=1 is 92.81.
The accuracy of the Stack (ut=1) classifier is the same as the accuracy of the Stack classifier.
The accuracy of the Stack (dt=1) classifier is higher than the accuracy of the Stack classifier.
The complexity of the German language is higher than that of the Dutch language for both the trigram and LSTM models.
The difference in complexity between the original and artificial languages is statistically significant for both the trigram and LSTM models.
The MRR@10 scores increase as the depth increases for all retrieval methods in both the MS MARCO passage and document tasks.
The DE-BERT retrieval method has a higher MRR@10 score compared to the BM25 retrieval method in the MS MARCO passage task at a depth of 200.
The hybrid retrieval model achieves the highest short answer exact match on the Natural Questions open-domain test set at a retrieval passage length of 200.
The ORQA lee-etal-2019-latent reading model achieves a short answer exact match of 33.3 when reading 8 text blocks of 400 tokens.
The hybrid model achieves the highest MRR score on the TRECCAR dataset.
DE-BERT outperforms ME-BERT in terms of MRR on the MS-Passage dataset.
Table 2 shows translation results (BLEU) for Fr ↔ De experiments.
The highest BLEU score for Fr*-De translation is achieved in the "Fr → De newstest2012" experiment.
Table 4 provides translation results (BLEU) for Fr ↔ De experiments evaluated on the newstest 2013 set.
The BLEU scores for Fr → De NMT, Fr → De SMT, De → Fr NMT, and De → Fr SMT are higher than the corresponding scores for Fr-De* and Fr*-De.
The Proposed CNN approach achieves the highest accuracy of 21.47 for the WUPS @0.9 metric.
The Human Answers approach achieves an accuracy of 50.20.
The Proposed CNN model achieves the highest performance in terms of accuracy, WUPS @0.9, and WUPS @0.0 among all the models evaluated in the table.
The GUESS model achieves the lowest accuracy among all the models evaluated in the table.
MDR performs the best on the OntoNote and Pro-Stereotype tasks among all the models.
MDR-Hard has the smallest difference in performance between the Anti-Stereotype and Pro-Stereotype tasks among all the models.
"MDR-GloVe" has the highest accuracy among all embeddings.
"MDR-Hard" has the smallest p-value among all embeddings.
The table provides the performance of different models on the IWSLT14 GE-DE test set.
The †‡Mae-7 model achieves the highest BLEU score of 35.5.
The table shows the performance of different variations of the Transformer model on WMT14 EN-DE translation test.
The †‡Mae-7 model achieves the highest BLEU score among all the models listed in the table.
Table 4 provides language modeling performance on the WikiText-103 test set.
The model "†‡Mae-7" has a perplexity of 18.71.
The table provides performance results for different models on the WMT14 development set when only one expert is used for each multi-head attention layer.
The table shows the performance decrease for different models on the WMT14 development set when only one expert is used for each multi-head attention layer.
The "FtAll" method achieves the highest BLEU score of [BOLD] 31.8.
The "NoFt" method does not require updating any parameters or performing any gradient steps.
The highest accuracy for the target domain is in the "CD" row with a value of 0.669.
The average accuracy across all domains is 0.601.
Our approach using Named Entity features achieves the highest accuracy on the Yelp dataset compared to all the baselines.
Our approach using Domain Adversarial Multi-Task Learning (DAML) achieves the lowest RMSE on the Clothing dataset compared to all the baselines.
The table shows the results of different models used in the experiment.
Data-dependent codebook reestimation leads to the best reconstruction measured in bits per dim.
Table I shows the relationship between codebook usage and phoneme error rate (PER) on the supervised WSJ task.
The "no bottleneck" model has a phoneme error rate (PER) of 11.6 and the code perplexity is not applicable (N/A).
The addition of batch normalization and codebook learning rate improves the reconstruction bits/dim performance for the 5×1024-codeword VQ bottleneck compared to the 8×128-codeword VQ bottleneck.
The addition of batch normalization, data-dependent reestimation, and codebook learning rate improves the reconstruction bits/dim performance for the 8×128-codeword VQ bottleneck compared to the 5×1024-codeword VQ bottleneck.
The BERT-MRC+DSC model achieves the highest F1 score of [BOLD] 93.33 for the English CoNLL 2003 NER task.
The BERT-MRC+DSC model achieves the highest F1 score of [BOLD] 84.47 for the Chinese OntoNotes 4.0 NER task.
BERT+DSC achieves the highest F1 score on SQuAD v2.0.
XLNet+DSC achieves a higher EM score on QuoRef than BERT+DSC.
Table 9 describes the effect of DL and DSC on sentiment classification tasks using the BERT model with different training objectives.
The BERT model trained with the cross-entropy objective achieves an accuracy of 94.90% on the SST-2 sentiment classification task.
The table shows important features for KNN search using KIF and salient conversation features improve performance on both datasets.
The model's performance is 24.6 on the "Wizard of Wikipedia" dataset and 13.3 on the "Engaging ImageChat" dataset when using only the previous utterance as a feature.
The frequency of pairs decreases as the number of pairs increases.
The percentage of pairs with the same label decreases as the number of pairs increases.
As the number of training samples increases, the performance of the QA model improves for all three evaluation sets.
The augmented evaluation set shows better performance in terms of F1 score compared to the base evaluation set for the real-world evaluation set.
The character-based B-LSTM outperforms humans in terms of F1 score for the 5 most frequent emojis.
The average F1 score for humans is equal to the F1 score for the character-based B-LSTM.
The "Adapted-seq2seq (FTND16)" model has the highest average accuracy among all the models.
The "Seq2Seq w/ Attention" model has a higher average accuracy than the "Seq2Seq" model.
The "biSSNT+" model has the highest ROUGE-2 score.
The table includes information about the perplexity of the Seq2seq and biSSNT+ models.
Increasing the values of H (hidden size) and L (number of layers) results in lower perplexity values for both Seq2seq and biSSNT+ models.
The model (biSSNT+) outperforms the previous state-of-the-art on each morphological inflection dataset.
The model (biSSNT+) achieves the highest performance on the FTND16 dataset among all the models.
The CO2 emissions for "Air travel, 1 passenger, NY↔SF" is 1984 lbs.
The CO2 emissions for "NLP pipeline (parsing, SRL)" is 39.
There are 842 positive instances in the test set.
There are 165 negative instances in the test set.
The BL1 technique has a higher recall score than the BL2 technique.
The SVM technique has a higher F1 score than the MNB technique.
The 2-word window context has the highest F-score among all the word contexts.
The precision for the word LSTM is slightly lower than the precision for the 2-word window.
Table 6 shows the influence of different pretraining methods on the performance metrics of the baseline model.
Table 2 represents ablation tests of irony markers for Twitter.
The table includes results for both irony and non-irony categories.
The table compares the knowledge-seeking turn detection performances between the Anomaly Detection and Classification methods.
The Classification method achieves a higher Accuracy than the Anomaly Detection method in knowledge-seeking turn detection.
The table compares the knowledge selection performances of different methods, including retrieval and classification methods.
The classification method using BERT achieves the highest knowledge selection performance in terms of MRR@5, R@1, and R@5.
Table 5 presents automated evaluation results on knowledge-grounded response generation using two different methods: GPT-2 without knowledge and GPT-2 with knowledge.
The perplexity of GPT-2 with knowledge is lower than the perplexity of GPT-2 without knowledge, indicating better performance in terms of language modeling.
Adding one surrounding sentence improves the accuracy of the BERT nearest neighbor model.
Adding one surrounding sentence improves the accuracy of the Simple model.
The "Simple (1sent+1sur)" system performs better on average than the "GLU (1sent+1sur)" system.
The "GLU (1sent+1sur)" system outperforms the "IMS Zhong and Ng (2010)" system on the SE07 dataset.
Table 7 shows the test error rates (%) with multi-task fine-tuning using different methods.
BERT-CDPT-FiT achieves the lowest test error rate for the DBP task compared to other methods.
The table shows test error rates (%) on the IMDb and Chinese Sogou News datasets using different methods.
The "head-only" method performs better in terms of test error rate compared to the "tail-only" method on both the IMDb and Sogou datasets.
The test error rates decrease as the decay factor (ξ) decreases.
The test error rates decrease as the learning rate decreases.
Table 5 shows the performance of in-domain and cross-domain further pre-training on seven different datasets.
The sentiment Yelp P. dataset has the lowest performance in the sentiment domain.
The BERT-Feat model shows an average improvement of 2.50% compared to the BERT model on all datasets.
The BERT-ITPT-FiT model achieves the lowest test error rate of 1.93% on the DBP dataset.
The table shows the test error rates (%) on five text classification datasets for different models, both with and without in-domain pretraining.
The BERTLARGE + ITPT model has a lower test error rate on the IMDb dataset compared to the ULMFiT + ITPT model.
The table compares the word error rates of GMM and DNN methods on 339 words isolated word recognition.
The AAE DNN method achieves the lowest word error rate among all the methods.
The table shows the word error rates in % of AAE based recognizers with different numbers of AAEs and GMM mixture.
The word error rate for 128 AAEs and mixture 32 is 13.09%.
The word error rate for Phone DNN is lower than for AAE DNN when no language model is used.
The word error rate for AAE DNN is higher than for Phone DNN when a bigram language model is used.
When y0=1, it indicates that the output is related to a human.
When y2=1, it indicates that the output corresponds to attacks on S2, S3, S4, and S5.
The method "Artetxe et al. (2016)" achieves the highest performance in the supervised category for English, German, Spanish, and Italian.
The method "Conneau et al. (2017)" achieves the highest performance in the unsupervised category for English, German, Spanish, and Italian.
Stemle and Onysko (2018) and Wu et al. (2018) outperform Model Baseline 1 (lemma) in terms of F1 score.
Stemle and Onysko (2018) achieves a higher recall rate than Wu et al. (2018) in Model Baseline 1 (lemma).
Table 1 shows the results of multi-task learning architectures on the machine translation task.
The architecture "shrd Enc" is used for the "POS + MT" task.
The adapted schedule improves the performance of all systems compared to the default schedule.
The adapted schedule significantly improves the performance of the "NE + POS + MT" system compared to the default schedule.
Table 3 presents the results of different multi-task architectures on the POS task.
The model "shrd Dec" performs better on the adaptation schedule test than on the default schedule test.
The "Pouting Face" emoji is associated with a high level of anger.
The "Smiling face with smiling eyes" emoji has a relatively high emotion score.
R-BERT has a higher F1 score compared to R-BERT-NO-SEP-NO-ENT, R-BERT-NO-SEP, and R-BERT-NO-ENT.
The F1 score of R-BERT is 89.25.
R-BERT has the highest F1 score among all the methods compared in the literature.
The FCM method has an F1 score of 83.0.
The Macro F1 score for the "Shimaoka et al. (2017) ‡" model is [BOLD] 78.9.
The Micro F1 score for the "+ hierarchy" model is [BOLD] 75.4.
The table compares the performance of the "CNN" and "CNN+Complex" models on low data and full data.
The "+ hierarchy + transitive" model performs better on full data than on low data.
The table shows the accuracy of different models on entity linking in MedMentions.
The accuracy of the "+ hierarchy" model is higher when considering only mentions that contain the gold entity in the candidate set.
The models "GCN char" and "Test-pair" have the highest Pearson correlation percentages.
The "Test-pair" model has a lower Pearson correlation percentage compared to the other models.
As the text length increases, the vocabulary size also increases.
The "T10" and "TT" corpora have the same text length but different vocabulary sizes.
The word "Margrethe" is the most frequently occurring word in the corpus.
The word "the" is one of the most frequently occurring words in the corpus.
The perplexity of BROWN.200 is lower than T10 in all the corpora.
The table compares the performance of using a window (BROWN) and not having a window (BROWN_NW) on two different corpora: T10 and T20.
The language models trained on the BROWN_NW corpus have lower perplexity values compared to the language models trained on the BROWN corpus.
The model configuration "+ BiLSTM + CRF + gradual unfreeze" achieves the highest F1 score of 95.2.
The addition of gradual unfreeze does not improve the F1 score for the model configuration "+ CRF".
Table 3 compares the performance of different ELMO cross-sentence embedding methods on dev sets of sentence pair tasks.
The "ELMo- +bi-attn." method outperforms the "w/o bi-attn." method in all four tasks (SICK-E, SICK-R, STS-B, MRPC).
Table 4 compares the performance of BERT- joint encoding and separate encoding on the dev sets of sentence pair tasks.
BERT- joint encoding outperforms separate encoding on the SICK-E and STS-B tasks.
Table 6 shows the accuracy of feature extraction and fine-tuning with BERT-base trained on training data of different MNLI domains and evaluated on corresponding dev sets.
The difference in accuracy between feature extraction and fine-tuning with BERT-base is negative for the "te", "go", and "sl" MNLI domains, while it is positive for the "tr" and "fi" domains.
SRU (8 layers) outperforms the best reported result on the SUBJ dataset.
QRNN (k=1) + highway achieves a higher accuracy than the best reported result on the MR dataset.
The MI-LSTM model has the highest reported test result among all the models.
The SRU (with projection) model with a size of 47m and 8 layers has a test result of 1.21.
The EM scores for the SRU model decrease as different components are removed.
Increasing the number of layers in the SRU model improves the EM score.
The table shows word analogy prediction accuracy on Google datasets according to different types of word pairs.
Our model achieves an accuracy of [BOLD] 86.7 in the "capital-country" type of word pairs.
Table 1 provides the main results on word similarity and analogy tasks for different types of models.
The "Ours" model performs the best among all the models in terms of word similarity and analogy tasks.
The values in the "Text" column are higher than the corresponding values in the "Occu" column.
The value in the "Inter" cell is higher than the values in the "Gloc" cell.
Table 6 shows the performance of feature concatenation and stacking on the development set.
The stacking method with feature combination "{3} + Inter" achieves the highest MAcc score.
The Reasoning model has a higher mean percentage of differences compared to the Literal (S0) model.
The increase in accuracy is more significant when going from 1 sample to 10 samples compared to the increase from 10 samples to 100 samples.
The "Reasoning (S1)" model has a higher accuracy than both the "Literal (S0)" and "Contrastive" models.
The "Reasoning (S1)" model has a higher accuracy than both the "Literal (S0)" and "Contrastive" models on the hard test set.
The character embedding size is 50 and the GRU/LSTM state size is 200.
The optimiser used is Adagrad and the initial learning rate for the main task is 0.1.
The table provides information on the accuracy of the seq2seq transducer on Arabic and Hebrew.
The table includes the evaluation scores for the accuracy of the seq2seq transducer on Arabic and Hebrew.
The segmentation accuracy for Arabic using the Dictionary method is 97.27%.
The segmentation accuracy for Hebrew without any transduction is 87.17%.
The "+att+descr" model performs better than the "+att" and "+descr" models in terms of F1 score in both the test_set (T3) and real_simulation (T3) scenarios.
The "+att+descr" model has higher precision and recall scores than the "+att" and "+descr" models in both the test_set (T3) and real_simulation (T3) scenarios.
Table 2 provides information about the average dependency length in the Convex-MST parses of the English test dataset.
The average dependency length in the Convex-MST parses is 1.627 when using joint training.
The model trained with joint decoding (DD) achieves the highest directed dependency accuracy on average.
The model trained with joint decoding (DD) achieves the highest directed dependency accuracy on sentences of length ≤ 15.
The accuracy for the "left+right-marked (+m+)" model is 0.56.
The masked LM loss for the "left-marked (+m)" model is 2.03.
The table provides the BERT Test performance of two different models: "left+right-marked (+m+)" and "left-marked (+m)".
The model "left-marked (+m)" has a lower pseudo perplexity value compared to the model "left+right-marked (+m+)".
The table shows the training perplexity scores for Tr-XL.
The table shows the test perplexity scores for two different models: "left+right-marked (+m+)" and "left-marked (+m)".
The table shows the test perplexity scores for two different memory segment lengths: "32-32" and "(prev best)".
The biGRU+ AC model outperforms the biGRU model on overall frame element identification on D3.
The biGRU+ AC model shows improved performance compared to the biGRU model on identifying frame elements with a nominal trigger on D3.
The scores for the LU "arriver" are higher for biGRU D1 and biGRU+ AC D1 compared to biGRU D3 and biGRU+ AC D3.
The score for the LU "expression" is the highest for biGRU+ AC D3 compared to the other models.
The table shows the SRL performance (Fmax) on CoNLL-2005, based on the He et al. (2017) model.
The AC dataset has a higher SRL performance (Fmax) than the WSJ and BROWN datasets in the He et al. (2017) model.
Among the different methods, "Text2Vis1" has the highest performance in terms of the averaged DCG for the "fc6" feature.
Among the different methods, "Text2Vis N" has the highest performance in terms of the averaged DCG for the "fc7" feature.
The Bleu@1 score for unpaired English image captioning without any reward is 42.7.
The Meteor score for unpaired Chinese image captioning with rflc+ rsrlv+ rcrlv rewards is 22.8.
The recall at 1 for the Image-to-Sentence retrieval on the AIC-ICC validation set is 52.8%.
The recall at 10 for the Sentence-to-Image retrieval on the MSCOCO test set is 48.7%.
The B@4 score for the Our SSR approach is 11.1.
The CIDEr score for the Our SSR approach is 28.2.
Table 1 shows the F-measure by different models for classifying sentences in a document as Subjective and Objective in the MPQA and MR datasets.
The BCDBN model achieves the highest F-measure of 93.2 on the MPQA dataset.
The automatic evaluation metric BLEU shows that both "Dialogue+Persona" and "Dialogue+Fine-tuning" achieve higher scores than "PAML".
The human evaluation for fluency shows that "Dialogue+Persona" outperforms "PAML".
The accuracy of the BiLSTM+softAtt model on the AG's News dataset is 0.9264.
The GA-Net model achieves the best performance on all datasets.
The "BiLSTM+AUX LSTM" model has the highest accuracy among the three models in GA-Net.
The "BiLSTM+AUX LSTM" model has the lowest density among the three models in GA-Net.
The BAN model outperforms all other models in terms of R@1, R@5, and R@10.
The Upper Bound performance of the Plummer et al. model is higher than the Upper Bound performance of all other models.
The BAN (ours) model achieves the highest Recall@1 performance across all categories.
The BAN (ours) model performs better than all other models in terms of Recall@1 performance for the "Animals" category.
TD-SpkBeam achieves the highest SDR (dB) score among all the methods.
TasNet with xvect input achieves a lower SDR (dB) score compared to the average score.
The WSJ dataset contains 52 female speakers and 49 male speakers.
The CSJ dataset has 15k test mixtures.
The SDR values for TD-SpkBeam + SI-loss are consistently higher than the SDR values for TD-SpkBeam across all mixture types.
The SDR values for TasNet (oracle) are consistently higher than the SDR values for TasNet (xvect) across all mixture types.
The table shows the accuracy rate obtained when distinguishing real papers from artificial papers using KNN, NBY, and C45 classifiers.
The JI classifier achieves an accuracy rate of 95% when distinguishing real papers from artificial papers.
Table 3 shows the results of different ablations of the multi-level cascaded predictor on the dev set of the NQ dataset.
All the ablations of the multi-level cascaded predictor in Table 3 achieve an F1 score of at least 73.0 on the dev set of the NQ dataset.
The F1 score for the long-answer (LA) task on the test set is higher than the F1 score for the short-answer (SA) task on the test set for the "BERTlarge + 4M synth NQ alberti2019synthetic" model.
The F1 score for the long-answer (LA) task on the dev set is higher than the F1 score for the short-answer (SA) task on the dev set for the "RikiNet-RoBERTa large (ensemble)" model.
Table 2 shows the ablations of DPDA reader on the dev set of the NQ dataset.
RikiNet-BERTlarge (K=256) and RikiNet-BERTlarge (T=2) achieve the highest F1 score in the LA task.
The model [ITALIC] UWB→ [ITALIC] Usrc+tgt→ [ITALIC] Utgt (DAP) has the highest average performance.
The model [ITALIC] Usrc+tgt →  [ITALIC] Utgt (DAP) performs better on W4tgt compared to the model [ITALIC] UWB→ [ITALIC] Utgt (DAP).
The table provides the performance of the Full-Transformer (UWB) model on seen and unseen entities from the training and validation worlds.
The accuracy of the Full-Transformer (UWB) model is higher on training worlds, seen entities compared to validation worlds, unseen entities.
Table 6 shows the performance on test domains with Full-Transformer.
The average syntactic stats precision, recall, and fscore for all verbs in the test set are 0.847, 0.727, and 0.78, respectively.
The semantic stats precision, recall, and fscore for the verb "crush" in the test set are 0.484, 0.436, and 0.459, respectively.
The MLP model has the highest f1 score among all the models in both the cross-validation and held-out datasets.
Humans outperform all the systems in terms of precision, recall, and f1 scores in both the cross-validation and held-out datasets.
Table 3 shows parsing scores on the PTB test-set using the Kiperwasser16 system and the Kiperwasser16 system with conjunction features.
The Kiperwasser16 system with conjunction features achieves higher UAS and LAS scores compared to the Kiperwasser16 system.
The attribute "Tone: Complex tone system" has the highest negative weight in the full regression model on all languages.
The attribute "Purpose clauses: Balanced" has the highest positive weight in the full regression model on all languages.
The table provides information about different attributes related to WALS.
The weight for the "Exponence: No case" attribute is 0.27±0.01.
Table 2 shows the performance with improved DNN feature learning.
The EER% P2 value for the DNN+PT+SEG model is 8.90.
The EER% cosine values for i-vector are consistently lower than the EER% cosine values for d-vector.
The EER% PLDA values for P1 are higher than the EER% PLDA values for P5 for both i-vector and d-vector.
The EER% for PLDA is 1.71.
The EER% for the combination method is 1.38.
XLM-R outperforms mBERT in terms of F1 score on the BUCC bitext mining task.
On average, XLM-R achieves a higher F1 score than mBERT on the BUCC bitext mining task.
The distribution of sentiment labels on Twitter is different from that on OSG.
The percentage of negative comments is higher on OSG compared to Twitter.
Our approach outperforms both the Majority Baseline and Random Baseline in terms of Accuracy, Precision, Recall, and F1 score.
Our approach achieves a higher Accuracy than both the Majority Baseline and Random Baseline.
Table 6 shows the distribution of sentiment labels of the final text of the original poster (OP) with respect to the comment's sentiment label.
For the OSG category, the percentage of positive sentiment is higher for Twitter compared to OSG.
Table 9 shows the supertagging accuracy on the CCGbank test set for various models.
The BiLSTM-LAN model achieves the highest supertagging accuracy on the CCGbank test set.
The "BiLSTM-CRF" model has variations with different values for label embedding size (E), hidden size (H), and number of layers (L).
The "BiLSTM-LAN" model has a higher accuracy than the "BiLSTM-CRF" model for all variations.
BiLSTM-LAN has the highest accuracy among all the models in the table.
The accuracy of BiLSTM-CRF, BiLSTM-softmax, and BiLSTM-LAN w/o attention are similar.
There are two models for POS tagging and two models for CCG tagging in the table.
The BiLSTM-LAN (CCG) model has a shorter training time than the BiLSTM-CRF (CCG) model.
Table 8 shows the F1 scores of various models on the OntoNotes 5.0 test set.
The BiLSTM-LAN model achieves an F1 score of 88.16 on the OntoNotes 5.0 test set.
The case study for DMD has fewer articles, articles with full-text, UMLS concepts extracted, UMLS SN relations extracted, Has-MeSH relations, and Mentioned-in relations compared to ADD and LC.
The number of UMLS concepts extracted for ADD is smaller than the numbers for DMD and LC.
The table compares the performance of two machine translation systems: Fairseq and ESPnet-ST.
Fairseq performs better in translating from English to German in the year 2012, while ESPnet-ST performs better in translating from German to English in the year 2013.
The ESPnet-ST (Transformer) model outperforms all other models in terms of BLEU scores in all four evaluation sets.
The ESPnet-ST model performs better than the Char RNN ASR → Char RNN MT model in terms of BLEU scores.
The "M2H-GAN" model achieves the highest accuracy on both the development and real test datasets.
The "M2H-GAN" model has the lowest standard deviation in its performance on the real test dataset.
The "Max Test" accuracy is always higher than the "Real Test" accuracy for all models.
The "M2H-GAN" model has the highest accuracy in both the "Dev." and "Real Test" columns.
The ReCoSa model achieves the highest BLEU score on the JDC Dataset.
The HRAN model performs better in terms of distinct-2 score on the JDC Dataset compared to the Ubuntu Dataset.
The model "ReCoSa-head3" achieves the highest precision at 1 on the JDC Dataset.
The model "ReCoSa-head5" achieves the highest F1 score at 10 on the JDC Dataset.
Table 7 shows the EM/F1 test-on-source scores over DRCDfilter (English) and DRCD (English).
The EM and F1 scores for the SQuAD approach are lower than the EM and F1 scores for the DRCD (English) approach.
The table presents an ablation study comparing the performance of three different models: "CLQG (no pretraining)", "CLQG", and "CLQG+ parallel".
The "CLQG+ parallel" model outperforms the other two models in terms of BLEU-4 score.
The "Transformer+pretraining" model outperforms the "Transformer" model in terms of BLEU-4 score for both Hindi and Chinese question generation.
The "CLQG+parallel" model outperforms the "CLQG" model in terms of ROUGE-L score for both Hindi and Chinese question generation.
Table 3 provides human evaluation results and inter-rater agreement for each model on the Hindi test set.
The CLQG model performs the best in all three evaluation criteria (Syntax, Semantics, and Relevance).
Table 5 compares the performance of Hindi QG only dataset with Hindi QG + English QG dataset.
The addition of English QG data improves the performance of the Hindi QG dataset, as indicated by the higher BLEU-4 score for the "Hindi QG + English QG" dataset.
The table presents the results of two different language models: RNNLM and VAE.
The perplexity score of the standard test data is higher for the VAE model compared to the RNNLM model.
The VAE (3x5 bm.) model has a lower adversarial error than the RNNLM (15 bm.) model for both Unigram and lstm.
The VAE (3x5 bm.) model has a lower adversarial error than the RNNLM (15 bm.) model for both Unigram and lstm.
The "Combine-st" method has the highest accuracy of 92.2.
The "cnn" method has the highest accuracy of 93.6.
BHAM-Category achieves the highest F1 scores for all currency pairs (USD-EUR, USD-JPY, USD-RMB, USD-GBP) compared to other methods.
NoGroup has a lower F1 score for the USD-GBP currency pair compared to LSTM+Attention, BHAM-Time, BHAM-Topic, and BHAM-Category.
The "CHIM - Encoder" method has the highest accuracy among all the competing models.
The "Bias - Attention" method has a perplexity of 44.00.
The CHIM (Ours) model with BiLSTM as the base model and using embedding as the injection achieves the highest accuracy on both the IMDB and Yelp 2014 datasets.
The HCSC model with BiLSTM+CNN as the base model and using attention2 as the injection achieves an accuracy of 54.2% on the IMDB dataset.
The mean number of summary-worthy sentences in a document for the CNN/DM dataset is 13.95.
The maximum number of randomly sampled sentences from the same document for the training set in the CNN/DM dataset is 337.
The table provides information about different tasks in the WHAM! dataset.
The SI-SDR oracle performance is higher for the "separate-noisy" task compared to the "enhance-both" task.
Table 2 provides a performance comparison of chimera++ networks on three different tasks: "enhance-single", "enhance-both", and "separate-clean".
The "separate-clean" task has the highest output values for both "18 kHz min" and "16 kHz max" cases.
The "enh-both + sep-clean-finetune" objective achieves the highest improvement in SI-SDR.
The SI-SDR improvement for the "LDC,W" objective is higher than the SI-SDR improvement for the "LDC,W, 0 weight on noise bins" objective.
The model "chimera++" has different outputs for the "18 kHz min" and "16 kHz max" datasets.
The delta value for the "chimera++" model is higher for the "16 kHz max" dataset compared to the "18 kHz min" dataset.
Table 3 shows the results of experiments with target-side syntax for German→English and Romanian→English.
The multitasking strategy with a shared encoder achieves a BLEU score of 29.0 for the Romanian→English ensemble.
The "IFrequency" method has the highest accuracy among all the methods.
The SVM classifier performs better than the Naive Bayes classifier across all methods.
The table provides accuracy scores for different feature combinations using SVM and NB classifiers.
The accuracy of the SVM classifier using only unigrams as features is 0.76.
Table 2 shows the results of offline MMA models on the dev sets with head-synchronous beam search decoding.
Model D3 achieves the highest WER score of 3.5 / 10.3.
The table presents the results of offline MMA models on the dev sets with standard beam search decoding.
The Rstr values for the experiments range from 0.0 to 15.8.
The streaming MMA model with data augmentation and a large model performs better on Librispeech and TEDLIUM2 compared to other streaming MMA models.
The streaming MMA model with a wide chunk performs better on TEDLIUM2 and AISHELL-1 compared to other streaming MMA models.
The "Bottom-Up Summarization" method has the highest R-2 score.
The "ML+RL*" method has the highest R-L score.
The Content Selector method performs the best in terms of R-1 and R-L scores.
The Top-3 sents (Cont. Select.) method shows a significant improvement in the R-2 score compared to other methods.
The percentage of novel words in the reference summaries is 14.8%.
The percentage of adjectives in the novel words of the summaries generated by the Bottom-Up Attention model is 6.5%.
The addition of each inference penalty improves the performance of the Pointer Generator model on the CNNDM dataset.
The R-L metric consistently outperforms the R-1 and R-2 metrics on the CNNDM dataset.
The table shows the F1 scores of different variations of the stacked model.
The table displays the difference in F1 scores between each variation of the stacked model and the baseline model.
Table 6 represents the class-level and overall results of the systems on the WNUT 2017 dataset.
The end-to-end model has a higher precision than the stacked model.
The accuracy for scenario S1 with model M1 is 0.5.
The test set ID for scenario S2 with model M2 is 6.
The total number of true positive instances included in the final resource is 1.
In the first test set, the prediction scenario S1 has a value of 1 for model M1.
The highest BLEU score is achieved at iteration 2.
The average length of the generated answers on the TEST set is 10.56 for iteration 1.
The strategy "TF×IDF+qc-sim" achieves the highest MAP score on the development dataset.
The average MAP score across all ranking strategies is 62.33.
The average score of CL-C3G, CL-CTS, and T+WA is higher than the average score of all unsupervised systems.
The score for the method M5P is higher than the scores for all other supervised systems.
The M5P method performs better on the SNLI dataset compared to the WMT dataset.
The table shows the results of three different methods: CL-CTS, Average, and M5P.
The prediction accuracy for answerable questions using BERT-base is 81.5% and for non-answerable questions is 78.3%.
The prediction accuracy for both answerable and non-answerable questions improves to 82.1% when using BERT-base with the Relation Module.
The models are listed in ascending order based on their performance on the SQuAD 2.0 development set.
The models with the highest values in both the "EM(%)" and "F1(%)" columns are "BERT-base + Relation Module" and "BERT-large + Relation Module".
The "Ruminating Reader" variant achieves the highest F1 score among all the ablation variants.
Removing the query ruminate layer has a slightly smaller impact on the F1 score compared to removing the context ruminate layer.
The "Bidirectional Attention Flow" model has the highest Test F1 score.
The "r-net" model has the highest Test EM score.
The LAS score for the VnCoreNLP model on the Auto POS task is 70.23 and the UAS score is 76.93.
The UAS score for the BIST-bmstparser model on the Gold POS task is 79.39.
The fully pre-trained BERT model performs better on entity and relation extraction than the BERT models with self-attention layers trained from scratch or with both layers and BPE input token embeddings trained from scratch.
Removing more components during pre-training leads to a decrease in performance on relation extraction.
The "Max" pooling method achieves the highest scores for both Entity F1 and Relation F1.
The performance of the models decreases as we move from "Max" pooling to "Sum" pooling to "Average" pooling.
Sockeye-1 and Sockeye-2 are trained using different toolkits.
Fairseq has a higher accuracy than Sockeye-1 and Sockeye-2.
The table provides the results of different models with small training data and replicate settings.
The Macro-average F-score for Medicine is lower than the Macro-average F-score for Space.
The Precision for Cryptography is higher than the Precision for Electronics.
The Multilingual Model News has the highest output score for both BLEU and chrF3.
The Combined Model News has a higher output score for BLEU compared to the Baseline Model News.
Table 5 presents the experiment results of English-Kurdish models, including a Bilingual Model Generic and a Multilingual Model Generic.
The Multilingual Model Generic achieved a BLEU score of 8.51.
SGReader (Ours) achieves a Hit@1 score of 35.9% on the 30% KB setting.
The F1 score for KV-KB+Text on the 10% KB setting is 17.7.
The table presents the results of different models' ablation on the dev dataset under the 30% KB setting.
The Full Model outperforms the other models in terms of Hit@1 and F1 scores under the 30% KB setting.
Increasing the model width from 2048 to 3072 improves the performance metrics (PPLword CV, PPLword Hub5’00, WER swb, WER chm).
Adding batch normalization to the model improves the performance metrics (PPLword CV, PPLword Hub5’00, WER swb, WER chm).
The "filter dup." step has a consistent WER of 7.5%.
The "filter noise" step improves the WER performance.
The WER for "weight decay" is lower than the WER for "zoneout (in dec.)".
The number of errors for "random. batch" is lower than the number of errors for "zoneout (in dec.)".
As the depth of the encoder increases, the number of parameters also increases.
As the model size decreases, the word error rate with language model also decreases.
Table 5 provides detailed results with the best performing systems.
The best performing system achieved a score of 6.4 on the hub5’00 swb dataset using external language models.
The baseline NMT model achieves a Meteor score of 38.6 and a BLEU score of 17.8.
The model proposed by Caglayan et al. achieves a verb accuracy (VAcc) score of 29.3.
The table shows the cross-lingual verb sense disambiguation accuracy of unimodal models and the multimodal model, as well as the performance of random chance baseline and majority label baseline.
The accuracy of the multimodal model for German is 55.6%.
The highest scores for all evaluation metrics are achieved when training with "BLEU-4 single V refs" and "CIDEr V refs" in the "Captioning without attention" condition.
The "Captioning without attention" condition with "V batch" consistently achieves lower scores for all evaluation metrics compared to the other conditions.
The highest BLEU-4 score in the "With attention" condition is [BOLD] 33.34.
The highest CIDEr score in the "With attention" condition is [BOLD] 103.81.
Table 3 provides tokenized BLEU scores on WMT'14 En-Fr and IWSLT'14 De-En datasets.
The "Hamming" and "BLEU-4" metrics are used for evaluation in the "Seq" column.
The highest score for Captioning with attention BLEU-4 is achieved when using V [ITALIC] refs.
The highest score for Captioning with attention CIDEr is achieved when using V [ITALIC] batch.
The table provides the Exact Match (EM) scores on NQ-open for different models.
The model "SpanSeqGen" has the highest EM score on the NQ-open test dataset.
The F1ans (multi) score for Karpukhin et al. (2020) is higher on the Dev set compared to the Test set.
The SpanSeqGen model achieves the highest NQ-open EM score and F1ans (all) score on the Test set.
Table 4 presents the Pearson and Spearman correlation coefficients of baseline, character and word-level models, and their ensemble for fear, anger, joy, sadness emotions, and average values.
The ensemble model of baseline, character, and word-level models has the highest correlation coefficients for fear, anger, joy, sadness emotions, and average values.
The table presents the single-label classification results on the EmoCT single-labeled version using two methods: BERT and BERT(ft).
BERT(ft) achieves a higher accuracy than BERT in the single-label classification results on the EmoCT single-labeled version.
BERT has the highest F1 Score among the selected models.
MultinomialNB has the highest Precision among the selected models.
There are 14 instances where violence is directed and the gender is specified.
There are 37 instances where violence is not generalized and the religion is not specified.
Table 3 provides empirical results utilizing exemplar auditing as a decision rule, showing the token-level precision, recall, and F0.5 score for different models.
The "uniCNN+BERT+S*+ExA" model has a higher token-level precision than the "uniCNN+BERT+ExA" model.
Table 6 provides accuracy results for predicting sentiment on the original and revised test sets.
The accuracy for predicting sentiment on the revised test set is higher than the accuracy on the original test set for the BERTBASE_uncased+FT model.
The accuracy of the "Orig.DISJOINT+Rev. (1.7k+1.7k) BASE_uncased" model is 75.2.
The accuracy of the "Orig.+Rev. (19k+1.7k)" model is 66.9.
The test set is divided into different subsets based on the combination of original and revised reviews.
The accuracy of predicting whether a review is a "counterfactual" re-write is higher for the "Orig.+Rev." subset compared to the "Rev." subset.
The F1 score of the uniCNN+BERT+S* model for token-level labeling is 70.86.
The precision of the MajorityClass model for token-level labeling is 100.
Table 23 provides token-level evaluation results for different groups on a subset of the synthetic test set.
The F1 score for token-level labeling on the Original-partial set is 60.12.
As the number of augmentations in TCVAE increases, the MRR of WDtext decreases.
The MRR of DBPtext increases as the number of triplets being generated in TCVAE increases.
The "Ours" model outperforms all other models in terms of Hits@10, Hits@5, Hits@1, and MRR on both WDtext and DBPtext.
The "ConMask" model performs better than "DKRL" and "GMatching" in terms of Hits@1 on WDtext.
The performance of the state-of-the-art models decreases from the "Normal" setting to the "ME" setting and further decreases to the "OE" setting for all benchmarks.
The state-of-the-art models achieve the lowest AUC value on the NYT-10 benchmark in all settings.
The "combined" model has the highest Spearman Correlation for both WordSim-353 and SimLex-999.
The "combined" model has a higher Spearman Correlation for WordSim-353 compared to both the "wac" and "GloVe" models.
Table 1 provides the experiment 1 and 2 results for accuracy scores of different models and composition approaches using the refCOCO data.
The "mlp relational" model has an accuracy score of [BOLD] 0.63 in experiment 2.
The table compares the results of the authors' approach with the best reported values by Joshi et al. (2015) and Riloff et al. (2013) on a dataset of tweets by Riloff et al. (2013).
The "Incongruous words-only Approach" performs better than the "All-Words Approach" in terms of precision, recall, and F1-score.
The "All-Words Approach" and the "Incongruous words-only Approach" are compared in terms of precision, recall, and F-score.
The precision, recall, and F-score values are given for the "P", "R", and "F" columns in the "Joshi et al. (2015)" row.
The Oracle approach outperforms the All-words approach in terms of precision, recall, and F-score.
The F-score for the Oracle approach is 63.42.
The TTS systems used in this experiment are OJT, MOO, MOC, MMC, and MMO.
The TTS system with the lowest V/U Error is MMO.
The TER score for the OSNMT baseline on the English→German translation task in newstest 2019 is 55.8.
The BLEU score for the RNN baseline on the English→Turkish translation task in newstest 2017 is 16.6.
The RNN baseline (500K) system outperforms the OSNMT (500K) system in the English→German translation task.
The addition of the "SRC_POP factor" improves the performance of the OSNMT (500K) system in the Ter newstest 2017 evaluation.
The method "EmbedRank [ITALIC] positional s2v" achieves the highest F1 score on the Inspec dataset at 15.
The F1 score of the method "EmbedRank [ITALIC] positional s2v" is highest on the NUS dataset at 15.
The "Bidirectional GRU+CNN-CRF" model has the highest F1-score among all the models.
The "GRU-CRF" model has the highest precision among all the models.
Table 16 shows METEOR scores between original and transformed documents from rule-based techniques.
The METEOR score for ParChoice with EBG45 transformation is 69.34.
The table presents the results of the ParChoice-LSTM and ParChoice-CNN techniques on the blog author dataset.
ParChoice-LSTM shows a higher source class accuracy decrease and target class accuracy increase compared to ParChoice-CNN.
The accuracy of the LSTM author profiler decreases after adversarial training for both the original and transformed test sets.
The accuracy of the LSTM author profiler is generally higher for A2 compared to the other authors.
The technique "ParChoice-LSTM" has a target author accuracy of 0.51.
For the source author "A3", the technique "ParChoice-LSTM" has a target author accuracy of 0.57.
BERTLARGE has the highest performance among all the models on the "Problem" label category.
BERTBASE performs better than word2vec, GloVe, FastText, and ELMo on the "Clinical dept" label category.
BERTlarge achieves the highest F1 score in the "i2b2 2010 General" and "i2b2 2012 General" tasks among all embedding methods.
BioBERT does not have F1 scores in the "i2b2 2010 MIMIC" and "i2b2 2012 MIMIC" tasks.
The BERTBASE model performs the best on the "Problem" category with a score of [BOLD] 89.61.
The BERTLARGE model performs the second best on the "Test" category with a score of [BOLD] 88.8.
The model "Table2Seq++" achieves the highest BLEU-4 score on the test set with a score of [BOLD] 38.23.
The verb "hit" has the highest count of 172.
The utterances for the verb "hug" include "he loves me so much" and "awww. I love you child".
The verb "hit" has the highest count of 172.
One of the top utterances for the verb "hug" is "Minister! It is so good to see you!".
EKSet BPS has a ranking performance of 83.8%.
The NTC value for EJSet MKS is 33.
The transliteration ranking for the data "de-i-teo" in the WF search engine is 94,100.
The transliteration ranking for the data "de-i-ta" in the NWF search engine is 0.2042.
Table 5 shows the link prediction results with MRR and Hit@n on WN18RR-sparse and FB15k-237-sparse.
The MRR for WN18RR-sparse using TransE (Bordes et al., 2013) is 14.6 (filter) and 12.4 (raw).
En-DT has the highest number of sentences among all the corpora listed in Table 1.
En-DT has the highest number of words among all the corpora listed in Table 1.
Our model achieves the highest UAS and LAS scores among all the models in the table.
The UAS and LAS scores of Dozat and Manning (2017) are higher than the scores of Ma and Hovy (2017).
The total number of samples increases as the score increases.
The number of samples chosen for the full score dataset is the same for each score.
The "NML-sequence" system has the lowest %WER (8.45) among all the systems listed in the table.
The "NML-step" system has a lower %WER (8.55) compared to the "Baseline" system (10.72).
The system with random initialization has a %WER of 11.15.
The system with "b f= 1 + minimum 6 epochs before LR halving" has a lower %WER (dev-clean) than the system with "b f= 1".
The system with "3 frame stack, stride =3 + 9-fold max perturbation" performs better than the baseline system in terms of %WER (dev-clean).
The system with "Baseline + 9-fold max perturbation" performs better than the baseline system in terms of %WER (dev-clean).
The table presents the results of experiments with different combinations of NML-step and Forward-step in the context of Stochastic dropout.
The NML-sequence + Forward-sequence combination achieves the lowest word error rate among the different combinations in the Stochastic dropout experiments.
The Unif. model has an average attachment score of 62.0.
The C=3 model has an attachment score of 65.9 for Italian.
The Arc-standard parsing algorithm performs better on average on the CoNLL X and 2007 test sets compared to the Left-corner full parsing algorithm.
The Arc-eager parsing algorithm performs better on Chinese compared to the Arc-standard parsing algorithm.
The table shows the BLEU scores of systems trained using additional monolingual data.
The BLEU score for the "En → My" direction after 3 iterations of ST + BT is 40.6.
The BLEU score for the My→En translation using the BT model is 33.1.
The BLEU score for the En→My translation using the BT + ST model is 40.3.
Each row in the table represents a different system or combination of systems trained on the provided parallel datasets.
The BLEU scores for translation from "My" to "En" are lower than the scores for translation from "En" to "My" in all systems or combinations of systems in the table.
The F1-score increases as the number of CNN layers increases.
The Hamming Loss decreases as the number of CNN layers increases.
The "2-CNN+2-Dense+Dropout" approach achieves the highest F1-score and Jaccard similarity among all the approaches listed in the table.
The combination of 2, 3, and 4-gram filter sizes achieves the highest precision, recall, F1-score, Hamming loss, Jaccard similarity, and exact matching scores among all the combinations listed in the table.
The "Segmenter" model achieves the highest F1 score on the StackOverflow NER dev set.
The "Segmenter" model outperforms the "Word Frequency Embed." model and the "Code Markdown Embed." model in terms of F1 score on the StackOverflow NER dev set.
The "– Character ngram LM" method performs better than the "– Word ngram LM" method in terms of F1 score for code token recognition.
The "Code Token Recognizer" method achieves the highest F1 score for code token recognition.
The BiLSTM model with SO-ELMo input representation has the highest F1 score.
The BiLSTM model with Newswire-Glove input representation has higher precision and recall than the model with Newswire-ELMo input representation.
The Attentive-BiLSTM model achieves the highest F1 score among all the ablation study models.
The precision score for the Attentive-BiLSTM model is 79.43.
The Attentive-BiLSTM(SO-ELMo) model outperforms the BiLSTM (SO-ELMo) model on the NER(20 Types) Dev dataset.
The Fine-tuned BERT model achieves higher performance than the BiLSTM (SO-ELMo) model on the NER(20 Types) Test dataset.
The model with a lexicalized previous utterance has the highest BLEU score for QA.1.
The model with slot types and values as encoders has the lowest Slot Error Rate for QA.2.
The table shows human evaluation results on three QA NLG datasets with increasingly larger ontologies.
The models with lexicalized previous utterance have higher average scores for Naturalness and Informativeness compared to the models with delexicalized previous utterance.
The table shows the performance of the constituent hierarchy predictor and the corresponding parser on the WSJ dev dataset.
The proposed model without ablation performs better than the other variations.
Table 4 shows the performance of the constituent hierarchy predictor and the corresponding parser on the WSJ dev dataset.
The performance of the constituent hierarchy predictor and the corresponding parser improves as the number of hidden layers increases.
The parser with lookahead feature outperforms the baseline parser for all phrase types.
The parser with lookahead feature performs better on the "s-type" constituent hierarchy compared to the "e-type" constituent hierarchy.
The "Max" system has the highest performance in terms of average score, Success@2, Success@3, and Success@4 values.
The "Max" system has the highest precision at 2.
The table provides evaluation results for different models and baselines.
The BLEU-1 and BLEU-4 scores increase when the value of γ is increased.
The examples in the "Polite Examples" section have higher politeness scores compared to the examples in the "Rude Examples" section.
The higher the politeness score, the more polite the response.
The original BiLSTM model achieved a performance of 64.70% on the MS Marco dataset with 2k samples.
The FBQAFA method with BERT achieved a performance of 94.81% on the WikiPassageQA dataset with 10k samples.
The ClinicalSTS task uses the metric "Pearson" for sentence similarity.
The BC5CDR disease task belongs to the biomedical domain.
The table shows the test results on clinical tasks for three different models: BlueBERTclinical, MT-BlueBERT-Refinementclinical, and MT-BlueBERT-Fine-Tuneclinical.
The MT-BlueBERT-Fine-Tuneclinical model achieves the highest scores on both the MedNLI and ClinicalSTS tasks.
There are three different models: BlueBERTbiomedical, MT-BlueBERT-Refinementbiomedical, and MT-BlueBERT-Fine-Tunebiomedical.
The MT-BlueBERT-Fine-Tunebiomedical model performs better on the BC5CDR chemical task compared to the other models.
The MT-BlueBERT-Fine-Tuneclinical model performs better than the other models on all clinical tasks.
The MT-BlueBERT-Fine-Tuneclinical model achieves the highest score of 0.840 on the ClinicalSTS task.
Table 6 compares the performance of different models on eight BLUE tasks.
The MT-BioBERT Fine-Tune model has the highest average performance across all BLUE tasks.
The average ROUGE-1 score for all platforms is 83.19±15.04.
The ROUGE-2 R score for Nintendo 64 is 21.11±9.37.
The LSTM model has the highest AUROC score among all the models.
The LSTM model performs better in terms of AUROC score compared to the "+oversamp." model.
The LSTM model has the highest recall for class 1.
The +oversamp. model has the highest precision for class 1.
The BART (MLE) model achieves a BLEU-4 score of 53.13 on the QG (SQuAD) task.
The T5-small (MLE) model achieves a ROUGE-L score of 40.37 on the Summ. (CNN/DM) task.
Figure 4 represents the probability that the generated text is human according to Dϕ on CNN/DM.
The fluency score for BART (MLE) is 3.80.
The ET4EL model outperforms all other models on the CoNLL development and test sets.
The Most Frequent Entity model performs better than the Cosine Similarity model on the CoNLL development and test sets.
Table 4 shows the accuracy on the CoNLL development set (testa) with different numbers of categories.
The ET4EL model achieves the highest accuracy on the Unseen-Mentions test set.
The model trained on the Wiki dataset achieves the highest accuracy on the Unseen-Mentions test set.
The LSTM model with the "last" encoding achieves an agreement score of 80.2±1.0 on the AGNews dataset.
The LSTM model with the "max" encoding achieves an agreement score of 33.7±1.7 on the Yelp dataset.
There are four different datasets in the table.
The number of labels for each dataset is different.
Table 3 provides information about the performances of downstream task classifiers trained on different inputs.
When the KL collapses, the F1 scores of downstream task classifiers trained on samples z∼N(μ,Iσ2) are higher than the F1 scores of classifiers trained on the mean μ.
The ratio of performance between using all words as features and only the first three words varies across datasets.
Increasing the value of FB in the δ-VAE-style free bits method improves the F1 scores in all data regimes.
Increasing the value of λ in the δ-VAE-style free bits method improves the F1 scores in all data regimes.
Increasing the value of λ leads to higher F1 scores for both N = 2 and N = 8.
Resetting the decoder improves the F1 scores for both N = 2 and N = 8.
Table 2 provides the evaluation results for four different text retrieval models.
SILM.Sentence outperforms the other three models in terms of MAP, P@5, and P@10.
The TF model with the Linear kernel has a test accuracy of 0.8789.
The table presents the results of two models, NoSup and Period Count.
The error rate for the Period Count model is lower than the error rate for the NoSup model.
The number of periods increases as the number of attributes increases.
The number of periods decreases as the number of attributes decreases.
The "mask top-2 weights" system has the highest Gigaword R-1 score.
The "averaged head" system has a higher Gigaword R-L score than the "non-synchronous head" system.
The table provides information on the validity of different values of k.
The number of valid instances for k=2 in DBpedia is 496964.
Table 9 shows the results on QALD-1.
The values in the "R BFQ" column represent the R BFQ scores.
The system (Bordes et al., 2014) does not have a precision value, while the system (Zheng et al., 2015) has a precision value of 0.38.
The system (Yao, 2015) has an F1 score of 0.44.
The table provides the results of different hybrid systems on QALD-3 over DBpedia.
The hybrid system KBQA+SWIP improves the precision, recall, and F1 score compared to the SWIP system alone.
The test accuracy of the MemN2N API call improves as the number of training bAbI+ dialogues increases.
The train accuracy of the MemN2N API call improves as the number of memory hops decreases.
The success rate of the "DeleteAndRetrieve" system on the Yelp dataset is 29%.
The attribute match score for the "TemplateBased" system on the Captions dataset is 4.1.
Table 4 describes the results using SUTime, with additional rules for predicting age expressions and when limiting the candidate expression set using the closest heuristic.
Adding age rules significantly improves the precision of SUTime.
The Logistic Reg. model has a lower precision score for diagnosis recency compared to the Linear SVM and GBT models.
The Linear SVM model has a higher recall score for condition state compared to the Logistic Reg. and GBT models.
The table shows the word ordering BLEU scores for seq2seq, BSO, ConBSO, and LSTM-LM models with Ktr=6.
The "Self-Attention Transformer" has the highest number of trainable parameters among the three models.
The "Self-Attention Transformer" has more trainable parameters than the "Attention Encoder-Decoder".
The combination of HISK and BOSWE with ν-SVR outperforms HISK with ν-SVR in terms of cross-domain automatic essay scoring.
The accuracy score for the Advanced method on the QuoraQP dataset is 80.47.
The relative improvement for the Leakage method compared to the Majority method on the MultiNLI Mismatched dataset is -14.79.
Table 2 presents the results of ablation experiments on four different datasets: SNLI, QuoraQP, SICKSTS, and ByteDance.
The inclusion of the "Majority" feature decreases the accuracy scores of the "SNLI" and "QuoraQP" datasets.
The biased model performs better than the debiased model in terms of evaluation results.
The debiased evaluation performs better than the biased evaluation in terms of accuracy scores.
The debiased model outperforms the biased model in terms of accuracy on the synthetic, MSRP, and SICKSTS datasets.
The synthetic dataset yields higher accuracy scores for both the biased and debiased models compared to the MSRP and SICKSTS datasets.
The SQLNet system has a higher average number of questions (#q) and a lower Qr% compared to the SQLova system.
The SQLova system has a lower Qr% for both p∗=0.8 and p∗=0.5 compared to the SQLNet system.
As the average number of questions increases, the accuracy of both the probability-based error detector (Accqm) and dropout-based error detector (Accqm) also increase.
The dropout-based error detector consistently has higher accuracy than the probability-based error detector for each average number of questions.
Table 1 shows the case-insensitive BLEU scores (%) on Chinese↔English translation.
The JT-NMT system achieves the highest BLEU scores in all translation directions (C→E and E→C) and on all datasets (NIST2006, NIST2003, NIST2005, NIST2008, NIST2012).
The ensemble (xi) approach achieves the highest F-Score for the class label "Sexism".
The recall values for the class label "Racism" are consistently lower than the recall values for the class label "Neutral" across all ensemble approaches.
Table 3 provides evaluation results for different approaches used in the study.
The approach "O + NS + RS + NR + NRS" achieves the highest F-Score among all the approaches.
UMML-SL achieves the highest accuracy in both the ReutersMLDC (MLDC) and MLParsing (MLDP) datasets.
The precision@1 for UMML-SL is [BOLD] 80.9 when the source language is French (xx-fr).
The average precision@1 for UMWE is [BOLD] 80.4.
UMML-SL outperforms UMWE in terms of precision@1 scores in all language combinations.
On average, UMML-SL achieves a higher precision@1 score compared to UMWE.
The G2S+AE method achieves the highest scores in both BLEU-4 and ROUGE-L metrics.
The G2S [ITALIC] edge +AE method achieves a higher METEOR score compared to the G2S+AE method.
The Ours model with the Multi-pass ST architecture achieves a BLEU score of 24.2 on the Test18 evaluation set.
The Ours model with the Oracle ST architecture achieves a BLEU improvement of 2.5 compared to the Baseline model.
The table compares the performance of different methods on NIST evaluation sets.
The inclusion of self-training improves the performance of the Transformer model.
The BLEU score for the BT model on MT06 is 51.29.
The difference in BLEU score between the Non-BT model and the Multi-pass ST model is 0.53.
The "Oracle ST" model achieves the highest BLEU score in both the validation and test sets.
The "BT" models show an improvement in performance compared to the "Non-BT" models.
The Transformer-based NMT (this work) outperforms the RNN-based NMT in terms of BLEU scores in all domains.
The Fine-tune method improves the performance of the Transformer-based NMT in the Thesis domain.
The average improvement in translation performance for all domains when adding each new feature is positive.
The average translation performance for all domains is highest when using the "Domain Supervision" feature.
The translation model performs better on De⇒En Law than on De⇒En Med.
The translation model performs slightly better on En⇒Fr Par. than on En⇒Fr Med.
The network with the highest clustering coefficient is BLH, while the network with the lowest clustering coefficient is SAS.
The network with the highest number of nodes is LDR, while the network with the lowest number of nodes is SAS.
The LSTM model outperforms the Transformer model in terms of BLEU scores for both word and BPE tokenization.
Adding both ReWE and ReSE to the LSTM model with BPE tokenization improves the BLEU score compared to adding only ReWE.
The Transformer models have higher BLEU scores than the LSTM models.
The Transformer model with ReWE(λ=20) has a higher BLEU score than the LSTM model with ReWE(λ=20) for the bpe version.
The Transformer models have higher BLEU scores than the LSTM models.
The Transformer model with ReWE(λ=20) has a higher BLEU score than the LSTM model with ReWE(λ=20) for the bpe version.
The table provides BLEU scores for two different models: LSTM and LSTM + ReWE(λ=2) + ReSE(β=2).
The LSTM model with BPE achieves the highest BLEU score compared to other models mentioned in the table.
Table 1 provides information about the loss and perplexity on the Achemenet dataset.
The test perplexity for the 2-Gram model on the Achemenet dataset is 22.87.
Table 7 provides information about the learning rate for text encoders on different datasets.
The learning rate for RoBERTa-Large is 1×10−5 on both CommonsenseQA and OpenbookQA datasets.
The cosine similarity between the nearest neighbors in the Synthesizer train set is lower than the cosine similarity between the nearest neighbors in the Speaker Encoder train set.
The SV-EER for the Speaker Encoder train set is lower than the SV-EER for the Synthesizer train set.
The naturalness mean opinion score (MOS) for the seen data in the joint training model is 3.72 ± 0.06.
The similarity mean opinion score (MOS) for the unseen data in the proposed model is 3.03 ± 0.09.
Increasing the dropout rate generally decreases the WER, except when the dropout rate is 0.1.
Decreasing the L2 regularization coefficient generally decreases the WER, except when the L2 regularization coefficient is 0.
The layer sizes 600 and 700 have the lowest WER among all the layer sizes.
The layer size 700 reached its best performance at epoch 18.
The WER5 value for the SGD method is 17.0.
The bWER value for the upd-mm-3-2 method is 14.3.
Table 5 compares the performance of pretraining and no pretraining for different numbers of layers.
Pretraining improves the performance compared to no pretraining for 4 layers.
The table shows the performance of different ρ functions on the Text8 dataset.
The vocabulary size increases as the corpus size increases.
The window size is constant at 10 regardless of the corpus size.
The table shows the performance of the best word2vec, GloVe, and WordRank models on six similarity tasks and Google semantic and syntactic subtasks.
The WordRank model outperforms both word2vec and GloVe models on the Word Similarity WordSim task.
The methods HE, DYPSA, ZFR, SEDREAMS, and YAGA are evaluated on multiple databases.
The method YAGA consistently has the lowest IDA values across all databases.
The table shows the relative computation time (RCT), in %, for different methods.
The computation time for female speakers varies across different methods.
As the percentage of noisy data increases, the balanced accuracy decreases.
As the percentage of noisy data increases, the F1-score decreases.
The "Balanced accuracy" metric performs better than the "Sum of all sentiment scores" metric for all the variants in the benchmark without RST.
The "Node reordering & leaf insertion" variant of the Tree-LSTM method achieves the highest "F1-score" compared to the "Node reordering" and "Leaf insertion" variants.
Table 4 shows the predictive performance reported for the test set from the IMDb dataset using different methods.
Table 4 shows the predictive performance reported for the test set from the IMDb dataset using different data augmentation techniques.
Table 5 presents the predictive performance reported for the test set from the Amazon Fine Food dataset, using different methods for sentiment analysis.
Different variants of data augmentation were applied to improve the predictive performance of sentiment analysis on the test set from the Amazon Fine Food dataset.
The method "LAS + LibriFullAdapt" achieves the lowest WERs in both "clean" and "other" conditions.
The use of LM improves the WERs in both "clean" and "other" conditions.
The "SpecAugment" method does not have performance results for any of the domains in the table.
The system "mt→pe×4 / src→pe×4 / pep∗" has the best performance in terms of TER and BLEU scores.
Increasing the number of translations from "mt→pe" to "mt→pe×4" improves the TER score and BLEU score.
The "mt→pe×4 / src→pe×4 / pep" system has the best performance in terms of TER and BLEU scores.
The "Standard Moses (baseline 2)" system performs better than the "Uncorrected MT (baseline 1)" system in terms of TER and BLEU scores.
Table 2 provides precision, recall, and F1 scores of selected concept extraction models.
The Binarized Neural Embedding CRF model achieves a precision score of 0.851.
The "+Reinforcement Learning" component improves the performance of the Seq2Seq model, as indicated by the higher BLEU score of 0.0042.
The addition of the "Skeleton Extraction Module" improves the performance of the Seq2Seq model, as indicated by the higher BLEU score of 0.0029.
The proposed model outperforms the state-of-the-art models in terms of BLEU score.
Among the models evaluated, the GE-Seq2Seq model has the lowest BLEU score.
The "+Reinforcement Learning" model has the highest G-Score among the three models.
The "+Skeleton Extraction Module" model has the lowest coherence score among the three models.
The F1 scores for the LIMSI, DFKI, and UNIBA algorithms are missing for some parts of speech.
The F1 scores for the BFS Baseline, Word2Vec (T:SemCor), LSTM (T:SemCor), and LSTMLP (T:SemCor, U:1K) algorithms are all present.
The F1 score for the LSTM LP model with NOAD,SemCor on MASC data is [BOLD] 0.873.
The F1 score for the LSTM LP model with NOAD on SemCor data is 0.822.
The mean prosodic variation for the "Mixed expressive" corpus is higher than the mean prosodic variation for the "Neutral" and "Newscaster" corpora.
The variance of prosodic variations for the "Mixed expressive" corpus is higher than the variance for the "Neutral" and "Newscaster" corpora.
The "News w/o CWE" system has the lowest MSD (dB) value among all the systems.
The "News with CWE" system has the highest FCORR value among all the systems.
The overlap1 metric achieves the highest score for BLEU-4 at beam size 1.
The smoothed_bleu4 (MBR) metric achieves the highest score for BLEU-1 at beam size 100.
Table 4 shows the BLEU scores on newstest2014 with range voting applied to the beams obtained with no-copy filtering.
The BLEU scores increase as the beam size increases in the "Diverse decoding" row.
The proposed methods (IMS + CBOW) have higher accuracy than the baseline method (Bing Translator + word alignment).
The proposed method with σ=0.1 has the highest accuracy among all the methods.
The biLSTM model has the highest F1 score on the C3 test.
The CNN model has the lowest mean comment length of false positives.
Table 5 compares the F1 scores on SOCC-a with a linear SVM.
C3 train dataset has a higher F1 score than NYT + YNACC* dataset.
The biLSTM model consistently outperforms the CNN and BERT models in terms of F1 score.
The experiments in Table 1 involve different types of word embeddings.
The experiments with BERT as the word embedding have lower word error rates compared to the other experiments.
(c) Proposed Fused Decoding has the lowest WER (%) for both Dev and Test.
(e) Criticizing-LM has a lower WER (%) than (a) Baseline for both Dev and Test.
The MRC method outperforms the Hum method in terms of ROUGE-2 scores in all three DUC datasets.
The Basel. method performs worse than the Syst. Av. method in terms of ROUGE-SU4 score in the DUC07 dataset.
Table 4 provides model baselines on the proposed hard and easy test sets for different models.
BERT achieves the highest accuracy on both the Full, Easy, and Hard test sets.
The BERT model achieves an accuracy of 75.0% on the easy test set.
The RoBERTa model achieves an accuracy of 87.2% on the full test set.
The model "InferSent+Guidance" has the highest accuracy on the "Hard" subset.
The model "InferSent+Guidance+Reweight" has the largest decrease in accuracy from the "Easy" subset to the "Hard" subset.
The model "InferSent+Guidance" performs better on the "Hard" subset compared to other models.
The model "+Guidance+Reweight" has the largest decrease in performance from the "Easy" subset to the "Hard" subset.
The BLEU score decreases when both the shared vocabulary and length penalty are applied in the AGENDA model.
The CHRF++ score is higher when only the length penalty is applied in the WebNLG model.
CGE-LW (Levi Graph) achieves the highest BLEU score among all the models.
CGE-LW (Levi Graph) is a variation of the CGE model.
HSCJN achieves the highest BLEU-3 and BLEU-4 scores among the three models.
VHRED achieves the highest distinct-1 score among the three models.
SpanBERT outperforms Our BERT-1seq on both SQuAD 1.1 EM and F1 scores.
Human performance surpasses Google BERT on both SQuAD 2.0 EM and F1 scores.
Table 2 shows the performance (F1) on five MRQA extractive question answering tasks.
The average performance of Our BERT-1seq across the five MRQA extractive question answering tasks is 79.7.
SpanBERT achieves a higher average F1 score than the previous state-of-the-art (lee2018higher) on the OntoNotes coreference resolution benchmark.
"Our BERT-1seq" achieves a higher CEAFϕ4 F1 score than "Google BERT" on the OntoNotes coreference resolution benchmark.
The table presents the test performance on the TACRED relation extraction benchmark.
Our BERT-1seq achieves the highest F1 score among all the models listed.
Our BERT-1seq outperforms Our BERT on all tasks except for QNLI.
SpanBERT achieves the highest performance on all tasks.
Table 6 shows the effect of replacing BERT's original masking scheme with different masking schemes.
Geometric Spans has the highest F1 scores for QA tasks compared to other masking schemes.
The Linear SVM and RBF SVM models have higher F1 scores compared to the other models.
The Linear SVM model has a higher F1 score compared to the LSTM model.
The "All" feature set achieves the highest F1 score of 0.791 in the feature ablation test.
The "POS tags" feature set achieves the lowest F1 score of 0.734 in the feature ablation test.
The "CEI [ITALIC] causal Only" model has higher precision and F1 score than the "CP + CEI [ITALIC] all" model.
The "CP + CEI [ITALIC] causal" model has higher recall and F1 score than the "CEI [ITALIC] all Only" model.
The table shows the BLEU scores on the synthetic dataset of typos for two translation methods: "FR → EN" and "EN → FR".
The BLEU score for the "AdvSR" method with a noise fraction of 0.4 is 25.7.
Table 2 shows BLEU scores for different language pairs.
The AdvSR (Adversarial Scoring Rule) method consistently improves the BLEU scores compared to the Base method.
SBAT (w/o Local) achieves the highest BLEU4 score on the MSVD dataset.
SBAT achieves the highest ROUGE score on the MSR-VTT dataset.
SBAT achieves a BLEU4 score of 42.9 on the MSR-VTT dataset.
SCN achieves a CIDEr score of 77.7 on the MSVD dataset.
The "CL + PT + WLM" system achieves the lowest PER%.
Table 8 provides the lowercased BLEU scores for English-German translation on newstest'16 dataset for three different models trained on 250k and 500k sentences news-mix data.
The morph-gen model performs better than the baseline model, and the morph-gen-split model performs slightly worse than the morph-gen model, when trained on 250k sentences news-mix data for English-German translation on newstest'16 dataset.
The "morphgen" system achieves higher BLEU scores than the "baseline" and "serialization" systems.
The "morphgen" system performs better than the "baseline" and "serialization" systems on the test set.
The table compares different combinations of text and audio modalities.
The F1 scores for slot filling are consistently high across all modalities and feature combinations.
The "Hierarchical Embedding" method performs better than other embedding approaches in predicting Star Rating of reviews.
The "Hierarchical Embedding" method has a lower MAE than other embedding approaches in predicting Star Rating of reviews.
The table provides the values for B1, B2, B3, B4, C, M, and R for the evaluation split.
The table provides the values for B1, B2, B3, B4, C, M, and R for the testing split.
The common reasons for a decrease in score for perturbations 1, 2, 3, 4, and 5 are related to "Organization" and "Relevance".
Perturbation 8 ("AddSong") has the highest score decrease percentage.
The table shows the performance of different methods for spoken term detection.
The MAP(%) values for spoken term detection differ based on the method and index used.
The addition of PDR to the AWD-LSTM model decreases the perplexity values for both PTB Valid and WT2 Valid.
The addition of PDR to the AWD-LSTM (NoReg) model decreases the perplexity values for both PTB Valid and WT2 Valid.
The table shows the performance scores of different variations of the AWD-LSTM+PDR model on the PTB and WT2 datasets.
The realized performance of the SUPERVISED (SENSE + HEUR) classifier is higher than the oracle performance on both the news and web portions of the development data.
The realized performance of the HEUR dictionary is higher than the oracle performance on the web portion of the development data.
Table 7 shows the ambiguity of target strings in the development dataset using different dictionaries and methods.
The LNRM dictionary has a value of 765 for multiple entities ambiguity.
The dictionary "GOOG" performs better than the dictionary "EXCT" on the news subset of the development dataset.
The dictionary cascade "LNRM" performs better than the dictionary cascade "FUZZ" on the news subset of the development dataset.
The table displays the performance of dictionaries on the news subset of the development dataset.
The performance of the dictionaries is slightly higher for web counts only compared to Wikipedia counts only.
Table 14 provides performance results on the 2010 test data for the best dictionary and supervised classifier, broken down for the news and web subsets.
The performance of the supervised classifier with lexical features and heuristics is 0.8125 for the news subset.
Incorporating speaker scoring is crucial to the performance of the model.
The navigation error decreases as more components are added to the model.
The success rate increases when the pragmatic inference component is added to the model.
The method "ours" outperforms other methods in all performance metrics.
The method "ours" performs better than the method "Random" in terms of Test (unseen) NE.
The WER (%) for French is higher than the WER (%) for Spanish.
The WER (%) for Portuguese after fine-tuning is lower than the WER (%) for Portuguese before fine-tuning.
The WER for the English language in the monolingual model is 38.6%.
The average WER reduction for the multilingual adversarial models compared to the monolingual models is 10%.
Higher values of k in the scheduled sampling schedule result in gentler decay schedules.
The scheduled sampling case where predictions at the previous step are always passed on as inputs to the next step results in the lowest NER (F1) score.
The F1 scores for NER on the development and test sets are the same for the Baseline CE training procedure.
The α-soft annealed training procedure achieves a higher F1 score for NER on the test set compared to the Baseline CE training procedure.
The highest BLEU score in the "Individual" category is achieved in the "Ru" language.
The table shows the BLEU scores of 23 different languages translated into English using multilingual models based on different methods of language clustering.
The Embedding method consistently achieves higher BLEU scores than the Random and Family methods.
The method "Mucko" achieves an overall accuracy of 73.06% on the FVQA dataset.
The method "Straight to the Facts (STTF)" achieves an overall accuracy of 62.20% on the FVQA dataset.
The accuracy of the Mucko model decreases when certain components are removed.
The inclusion of both the Semantic Graph and Visual Graph improves the performance of the Mucko model.
Table 3 shows the results for shuffled and partial data training.
The P-Shuffle method achieves a score of 86.5 on the MCScript dataset.
The AddSent2Pas-Shuffle attack method has the highest average drop in performance across all datasets.
The probability of the answer is always higher than the probability of the distractor in the given table.
The probability of the answer decreases as more tokens are added from the AddSent2Opt-Shuffle distractor.
The word similarity score for Dcos increases with the number of reconstruction epochs.
D2 achieves the highest word similarity score among all the metrics at 50 reconstruction epochs.
The "random" embedding has a Syntactic score of 1.13.
The "D2" embedding has a Semantic score of 2.47.
The number of lookup parameters increases as we go from "full" to "full+emb" to "char" to "char(D1)" to "char(D2)" to "char(D∞)" to "char(Dcos)".
The "full+emb" and "char(D1)" models have the highest accuracy.
RoBERTa-Large achieves the highest "Core hit@1" and "Core + Adversarial hit@1" scores among all the models.
The models outperform the human performance in terms of "Core hit@3" and "Core + Adversarial hit@3" scores.
The JST model has the lowest performance among all the models in the Yelp Restaurants and Shopping datasets.
The MicroASM model has the highest performance among all the models in the Yelp Restaurants and Shopping datasets.
The table shows the performance of different models (JST, ASUM, JAST, MicroASM) on the Yelp Restaurants and Shopping datasets.
The MicroASM model achieves the highest performance on average across the Yelp Restaurants and Shopping datasets.
ASUM performs better than JST on average.
MicroASM performs better than JAST on Fantasy.
The table shows the aspect category assignment metrics on the Yelp Restaurant dataset for four different metrics: Diversity, Specificity, Agreeability, and Average.
The MicroASM metric has the highest value among all the aspect category assignment metrics on the Yelp Restaurant dataset.
The table presents the performance scores for different genres in the Naver Movie datasets.
The fraction of novel N-grams in the ground truth SOAP note increases as the value of N increases.
Algorithm LABEL:algo:task1 performs better in terms of the fraction of novel N-grams as the value of N increases.
The table presents the experimental results of different BERT models on the SCT_v1.0 test dataset.
Among the BERT models mentioned in the table, BERTLARGE (monolingual, cased) achieved the highest accuracy on the SCT_v1.0 test dataset.
The "BERTLARGE + MNLI (Ours)" method achieves the highest accuracy of 91.8%.
The "BERTLARGE (Our Implementation)" method achieves a higher accuracy than all the "BERTBASE" methods.
Table 5 provides experimental results with different natural language inference categories on the SCT_v1.0 test dataset.
The accuracy of the BERTBASE + MNLI model on the SCT_v1.0 test dataset is 90.6%.
The models used for the experiments are mBERT, BETO, and DistilledBETO.
The F1 score for mBERT (2.0 Sm TAR) is 66.30 and for mBERT (2.0 Def TAR) is 67.17.
BETO achieves the highest scores for MLQA EM and MLQA F1.
mBERT (1.1 MT) performs better than mBERT (2.0 MT) on XQuAD EM.
Table 4 provides a sample of TAR and MT translations error examples annotated in the user study.
The "English" column in Table 4 contains the English sentences used in the TAR and MT translations error examples.
Table 4 provides a sample of TAR and MT translations error examples annotated in the user study.
The motion picture, television, and music industry is centered in Los Angeles, California, with companies like The Walt Disney Company, Sony Pictures, Universal, MGM, Paramount Pictures, 20th Century Fox, and Warner Brothers having their headquarters there.
The table provides information about Tesla's treatise on charged particle beam weapons and his attempt to interest the US War Department, the United Kingdom, the Soviet Union, and Yugoslavia in the device.
The table provides information about Tesla's treatise on charged particle beam weapons and his attempt to interest the US War Department, the United Kingdom, the Soviet Union, and Yugoslavia in the device.
Both k-means (KM) and k-medoids (KD) perform poorly on the word embeddings.
Both k-means (KM) and k-medoids (KD) have negative reranking scores for BERT 12 (average).
The model "JDI-T (ours)" has a higher mean opinion score compared to other models.
The internal evaluation score for the model "FastSpeech" is lower compared to other models.
Table 8 presents the results of an ablation study on different representations (W, H, L, G) and their impact on Dev Edge-F1 and Dev Ancestor-F1 scores.
The combination of lexical representation (L), word embedding (W), head word semantics (H), and structural graph-based representation (G) achieves the highest Test Edge-F1 and Test Ancestor-F1 scores.
Table 2 provides a performance comparison in the open-world crowdsourcing evaluation on 1,000 sampled items.
Octet achieves significantly higher precision and the best F1 score in the open-world crowdsourcing evaluation.
The Octet method has the highest values for both Edge-F1 and Ancestor-F1.
Table 6 shows the open-world expert evaluation results.
The "TransE Comp" model achieves the highest accuracy among all the models.
The Bilinear model performs better in the Path query task on the WordNet dataset with compositional training.
The Bilinear-Diag model shows a higher reduction in error compared to the Bilinear model in the KBC task on the Freebase dataset.
The "Logistic Regression" method has a higher segmentation accuracy than the "Naive n-gram" method.
The "Logistic Regression" method has a higher query accuracy than the "Naive n-gram" method.
The "GloVe web crawl" word vectors have the highest segmentation accuracy and query accuracy among the pretrained word embeddings.
The "GloVe web crawl" word vectors have a higher segmentation accuracy compared to the "Google News word2vec" and "Facebook Fasttext wikipedia" word vectors.
Table 7 shows the weights of different negotiation tactics in both stages.
The feature "⟨ \textsl [ITALIC] donotproposefirst⟩" has a weight of -0.62 in the 1st stage.
The table shows test results using different initial systems for warmup, before and after iterative back-translation.
The standard deviation for DE-EN final is 1.7.
The final system for DE-EN translation has a higher similarity score compared to the initial system, while the final system for EN-DE translation has a lower similarity score compared to the initial system.
The NMT (100k) system has a lower similarity score compared to the SMT (full) system.
BERT outperforms the baseline, ELMo Peters (2018), and BERT (w/ probe) in terms of F1 score.
BERT (w/ probe) achieves higher accuracy on the semantic probe task compared to the baseline and ELMo Peters (2018).
The table shows F1 scores for the WSD task and semantic probe accuracy on the final-layer BERT-base.
The F1 score for the WSD task with a trained probe at 512 is 71.52.
The student model achieves higher performance than the teacher models on both MNLI m and MNLI mm tasks.
The ensemble model achieves higher performance than any individual teacher model on the SQuAD EM task.
T6 has the highest performance on both MNLI m and MNLI mm.
BERT BASE has the highest exact match score on the SQuAD dataset.
The table shows the model sizes of teacher and students, including the number of layers, hidden size, feed-forward size, number of parameters, and relative size.
The relative size of the T4-tiny model is 13% compared to the BERT BASE (teacher) model.
The RoBERTa-wwm model achieves the highest accuracy in both XNLI and LCQMC tasks.
The T4-tiny model with +DA achieves the highest F1 score in the CMRC 2018 task.
The table shows the classification F1 score on MRPC for different methods.
The F1 score for the original data is higher than the F1 score for the artificial data.
There are three different methods used in the experiment.
The accuracy for the +Spell-Checker method with artificial noise is 63.0.
The L2-CLL model performs better than the CLL model on average across all languages.
The L2-CLL model with 8 iterations of belief propagation performs better than the CLL model for all languages.
Table 10 represents the correlations for non-verbalized items, gold standard verbalized items, and system verbalized items in the first user study.
The correlation coefficient for non-verbalized items is 0.83, for SYS verb is 0.92, and for GS verb is 0.90.
The alignment score for the Headlines dataset is higher than for the Images dataset.
Combining the Type and Score for inter-tagger agreement results in a lower agreement score compared to Type and Score individually.
The score for "Images ALI" is higher than the score for "Headlines ALI" in the "Base+" system.
The score for "Images SCORE" is higher than the score for "Headlines SCORE" in the "Full" system.
The "Full" dataset achieves the highest score in the "Headlines SCORE" column.
The "Full" dataset achieves the highest score in the "Images T+S" column.
The table represents the results of the second user study on agreement level with non-verbalized items, gold standard verbalized items, and system verbalized items.
The agreement level is higher when the items are verbalized by the system compared to when there is no verb or when the gold standard verbalizes the items.
The "ReplaceOnly" method achieves the highest accuracy for Yelp reviews.
The "Full" method achieves the highest BLEU score for Amazon reviews.
The CrossAligned model achieves an accuracy of 74.7% and a BLEU score of 9.06 for Yelp classification.
The BackTranslate model achieves an accuracy of 76.7% for Amazon classification.
The "Point-Then-Operate" method has the highest success rate for both Yelp and Amazon datasets.
The "UnsuperMT" method has the highest score for the "Content" category in both Yelp and Amazon datasets.
Table 5 shows the distribution of input type switches (%) for different error types.
The percentage of input type switches for the "No error" category is 1.3% for V2V.
The table shows the distribution of correction types for different error categories.
The percentages for each correction type in the "No error" row add up to 100%.
The NLU error has the highest number of errors in the reformulation cause prediction.
The LG error has the lowest number of errors in the reformulation cause prediction.
The NLU error category has the highest number of errors in the confusion matrix.
The proposed method has fewer errors in predicting reformulation cause compared to the baseline method in the LG error category.
The proposed method outperforms the baseline in terms of F1-measure for all labels.
Adding NLU to the baseline decreases the F1-measure score for the "LG" label.
The Pipeline GRU has a higher BLEU score than the Pipeline Transformer.
The T5-Large model has a higher METEOR score than the Pipeline Transformer.
NESA outperforms RF, SVM, LogReg, and MLP in terms of Recall@1, Recall@5, MRR, and IEuc.
NESA achieves the highest values for Recall@1, Recall@5, MRR, and IEuc among all the models.
The NESA model has the highest recall at 1 among all the models in Table 3.
The NESA model has the highest Mean Reciprocal Rank (MRR) among all the models in Table 3.
The NESA model with Context L. has the lowest Recall@1 and the highest negative difference percentage compared to other ablations.
The NESA model with Word E. has the highest MRR compared to other ablations.
The table shows the performance metrics of different models in the baseline model ablation experiment.
The table shows the percentage differences in performance metrics of different models compared to the MLP model in the baseline model ablation experiment.
The class labels in the "class label" column represent different cognate clusters for the concept "person".
The "doculect" column lists different languages and their respective words for the concept "person".
The PMI score for each concept is the same for both alignments.
Each concept has two different alignments.
The highest performance in each domain is shown in bold in Table 2.
The average performance across all domains in Table 2 is 82.75.
The WS-UDA model outperforms other models in most domains.
The ASP2 model performs better than the ASP1 model in the "dvd" and "apparel" domains.
The data profile in Table 1 is based on a fixed number of 75 speakers.
The average duration of "‘Tsk-tsk’" utterances is longer than the average duration of "Cough" utterances in the CSLT-TRIVIAL-I dataset.
Table 4 shows the EER results on CSLT-TRIVIAL-I with the i-vector and d-vector systems.
The EER% for the "Cosine" metric is 5.88 for "Cough" and 11.91 for "Sniff" in the d-vector system.
The table presents EER results on CSLT-DISGUISE-I with the i-vector and d-vector systems for three different metrics: Cosine, LDA, and PLDA.
The LDA metric performs better with the d-vector system in terms of EER% compared to the Cosine and PLDA metrics.
Table VI shows the TD-SV results for the score-level fusion of MFCCs and BN features on the m-part-01 task of the RedDots database using the GMM-UBM method.
The MFCC & PHN-BN3 (39) + clustering fusion method achieves the lowest EER/MinDCF value among all the fusion methods.
In the English-Spanish translation task, the AL warping function performs better than the Linex warping function in both the optimistic and pessimistic settings.
In the French-English translation task, the Warped GP with the log warping function performs better than the standard GP.
The English-Spanish dataset has lower NLL values compared to the French-English and English-German datasets.
The English-Spanish dataset has higher MAE values compared to the French-English and English-German datasets.
The BiDAF model performs better on the DailyMail dataset than on the CNN dataset.
The AS Reader model performs better on the DailyMail dataset than on the CNN dataset.
Table 5 shows the variations of similarity function α (Equation 1) and fusion function β (Equation 2) and their performance on the dev data of SQuAD.
The dot product similarity function has an EM score of 65.5 and an F1 score of 75.5.
Table 5 shows the BLEU scores (%) of model ablation.
OpAtt outperforms the Seq2Seq+copy model on both the ESPN Dev and ESPN Test datasets.
The ROTOWIRE dataset is more challenging for all models compared to the ESPN dataset.
The model with both NER and Entity category embeddings has the highest F1 score.
Adding NER embeddings improves the Precision and F1 score.
The model "Self-attn††" has a higher recall than the model "CNN†".
The model "Knwl+Self (SI)" has the highest F1 score among all the models listed.
The table compares the performance of different models on the SemEval2010-Task8 dataset.
The models Knwl+Self (SI) and Knwl+Self (KISA) show statistically significant improvements over the Self-attn model on the SemEval2010-Task8 dataset.
The Cross-VAEs method outperforms the BERT QA and USE-QA methods on all metrics in the subset of SQuAD.
The Cross-VAEs method has the highest Mean Reciprocal Rank (MRR) among all methods in the subset of SQuAD.
The "Cross-VAEs" method achieves the highest performance in terms of SQuAD MRR, R@1, and R@5.
The "QA-Lite" method achieves a higher score in SQuAD R@1 compared to the "SenBERT" method.
The aspect "Waiters" is tagged as "S" (single) and the sentiment "friendly" is tagged as "O" (outside).
The position index for the aspect-opinion pair "(fugu sashimi, friendly)" is "0" for all the tags.
Table 3 shows the stage one results of aspect extraction and sentiment classification.
The F-score for aspect extraction in 15res using Our–TG is [BOLD] 74.19.
The F-score for the 15res dataset in the Our–TG method is [BOLD] 83.21.
The precision for the 14res dataset in the Our method is [BOLD] 78.02.
The F1 scores for the Classifier in all settings are the same, with a value of 97.59.
The F1 score for the Li-unified-R+ model in the pair setting is 56.85.
Table 1 compares the perplexity and topic coherence of different topic models on the 20News and RCV1-V2 datasets.
The iTM-VAE-Prod topic model achieves a lower perplexity on both the 20News and RCV1-V2 datasets compared to other topic models listed in Table 1.
The number of classes increases with higher values.
The expected value of the posterior distribution of α increases with higher values.
Table 1 provides the mean and standard deviations for the absolute performance of each probing task across languages and encoders.
The mean absolute performance for the Tense probing task is 0.708 with a standard deviation of 0.124.
Table 4 presents the results of model ablation tests for different variations of the model.
The "Originality" values for the "Full", "-ins", "-del", and "-both" variations of the model are 86.0, 85.1, 86.3, and 85.4 respectively.
Table 4 presents the results of model ablation tests for different variations of the model.
The "Originality" values for the "Full", "-ins", "-del", and "-both" variations of the model are 86.0, 85.1, 86.3, and 85.4 respectively.
The accuracy of the models increases as the number of layers in the DenseNet architecture increases.
The non-static version of the "dweNet GLoVe 50" model performs better than the static version.
Table 2 compares the performance of different models on two versions of the SARC dataset.
The CASCADE model achieves the highest accuracy and F1 scores among all the models.
The Stack-RNN+Softmax-Temp model achieved full accuracy on the test sets in 8 out of 10 times.
The Baby-NTM+Softmax model has a higher mean performance on the training set compared to the test set.
The "Stack-RNN+ Softmax" model has the highest performance in both the training set and the test set.
The "Stack-LSTM+ Softmax" model has a higher mean performance in the training set compared to the test set.
The table shows the performances of various vanilla and memory-augmented recurrent models on the deterministic homomorphic palindrome language.
The language with the highest BLEU score in the CoVoST Test is "Fr" with a score of 29.8.
The BLEU score for the language "De" in the CoVoST Test is 8.0.
The CoVoST Test WER and CER scores are different for different languages.
The English model has a WER of 36.1 and a CER of 20.3.
FriendsBERT uses BERT-uncased as pre-trained weights, while ChatBERT uses BERT-cased.
FriendsBERT is trained for 3 epochs, while ChatBERT is trained for 2 epochs.
The method "AdaGram" outperforms the method "Watlink" in both the "bts-rnc" and "wiki-wiki" datasets.
The "Dense" representation type performs better than the "Sparse" representation type in all datasets.
"FastFeatsNN5" has the highest performance in terms of mean rank and MRR among all the features.
"All features" has the highest recall at 150 retrieved lines among all the features.
The table provides average performance metrics for different models trained using either Internal Swap (IS) or External Swap (ES) on the Switchboard Coherence corpus.
The number of source dialogues is consistent across the train, dev, and test sets.
The number of positive/negative pairs varies across the train, dev, and test sets.
The performance of the biGRU model with the combination of entity role, entity word, and turn is better when using the External Swap methodology compared to the Internal Swap methodology.
The performance of the biGRU model with the combination of entity word, DA, and turn is better when using the External Swap methodology compared to the Internal Swap methodology.
Table IV shows the PERs (%) for segmental models trained end to end with multiple tasks, using the marginal log loss plus either the frame-wise cross entropy or the CTC loss.
For λ=0.84, the CTC test PER is [EMPTY].
The "marginal log loss" values are consistently lower than the "log loss" values in the "FC" column.
The "2s+ft" values are consistently lower than the "2s" values in the "pyramid" column.
The hinge loss is not reported for the test set.
The marginal log loss for the model trained with two-stage training followed by fine-tuning is 20.2.
Table III compares segmental models trained end to end with different losses with the CTC model.
The table provides information about the average real-time factor per sample to compute gradients for different losses.
The average real-time factor per sample to compute gradients for the pyramid loss in SRNN is 3.253.
CorefBERTBase and CorefBERTLarge achieve the highest scores for Dev F1 and Test F1.
BERT-TSBase + performs better on the Test F1 metric compared to the Dev F1 metric.
CorefBERTBase performs better than BERTBase in all MRQA extractive question answering benchmarks.
CorefBERTBase has a higher average F1 score than BERTBase in the MRQA extractive question answering benchmarks.
KGAT (CorefRoBERTaLarge) achieves the highest FEVER score on the FEVER test set.
The FEVER score tends to improve as the model architecture becomes larger.
Table 6 provides test set performance metrics on the GLUE task.
The matched accuracy for MNLI using BERTBase is 84.6% and the mismatched accuracy is 83.4%.
CorefBERTBASE outperforms BERTBASE and -NSP on all benchmark datasets.
-NSP +WWM performs better than -NSP on HotpotQA dataset.
The table compares single and multi-step inference strategies on MultiNLI, SNLI, Quora Question, and SciTail dev sets.
The single-step inference strategy performs better on SNLI compared to the multi-step inference strategy.
The BERT model outperforms other models on both the MultiNLI Test Matched and MultiNLI Test Mismatched datasets.
The SAN model performs well on the Quora Question Dataset.
The model "SegRNN" achieves the highest F1 score for DE-TR Segmentation.
The model "CharBiLSTM" achieves the highest accuracy score for ES-WIX Character-level Tagging.
The proportion of unique word types for the "ES" language tag is 45.76% and the total number of tokens classified as "ES" is 4218.
The number of unique word types for the "NE.WIX" language tag is 49 and the total number of tokens classified as "NE.WIX" is 77.
Table 3 shows the segmentation and LID test results for mixed words only.
The BiLSTM+Seq2Seq model achieved the highest F1 score for DE-TR Segmentation.
Table 5 presents the results of an ablation study of the two component model in HNN, with performance reported on dev and test sets for WNLI, WSCR, WSC, and PDP60.
The two component model in HNN achieves the highest scores of 77.1 for WNLI, 85.6 for WSCR, 75.1 for WSC, and 90.0 for PDP60.
Table 6 provides the results of an ablation study of the ranking loss on four different tasks: WNLI, WSCR, WSC, and PDP60.
The highest scores in the ablation study of the ranking loss are achieved by HNN for each task: WNLI, WSCR, WSC, and PDP60.
Table 2 provides the BLEU scores by reranking on the BTEC Chinese to English machine translation task.
The DGLSTM model outperforms the baseline model on the development set.
The Bleu score for the left-right translation using the Oracle model is 29.47.
The Ribes score for the +⟨end⟩-tuning translation in the test set is 43.98.
The "annealed" method outperforms the "left-right" and "uniform" methods in terms of Bleu score, F1 score, and exact match on both the validation and test sets.
The Bleu scores for all three methods decrease when evaluated on the test set compared to the validation set.
The polarity seeds used for sentiment polarity classification are "{cat},{dog}" and "{waitress},{waiter}".
The accuracy for aspect classification is 0.642 for "{cat},{dog}" and 0.635 for "{waitress},{waiter}".
LocLDA performs better than ME-LDA and W2VLDA in terms of precision, recall, and F-1 score for the "Topics Staff" category.
W2VLDA performs better than ME-LDA and LocLDA in terms of precision and recall for the "Topics Ambiance" category.
Using all three seed words for every aspect improves the accuracy of domain aspect classification.
Using the exact aspect words for polarity classification improves the accuracy of domain aspect classification.
Table 12 shows the impact of different polarity seed words for sentiment polarity classification.
The average accuracy for aspect classification is 0.698 and for polarity classification is 0.732.
The baseline performance for the MNLI task is 84.3/85.6.
The standard deviation of the supermask performance on the STS-B task is ±0.1.
As more layers are excluded from fine-tuning, the F1 score decreases.
As the percentage of task-specific parameter storage decreases, the F1 score decreases.
Table 6 compares weights pruned with low-sparsity supermasks and weights pruned with magnitude-based pruning at the same final sparsity.
The overlap between the supermask and the magnitude-based pruning mask is 11.1% for MNLI.
Table 8 shows the difference in the confusion matrix counts for ACE05 entity extraction associated with adding CorefProp.
The count for GPE decreases significantly when adding CorefProp.
Our method achieves a Rank@1 score of 26.24 on the validation set.
TGN achieves a mIoU score of 38.62 on the test set.
ResNet-152 outperforms VGG in terms of all the mentioned metrics.
FastText outperforms ELMo in terms of all the mentioned metrics.
The chance performance for the MSR-VTT dataset is 0.10 for R@1, 0.50 for R@10, and 1.00 for R@100.
The MEE method trained on DiDeMo achieves a mean rank of 186 on the DiDeMo dataset.
Table 1 compares different feedback methods (chunk-level feedback and sentence-level feedback) in terms of their impact on BLEU and TER scores for translation tasks.
The chunk-lcs feedback method achieves the highest scores among all the methods in terms of BLEU and TER for both De-En and En-Es translation tasks.
As the selection ratio increases, the De-En BLEU score decreases.
As the selection ratio increases, the En-Es TER score increases.
The table includes data labeling results for three different datasets: CoNLL (en), CoNLL (sp), and MUC.
The "Ours+decode" model has a lower SRL F1 score compared to the gold standard.
The "He (PoE)" model has a higher number of unique core role violations compared to the gold standard.
The "Ours+decode" model has a lower SRL F1 score compared to the gold standard.
The "He (PoE)" model has a higher number of unique core role violations compared to the gold standard.
Using a perfect parser improves the F1 score for the "Spouse" relation compared to not using a perfect parser.
Using features instead of data programming improves the F1 score for the "Protein" relation.
BLSTM-CRF-ILP multi achieves the highest F-score for both Key Argument Detection and All Argument Detection.
MaxEnt achieves the highest precision for Key Argument Detection.
The table presents the evaluation results of three different settings on the APRC test set.
The KL values decrease as we move from Hybrid to Hybrid + Aux to Hybrid + Ours.
The VRAE + BOW + KLA method has the highest KL score among all the methods.
The ATT-joint model outperforms all other models in terms of P@1, P@10, and MRR in all language pairs.
The GRU-MTL model performs better than the GRU and ATT models in terms of P@1, P@10, and MRR in all language pairs.
The accuracy and F1 scores for En&Fr are higher than the scores for En&Es in the BilDRL-ATT-joint model.
The F1 score for En&Fr is higher in the BiBOW model compared to the BiCNN model.
The "Kaminski" corpus has the highest number of documents and tokens compared to the other corpora.
The "NIPS 2015" corpus has the longest runtime compared to the other corpora.
Table 6 shows the runtime of the LFLDA and the N-VAE on the N20short dataset with various numbers of topics.
As the number of topics increases, the runtime of both LFLDA and N-VAE also increases on the N20short dataset.
The table shows the MAP result of Entity Classification for different models on the FB15K and FB20K datasets.
Table 5 shows the MRR scores obtained using REE for FB15k and FB15k-237 for various dimensions of the embedding space.
The MRR score for FB15k with a dimension of 500 is 0.816.
The model "DMRM" achieves the highest MRR and R@10 scores among all the models.
The model "DMRM" has the lowest Mean score among all the models.
The model "DMRM" outperforms all other models on MRR, R@1, and R@5.
The model "RvA" outperforms all other models on R@10.
The DMRM model outperforms the other variations in terms of MRR, R@1, R@5, R@10, and Mean.
The DMRM model with 1-hop reasoning performs worse than the other variations in terms of MRR, R@1, R@5, R@10, and Mean.
Method S2 achieves the highest STOI score among all the methods compared in Table 3.
Method CRN achieves the second-highest PESQ score among all the methods compared in Table 3.
The training set contains 5,749 data points.
The dataset contains a total of 8,628 data points.
P-thought with one layer-Bi RNN has the highest Pearson correlation score.
Sent2vec has a higher Pearson correlation score than PV-DBOW.
The table shows the distribution of action types in the proxy section of the newswire section of the LDC2014T12 dataset, generated from automatically aligned data.
The "NONE" action type has the highest number of tokens in the proxy section of the newswire section of the LDC2014T12 dataset, generated from automatically aligned data.
DeAtt (d=200) outperforms other models in terms of NLI Accuracy and Paraphrase Accuracy.
Q-Att (d=50) performs better than DeAtt (d=50) on SciTail in terms of NLI Accuracy.
Table 2 compares the performance of STEAM against the baseline methods on three different datasets.
STEAM achieves the highest accuracy in the Environment category and the highest Wu&P score in the Food category.
The STEAM variant performs better than other variants in terms of accuracy, mean reciprocal rank (MRR), and Wu&P metric on all three datasets.
The Concat-D variant performs better than other variants in terms of accuracy on the Food dataset.
The performance of BERT-wwm-ext-L is higher than the performance of BERT-wwm on both the Dev QAC and Test QAC datasets.
The performance of RoBERTa-wwm-ext-L is higher than the human performance on both the Dev QAC and Test QAC datasets.
Table 3 provides a comparison of the performance of different models with previous state-of-the-art models.
Gated C+ P2C outperforms the other models in terms of DC Top-1, DC Top-5, DC Top-10, and DC KySS metrics.
The Gated C+ P2C module outperforms other modules in terms of both DC Top-1 and PD Top-1 scores.
The CoCat module outperforms other modules in terms of both DC KySS and PD KySS scores.
The SemBleu score increases as the n-gram order increases.
The SemBleu score with n=4 is higher than the overall SemBleu score.
The "3x40" embedding size has the highest MRR value among all the different embedding sizes.
The "3x40" and "1x82" embedding sizes have the highest H@ 10 values among all the different embedding sizes.
The normalization accuracy for zero-shot experiments is higher than the identity experiments.
The English dataset has the highest identity accuracy.
Adding the adversarial component improves the overall F1 score for both the "COMMA" and "PERIOD" tasks.
Adding the adversarial component improves the F1 score for both the "COMMA" and "PERIOD" tasks in the ASR model.
Transferring parameters from pre-trained BERT to BERT-CRF improves the performance in terms of P, R, and F1 scores.
Punctuation predicting models that use pre-trained BERT consistently achieve higher F1 scores compared to the models that use random initialization.
The word "they" appears 17,896 times in the German–English sentences before filtering.
The table shows question-answering accuracies using different types of QA pairs and different source inputs.
The accuracies for NER are higher when using "QASumm+NoQ" and "GoldSumm" as the source input compared to "NoText" and "FullText".
The "QASumm+NER" system has a higher accuracy than the "Pointer+Cov." system.
The "Pointer+Cov." system takes more time than the "QASumm+NER" system.
The accuracy of the TotalProb method is 0.65.
The accuracy of the Human performance is 0.64.
The top-k evaluation performs better than the nucleus and random evaluations for the "Train top-k" strategy.
The mixed dataset performs better than the nucleus and random evaluations for the "Train mixed" strategy.
The LF-MMI model achieves a score of 5.2 on the Bg LM Dev93 dataset.
The average value of the context gates remains the same for correct and incorrect entities.
The word/span gate has a much higher value when the prediction is correct for both entities and non-entities.
The "Full system - FW context - BW context - Glove finetuned" performs the best in terms of F1 score on both CoNLL and Wikipedia datasets.
The "FW context - BW context - Glove finetuned" has the highest recall score on the MUC-6 dataset.
The Pooling model with GloVe embeddings achieves the highest MCC score on the CoLA task.
The LSTM LM WLPM model with BNC embeddings achieves the highest MCC score on the SVO phenomenon.
The accuracy and MCC for the in-domain and out-of-domain tasks are lower when using GloVe embeddings compared to ELMo-Style embeddings.
The average and aggregate human evaluations have higher accuracy and MCC compared to all the other models and embeddings.
Split 5 has the highest overall accuracy among all the splits.
Split 2 has the most diverse sources for out-of-domain sentences.
Table 1 shows the validation and test results of different models on three language modeling tasks.
The model "3-HM-LSTM + LN" performs better than other models on the Char-PTB test.
"Zoneout" has a lower error rate than "Recurrent dropout" and "Recurrent batchnorm". The combination of "Recurrent batchnorm & Zoneout" has the lowest error rate among all models.
The "Recurrent batchnorm & Zoneout" model has the lowest error rate on the test set.
The BERT model outperforms the DeFormer-BERT model in both the Base Model and Large Model categories.
The layerwise representation similarity loss (LRS) contributes to the performance of the models in both the Base Model and Large Model categories.
BERT-large performs better on SQuAD than BERT-base.
BERT-base has a lower speed than DeFormer-BERT-large.
Table 4 provides information on the inference latency of BERT-base and DeFormer-BERT-base on SQuAD datasets, measured in batch mode, with different hardware configurations.
DeFormer-BERT has lower inference latency than BERT on both the GPU and CPU.
The SDEN + DR model has the highest F1 score for both Intent and Slot Token, and the lowest Frame Error Rate among all the models.
The MN + DR model has a higher F1 score than the ED + DR model.
The "Word shallow-and-wide CNN" model achieves the highest accuracy scores among all models for most datasets.
The "fastText (Joulin et al., 2016)" model achieves the highest accuracy score for the AGNews dataset.
The table includes case studies of different numbers of specific topics.
The table shows the accuracy percentages for different tasks.
The proposed method outperforms all previous methods in all language pairs.
The proposed method has the highest average performance across all language pairs compared to all previous methods.
The proposed method outperforms previous retrieval methods in terms of P@1 score for all language pairs.
On average, the proposed method achieves a higher P@1 score compared to all previous retrieval methods.
The table shows the dependency parsing scores for different variations of the RNG Transformer model on the development set of UD Turkish Treebank.
The DepBERT+RNG-Tr(T=1) model has a higher UAS score than the DepBERT model.
The addition of the RNG-Tr refinement improves the performance of the Mono DepBERT model on the English language.
The Multi+Mono UDify+RNG-Tr model performs better on the Russian language compared to the Mono DepBERT+RNG-Tr model.
Table 5 shows the F-score and relative difference of UDify and its refinement with RNG-Tr for a subset of dependency types on the concatenation of UD Treebanks.
The F-score for the "aux" dependency type is 98.40 for UDify+RNG-Tr, which is a relative increase of 67.1% compared to UDify.
Exp. 1 has 10,749 training images.
Exp. 2 has 25 evaluation attributes.
The sMlp and sMup models have higher Pairwise-BLEU scores than the hMlp and hMup models.
The sMlp and sMup models have higher BLEU E&M scores than the hMlp and hMup models.
Table 2 shows the results on three WMT datasets: En-De, En-Fr, and Zh-En, with 10 human references for En-De and En-Fr, and 3 human references for Zh-En.
The BLEU score for En-De is higher when using the "beam" sampling method compared to the "Pairwise-BLEU" method.
The accuracy values in the "Accuracy" column are sorted from worst to best.
The "Contextual-All" model has the highest accuracy among all the models.
RAML performs better than ML in terms of the "Parsing E.M." metric.
RAML performs better than ML in terms of the "MT C-B" metric.
The RAML Reward and RAML BLEU scores increase as the value of τ increases.
The RAML Reward and RAML BLEU scores are consistently higher than the SQDML Reward and SQDML BLEU scores for each value of τ.
The method with a threshold of 0.2 achieves the highest accuracy on the test results.
The ML Baseline achieves an accuracy of 98.2 on the development results.
The ML Baseline method performs better on the development set than on the test set.
As the threshold τ increases, the UAS generally increases, except for when τ=0.4.
As the value of τ increases, both the sentence-level BLEU (S-B) and corpus-level BLEU (C-B) scores also increase.
The τ=0.4 row has the highest sentence-level BLEU (S-B) score and corpus-level BLEU (C-B) score.
The Proposed Model outperforms the ML Baseline method in both the S-B and C-B metrics.
Li et al. (2017) achieved the highest score in the S-B metric.
The RAML method with importance sampling using τ=0.70 has a higher Bleu score than the ML Baseline method.
The RAML method with N-Gram has the highest Bleu score among all the methods.
The NeuralCRF(Our model) achieves higher F1 scores on both the CTB5 and CTB7 datasets compared to other methods.
The CRF(character) method achieves lower Precision scores on both the CTB5 and CTB7 datasets compared to other methods.
The Gaussian Transformer model achieves a performance score of 89.2 on the SNLI dataset.
The BERT baseline model achieves a performance score of 84.6 on the matched test dataset of MultiNLI.
The model with the auxiliary loss (i.e., ℓce+ℓmargin) performs better than the model without the auxiliary loss on both SMPI and IMPI.
The model without the auxiliary loss has a slightly lower performance compared to the model with the auxiliary loss on both SMPI and IMPI.
Table 4 presents the results of a linguistic error analysis on MultiNLI development matched sets.
The SMPI and IMPI models outperform the baseline model in most linguistic error categories.
The IMPI model outperforms the Baseline model in terms of overall performance.
The IMPI model outperforms the Baseline model in handling antonym-related tasks.
The "Voting" method with "SkipThought + Image" features has the highest performance in the sequencing task.
The "Random Order" method has the lowest performance in the sequencing task.
The models B0, B1, B2, and B3 represent different experiments with different training data sizes.
The CER performance for the B0 + M0 model improves as the training data size increases.
The models used in the experiment are denoted by A0, A1, A2, A3, and A4.
The CER performance decreases as the models progress from A0 to A4.
The odds ratios for all keywords in Table 1 are statistically significant.
The keyword "fever" has the highest odds ratio among all keywords in Table 1.
There are 17 tweets that are actually sick but predicted as not sick.
There are 66 tweets that are actually not sick but predicted as sick.
The classifier predicted 27 instances as "Not Sick" when the true value was "Sick".
The classifier predicted 192 instances as "Sick" when the true value was "Not Sick".
The analysis of variance shows a significant difference between the groups.
The choice of classifier has the largest impact on the area under the ROC curve.
The model "PL-FT" has the highest average score among all the models.
The model "PTGen+Cov" has the highest variability in scores among all the models.
The "basic" model performs better than the "Du et al., 2015" and "A&M, 2015" models in all three tasks (DM, PAS, PSD).
The average performance of the "basic" model is higher than the average performance of the "Du et al., 2015" and "A&M, 2015" models.
Table 5 shows the pairwise structural similarities between the three formalisms (DM, PAS, and PSD) in terms of unlabeled F1 score.
The unlabeled F1 score for PAS in the directed DM formalism is 54.9.
The unlabeled parsing performance of freda1 and freda3 is higher than the labeled parsing performance for PAS.
The unlabeled parsing performance of freda1 and freda3 is higher than the labeled parsing performance for DM.
The method "sum * pm" has the highest logit and log prob values compared to other fusion methods.
The method "sum * pm" has the highest SPL value compared to other fusion methods.
The "Our baseline SMNA" model performs worse than the "Seq2seq" model on the "Validation Seen NE" task.
The "Greedy Speaker-Follower" model performs better than the "Beam Speaker-Follower" model on the "Test Unseen SR" task.
The "+ + Fast" model improves the SR (%) performance by 6 percentage points compared to the "Validation Unseen Speaker-Follower" model.
The "+ + Fast" model improves the SPL (%) performance by 1 percentage point compared to the "Validation Unseen Speaker-Follower" model.
The "Ours: Fine-tuned augmentation" method achieves higher accuracy than other data augmentation methods in all three datasets (SST-5, IMDB, TREC).
Adding validation data to the base model improves the accuracy in all three datasets (SST-5, IMDB, TREC).
The "Ours" model consistently outperforms other models in terms of accuracy across different proportions.
Increasing the proportion of training examples in each class leads to higher accuracy for all models.
Our model outperforms the base model and the ren2018learning model in terms of accuracy for all three proportions.
The accuracy of all models improves as the proportion of training examples increases.
The throughput of the LSTM-2048 model is 169 tokens/s.
The responsiveness of the GCNN-8 Bottleneck model is 45,878 tokens/s.
"BIG GLSTM-G4" has the lowest Test PPL value among all the models in the table.
"GCNN-14 Bottleneck" has a lower Test PPL value than "GCNN-13".
The GCNN-14 model has the lowest Test PPL value among all the models listed in the table.
The GCNN-14 model is trained using 4 GPUs, while the other models are trained using only 1 GPU.
The SemSentSum model performs the best in terms of ROUGE-1 and ROUGE-2 scores among all the models.
The FreqSum model performs the worst in terms of ROUGE-1 and ROUGE-2 scores among all the models.
The "SemSentSum" model achieves the highest ROUGE-1 score of [BOLD] 39.12.
The "TCSum" model achieves a ROUGE-2 score of [BOLD] 9.66.
Table 4 shows the performance of Transformer models with different layers in the encoder.
The BLEU score improves as the number of layers in the encoder increases.
The HRL-16 method outperforms the RL-baseline and XE-baseline methods in all metrics except for CIDEr.
The RL-baseline method performs better than the XE-baseline method in terms of the BLEU-4 (B@4) metric.
"HRL (Ours)" outperforms all other methods in terms of BLEU@4, METEOR, ROUGE-L, and CIDEr scores.
"HRL (Ours)" performs similarly to "XE-baseline" in terms of BLEU@4 score but achieves higher METEOR, ROUGE-L, and CIDEr scores.
Table 2 compares the Zero-Shot (ZS) and Few-Shot (FS) performance of Vanilla BERT (multilingual) on different datasets.
The Few-Shot (FS) performance of Vanilla BERT (multilingual) on the Spanish 3 dataset is higher than the Zero-Shot (ZS) performance.
The highest accuracy is achieved with a batch size of 96 and a learning rate of 3.00E-05.
The highest F1 score is achieved with a batch size of 96 and a learning rate of 3.00E-05.
The model "RoBERTa-large" achieves the highest F1 score of 91.48±0.51.
The model "ALBERT-xxlarge" achieves a precision score of 97.99±0.15.
The average classification accuracy across all source-target pairs is 0.858.
The classification accuracy when the source is "Kitchen" and the target is "Books" is 0.845.
Table 5 provides information about the performance over POS tagging and NER.
The model with HRN achieves the highest performance for POS tagging.
BERT performs better than BERTCONS based on the F1 scores.
VADER-Sent-−−→←−−GRU performs better than NRC-Emotion-−−→GRU based on Precision and Recall scores.
The table compares the performance of two models: VADER-Sent-−−→←−−GRU and NRC-Emotion-−−→GRU.
The model NRC-Emotion-−−→GRU has a higher F1 score compared to VADER-Sent-−−→←−−GRU.
The table shows the test accuracy (%) on SCAN splits.
The test accuracy for "jump" using CNN is [BOLD] 69.2±8.2.
The average MAP scores for element mappings at various granularity levels increase as the value of k increases.
The average MAP scores for element mappings are higher for expressions than statements at all levels of granularity.
The mBART-based model performs well on the Czech language, achieving an F0.5 score of 73.52.
The mBART-based model does not perform well on the Russian language, achieving a low F0.5 score of 15.38.
The BART-based model achieves the highest precision on the CoNLL-14 (M2) dataset compared to other models.
The model with ensemble achieves the highest precision on the BEA-test dataset compared to the MASS-based model.
The table compares the performance of MASS-based and BART-based models on GEC accuracy to BEA-valid for each error type.
The F0.5 score for the MASS-based model is 22.34.
The precision and F1 score for the constituency level are both 87.6.
The recall and F1 score for the discourse level are both 40.2.
Table 3 presents experiments using gold segmentations and provides information about different systems and their performance on syntactic feats.
The neural system with MALT as the syntactic feature achieves a performance of 81.6 on structure.
Table 4 provides human evaluation results on the Gigaword dataset, comparing the readability and informativeness of the model with two annotators.
Annotator #2 rated the model higher in both readability and informativeness compared to Annotator #1.
Table 1 shows the correlation between empathy and various coordination measures.
The correlation between empathy and uCLiD, nCLiD, and global WMD is statistically significant.
The R-1 score for the Electronics dataset without length normalization is 13.36.
The R-SU4 score for the Movies&TV dataset with length normalization is 5.71.
The NRT algorithm achieves the lowest MAE value for the Books category.
The PMF algorithm achieves lower MAE values than the LRMF algorithm in all categories.
The table includes different approaches, including baselines and variants of the proposed approach.
The F1 score for the "Ensemble with (Berant & Liang, 14)" method is higher than the F1 score for the "Without multiple predictions" method.
When the minimum count is set to 5, the training corpus from Wikipedia has a higher score compared to the training corpus from NRLS.
Table 5 provides a comparison of BERT-based models.
The BERT-attention model achieves the highest P@1 score on Test2 in the insuranceQA dataset.
"AP-LSTM" outperforms "QA-LSTM (our impl.)" in terms of P@1 (Test1) score.
"MULT" achieves the highest P@1 (Test2) score among all models.
The model "KAN" achieves the highest P@1 and MRR scores among all the models in Table 4.
The model "KAN" achieves the highest scores in both P@1 and MRR metrics compared to the other models in Table 4.
The model "HAS" achieves the highest MAP and MRR scores on the wikiQA dataset.
The model "SUM [ITALIC] BASE, [ITALIC] PTK" outperforms the "LRXNET" model in terms of MAP and MRR scores on the wikiQA dataset.
The table presents the performance of three different versions of the proposed model on the MS COCO split test set.
The "OBJ2TEXT-GT" version of the proposed model achieves the highest Bleu_4 score on the MS COCO split test set.
Table 9 shows the results of STS training on English and testing on Spanish.
The embedding "N" performs better than other embeddings in terms of test scores for both English and Spanish.
The coverage for all embedding types in the word similarity datasets is above 90%.
The coverage for FastText (FT) and GloVe (GV) in the word similarity datasets is the same.
The combination "FT+UKB+P" has the highest score for the "Rel" column.
The "AR" source embeddings have the highest score for the "Sim" column.
The table compares the performance of different meta-embeddings in the English Word Similarity task.
The FT+UKB+N meta-embedding achieves the highest similarity score among all the meta-embeddings in the English Word Similarity task.
Table 6 presents the results of multilingual and cross-lingual word similarity using cross-lingual meta-embeddings.
The word similarity score for the EN-ES pair using the MUSE model is 62.4.
The combination of Flair and GV (news) achieves a word accuracy of 97.23 for Spanish.
The combination of Flair and various embeddings achieves a word accuracy of 96.96 for English.
The "MHier" model achieves the highest F1 scores for both the "I2B2’06" and "I2B2’14" test sets.
The "MHier" model achieves statistically significantly better results than other models for both the "I2B2’06" and "Physio" datasets in the "I2B2’14" test set.
The Hier model performs better than the Indep model in the tag-set extension experiment.
The Indep model performs better than the Hier model for the Hospital Total dataset.
The Hier model performs better than the Indep model in the tag-set extension experiment.
The Indep model performs better than the Hier model for the Hospital Total dataset.
The model "Attention + Tsf LM" outperforms the model "CTC + Policy" on both "test-clean w/ LM" and "test-other w/ LM".
The model "Jasper" has a lower word error rate (WER) on "test-clean w/ LM" compared to the model "CIF + SAN (big)".
Table 2 presents the results of ablation studies on Librispeech, where different strategies or components are removed from the full model.
The full model without any ablations achieves a WER of 4.48% on the test-clean set and 12.62% on the test-other set.
Table 1 provides SDR and SI-SNR values for the min and max test datasets of the WSJ0-2mix database.
The SDR value for the "WSJ0-2mix min (ours)" dataset is 15.3 dB.
There are four different models labeled MM (1), MM (2), MM (3), and MM (4).
The Mean Square Error (MSE) for the True Marks (TM) is consistently lower than the MSE for the Mean Marks (MM) across all four models.
Most models in Table 6 show an improvement in performance compared to the baseline model.
The model with the highest performance in Table 6 is "+post processing" with a score of 33.0 on newstest2019.
The algorithm "CNN" achieves the highest Micro-F1 score on Twitter.
The algorithm "PTE(Gwd+Gwl)" achieves the highest Macro-F1 score on MR.
There are four different classes in the perpetrator labeling task.
The F1-measure for class O is higher than the F1-measure for class I in the perpetrator labeling task.
The table presents the ranking evaluation of sexual violence reports for different values of k.
The precision of the ranking evaluation for sexual violence reports is consistently high across different values of k.
Table 7 shows the performance of different classifiers on the detection and characterization tasks.
The F-1 score for the "Victim Category" is consistently higher than the F-1 scores for the other categories.
The table shows the results of an ablation study on four different models: DAMP, DAMP−dis, DAMP−att, and coarse2fine.
The coarse2fine model has the lowest EM rate compared to the other models.
The table presents results on the DUC 2004 task 1 for the "LSTM EncDec+WFE" and "Transformer+LRPE+PE" methods.
The "+ALONE (Real number)" method with a "Dinter" value of 1024 achieves the highest score in the R-2 metric.
The method "Transformer+ALONE (Binary)" with 8192 dimension performs better than the method "Transformer+ALONE (Real number)" with 8192 dimension.
The model with factorized embeddings (4.3M) performs better than the model with conventional word embeddings (16.8M).
Table 2 provides performance results for different models.
The Soft-attention Model has the highest F1 score and AUC_ROC value among all the models.
The systems are ordered by their FPN1 score.
The systems are ordered by their ρPN score.
The best system for Subtask A "Message Polarity Classification" in 2016 achieved a score of 0.671 on the "Tweet" dataset.
The best system for Subtask A "Message Polarity Classification" in 2015 achieved a higher score on the "Tweet" dataset compared to the best system in 2016.
The "RMSNorm" model achieves the highest score on the Test17 dataset with a score of [BOLD] 23.7.
The "RMSNorm" model has a speedup of 24.7% compared to the "LayerNorm" model.
Model M performs better than the baseline.
The LayerNorm model performs worse than the baseline.
Table 1 shows the automatic evaluation results of different methods on the SQuAD test set.
The G2S [ITALIC] sta+BERT+RL method achieves the highest BLEU-4 score among all the methods.
The model "swayamdipta-etal-2016-greedy" outperforms other models in terms of joint dependency syntax and SRL on the CONLL08 English dataset.
The model "johansson-2009-statistical" utilizes joint inference as its main feature.
The models in Table 2 are categorized into three groups: Graph-based, Statistical Models; Transition-based, Statistical Models; and Graph-based, Neural Models.
The model "zhou-zhao-2019-head" achieves the highest UAS score of 97.2 on the PTB dataset.
Table 2 shows the results on the public FCE test set when incrementally providing more training data to the error detection model.
The F0.5 score for the test set increases to [BOLD] 64.3 when adding the CAE training data.
The precision of Annotation 1 is higher for CAMB than for Bi-LSTM (full).
The recall of Annotation 1 is higher for CUUI than for P1+P2+S1+S2.
The model trained on the original training data performs better on the evaluation data compared to the model trained on the adversarial training data.
The model trained on the original training data performs better than the model trained on the adversarial training data.
The "DecompRC" model achieves the highest F1 score on the "Distractor" task.
The "Cognitive Graph" model achieves the highest F1 score on the "Open" task.
The F1 score for the Open-domain 10 Paragraphs setting is 67.08.
Adding a Gold Paragraph to the Open-domain 500 Paragraphs setting increases the F1 score to 53.12.
The table shows attribute capturing scores with different methods of attribute fusion.
Attr-A (Both) has a higher attribute capturing score than the Baseline and Attr-T (Both) methods.
The "Attr-A" model achieves the highest BLEU score for all aspects and user categories.
The "Both" user category achieves the highest BLEU score for the "Attr-A" model.
Table 5 shows the results of model ablations on three different components: "knowledge," "user category," and "aspect."
The diversity score for the "knowledge" component is higher than the diversity scores for the "user category" and "aspect" components.
The proposed model KOBE outperforms the baseline model in terms of fluency, diversity, and overall quality.
The proposed model KOBE achieves a higher overall quality score than the baseline model.
Table 7 shows the experiment results of Context Model using different pretrained word vectors.
The dev F1 score for the Context Model using ELMo as the pretrained word vectors is 73.16, and the test F1 score is 66.35.
Table 6 shows the experiment results of Context Model using different pooling functions.
The BiLSTMMean model achieves a dev F1 score of 61.95.
Table 9 compares the performance of the Context Model (CTX) and the Combined Model (CMB) on the dev/test sets.
The Combined Model (CMB) has a higher number of correct predictions compared to the Context Model (CTX).
The probability of a blame tie existing between an entity and itself is always 1.
The probability of a blame tie existing between e1 and e2 is 0.82, between e1 and e3 is 0.02, and between e1 and itself is 0.00.
The accuracy of the model in UCLMR's FNC-1 submission is 88.46%.
The number of 'agree' predictions in UCLMR's FNC-1 submission is 1,593.
The "Transfer Fine-Tuning" model outperforms the "BERT-base" and "BERT-large" models in most tasks.
The "BERT-large" model performs better than the "BERT-base" model in the "Semantic Equivalence STS-B" task.
Table 6 shows the test results on semantic equivalence assessment and NLI tasks scored by the GLUE evaluation server.
The "Simple Transfer Fine-Tuning" model achieved a score of [BOLD] 87.7 on the Semantic Equivalence STS-B task.
The score for Simple Transfer Fine-Tuning on the MRPC task is 88.6.
The score for BERT-base on the QNLI task is 89.1.
The new method outperforms the two previous models in terms of overall performance.
The new method achieves higher precision, recall, and F1 scores for both named entity and nominal mention tasks compared to the two previous models.
The table provides results for different intensity prediction tasks such as anger, fear, joy, sadness, and valence.
There are different numbers of submissions for each intensity prediction task for which the average score difference between AA and EA is not significant.
The Baseline model has the lowest average distance between each time period's center and the year that minimizes the perplexity value of the corresponding LOWESS curve.
The LSTM model has the highest average distance between each time period's center and the year that minimizes the perplexity value of the corresponding LOWESS curve.
The test accuracy is higher when the network includes year information as input and has LSTM connections in the hidden layer.
The test accuracy is lower when the network does not include year information as input and has feedforward connections in the hidden layer.
Table 3 presents the classification accuracies on 10 probing tasks evaluating the linguistic properties of "Surface", "Syntactic", and "Semantic".
Our model achieves a slightly lower accuracy of 92.11% compared to the base model's accuracy of 92.20% in the "Surface" task.
GroupFunneling (Rand. Initialized + Emb. Distil.) achieves the highest BLEU score among the three models.
The BLEU score for GroupReduce is 42.13.
The Funneling with Emb. Distillation model achieves the highest BLEU score of 42.60.
The Approach AI performs better than the Approach Baseline and Approach B1 in terms of WER in all accents.
The average WER for all approaches is higher in the "Non-US" accent compared to the "US" and "CA" accents.
The WER (%) for approach B3 is higher than the WER (%) for approach B2.
The WER (%) for approach AI is lower than the WER (%) for approach Baselines.
The table represents decisions (D1, D2, D3, D4, D5) and their corresponding references (r1, r2, r3, r4, r5).
Decision D1 has a high similarity score with reference r4.
The D-PAGE models have higher distinct-1 scores compared to the other models.
The D-PAGE models have higher JD scores compared to the other models.
The average length of the sources is the same for all models.
The D-PAGE-4 model on PPDB has the largest difference in average length of outputs of kth decoder.
D1 has a strong reference to r4.
D5 has a moderate reference to r4.
The table shows the results of an ablation study on different variations of the model architecture.
The model variation with 1-layer GCN on DT + 2 SA-GCN blocks (head-ind) achieves the highest accuracy on the restaurant aspect.
BERT-Single performs better on the Restaurant dataset compared to the Laptop dataset.
The model "1-layer GCN on DT + 2 SA-GCN blocks(head-ind)⋄" performs better on the Restaurant dataset compared to the Laptop dataset.
CMR achieves the highest accuracy on both the dev and test sets in NLVR2.
All models achieve an accuracy above 50% on the dev and test sets in NLVR2.
The model "CMR" has the highest accuracy on both the Dev% Overall and Test Standard% Overall.
The model "LXMERT" has the highest accuracy on the Test Standard% Other.
As the dataset size decreases, both the across-speaker and within-speaker percentages increase.
The average across-speaker percentage is 19.856% with a standard deviation of 0.449, and the average within-speaker percentage is 12.986% with a standard deviation of 0.186.
As the percentage of the French dataset used for training decreases, the number of hours and the number of samples for the training set and the validation set also decrease.
As the number of hours that the percentage represents decreases, the number of samples for the training set and the validation set also decreases.
The model "E3 (ours)" has the highest values for both Micro Acc. and Macro Acc.
The BERTQA model achieves a BLEU1 score of 46.2.
The model "LSTM" has the highest training accuracy among all the models.
The test accuracy decreases when replacing words w with unseen synonyms s(w) in the test set.
The cosine distance between words w and their unseen synonyms s(w) is relatively low.
The test accuracy is higher when the word w is present compared to when it is replaced by unseen ontological twins t(w).
The cosine distance between word w and its unseen ontological twin t(w) varies for different word pairs.
The "ensemble of 12" translation system has a higher Bleu score for English→German translation than the "parallel (greedy)" system.
The "parallel (beam 12)" translation system has a higher Bleu score for English→German translation than the "none" system.
The "Gigawordmono" dataset has the largest number of sentences.
The "WIT" dataset has the smallest number of sentences.
The "Our model" method has the highest accuracy among the methods listed in the table.
The "Sequential short-text classification" method has a higher accuracy than the "Neural attention" method.
The table shows Spearman's rank correlations ρ with Simlex-999 when paraphrases of different types were used.
The correlation between paraphrases and Simlex-999 is higher when both equivalence and entailment are considered compared to when only equivalence is considered.
The correct answer rate for word analogical reasoning tasks is 44.96% when considering equivalence, entailment, other, and exclusive paraphrases.
The correct answer rate for the syntactic part of word analogical reasoning tasks is 40.22% when considering equivalence, entailment, other, and independent paraphrases.
The table shows the best correct answer rates in the word analogical reasoning task for the text8 and enwiki9 datasets.
The correct answer rate for the enwiki9 dataset is higher than that for the text8 dataset in the word analogical reasoning task.
The proposed method outperforms CBOW, GloVe, Faruqui, JointReps, and CBOFP in terms of the total percentage.
Enriched CBOW performs the best in terms of the syntactic percentage among all the methods.
The perplexity of the personalized model is lower than the user-agnostic model.
The personalized model has a larger size than the user-agnostic model.
Table 6 shows the effect of K on word perplexity of test performance for different methods.
ConGraD with K=5 has the lowest word perplexity among all methods.
The "Attention-based Enc-Dec (LV)⋆" model has the highest BLEU score of 37.19.
The "Attention-based Enc-Dec (LV)⋆" model has a relative improvement of +106.0% compared to the "Simple Enc-Dec" model.
The ablation test on entity salience features explores different combinations of features.
The combination of the "1st_loc" feature and "head_count" feature achieves the highest accuracy in the ablation test.
The model with "+ entity salience" has a higher accuracy than the model without it.
The model with "EventComp-40M" has a lower accuracy than the model with "EventComp-8M".
The highest accuracy on ON-Long is achieved when using "+ entity salience" with "EventComp-40M" model.
The "MostFreq" model has a higher accuracy than the "Random" model on ON-Long.
The table shows the evaluation results on the G&C dataset.
The F1 score for the model with entity salience added in the EventComp-8M setting is 46.1.
The table shows the number of parameters for each model, including LSTM (baseline), CNN Non-Inception, CNN Inception (word), and CNN Inception + Gate.
The LSTM (baseline) model has 13,819,904 parameters, the CNN Non-Inception model has 1,845,248 parameters, the CNN Inception (word) model has 2,152,448 parameters, and the CNN Inception + Gate model has 4,304,896 parameters.
The CNN Inception + Gate model has a higher accuracy than the LSTM (baseline) model.
The fastText models have lower accuracies than the LSTM (baseline) model.
The proportion of word matching questions is higher in SQuAD compared to NewsQA.
The proportion of paraphrasing questions is higher than the proportion of inference questions in both NewsQA and SQuAD.
The table presents human performance on the SQuAD and NewsQA datasets, as well as machine performance on the human-evaluated subsets.
The machine performance on the SQuAD dataset (0.650) is higher than the machine performance on the NewsQA dataset (0.465).
As the number of sentences in the document increases, the average number of sentences also increases.
The sentence-level accuracy decreases as the number of sentences in the document increases.
The Snapdragon 855 platform has better runtime performance than the Snapdragon 820 platform for all frameworks.
The Snapdragon 855 platform with CPU Quant has lower runtime than the Snapdragon 820 platform with CPU Quant for AlexNet.
Xiaomi 9 has a higher SOC than Xiaomi 5.
Xiaomi 9 has more memory than Xiaomi 5.
The model size of BNN is smaller than the full-precision CNN for all three models mentioned in the table.
The precision of BNN is lower than the full-precision CNN for all three models mentioned in the table.
The table provides information about the energy consumption per image frame for the YOLOv2 Tiny network on the Snapdragon 820 platform.
Tensorflow Lite CPU Quant has the highest efficiency among the listed options.
As the distance constraint increases, the F1 scores also increase.
The F1 scores are the same when the distance constraint is ∞.
Table 2 shows the development-set F1 scores for different distances of attention constraint in the sentence at test time.
The F1 scores for both strict and relaxed settings are 93.15 when the attention distance constraint is set to infinity.
The F1 scores for the different approaches to handling morphology are higher when word embeddings are used compared to when they are not used.
The CharLSTM approach performs better in terms of F1 score compared to the None approach.
The "Ours (CharLSTM)" model achieves the highest F1 score among all single models on the WSJ test set.
The "Ours (ELMo)" model achieves the highest F1 score among all multi-model/external models on the WSJ test set.
The base learning rate for English is 0.0008.
The base learning rate for Hebrew is 0.002.
The KAQA (ARC-I) model has different variations, including (entity), (triple), and (all).
The highest accuracy achieved by the KAQA (ARC-I) model is 0.773.
KDCoE-cross achieves the highest Hit@10 score for all languages.
TransE has the lowest MRR score for all languages.
The "KDCoE (term)" method performs the best in cross-lingual entity alignment, achieving the highest scores for all metrics in both language pairs.
The "OT" method performs poorly in cross-lingual entity alignment, achieving a low score for the "Hit@1" metric in the En-Fr language pair.
KDCoE (term) achieves the highest performance in terms of Hit@1, Hit@10, and MRR for the English-French language pair.
KDCoE models outperform Single-layer, CNN, GRU, and AGRU-mono models in terms of Hit@1 for the English-German language pair.
PCNN+PU has the highest accuracy score among all the models.
APCNN+D has a higher score than MIML in the Top 200 evaluation.
The accuracy of PCNN+PU is higher than the accuracy of CNN+PU.
The top 300 performance of PCNN+PU is better than the top 300 performance of CNN+PU.
The table shows the results of the ItemKNN approach on the Movielens dataset using different strategies.
The strategy "Click-Stream weight SG 200w 200v 4d" achieves the highest precision among all strategies.
The model "MemDec" achieves the highest average BLEU score of 36.97.
The model "+DeconvDec" achieves the highest BLEU score of 39.75 on the NIST Machine Translation task in 2004.
The "+DeconvDec" model has a BLEU score of 28.47.
The "RNNSearch-1" model has a BLEU score of 23.30.
The LSTM model uses the "adadelta" optimizer for the News Articles dataset.
The dropout rate used by the LSTM model and the EIN model for the Stop_Clickbait dataset is 0.2.
The EIN model outperforms the baselines in terms of accuracy on both the News Articles and Twitter datasets.
The LSTM model achieves the highest macro-F1 score on the News Articles dataset.
LSTM performs better than Stop_Clickbait in terms of accuracy on the clickbaits dataset.
EIN has higher precision than Stop_Clickbait on the clickbaits dataset.
Table 8 shows the WEAT positive-negative test scores before and after debiasing.
After debiasing, the WEAT positive-negative test scores decrease compared to before debiasing.
The original method performs worse on Google Analogy than on EQT.
The ECT test has the highest value among all the tests.
The f1 values for the WEAT and Google Analogy tests are higher than the f1 values for the Simlex and WSim tests.
The query "st paul saints" has a positive gain in NDCG at cut-offs 10 and 100.
The query "mr rourke fantasy island" has a negative gain in NDCG at cut-off 10.
The query "st paul saints" has a positive gain in NDCG at cut-offs 10 and 100.
The query "mr rourke fantasy island" has a negative gain in NDCG at cut-off 10.
The LSTM network outperforms the CNN network in terms of MRR and P@1.
The CNN network underperforms compared to the LSTM network in terms of MRR and P@1.
Table 2 compares different models over the Robust04 collection with title queries.
PV-LM performs better than QL in terms of MAP, nDCG@20, and P@20.
The baseline accuracy is consistently lower than the accuracy under the "Under" condition.
The accuracy under the "Over" condition is consistently higher than the accuracy under the "Under" condition.
The Cavity Filling method produces higher scores for Accuracy, Precision, Recall, and F1 compared to the baseline, undersampling, oversampling, and SMOTE methods.
The undersampling method produces the lowest scores for Accuracy, Precision, Recall, and F1 compared to the baseline, oversampling, SMOTE, and Cavity Filling methods.
The baseline accuracy remains the same for all experiments.
The accuracy is higher when the dataset is undersampled compared to when it is oversampled.
Table 2 provides the mean minor accuracy for different methods.
The Recursive model with WNN has the highest accuracy of 0.778.
The accuracy for the Standard model with GLOVE embeddings is 0.730.
The table shows the performance of different models compared to other methods on the Stanford sentiment treebank dataset.
The accuracy of BENN-recursive is higher than the accuracy of WNN-recursive.
The accuracy of the recursive model is lower than the accuracy of BENN-recursive.
The number of epochs increases as the model number increases.
The batch size decreases as the model number increases.
The cv_stem classifier achieves the highest roc_auc score among all the classifiers.
The tfidf12 classifier achieves the highest precision score among all the classifiers.
The highest roc_auc value for each model type is marked as [BOLD].
The precision value for the lstm_fpb model is marked as [BOLD].
Table 4 presents the experimental results of the four sampling algorithms evaluated with the two best performing GBM and LSTM models from Tables 2 and 3.
The precision/recall scores for the "seq11" model with a document length of 200-500 words and a h-coverage of 0.5 are 0.65/0.12.
Our method achieves the highest accuracy on the R52 and Obsumed datasets.
The CNN model achieves the highest accuracy on the TREC dataset.
Table 6 shows the captioning performance with different numbers of topics.
The captioning performance metrics (BLEU@4, METEOR, ROUGE-L, and CIDEr) have higher values for 20 topics compared to 10 and 30 topics.
The best caption model with topic guidance achieves the highest BLEU@4 scores for the predicted topics in Table 2.
The best caption model with topic guidance achieves the highest CIDEr scores for the predefined categories in Table 2.
Table 3 compares the captioning performance among caption models with topic guidance using predicted data-driven topics.
The caption model with topic guidance using predicted data-driven topics has the highest BLEU@4 score of 0.4397.
The predicted value for BLEU@4 is 0.4397.
The annotated value for ROUGE-L is [BOLD] 0.6339.
The BLEU@4 score without using speech modality for topic prediction is 0.4266.
The CIDEr score with using speech modality for topic prediction is 0.4970.
The F1 score for the Tf-Idf model is 0.270.
The F1 score for the ExpandRank model is 0.269.
The table provides the performance of RNN and CopyRNN models on five different datasets.
The CopyRNN model outperforms the RNN model in terms of R@10 performance for all datasets.
Table 1 provides results on Yelp "style" (sentiment) transfer for various models, including shen-1, CAE, fu-1, Multi-decoder, Style embedding, simple-transfer, Template, Delete/Retrieve, yang2018unsupervised, LM, pang2018learning, and CAE+losses (model 6).
The "Untransferred" model achieves the highest BLEU score of 31.4 among all the models listed in Table 1.
The table shows accuracy results in the development set of the DSLCC for various languages.
The "AR" variety has the highest number of instances classified as AR.
The "AR" variety has a higher number of instances classified as "CL" compared to the "ES" variety.
The "+ELMo Idwiki" model has a higher F1 score on the development set compared to the "CNN-BiLSTM-CRF" model.
The "+Flair KT" model has a lower F1 score on the test set compared to the "+Flair Idwiki" model.
LM[Idwiki]_Sup[TO-ST] achieves the highest F1 scores in all target domain dev datasets.
LM[TOL]_Sup[ST] achieves the highest F1 scores in the 25%, 50%, 75%, and 100% target domain dev datasets.
The "unsupervised real" and "unsupervised +synthetic" settings have higher average test UAS scores compared to the "(weakly) supervised real" and "(weakly) supervised +synthetic" settings.
The "unsupervised +synthetic" setting has a higher test UAS score compared to the "unsupervised real" setting.
The table shows the performance of three different models (BR, Seq2Seq, and Seq2Set) on the rebuilt uncorrelated RCV1-V2 test set.
Seq2Set (simp.) has the highest F1 score among the three models on the rebuilt uncorrelated RCV1-V2 test set.
The Seq2Set model has the lowest hamming loss (HL) value among all the models.
The Seq2Set (simp.) model has higher micro-precision (P), micro-recall (R), and micro-F1 (F1) values compared to other models.
Seq2Set (simp.) has the highest F1(+) score among all the models.
Seq2Set (simp.) has the lowest HL(-) score among all the models.
"Seq2Set (simp.)" has the highest F1(+) score among the models.
"BR" has the highest HL(-) value among the models.
The "CDCK2" feature has a feature dimension of 256.
The summarization technique used for all features is average pooling.
CDCK5 has the lowest model size among the three CPC models.
CDCK6 has the highest NCE loss on the dev set among the three CPC models.
The features used for speaker verification in the experiment are MFCC, CDCK2-60, CDCK5-24, CDCK6-60, MFCC + CDCK2-36, MFCC + CDCK5-24, and MFCC + CDCK6-36.
The accuracy of the Oracle model is higher than the accuracy of the Majority Vote model.
The Majority Vote model has a higher accuracy than the Greedy Action and Parser Switch models.
Table 2 shows different ensembling strategies and their corresponding accuracies.
The "Greedy Action" ensembling strategy achieves an accuracy of 83.84.
The ELMo layer with "Average" has the highest accuracy of 83.85.
The "First" ELMo layer has a higher accuracy than the "Last" ELMo layer.
Table 3 shows the different methods used for LM-based re-ranking with beam size 5.
The SVM ranker method achieves an accuracy of 84.26.
The "LM re-ranked" method has an accuracy of 86.30.
The "Oracle top-2 beam" method has an accuracy of 91.53 and the "Oracle top-5 beam" method has an accuracy of 95.04.
Table 5 compares different source reordering strategies and shows that the proposed approach outperforms the baseline monotone and random rearrangement strategies.
The "Ground Truth" source reordering strategy has the lowest oracle-ppl and highest oracle-BLEU values among all the strategies compared in Table 5.
SCPN performs worse than the copy-input model in terms of both BLEU and ROUGE-1 scores.
The + diverse-decoding model has a higher percentage of rejected paraphrases compared to the copy-input model and exhibits lower pairwise diversity in terms of self-BLEU.
SCPN iyyer2018adversarial performs the best in generating grammatical paraphrases and the worst in generating non-paraphrases.
Transformer seq2seq performs the worst in generating ungrammatical paraphrases.
The system that makes use of lexical resources achieves higher accuracy on the SNLI task compared to the Edit Distance Based and Classifier Based systems.
The system that makes use of lexical resources achieves higher accuracy on the SICK task compared to the Edit Distance Based and Classifier Based systems.
The "Unigrams Only" system has the lowest accuracy on both SNLI Test and SICK Test.
The "Lexicalized" system has the highest accuracy on SNLI Train.
Table 4 represents the pairwise preference of three persona-based models.
The Per.-CVAE model has the highest preference percentage among the three persona-based models.
Table 6 shows the effect of different sampling mechanisms on the WMT14 En→De and IWSLT14 De→En datasets.
"TF" stands for Teacher Forcing, "SS" stands for Scheduled Sampling, and "DSS" stands for Differentiable Scheduled Sampling.
The "Static Decoding" strategy performs better than the "Sequential Decoding" strategy for the "W’14 En→De" model.
Table 4 represents the human evaluation results of Unnecessity, Redundancy, Readability, and Overall.
The SSE model performs better in terms of Redundancy compared to the BERT-SENT model.
The R-2 scores for Gigaword and Newsroom are higher in case (b) compared to case (a).
The average training loss for Gigaword is higher in case (h) compared to case (d).
The systems are ranked based on their performance on the Gigaword test set.
The system with Beam+BPNorm (c=0.55) achieves the highest R-1 and R-2 scores among all the systems.
The system "Ours (best-abs)" achieves the highest scores in all three metrics (R-1, R-2, R-L) for both the Newsroom and Giga datasets.
The system "Ours (pure-ext)" has a higher Bert-S score than the system "Ours (best-abs)" for both the Newsroom and Giga datasets.
The "Ours (pure-ext)" system has the highest score in the "Inform." category.
The "Ours (best-abs)" system has the highest score in the "Truthful." category.
The "Background" function has the highest count in the dataset.
The "Continuation" function is more likely to be essential than to be positioning.
The coefficient for the "Uses" role is 0.0400.
The p-value for the "Continuation" role is 0.994.
Table 2 shows the test error rates on document classification tasks for different models.
The S-ACNN model has a lower test error rate on the DBpedia dataset compared to other models.
The model "Enc-Dec + PhoneDec-3 + State-2" achieves the lowest WER (%) scores for all three datasets: SWB, CHE, and Full.
The model "Enc-Dec" performs worse than the model "Enc-Dec (baseline)" for all three datasets: SWB, CHE, and Full.
The VD-BERT model has the lowest mean value in the Generative Setting.
The RvA model has the highest value in the Discriminative Setting R@10↑.
The exact match accuracy for the SQuAD dataset without any additional training data is 80.95.
The exact match accuracy for the model trained on SQuAD dataset with an additional 2K SearchQA training data is 59.80.
For the SST2 dataset, increasing the sample size decreases the accuracy, except for the sample size of 5000 where the accuracy is the same as the baseline accuracy.
For the YELP dataset, increasing the sample size does not significantly affect the accuracy, except for the sample size of 5000 where the accuracy is slightly higher than the baseline accuracy.
Table 4 provides the baseline accuracies of the FastText and Bi-LSTM models on four datasets.
Table 6 shows the F1 scores on 100-word summaries for DUC 2002 documents for various algorithms.
The algorithm "KKV" achieved an F1 score of 0.490 for ROUGE-1 and an F1 score of 0.228 for ROUGE-2 in the evaluation on 100-word summaries for DUC 2002 documents.
The running time of the "greedy size+d" algorithm is always equal to or greater than the running time of the "greedy size" algorithm.
The REPEL algorithm achieves the highest scores for all metrics in both the Wiki + Freebase and NYT + Freebase datasets.
The REPEL algorithm achieves a higher F1 score in the Wiki + Freebase dataset compared to the NYT + Freebase dataset.
Table VIII provides the number of times the model using hierarchical embeddings predicts the phonetic sub-unit is on the right of a logograph that follows the left-right arrangement.
The model using hierarchical embeddings prefers to predict the phonetic sub-unit on the right of a logograph that follows the left-right arrangement in most scenarios.
There are two datasets with simplified Chinese and three datasets with traditional Chinese.
The MSR (Simplified) dataset has the highest number of sentences in the test set.
The AWD-LSTM model with hierarchical embedding performs better than the AWD-LSTM model with standard embedding on all datasets.
The AWD-LSTM model with hierarchical embedding performs better than the AWD-LSTM model with standard embedding on the CITYU (Traditional) dataset.
The "fastText" embedding algorithm has the highest average AUC performance across all word lists.
The "GloVe" embedding algorithm performs best for the "Lrelative" word list.
The size of the Lfamily word list is 54.
The "Lrelative" LIWC word list has the highest accuracy and the lowest false positive rate among all the LIWC word lists.
The "Lrandom(max)" LIWC word list has a higher accuracy and recall than the "Lrandom(avg)" LIWC word list.
The linear classifier using LIWC word lists on word2vec embeddings performs the best in terms of accuracy on the "Lrelative" set.
The linear classifier using LIWC word lists on word2vec embeddings has the lowest false positive rate on the "Lrandom(max)" set.
The accuracy of the linear classifier using LIWC word lists on fastText embeddings to identify members of its own set is higher for "Lfamily" compared to "Lrandom(max)".
The recall of the linear classifier using LIWC word lists on fastText embeddings is higher for "Lnegemo" compared to "Lposemo".
The pitch values in the table range from 60 Hz to 180 Hz.
The table includes the vowel sounds /a/, /@/, /i/, and /y/.
The table shows the ratings and generated review texts for different products.
The review texts in the table are generated based on the given ratings.
The experimental results compare the performance of three different methods (Rnp, 3Player, and InvRat) on highlight lengths of 10, 20, and 30.
The method 3Player achieves the best F1 scores for the "Appearance" aspect at lengths 10, 20, and 30.
The table shows the results on the synthetic IMDB dataset.
The Rnp metric has a lower value compared to the Test Acc metric.
Table 3 provides the values of different regularization parameters used in three models.
The "Joint AB-LSTM" model has a dropout value of 1.0.
The performance of all models improved after filtering negative instances from the dataset.
The precision scores of all models increased after filtering negative instances from the dataset.
Table 6 presents the performance comparison of proposed models with existing models on the complete dataset for the DDI classification task.
SCNN2 has a precision of 68.5, recall of 61.0, and F1 score of 64.5.
The Joint AB-LSTM1 method has the highest F Score among all the methods.
The AB-LSTM1 method has the highest Precision among all the methods.
Table 9 shows the contribution of each feature in the Joint AB-LSTM model.
The F Score for the Joint AB-LSTM model without the random position embedding (P) is 68.66.
The F1 score of the best performing neural model is 91.25.
The Full model has the highest F1-value and SSA on the test set.
The LSTM+char model shows the highest improvement in F1-value and SSA compared to the Baseline model.
The table provides the hyperparameters of the models used in the experiments.
The table provides the values of various parameters used in the models.
The BLEU score for the Im2Tex-tok model on the Im2latex-100k dataset is 73.71.
The exact match accuracy after deleting whitespace columns for the Im2Tex-C2F model on the CROHME14 dataset is 33.87.
Table 3 compares different models and their performance on train and test perplexity, as well as image match accuracy.
The model with the ablation "-PosEmbed" has a higher image match accuracy compared to the model without any ablation.
The table provides information about different models and their average number of coarse and fine attention computations.
The table compares the average number of coarse and fine attention computations for different types of attention.
Increasing the beam size from 1.5 to 2.3 reduces the Word Error Rate (WER).
Increasing the beam size from 1.5 to 2.3 slightly increases the real-time factor at 40.
The computational efficiency, measured by rtf@40, increases as the DT values decrease.
The word error rate (WER) increases as the DT values decrease.
MacBERT-large performs better than RoBERTa-wwm-ext-L on the CMRC 2018 EM score.
MacBERT-large has a higher average F1 score across all metrics compared to RoBERTa-wwm-ext-L.
As the value of k increases, the accuracy of Subject AC and Object AC concepts decreases.
The accuracy of Subject BL concept is consistently lower than the accuracy of Subject AC concept for all values of k.
Probase AC performs better than WordNet AC in terms of accuracy for both Subj and Obj categories.
Probase SP performs better than Probase BL in terms of accuracy for all categories and k values.
The average rating for the Seq2Seq system is 3.31.
The Re-Ranking system has the highest average rating among all the dialogue systems.
The Mean Absolute Error (MAE) between the judgements predicted by AutoJudge and the human judgement is 0.984 in the System Split.
The total number of situations in the "search" category is 327.
The number of situations in the "dev" split is 724.
Random Forest has the highest accuracy score among all the classifiers.
All the classifiers perform better when using all the features compared to when excluding the additional features.
Table 5 shows the accuracy scores of the Random Forest classifier on the RumourEval dataset with each feature removed in turn.
Removing the "All without AF" feature has the most significant impact on the accuracy of the Random Forest classifier on the RumourEval dataset.
The "Vanilla" translation performs better than the "French Nat Spelling" and "German Nat Spelling" translations.
The "Czech Nat Spelling" translation has the lowest performance compared to the other translations.
The number of French words in the dataset is 65,156.
The error rate for German in the dataset is 2.5%.
The compression ratio increases as the number of bits decreases.
The precision decreases as the number of bits decreases.
The experiments in Table 2 used different models.
The quantized activation has the same value for both "No Bucketing" and "Bucketing" cases.
The perplexity for the "Decoder" with "Input Embedding + Positional Encoding" is 3.20 for the "No Bucketing" case.
The larger bilingual model (bi*) improves the translation quality and classification of linguistic properties slightly compared to the regular bilingual model (bi).
The larger multilingual model (multi*) improves the translation quality and classification of linguistic properties substantially compared to the regular multilingual model (multi).
Table 13 compares the BLEU scores and morphological classifier accuracy across language pairs for fully character-based LSTM and charCNN LSTM models.
The BLEU score for the de-en language pair is higher than the BLEU score for the cs-en language pair in the "char → bpe" model.
There is a positive correlation between the POS and morphology scores and the BLEU score for the word-based models.
There is a positive correlation between the POS and morphology scores and the BLEU score for the character-based models.
Table 15 shows the impact of changing the target language on POS tagging accuracy with classifiers trained on the encoder side.
The POS tagging accuracy for the German source language when the target language is Arabic is 89.3.
The CCG tagging accuracy improves as the value of "k" increases for all target languages.
Considering all target languages leads to the highest CCG tagging accuracy.
The method "Ours (Ensemble)" achieves the highest F-score for both Entity (Primary) and Relation (Relaxed).
The combination of "Training-22 + NLM-180 + HS2" achieves the highest precision, recall, and F-score for Entity (Primary) and Relation (Relaxed).
The Char-CNN + Stacked Bi-LSTM model with Training-22 and NLM-180 achieves an F-score of 53.74% for Entity (Primary) and 46.00% for Relation (Primary).
The upperbound model, which incorporates simplifying assumptions, achieves the highest Precision, Recall, and F-score for Entity (Primary) compared to other models.
Different methods are applied to the "Training-22 + NLM-180 + HS2" dataset.
The F-score for the "Relation (Primary)" task is highest for the "Training-22 + NLM-180 + HS2" dataset with the "Tang et al." method.
The table provides the average scores and standard deviations for different questions in the polishing stage questionnaire.
Question 3 in the polishing stage questionnaire has an average score of 5 and a standard deviation of 0.
The average score for Q. 1 (a-d) is (5 5 5 5) with a standard deviation of (0 0 0 0).
The average score for the GQ question is [BOLD] 4.83 with a standard deviation of [BOLD] 0.18.
The average F1 score of BERTBase-PN5cls is higher than the average F1 score of BERTBase-PN.
The Spearman correlation of BERTBase-PNsmth is higher than the Spearman correlation of BERTBase.
The BLEU scores for the MTNT (mtnt2019) EN→JA translation task are higher than the scores for the MTNT (mtnt2019) JA→EN translation task.
The BLEU scores for the 4SQ FR→EN translation task are higher than the scores for the MTNT (mtnt2019) FR→EN translation task.
BERT_F-DA+text achieves the best precision and F1 score among all the models.
BERT-SwDA_F achieves the best recall score among all the models.
Table 5 shows the ablation test results for the proposed five curriculum learning attributes on the validation set of the DailyDialog dataset with the SEQ2SEQ model.
The "Multi-curricula" attribute has the highest BLEU score among all the curriculum learning attributes.
The full method achieves the highest BLEU score and Dist-1 score compared to the other methods.
The full method achieves a high Intra-3 score and average score.
The percentage of scores within one standard deviation of the mean score for the reference response is 71.65.
The mean ADEM score for reversing the response is 2.75.
The ADEM mean score for the ground-truth response is 2.75.
The ideal score for the context repeated as response is 1.
The variant "Named entities removed" received a better score than the original response 11.2% of the time.
The variant "Replace words with synonyms" has a higher correlation score with the original response compared to the variant "NLTK stopwords removed".
The table provides the results of ablated BiERU on the IEMOCAP dataset.
The values in the "Accuracy" and "F1-score" columns represent the weighted averages.
The combination of word ngrams, word embeds, and all lexicons has the highest Pearson correlation r for the emotion "joy" with a value of 0.71.
NRC-Hash-Emo has the highest Pearson correlation r for the emotion "anger" with a value of 0.55.
Table 2 provides the Split-half reliabilities (as measured by Pearson correlation and Spearman rank correlation) for the anger, fear, joy, and sadness tweets in the Tweet Emotion Intensity Dataset.
The NLLoracle value decreases as the mixture weight m increases until it reaches 3/4, and then starts to increase again.
The NLLtest value decreases as the mixture weight m increases until it reaches 3/4, and then remains the same.
The "Our CNN model" outperforms "Word overlap" and "[ITALIC] idf-weighted word overlap" in terms of MAP and MRR on the TrecQA dataset.
The "Our CNN model" performs better than "Rao et al. (Rao et al., 2016)" in terms of MAP and MRR on the TrecQA dataset.
The "Top1k-A" augmentation performs the best in terms of overall performance, other performance, and number performance compared to the other augmentations.
The overall performance of the model improves when using all the augmentations compared to the baseline model.
The CBOW model performs better on the word similarity task than on the word analogy task.
The GloVe model performs better on the semantic task than on the syntactic task.
Table 2 provides an evaluation of phrase representations for different models using 100-dimensional phrase vector representations on word retrieval.
"Our model" outperforms all other models in terms of recall rates at 1, 5, and 10.
Naive Bayes has the lowest F1 score for the "Denying" class.
Tree CRF has the highest F1 score for the "Querying" class.
The table includes performance results for 7 different events.
The "charliehebdo" event has the highest Macro-F1 score in the Tree CRF model.
The ET classifier outperforms other methods in terms of AUC, Accuracy, Precision, Recall, and F1 score.
The methods "PropSurf" and "PropLem" perform worse in terms of AUC, Accuracy, Precision, Recall, and F1 score.
In Table 13, SemanticILP consistently outperforms baselines on elementary level science exams.
SemanticILP performs better than various baselines on the ProcessBank dataset.
Proread has a test score of 68.1.
The ET classifier outperforms all supervised and unsupervised baselines in terms of AUC, Acc, P, R, and F1 scores.
The "PropLem" method has a higher F1 score compared to the "PropSurf" method.
The minimum threshold for cell-cell alignment is higher than the minimum threshold for title-title alignment.
The minimum threshold for cell-question consistency alignment is lower than the minimum threshold for cell-question choice alignment.
The table shows the effects of lexical memorization on the output of regression models for pairs of concepts.
The word pair "chemistry / science" has the highest rating of 10.0 in the HyperLex dataset.
The word pair "ear / head" has a rating of 0.00 in the HyperLex dataset.
The category "animal" has the highest score for all instances in the table.
The category "plant" has a higher score than the category "food" in the table.
Table 11 shows the results of the graded LE task using various models.
IAA-1 achieves a Spearman's ρ correlation score of 0.854 in the graded LE task.
The "IAA-1" model has a higher score than the "IAA-2" model in both the "Random" and "Lexical" categories.
The "Random" score is higher than the "Lexical" score in the "FR (α=0.02, θ=0.25)" row.
The training mechanism "PR" achieves the highest accuracy for the "Long to Short Transfer Task".
The training mechanism "PR" achieves the highest accuracy for the "Short to Long Transfer Task".
Table 1 describes the transfer loss (TL) of different classifiers for each transfer task for all datasets described in Section 2.1.
The transfer loss for the Bag-of-Words (BoW) classifier on the "Mov_en" dataset is 4.2.
LeTraNets outperforms the second best model in terms of P_Acc, F_Acc, and F_RMSE for the Short to Long Transfer Task in the English dataset.
LeTraNets outperforms the second best model in terms of F_Acc for the Long to Short Transfer Task in both the English and Korean datasets.
The method "Ours" performs better than the other methods in terms of the "Overall" score.
The method "Ours" performs worse than the other methods in terms of the "Content" score.
The proposed framework has three different design choices: sharing-decoder, sharing-encoders, and full.
The "full" design choice has the highest style score and content score.
The method "CAE [ITALIC] σ=0.001" performs the worst in terms of style transfer, as it has the lowest scores in all categories.
The method "Ours" is preferred over the method "No Pref." in terms of overall style transfer performance.
The reconstruction loss is not considered in the proposed objective function for the one-to-many style transfer task.
The style scores of the generated outputs vary significantly in the proposed one-to-many style transfer task.
The "Tacotron2 + WaveGlow" model has the lowest MOS score among the listed models.
The table shows the Mean Opinion Score (MOS) of different variations of the model.
The baseline model (MelGAN) has the highest Mean Opinion Score.
The "Original" model has the highest Mean Opinion Score (MOS) among all the models listed.
The confidence interval for the "Original" model is [BOLD] 0.04.
Table 2 provides information about the computational time of TYC, HotFlip, and TextFooler when attacking different datasets.
TYC, HotFlip, and TextFooler are different methods used for attacking datasets.
The table shows the results of translation quality for all tasks in terms of TER and BLEU.
The TER score for En→De translation using PB-SMT is 62.2±0.3.
Table 3 shows the number of words falling in each bucket of three bucket sets.
The number of words in each bucket is the same for all bucket types.
The table shows the bucket-wise precision/recall for four different types of buckets: SB, LB, BL, and LM.
The UUR (User-User Recommendation) method outperforms the baseline method in terms of precision/recall for all bucket types.
The table provides the overall macro and micro precision and recall for two different ground truths.
The UUR is higher for the "Young" ground truth compared to the "Elder" ground truth.
There are three different user buckets: High, Mid, and Low.
The Low bucket has the highest number of users.
The model with penalization on parameter matrices has a higher performance on SNLI compared to the other models.
The model without penalization has a higher performance on Yelp compared to the other models.
The model with the highest test accuracy on the SNLI dataset is the 1200D BiLSTM generalized pooling.
The 1200D BiLSTM generalized pooling performs better than the 1200D BiLSTM max pooling on the SNLI dataset.
The "BiLSTM gated-pooling" model has the highest accuracy on both the "In" and "Cross" tasks.
The "Shortcut stacked BiLSTM" model has a higher accuracy on the "In" task, while the "BiLSTM max pooling" model has a higher accuracy on the "Cross" task.
The hyperparameter values for SGD are the same for both BN50 and SWB300.
The value of λ is 400 for all combinations of parameters.
The table provides information about different models and their corresponding CE loss and WER values.
The CE loss decreases for each model when using ESGD with anchors compared to the corresponding baseline.
The table shows the CE loss and WER of two different variations of the ESGD model on SWB300.
The WER values decrease as we move from the "Single SGD baseline" to the "ESGD with anchor (iterated anchor)" model.
The St a n z a system performs well across all 100 treebanks.
The St a n z a system outperforms UDPipe and spaCy in most metrics for the English-EWT treebank.
MemDec performs better with pre-training compared to without pre-training.
MemDec performs better on average with a memory size of 8 compared to other memory sizes.
The MemDec system achieves the highest BLEU scores on Chinese-English translation tasks.
All translation systems outperform the Moses system in terms of BLEU scores.
The model with "Combinatorial Reward + rerank" achieves the highest scores for all three metrics (R-1, R-2, R-L).
The scores for all three metrics (R-1, R-2, R-L) are similar between the "Sentence-level Reward" and "Combinatorial Reward" models.
Table 3 compares different methods for building an upper bound for the full model.
The values for "Sentence-matching", "Greedy Search", and "Combination Search" in the "R-1" column are increasing.
The model "BERT-ext + abs + RL + rerank (ours)" has the highest scores in both "Relevance" and "Readability" compared to other models.
The model "BERT-ext + abs + RL + rerank (ours)" has the highest total score compared to other models.
Table 6 shows the performance on the NYT50 test set using the limited length ROUGE recall score.
BERTSUM Liu (2019) achieves a R-1 score of [BOLD] 46.66.
The ROUGE-1 score for BERT-ext + abs + RL (ours) is 43.39.
The ROUGE-L score for Pointer Generator See et al. (2017) is 33.90.
Table 2 shows the F-scores between scene graphs parsed from region descriptions and ground truth region graphs on the intersection of Visual Genome and MS COCO validation set.
The "Oracle" method has an F-score of 0.6985.
The table shows the model performance with perfect tokenization.
The Flair model performs better on TNT than on BTC.
The F1 score for NLTK with LM-LSTM-CRF tokenizer is 80.85.
The F1 score for Flair with BTC tokenizer is 68.33.
The table presents the results of an ablation study on representation modules, comparing the performance of different methods.
The "Neural-Char-CRF (Match)" method performs worse than the other two methods in terms of BTC.
The number of tweets with emojis is different for each language.
The correlation values between the sentiment scores of emojis in the 13 languages and the Emoji Sentiment Ranking vary.
Table 5 evaluates different embedding algorithms for query classification.
The F1-measure for the Rel.-based Embedding - RPE algorithm is the highest among all the evaluated algorithms.
Table 1 compares the performance of the proposed network of DNN approach with other competitive DNN-based systems on TIMIT and WSJ datasets.
The proposed network of DNN approach outperforms other competitive DNN-based systems on TIMIT, WSJ rev, and WSJ rev+noise datasets.
The table provides the performance of the proposed network of DNN achieved at various levels of the architecture for three different datasets: TIMIT rev, WSJ rev, and WSJ rev+noise.
The performance of the proposed network of DNN decreases as we move from TIMIT rev to WSJ rev to WSJ rev+noise.
Table 3 provides the performance results with the ResNet-inspired architecture.
The ResNet-inspired architecture performs slightly worse than the original architecture.
Table 7 compares the performance of the work in this paper using Bible and Europarl data with previous work.
The average accuracy for all datasets except English is 77.7 and the LAS score is 83.9.
NOUN is the most frequent part of speech in the G1 languages.
AUX has the lowest accuracy among all parts of speech in the G3 languages.
Table 12 shows the precision, recall, and f-score of unlabeled dependency attachment for different POS tags as head for three groups of languages in the universal dependencies experiments.
The frequency percentage of NOUN in G1 languages is 43.9%.
The Transfer-T model outperforms the LostInConv model in terms of the H@1 metric.
The Transfer-T model has a lower perplexity than the LostInConv model.
The table represents the performance variation of the self-conscious agents according to β and different updates for world prior pt(i).
The performance of the self-conscious agents in the "S1+DM" scenario is higher for both the LostInConv and Transfer-T models compared to the other scenarios.
The self-training model outperforms the standard model on the "bg_btb" treebank.
The self-training model achieves better MLAS performance than the standard model on the "uk_iu" treebank.
The table provides official results of a system in the CoNLL shared task for different categories.
The LAS score for the "Small" category is lower than the LAS score for the "All" category.
Table 2 compares the models trained with and without the additional loss function on different treebanks.
The average UAS score is slightly higher with the additional loss function compared to without it.
Table 5 shows the paraphrase detection performance on the Quora dataset.
The precision score for the model with "P2, NP1" and "Learned + Linguistic Features" is [BOLD] 95.5.
Our model has the highest accuracy compared to previously published performances on the Quora dataset.
Wang et al. (2017) (BiMPM) has an accuracy of 88.2 on the Quora dataset.
The accuracy of paraphrase detection improves when linguistic features are added to the learned features.
Adding NP2 to the linguistic features does not affect the precision of paraphrase detection.
Our model achieves an F1-score of 84.8 on the MSRP dataset.
Arora and Kansal (2019) achieve an accuracy of 79.0 on the MSRP dataset.
The performance metrics (ACC, MCC, AP) are consistent across different numbers of features in the subfamily test set.
The highest MCC value is achieved when there are 3 features in the subfamily test set.
The accuracy increases as the value of the feature increases, except for the last row where the accuracy is higher than the previous row.
The accuracy of the last row is higher than the baseline accuracy.
Table 4 displays the five-fold cross-validation accuracy for a combination of HK features and various subsequence lengths.
The accuracy for the HK+4 subsequence length is the highest among all subsequence lengths.
Table 8 shows the QD and GQD for the top performing models in the subfamily experiment.
The QD and GQD for the HK (binary) model are both bolded in Table 8.
The minimum difference between the accuracies of entities seen in the English NER training data and entities that only consist of up to two tokens, only consist of Latin characters, or occur at least twice in the dev data is 0.1.
The accuracy of entities that occur at least twice in the dev data is higher than the accuracy of entities that were not seen in the English NER training data.
Table 4 shows the accuracy of mBERT on POS tag trigrams and 4-grams in the target language dev data.
The difference in accuracy between trigram, seen and trigram, unseen is 39.7.
Table 14 provides information on three types of sentence embeddings from mBERT in BUCC tasks.
The performance of mBERT sentence embeddings in the BUCC tasks is consistently lower for the Chinese language compared to other languages.
The table shows BLEU scores for different language pairs on the IWSLT 14' datasets.
The configuration with cosine similarities has a higher BLEU score than the baseline configuration.
The configuration with l2-normalized input embedding has a higher BLEU score than the configuration with square-normalized output embedding.
The table provides results on three datasets: MSCOCO, Yelp Medium, and Yelp Large.
The PPL values decrease as the value of α increases.
The LSCD Spearman's ρ for the development experiment with the Context-Free model is 0.7263.
The LSCD Spearman's ρ for the submission experiment with the sw-semeval model is 0.5467.
The LSCD Spearman’s ρ submission values for "de-surel" and "de-semeval" are higher compared to "en-semeval" and "ln-semeval".
The CIRCE Weight submission values for "de-surel" and "de-semeval" are higher compared to "en-semeval" and "ln-semeval".
The model "Transformer-1024-8192" has the highest number of parameters.
The model "LSTM-2-1024" has the highest test log perplexity.
The model "Transformer-2048-8192" has the highest number of parameters.
The model "Transformer-2048-8192" has the lowest test log perplexity.
The table compares the performance of the "Syntactic Attention" model to the models of "LSTM (Loula et al.)" on template splits of the SCAN dataset.
The "Syntactic Attention" model performs better than the "LSTM (Loula et al.)" model on the "Template split [BOLD] P [ITALIC] right" of the SCAN dataset.
The objective of the LSTM-P model with WGAN is to achieve a 97.96% accuracy and a uniqueness measure of 0.861.
The LSTM model with WGAN-GP has a higher accuracy (99.21%) in generating samples of length 5 compared to the CNN model with WGAN-GP (98.59%).
Table 4 is an ablation study on neighbor entities, comparing the performance of DCA with or without neighbor entities.
The "In-KB acc. (%)" for RL in the "ETHZ-Attn + 2-hop DCA" system is 93.76.
The three systems (DCA + Average Sum, DCA + Soft Attention, DCA + Soft&Hard Attention) have similar accuracy percentages in terms of SL In-KB acc.
The three systems (DCA + Average Sum, DCA + Soft Attention, DCA + Soft&Hard Attention) have similar accuracy percentages in terms of RL In-KB acc.
The performance of the models improves as different semantic scaffolds are added to the base model.
The SPNet model outperforms the other models in terms of ROUGE-1 and ROUGE-2 scores.
The PT&FT model achieves the highest F0.5 score.
The JFLEG GLEU score for the ST (up-sampling) model is 61.37.
The combination of BT, M-Task, and F-Dis achieves the highest BLEU score for both E&M and F&R models.
The combination of BT, M-Task, and F-Dis improves the BLEU score for both E&M and F&R models compared to the Original data.
Our approach outperforms the state-of-the-art result for FST in terms of both E&M and F&R scores.
The "No-edit" system has the lowest scores for both E&M and F&R.
The table provides results of human evaluation of FST for two models: NMT-MTL and Ours.
The SOTA model in the WMT14 English-German dataset is FAIR Edunov et al. (2018) with a BLEU score of 35.0.
Our model performs better with PT&FT (Pre-training and Fine-tuning) technique, achieving a BLEU score of 32.2.
The model "OURS-bm" outperforms all other models in terms of relevance and diversity metrics.
The model "TFM-S2S" performs better than the model "LSTM-LM" in terms of relevance metrics.
"OURS-tk" has the highest score among all models in the evaluation metrics.
"OURS-tk w/o SSA" has a lower score in the fluency evaluation metric compared to "OURS-tk w/o TI".
MuseSupervised has a higher precision than MuseUnsupervised.
The precision at the top 10 is higher than the precision at the top 5.
The ACP+Bi-view AT system outperforms other systems in all language translation directions.
The Word-by-word using ground-truth dictionary system outperforms the Word-by-word using artificial dictionary system in all language translation directions.
The "Att" model performs better than the "No Att" model in terms of BLEU score for all lengths.
There is a positive correlation between the length of the sequence and the computation time.
The Memory Attention model with Positional Encoding (PE) achieves the highest BLEU score on the en-de dataset.
The Memory Attention model achieves a BLEU score of 15.87 on the en-fi dataset.
The decoding time for the Attention model is higher than the decoding time for both K=32 and K=64.
The decoding time for K=32 is slightly lower than the decoding time for K=64.
The method "FA5 + Demog" outperforms other methods in terms of FriendSize, Income, IQ, and Likes.
The method "Big5+Demog" performs the best in terms of Income.
The table compares the performance of different methods on Big5Questions, Sat. W/ Life, and Depression.
The Big5Questions scores for the Big5+Demog method are higher than the scores for the Big5 method.
The table shows the mean Pearson R over 10 random train-test splits for different methods, including Big5Questions, Sat. W/ Life, and Depression.
Including demographic factors along with Big5-10 questionnaire-based factors leads to higher mean Pearson R values.
The method "FA5 + Demog" outperforms other methods in terms of FriendSize, Income, IQ, and Likes.
The method "Big5+Demog" performs better than other methods in terms of FriendSize, Income, and IQ.
The Baseline RNN model achieves an average F1 score of 50.0 on the CDCP dataset.
The Baseline SVM model achieves an F1 score of 68.9 on the UKP dataset for proposition classification.
The DCM model has the highest accuracy for both the "Accident" and "Earthquake" categories.
The average accuracy of the Recursive model is 0.920.
The F-measure (Fmax) on Frame Ident. PODCAST is higher for biGRU+adv compared to biGRU.
The F-measure (Fmax) on Argument Ident. PODCAST is higher for biGRU+adv compared to biGRU.
The F-measure for Frame Identification is higher when using the "CALOR+PODCASTBOTH" training dataset compared to the "CALOR" training dataset.
The F-measure for Argument Identification is the same for both the "GOLD" and "ASR" systems.
Using ASR with adversarial training improves the performance of frame identification for the LU "dire" compared to using the GOLD biGRU model.
The frame identification score for the LU "donner" using the GOLD + [ITALIC] adv model is 77.4.
The biGRU+adv model outperforms the biGRU model in terms of overall performance.
The biGRU+adv model performs better than the biGRU model in identifying non-core functional elements.
Table 6 provides the Frame Element Identification results (Fmax) on the PODCAST-ASR corpus under different Word Error Rate (WER) conditions.
ELMo achieves the highest accuracy (71.2%) on the CoNLL dataset compared to Gupta et al. (2017) and Deep ED.
Gupta et al. (2017) achieves the lowest accuracy (65.1%) on the CoNLL dataset compared to ELMo and Deep ED.
The table provides experiment results on internal video ASR tasks.
The AMTrf model achieves a WER of 23.3 on the noisy Russian dataset.
As the segment length increases, the performance on both "test-clean" and "test-other" decreases.
As the left and right context length increases, the performance on both "test-clean" and "test-other" decreases.
The AMTrf model performs better on the test-clean and test-other datasets compared to the Trf model.
The Trf model performs worse on the test-clean dataset compared to the AMTrf model.
The average purity of clusters obtained with k-means is 0.600.
The purity of clusters obtained with agglomerative clustering for the category "Politicians" is 0.510.
The word embeddings generated using aW2V have the highest correlation with human scores on the word similarity datasets.
The word embeddings generated using rW2V have the highest correlation with human scores on the Similarity353 dataset.
Table 2 shows the word analogy results and the variation in which predictions are limited to the correct entity type.
The highest prediction accuracy for the normal analogy tasks in the "DW [ITALIC] log" variation is 0.006.
SGNS25 performs better than SGNS15, SGNS5, and SGNS1 based on the RPD values.
SGNS25 outperforms SVDPMI and SVDLC based on the RPD values.
The Word Error Rate Reduction relative to the Raw-1ch model is higher for Set1 noPB compared to Set2 PB.
The Word Error Rate Reduction relative to the Raw-1ch model is lower for FAN-Max on the test set compared to the dev set.
The RSPQ model has the highest performance on full FlickrTag.
The RSPQ model performs better than the PQ model.
Based on the reviews, it can be inferred that the restaurant has poor service and food quality.
Based on Review 5, it can be inferred that the restaurant has excellent food quality.
BERT performs better on the SNLI test dataset compared to the SciTail test dataset.
DA+LF performs better on the MNLI-mm dataset compared to the MNLI-m dataset.
BERT achieves an accuracy of 0.1 on the Non-entailment Lexical task.
The average accuracy for the DA+SA model is 53.4.
The attention-based E2E model performs better in terms of PER and CER compared to the CTC-based E2E model.
The MT approach achieves the highest BLEU score on the TED English-Chinese test.
The "End-to-end" approach outperforms the "Pipeline" approach in terms of BLEU score on the TED English-Chinese test.
The knowledge distillation (KD) method outperforms the other methods in terms of the greedy and beam search techniques.
The "Ours" method performs better than the "Bérard" method in terms of both the end-to-end and pre-trained approaches.
The table describes the effect of teacher model weight on ST results.
As the value of λ increases, the BLEU score also increases.
The median rank decreases when the target vector changes from single-sense to multi-sense.
The accuracy at rank 10 increases when the target vector changes from single-sense to multi-sense.
The "multi-sense (dot product similarity)" input vector performs the best in terms of median rank, accuracy @10, accuracy @100, and mean reciprocal rank.
The "multi-sense (dot product similarity)" input vector has the lowest median rank among all the input vectors.
The table shows the classification scores obtained after evaluating the final Logistic Regression model on test data.
The precision for the "Offensive" and "Clean" classes is higher compared to the precision for the "Hateful" class.
The highest accuracy is achieved when the value of Alpha (α) is 0.1.
As the value of Alpha (α) increases, the accuracy decreases.
The final Logistic Regression model accurately classified most instances as hateful.
The final Logistic Regression model accurately classified most instances as clean.
Table 6 compares the PPL and ranking order between two fidelity-based metric instantiations (Base and SA) on three WMT datasets: Zh⇒En, De⇒En, and Fr⇒En.
The Pd method has the lowest perplexity (PPL) for all three WMT datasets: Zh⇒En, De⇒En, and Fr⇒En.
SPM has the highest accuracy for the "different" category.
BoW has the highest accuracy for the "same" category.
"uni-skip" has the highest accuracy on the "Subj" dataset.
"SPM" has the highest accuracy on the "Subj" dataset.
The classifiers used in the experiments are logistic regression, SVM-rbf, SVM-linear, KNN(n=5), DT, and bernoulli-NB.
The accuracy of logistic regression on Dataset-4 TF-IDF is 82.633.
The logistic regression classifier performs better on Dataset-1 than on Dataset-2.
The SVM-rbf classifier performs better on Dataset-3 than on Dataset-2.
LKB-BERT and KEML-S have the highest F1 scores for the HYP relation.
The average F1 score for all methods is 0.485.
The table shows the experimental results of different cross-domain transfer methods on the Chinese-English translation task.
The IDDA framework outperforms other cross-domain transfer methods in terms of average score on both the TED Talk (In-domain) and News (Out-of-domain) datasets.
The "IDDA" model has a higher average score compared to its two variants.
The "IDDA-fixTea" variant has a slightly higher average score compared to the "IDDA-unidir" variant.
Table 3 displays the average evaluation scores for AREL stories using human-edited stories as references.
The human rating scores are higher for the original "AREL" stories compared to the stories edited by LSTM(T).
Table 1 provides the average number of tokens with each POS tag per story, as well as the differences between post- and pre-edit stories.
The Chunk Merging Evolved Transformer outperforms the Non-Chunk Merging Evolved Transformer in terms of F1-score for all classes.
The Chunk Merging Evolved Transformer has higher recall than the Non-Chunk Merging Evolved Transformer for all punctuation classes.
Table 2 compares the performance of the "Chunk Merging Seq2seq LSTM" model and the "Non-Chunk Merging Seq2seq LSTM" model on plain text format.
The table compares the results of two different models: "Encoded Text Chunk Merging Evolved Transformer" and "Plain Text Chunk Merging Evolved Transformer".
The "Plain Text Chunk Merging Evolved Transformer" model outperforms the "Encoded Text Chunk Merging Evolved Transformer" model in terms of F1-score for all class labels.
There is no statistically significant difference in relevance between System 1 and System 2.
System 2 performs statistically significantly better than System 1 in terms of human-like responses.
The "Tips + Title (multi-attention)" model achieves the highest F1 score among all the models.
The "Single Tips Attention" model performs better in terms of precision and recall compared to the "Single Title Attention" model.
Table 3 provides information about the effect of Geo-PL for linking.
Using Geo-PL improves the precision, recall, and F1 score for location linking.
The table shows the bits per word on noun representations for unigram and bigram models.
The unigram model has a higher bits per word on noun representations for the "Ubuntu" dataset compared to the bigram model.
"MrRNN Noun" has the highest scores for "Entity P", "Entity R", and "Entity F1".
"HRED + Act.-Ent." has a higher human evaluation fluency score than "VHRED".
The method "anchor phrases" has the highest average score of 0.792.
The method "anchor phrases" has the highest standard deviation of 0.077.
The method "elastic search2vec, K=10" has the highest average score and STD.
The method "elastic search2vec, K=10" has the highest STD.
The "search2vec" method has a higher oAUC score than the "word2vec" method.
The "search2vec" method has a higher Macro NDCG@5 score than the "word2vec" method.
The training times for the Transformer+Pascal model are slightly higher than the training times for the Transformer baseline model on each dataset.
The WMT16 En-De dataset has the highest training time for both the Transformer baseline and Transformer+Pascal models.
Table 1 shows the different methods used for translation.
The Transformer method achieved a score of 13.1 on the WMT18 En-Tr dataset.
The table provides hyperparameters for different language pairs in machine translation models.
The parent ignoring probability (q) is different for each language pair.
The mAP values increase as we move from the first row to the second row, and then to the third row.
The values in the third row of the "B-4" column are higher than the values in the first and second rows.
The model "v2t_navigator" achieved a B-4 score of 40.8 and an overall score of 0.9325 on the MSR-VTT Challenge 2017 dataset.
Our model achieved an M score of 28.9 and an overall score of 0.9935 on the MSR-VTT dataset.
The mAP values increase as we move down the table.
The overall scores are consistently high throughout the table.
There are three different training methods used in the experiment.
The Multinomial training method achieved the highest score in the B-4 metric.
The "argmax" training method achieves the highest overall score for all metrics.
The "Multinomial" training method achieves the highest scores for the "B-4", "C", and "M" metrics.
Table 7 provides percentile thresholds κ for predicting an emotion for the ℂ𝕆𝕄𝔼𝕋 - CA and ℂ𝕆𝕄𝔼𝕋 - CGA models.
The ℂ𝕆𝕄𝔼𝕋 - CA model has a κ value of -0.0144 for the emotion "trust".
The UAS and LAS scores for English are higher than those for Chinese and German in the "This work: Full" row.
The UAS and LAS scores for Chinese are higher than those for English and German in the "dozat2016deep" row.
The "+Char" model performs better than the "Basic" model in terms of UAS and LAS scores for all three languages.
The "Full" model performs better than the "Basic" and "+Char" models in terms of UAS and LAS scores for the German test set.
The table shows the parsing performance on PTB with different training objective functions.
The Dev UAS and Dev LAS scores for the cross-entropy training objective function are 94.10 and 91.52 respectively.
The accuracy of DANN is higher than the accuracy of SVM.
The accuracy of DNN is higher than the accuracy of DANN.
Table 2 presents the results of behavioral coding experiments for different tasks.
The table compares the performance of two different methods, BLSTM-focus and + dual PL + dual learning, on the ATIS and SNIPS datasets.
The + dual PL + dual learning method performs better than the BLSTM-focus method on the SNIPS dataset for slot F1 scores.
The table provides evaluations of the SSG model for the dual task of NLU, supervised by full training sets on ATIS and SNIPS.
The SSG model achieves a slot accuracy of 97.72% on the ATIS dataset when supervised.
The "+ dual PL + dual learning" method achieves the highest BLEU scores for both ATIS (10%) and SNIPS (5%).
The "supervised SSG" method achieves the lowest Slot Acc scores for both ATIS (10%) and SNIPS (5%).
The table presents the results of ablation studies on the dual pseudo-labeling method.
The variation of the dual pseudo-labeling method with "wi=1" performs better in terms of SNIPS (5%) Slot F1 compared to the variation without iterative generation.
The "+ dual learning" method achieves the highest values for both "SNIPS (5%) Slot F1" and "SNIPS (5%) Intent Acc" compared to the other methods.
All methods achieve a higher accuracy than the baseline value of 97.71% for "SNIPS (5%) Intent Acc".
Our method with BERT achieves the highest performance on both ATIS Intent and SNIPS Intent.
ELMo-Light for SLU achieves a higher performance on SNIPS Intent compared to SNIPS Slot.
The table shows test accuracies on ATIS and Overnight in semi-supervised settings with a 50% ratio of fully labeled data.
The table includes two different methods that involve "+ dual learning".
The word "time" has the highest rescaled count in the table.
The rescaled count of the word "answer" is 0.3684210526315789.
The word "however" has the highest rescaled count among the words listed.
The rescaled count of the word "let" is 0.3076923076923077.
The BASELINE-eq method has the highest winR score among the three methods.
The Stacking method has the lowest windowDiff score among the three methods.
Table 1 shows the performance of each model on automatic measures.
The kgCVAE model performs better than the Baseline and CVAE models in several metrics.
The model "BOW+KLA" has the lowest perplexity compared to other models.
The model "BOW+KLA" has the highest KL cost compared to other models.
OL-NMT outperforms NMT on the XRCE dataset for the En→De translation task.
OL-NMT outperforms NMT on the UFAL dataset for the En→Fr translation task.
The PB-SMT system outperforms the NMT system in terms of BLEU score for the XRCE En→Fr task.
The PB-SMT system outperforms the NMT system in terms of TER score for the UFAL En→De task.
Table 7 provides information about vocabulary coverage, restricted repetition rate (RRR), and unseen n-gram fraction (UNF) with respect to the Europarl corpus.
For the language "En", the vocabulary coverage during training is 92.5% and the restricted repetition rate (RRR) during testing is 10.5%.
The cLSTM units perform better than the regular LSTM units in terms of TER score for the Europarl En→De translation task.
The cLSTM units perform better than the regular LSTM units in terms of BLEU score for the UFAL En→Fr translation task.
The "R-GCN" model achieves the highest score in the "Entity Predication Hits@10" metric for both "Raw" and "Filter" settings.
The "KANE (LSTM+Concatenation)" model achieves the highest score in the "Relation Predication Hits@1" metric for both "Raw" and "Filter" settings.
CoCon has the highest topic accuracy among the evaluated models.
PPLM has the lowest perplexity among the evaluated models.
The table presents the content similarity and quality of generated content-conditioned samples for four different models.
The perplexity of the models decreases from GPT-2 to CoCon, CoCon-Webtext, and Prompt-Content.
As the value of θ increases, the accuracy, precision, and recall for both B and E decrease.
The accuracy, precision, and recall for B are consistently higher than those for E for all values of θ.
The f-score for the binarized parser with CC (gold) is 53.9.
The speedup for the unbinarized parser with supertag (k=3) is 69.4x.
The model Resnet18 achieves the highest accuracy among all the classification models.
Alexnet has the lowest standard deviation among all the classification models.
The model Resnet18 achieves the highest accuracy among all the classification models.
Alexnet has the lowest standard deviation among all the classification models.
Table 5 displays the average scores given by human experts for each method for 4 different questions.
The "Our" method has a higher agreement score (0.89) compared to the "method" (0.88) for Q3.
The table shows evaluation results on relation prediction.
The PTransE (ADD, 3-step) model achieves the highest Hits@1 (%) Filter score.
PTransE (ADD, 2-step) achieves the highest scores for predicting head entities in the 1-to-1, 1-to-N, and N-to-N categories.
TransR achieves the highest score for predicting tail entities in the N-to-1 category.
The classification model based on character 4-grams achieves high precision, recall, and F1 score for all languages.
The classification model based on character 4-grams achieves perfect precision, recall, and F1 score for the Croatian language.
The table presents experimental results for all languages averaged across the five best models in terms of perplexity on the validation set.
The highest percentage accuracy for the "En" language is 65.9%.
Table 5 shows the average BLEU scores for different models on definitions evaluated using different reference dictionaries.
The "NE-GC" model performs the best in terms of the GC metric.
The perplexity values decrease as the model size increases.
The model with the addition of "S+G+CH+HE" has the lowest perplexity value.
The table provides information about the performance of different models on the Reverse (RVD) and Forward (FWD) Dictionary tasks.
The "S+G+CH+HE" model has the highest performance on both the Reverse (RVD) and Forward (FWD) Dictionary tasks.
Table 7 shows the percentage of times a definition is manually ranked in each position and the average rank for different options.
Table 2 presents the results of an ablation study on the Query Vectors (QV) and Concatenation (Conc.) parts of the Label Attention Layer on the PTB test set.
When the Query Vectors (QV) are present, the precision is 96.53.
Enabling the Position-wise Feed-forward Layer (PFL) improves the performance metrics of precision, recall, F1 score, UAS, and LAS.
Disabling the Residual Dropout (RD) decreases the performance metrics of precision, recall, F1 score, UAS, and LAS.
Table 2 provides expected and empirical overlap scores for adult- and autoencoder-generated language with varying levels of dropout.
The empirical overlap for autoencoder-generated language with 30% dropout is 59.4%.
Table 3 shows the F1 performance of the BSAE model trained on datasets with different answer generation strategies.
The F1 performance for the Dynamic-FakeAns strategy is [BOLD] 77.45.
The BSAE model trained with the "AddSentPrepend" distractor placement strategy has the highest F1 performance.
On average, the BSAE model trained with the "InsRandom" distractor placement strategy has the highest F1 performance.
The average F1 performance of the BSAE model trained on the "InsFront-3" dataset is 74.13.
The average F1 performance of the BSAE model trained on the "InsFront-6" dataset is 71.10.
The addition of "Ours" consistently improves the BLEU4 scores of all models.
The "Up-Down + Ours" model achieves the highest METEOR score.
Our method outperforms the baseline without using unlabeled data.
Our method outperforms the baseline with the use of unlabeled data.
The table provides an illustrative sentence with its nearest neighbor at layer 1 and layer 3.
The mean position for layer 2 is 14.9.
The models used in the experiment are Word Sum, Word GRU, and Phon GRU.
The accuracy at 5 for Word GRU is higher than the accuracy at 5 for Word Sum and Phon GRU.
The table shows the accuracy, precision, and recall for three different models: Majority, Phon GRU, and n-gram.
The accuracy decreases as the value of "n" increases in the n-gram model.
The correlation coefficient between word-word cosine similarity and human similarity judgments is higher for frequent words compared to all words.
The correlation coefficient between word-word cosine similarity and human similarity judgments is higher for frequent words compared to all words at Layer 3.
Table II shows the average αU for students' annotations on gold standard texts, before and after the removal of less-devoted students' annotations.
The average αU for gold 4 after the removal of less-devoted students' annotations is 0.1986.
The hyperparameter "Dense Units L1" has a value of 512.
The optimizer used is "Ada".
Table 1 represents the training, validation, and testing distribution of the RCV dataset across different languages.
The total number of data samples for the English language is 653,947 in the RCV dataset.
The average gain for the "DE" language is higher than the average gain for the "EN" language.
The F1 score for "EN best" is higher than the F1 score for "EN mono" in the "bi-GRU-Att+" model.
The kernel sizes used for text classification architectures are 3, 4, and 5.
The optimization algorithm used for text classification architectures is Ada.
The hyperparameter "GRU activation" has the value "tanh".
The hyperparameter "Batch" has the value 64.
The hyperparameter "Optim Task 1" has a value of "Ada (10-3)" and the hyperparameter "Optim Task 2" has a value of "Ada (10-2)".
The batch size for the text classification architectures and sentence alignment is 15.
Table 5 provides hyperparameters for different text classification architectures and sentence alignment.
The number of epochs for training the text classification architectures is set to 50.
Our Model outperforms all other models in terms of accuracy @2, @5, @10, @20, and map on the HotpotQA benchmark.
Our Model shows significantly better retrieval performance than the other models on the HotpotQA benchmark.
Table 5 provides the test BLEU scores for High/Medium/Low resource language pairs in a many-to-many setting on OPUS-100 (100 languages).
The BLEU scores generally increase as the model architecture becomes more complex.
The "6-to-6" setting only includes the zero-shot languages in the test set.
The table shows the STAT and Accuracy (MP@1) for the analogies in T1 for three different models: SW2V, TW2V, and CADE.
The table provides the STAT and Accuracy (MP@1) for the analogies in T1 for three different subsets: All, Static, and Dynamic.
Table 5 provides MRR and MP values for four different models: SW2V, TW2V, OW2V, and CADE.
Among the four models, CADE has the highest MP1 value in the dynamic analogies subset.
The table shows the performance of two algorithms, CADE and MUSE, on the British-American spelling test.
The table compares the performance of two algorithms, CADE and MUSE, on the British-American spelling test with words with a frequency higher than 20 in the corpus.
The table shows the STAT and Accuracy (MP@1) for three different models: SW2V, TW2V, and CADE.
The Dynamic subset has a higher STAT value for CADE compared to TW2V.
The One-Step DCCA algorithm achieves the highest accuracy and F-score on the CMU-MOSI dataset.
Combining multiple modalities (audio, text, and video) improves the accuracy and F-score.
The "Pre-trained" initialization consistently outperforms the "Random" initialization in terms of accuracy and F1 score for all input types and datasets.
The performance of the model is generally better when using "Word" input type compared to "Sense" or "Supersense" input types for all initialization types and datasets.
Our model achieves higher accuracy than frequency analysis (with keys) in deciphering the Brown-W200 dataset.
Our model achieves higher accuracy than frequency analysis (with keys) in deciphering the Brown-W200 dataset.
Table 2 shows the performance of different variations of the AWI model in terms of BLEU and IDF scores.
The "AWI + sampling" variation of the AWI model has a higher BLEU score compared to the "AWI + IDF" variation, while the "AWI + IDF" variation has a higher IDF score.
Table 1 shows the results on the test set for different models.
The AWI model has the highest BLEU score among all the models.
Our method achieves the highest BLEU score on all three translation tasks compared to the other methods.
The Tied Transformer method does not have a reported BLEU score for the Hebrew to English translation task.
Our method achieves a BLEU score of 29.20 on the WMT14 En→De translation task.
The Layer-wise Transformer outperforms the Tied Transformer with a higher BLEU score on the WMT16 En→Ro translation task.
The third row represents the method without the error correction mechanism (ECM).
The performance of the method without scheduled sampling (SS) is lower than the performance of the method with SS.
The false rejection rate for the supervised labeling strategy is lower than the false rejection rate for the teacher labeling strategy.
The false accept rates for FedAvg, FedAvg + NAG, and FedAdam are all 0.21.
FedYogi has the lowest false reject rate among all the optimizers.
Our work (disco2) outperforms all the competitive systems in terms of F1 score for the first-level discourse relation identification.
Our work (disco2) achieves higher accuracy than all the competitive systems for the first-level discourse relation identification.
The "Baseline" CS detection system has a lower EER than the "Phone posterior" and "Language posterior" systems on both the development and test data.
The "Baseline" CS detection system has a higher EER than the "Phone posterior" and "Language posterior" systems on the development data.
The table shows the performance of different system agents in the interaction with the benchmark user policy.
CRL and GDPL have higher match scores in the interaction with the benchmark user policy.
Table 4 provides the performance of the interaction between each user agent and the benchmark system policy.
CRL has the highest match success rate among all user agents.
Table 5 shows the human preference on dialog session pairs that MADPL wins, draws with, or loses to baselines with regard to quality and success by majority voting.
System Q W performs better than System Q D and System Q L in terms of quality.
The "RESIDE" system has the highest AUC and P@100, P@200, P@300, and P@500 scores among all the systems.
The "PCNN+ATT" system has higher AUC and P@100, P@200, P@300, and P@500 scores compared to the "Mintz" system.
The "MRA xue2018multimodal+" model has the highest BLEU-4 score among all the models in Table 1.
The "Vispi" model achieves the highest scores in all evaluation metrics among all the models in Table 1.
Vispi outperforms TieNet* in disease classification performance based on the AUROC scores.
Vispi performs better than TieNet* in classifying Effusion based on the AUROC score.
The values in the "Steps" column represent different steps of an algorithm.
The scores in the "Mindless" row decrease from step 0 to step 4 and then increase at step 8.
The values in the "Steps" column represent different steps of the inference algorithms.
The table shows precision scores for different summarization methods.
The Oracle method achieves the highest precision scores for all Rouge metrics.
SumBasic has a lower Rouge-1 score than the Oracle score.
DOC + SUM has a higher Rouge-L score than TextRank.
The Rouge-1 score for SumBasic in the Congressional Bills dataset is 35.47.
The Oracle system has the highest Rouge-1 score among all the systems.
The Rouge-L scores for the SumBasic, LSA, TextRank, DOC, SUM, and DOC + SUM systems follow an increasing trend.
The Rouge-1 score for the Oracle system is higher than the Rouge-1 scores for SumBasic, LSA, TextRank, DOC, SUM, and DOC + SUM.
The Rouge-L score for the SUM system is higher than the Rouge-L scores for SumBasic, LSA, TextRank, and DOC.
The Oracle system achieves the highest Rouge-1 score among all systems.
The DOC + SUM system achieves the highest Rouge-L score among all systems.
The Oracle system achieves the highest Rouge-1 score among all systems.
The DOC + SUM system achieves the highest Rouge-L score among all systems.
The table shows the ROUGE F1 scores on the combined AFP/APW/XIN/NYT in-domain test set.
The scores for Rouge-1, Rouge-2, and Rouge-L in the SP row are higher than the scores in the other rows.
Table 2 shows the ROUGE F1 scores on out-of-domain style test sets CNA and LTW.
The ROUGE F1 scores for CNA Test Rouge-1, CNA Test Rouge-2, CNA Test Rouge-L, LTW Test Rouge-1, LTW Test Rouge-2, and LTW Test Rouge-L are higher for M-SP compared to S.
Table 4 shows the performance of our model and two baselines on the ISI Language Grounding dataset.
Our model has a higher policy quality on the ISI Language Grounding dataset compared to the baselines.
The policy quality of our model is higher when trained on both local and global instructions compared to when trained on either local or global instructions separately.
The combined distance of our model is lower when trained on both local and global instructions compared to when trained on either local or global instructions separately.
Our model without gradient has lower local and global MSE values compared to the other models.
Our model has lower local and global distance values compared to the other models.
The "In-order parser" consistently outperforms the "Top-down parser" and "Bottom-up parser" in terms of F1 score.
The "In-order parser" performs better than the "zhu:2013" and "wang:2014" parsers in terms of F1 score under semi-supervision.
The table shows the performance results of three different types of parsers on WSJ section 23.
The rerank models achieve higher F1 scores than the fully-supervised models for all three types of parsers.
The table displays the results for the RST Discourse Treebank, comparing different systems.
The Full system outperforms the First k words system in both ROUGE-1 and ROUGE-2 metrics.
The table shows results on the NYT50 test set from the New York Times Annotated Corpus, including metrics such as ROUGE-1, ROUGE-2, clarity/grammaticality, and number of unclear pronouns.
The values in the "R-1", "R-2", "CG", and "UP" columns for the "Full" row are statistically significant gains compared to the "Ablations from Full" row, with p<0.05.
The evaluation metrics used for the models in Table 7 are "Morph-morph gold", "Level-morph hybrid", "token-multi", "Token-morph gold", "Level-morph hybrid", "token-multi", and "token-single".
The model "Morph-morph gold" performs better on the test dataset compared to the model "Level-morph hybrid".
The optimizer used in the experiment is SGD.
The batch size used in the experiment is 8.
Table 6 shows token-level evaluation on the development set by entity category using fastText.
The F1 score for the "GPE" entity category in the "morph standard" model is 78.34±0.7.
The model "DCC" performs better than the model "LRCN" in terms of the average F1 score.
The model "-(One hot+Glove)" performs better than the model "-(One hot)" specifically for the object "couch".
The method "w/ beam search (Morishita et al., 2017)" achieves the highest BLEU score of 41.42.
The method "Cross entropy (1.3M) w/ beam search" achieves a lower BLEU score of 39.42.
Table 2 provides BLEU scores for the development splits of six datasets in the context of translation and image captioning tasks.
Table 5 provides information on the training time and maximum memory consumption for text generation models on different datasets.
The training time for the Small softmax method on the En-Ja (100K) dataset is 4.6 minutes per epoch.
The "BERT + GSAMN+ Transfer" model achieves the highest scores in both TrecQA MAP and WikiQA MRR.
The "BERT + Transformers" model outperforms the "ELMo + Compare-Aggregate" model in both TrecQA MAP and WikiQA MAP.
Enabling external memory generally improves the EM accuracy for all models.
ResNet 3000 achieves the highest performance in terms of EM, Multiple-choice, and Open-ended metrics compared to other ResNet models.
The "Ours-ResNet" method achieves the highest accuracy of [BOLD] 65.2 in the multiple-choice task on the test-dev dataset.
The "MRN-ResNet" method achieves the highest accuracy of [BOLD] 61.8 in the open-ended task on the test-standard dataset.
Table 5 compares the performance (H@3 in percent) of different hop numbers for various datasets.
The UPGAN model shows improvement over the strongest baseline models (DistMult and ConvTransE) in terms of H@3 for all datasets.
The table compares the performance of different KGC models on three datasets.
UPGAN achieves the highest performance among all models in the Music dataset.
UPGAN shows a higher improvement ratio over the strongest baseline compared to DistMult and ConvTransE in the Book dataset.
The "Weather" domain has the highest value in the "Joint GA" column.
The "Messaging" domain has the lowest value in the "Avg. GA" column.
The model trained with both single and multi-domain dialogues has the highest joint goal accuracy.
The model trained with both single and multi-domain dialogues performs better in terms of request F1 score compared to the baseline model.
The table includes the performance of two models: Baseline and D + S.
The F1 score for non-categorical slot types is higher than the F1 score for categorical slot types.
Table 3 shows the incremental results after adding each error type model and applying them on the development set.
The F1 score increases as each error type model is added.
The model combination of M and BT achieves the highest accuracy on the small-scale dataset under setting 1.
The model combination of BK and E achieves a higher accuracy than the model combination of BT and E on the small-scale dataset under setting 1.
The DAS-EM algorithm achieves the highest accuracy on the small-scale dataset under setting 2.
The Naive algorithm consistently achieves lower accuracy on the small-scale dataset under setting 2.
The full model performs the best in both the single-sentence and multi-sentence tasks.
Removing the encoder component decreases the performance of the model in both the single-sentence and multi-sentence tasks.
Table 1 shows the overall accuracy of different methods for both single-sent and multi-sent tasks.
Our model (vTest) achieves the highest overall accuracy among all methods for the single-sent task.
The accuracy decreases as the distance from the destination increases for both single-sentence and multi-sentence tasks.
The label count for "PubRec" after merge in each category is 60.
The label count for "GenMedAdv" after merge in each category is 177.
The table provides information about different metrics such as Accuracy, Perplexity, and Coherence.
The Accuracy for BERT is 89.54.
MemNN-S+NMT has the highest mean performance score among all the models.
MemNN-S performs better than MemNN-R on the Basic Coreference task.
The table shows the results on TAC 2010 entity extraction with N-to-1 mapping for N=1 and N=5.
The F1 score for the 5-to-1 mapping in ProFinder is higher than the F1 score for the 1-to-1 mapping.
The table presents the results on MUC-4 entity extraction.
The precision (P) for ProFinder (This work) is 32 and the recall (R) is 37.
The token-based F1 score is consistently higher than the MWE-based F1 score for all languages and filtering conditions.
The token-based approach generally achieves higher precision than the MWE-based approach for all languages and filtering conditions.
The TRANSITION model outperforms the CRF model in terms of MWE-based F1 score for all languages.
The ConvNet+LSTM+CRF model achieves the highest Token-based F1 score for the DE language.
The F1 score for Token-based analysis is higher than MWE-based analysis in the "Total (official)" row.
The F1 score for Token-based analysis is lower than MWE-based analysis in the "EN*" row.
The "PHEME" dataset has the highest number of tweets compared to the "Twitter15" and "Twitter16" datasets.
The "PHEME" dataset has the highest average tree depth compared to the "Twitter15" and "Twitter16" datasets.
The table shows the percentage of claims containing each word in three different datasets: Twitter15, Twitter16, and PHEME.
The percentage of claims containing the word "lies" is 2.1 in the PHEME dataset.
The tweets in the "Important Tweets 1" column are all related to the claim "Surprising number of vegetarians secretly eat meat".
The tweets in the "URL" column are all related to the claim "Officials took away this Halloween decoration after reports of it being a real suicide victim. It is still unknown."
The table provides the RSA values between ORIG-FOIL for different tasks.
The RSA value for the "Scramble Text" task in the "Random" row is 0.002348.
Table 1 provides the precision scores for different tasks on the validation set.
The precision score for the VQA task on the validation set is 0.14.
The number of epochs for the VQA task is 89.
The Guesser task has a higher overall accuracy than the RefCOCO task.
The table shows the mean nearest neighbor overlap for the tasks Random, VQA, Guesser, and RefCOCO.
The table provides results of the transformer model trained with or without the time restriction.
The window size for the Transformer model is [-∞,0].
The Transformer model uses both convolution (Conv) and layer normalization (Norm).
The hyperparameter "Hidden size" has a range of {16, 32, 50, 64, 100, 128, ⋯}.
The adopted value for the hyperparameter "Hidden size" is 100.
The AMCNN-3 model achieves a score of 47.58 on the SST-1 dataset.
The CNN-multichannel model achieves a score of 89.4 on the MPQA dataset.
The average article size in Cluster 3 is 838 words.
Cluster 6 is associated with corporate, business, company, and automotive topics.
Table 9 shows the Rouge F1 scores for the seq2seq model and seq2seq_point models on the teaser task.
The seq2seq_point model outperforms the seq2seq model in terms of Rouge-1 scores on the validation set.
The total count of initial sarcastic items in the L&W dataset is 115.
The count of sarcastic items increases and the count of not-sarcastic items decreases from the initial to the final labeling in the L&W dataset.
The OpenIE model has an accuracy of 87.26.
The E2E model has a ROUGE-1 score of 65.44.
The "End-to-end" model has a higher percentage of true positives compared to the "Relation Classifier" model.
The "End-to-end" model has a percentage of true positives equal to 77.8.
The mean performance increases as the training data size increases.
The maximum performance achieved is 0.544 at a training data size of 100k.
Table 4 presents the results of the baseline CNN performance with different window sizes and varying numbers of filters per window on a 10k valid set.
The filter sizes of 5 and 100 have the highest performance in terms of 4-gram.
The table compares the performance of different activity representations and attribute training on different types of composites.
The table compares the performance of SVM and NN models without using attributes.
The NumGNN method has a higher EM score compared to the GNN method.
The F1 score for the comparing question subset is higher than the F1 score for the number-type answer subset.
The task-specific attention model outperforms the single-data baseline in both the "shared" and "paired" scenarios.
The task-specific attention model outperforms the single-data baseline in the "target-specific" scenario, but not in the "source-specific" scenario.
The task-specific attention model variants outperform the single-data baseline in terms of BLEU scores.
The target-specific attention model variants achieve higher BLEU scores than the source-specific attention model variants for the English-French translation task.
Table 8 provides BLEU scores for multilingual NMT 'zero-shot' translations comparing the baseline shared attention model to target-specific and source-specific attention models.
The highest BLEU score for the Fr-Es translation in the target-specific attention model is [BOLD] 15.31.
Table 2 shows the classification results for substances and effects, with varying the number of seeds.
The F1 score increases as the number of seeds increases.
The F1 score increases as the number of seeds increases.
The recall values are consistently higher than the precision values.
Retraining improves the BLEU score in the 4-bit Transformer quantization performance for English to German translation.
The baseline BLEU score in the 4-bit Transformer quantization performance for English to German translation is 35.66.
Table 3 shows improvements after using the dictionary of the development sets.
The BLEU score for BOLT Zh→En translation increases when using the dictionary.
Each row in the table represents a different method used in the ablation study.
The full model with constituency graph achieves the highest accuracy on the JOBS dataset.
The "Graph2Tree" method with constituency graph has the highest accuracy of 78.8.
The "BiLSTM" method has a higher accuracy of 62.8 compared to the "Self-attention" method with an accuracy of 60.4.
Table 2 presents the results of an ablation study on NSnet performance with Symbolic Lookup and Symbolic Matcher.
The combination of Symbolic Lookup and Symbolic Matcher has a negative impact on NSnet performance.
The Interactive module is used in all cases in the ablation study.
The TER (Translation Edit Rate) decreases as we move from ID 1 to ID 7 in the ablation study.
CopyNet has the highest relevance score and wins in the manual K2Q evaluation.
The method with the highest Cohen's kappa score also has the highest wins in the manual K2Q evaluation.
CopyNet outperforms all other methods in terms of ROUGE-L, ROUGE-1, ROUGE-2, and BLEU scores.
The CNN classifier achieves an accuracy of 99.65% on the Reddit dataset.
The Logistic classifier does not have a reported accuracy on the Politics dataset.
The table shows the edit distance after masking out pivot words in the CG and CA models for Yelp and Amazon datasets.
The gender distribution is different for Yelp and Amazon datasets.
Table 1 provides information about the accuracy of existing systems on the datasets for all questions and questions at all positions within the sequence.
The "neural" model has an accuracy of 17.4% on all questions.
There are 8 different input feature vectors used in the NN ranking model.
The dimensionality of each input feature vector used in the NN ranking model is either 100 or 10.
The F-score for System1 (fusion of 2 and 3) is 0.759.
System2 (cond. RNN-LM) has a higher precision than System3 (attn. enc-dec).
The Dynamic Fusion framework (Ours) achieves statistically significant improvements over all baselines in terms of SMD BLEU, SMD F1, SMD Navigate F1, SMD Weather F1, and SMD Calendar F1.
The Shared-Private framework (Ours) achieves higher F1 scores than all baselines in the Multi-WOZ 2.1 dataset, including the Restaurant F1, Attraction F1, and Hotel F1.
The entity F1 score for the full model on the test set is 62.7%.
The difference in entity F1 score between the model without domain-shared knowledge transfer and the full model is 3.7%.
The model "Our framework" has a fluency score of 4.2.
The model "GLMP" has an agreement rate of 53%.
The Transformer model performs better than the biLSTM model in terms of WER.
There is no WER value provided for the Contextual Block Transformer model.
Table 2 compares the performance of MLE and SST on different language directions.
Table 2 compares the performance of translation from Chinese to English (zh → en) and from English to Chinese (en → zh) using MLE and SST.
Table 1 compares the performance of GroundHog and THUMT.
THUMT with Adam optimizer outperforms GroundHog with Adam optimizer in all machine translation tasks.
The baseline model achieves higher accuracy on the GENIA-POS dataset compared to the PTB-POS dataset.
The model with LMcost achieves the highest accuracy on the UD-FI dataset.
The "+ LMcost" modification improves the performance of the baseline on all metrics.
The F0.5 score is higher for the "+ LMcost" modification compared to the baseline on both FCE TEST and CoNLL-14 TEST datasets.
Table 2 shows the performance of alternative sequence labeling architectures on NER and chunking datasets, measured using CoNLL standard entity-level F1 score.
The architecture with LMcost has higher F1 scores compared to the baseline architecture in all datasets.
Table 3 shows the effect of using different UNK post-processing methods on the English development set.
The "Copy" method has the highest values for both R1 and R2 among all the UNK post-processing methods.
The Full-vocab restriction achieves the highest R1 and R2 scores compared to the Input-only and Extended-input restrictions.
The Input-only restriction achieves the lowest R2 score compared to the Extended-input and Full-vocab restrictions.
The ABS system performs better than the "this work" system on the DUC-2004 R1 R metric.
The "this work" system performs better than the ABS+ system on the Gigaword R2 F1 metric.
The system "[ITALIC] this work" achieves the highest F1 score in both R2 and RLF1 on the LCSTS dataset.
The system "COPYNET" uses the G-RNN encoder and G-RNN decoder along with a Copy mechanism.
"Our Model (Crawl Embedding)" outperforms "Disan (Crawl Embedding)" in terms of F1 scores for PER, LOC, and ORG.
"Disan (Crawl Embedding)" performs better than "Adaptive Co-Attention Network" in terms of F1 score for MISC.
The CWBLSTM model achieves the highest accuracy in disease named entity recognition.
The CharCNN model achieves the highest F1 score in clinical named entity recognition.
The performance of BERT with training data S,M⋆1 is 44.2 on A1.
The performance of RoBERTa with training data S,M,F,ANLI is 92.9 on A1+A2+A3.
Table 8 provides information about the percentage of development set sentences with tags in several datasets.
The percentage of sentences with the tag "Negation" is higher in SNLI than in the other datasets.
The model "Back-translation [edunov2018understanding]" has the highest accuracy of 35.0.
The accuracy of the models increases from "Convolutional Seq-to-Seq [gehring2017convolutional]" to "DeepL Translation Machine".
Table II provides the accuracy of various POS tagging models evaluated on the WSJ-PTB dataset.
The model "Meta-BiLSTM" achieves the highest accuracy among all the POS tagging models evaluated on the WSJ-PTB dataset.
The model "Self-Attentive Encoder" achieves the highest accuracy of 95.1.
The model "Model Combination and Reranking" has a higher accuracy than the model "In-order traversal over syntactic trees + LSTM".
The model "[BOLD] Argumented Representations + BiLSTM [he2018jointly]" achieves the highest accuracy of [BOLD] 85.3.
The model "Contextualized Word Representations [peters2018deep]" achieves an accuracy of 84.6.
The Universal Language Model Fine-tuning (ULMFiT) achieves the highest accuracy among all the state-of-the-art methods on the AG News Corpus dataset.
The Deep Pyramid CNN achieves a higher accuracy than the CNN from Conneau et al. and the CNN from Johnson et al. on the AG News Corpus dataset.
The table provides detailed results of Croatian word analogy corpus for four different models: CBOW, Skip-gram, fastText-Skip, and fastText-CBOW.
The BLEU score for the Baseline (18M) system on the newstest17 test set is 24.2.
The BLEU score for the + Iterative BT/KD system on the newstest18 test set is 30.5.
Table 1 shows the results of English↔German translation using sacreBLEU.
The baseline scores for En→De news16, En→De news18, De→En news16, and De→En news18 are 37.4, 45.6, 41.9, and 44.9, respectively.
Table 4 compares the BLEU scores for English↔Lithuanian translation on the newsdev19 set for four different systems.
The + Reranking system achieves the highest BLEU scores for both English→Lithuanian and Lithuanian→English translation on the newsdev19 set.
The BLEU score for the KDE4 domain is 27.26±0.36 when Meta-MT is performed.
The metric "NIST" has the highest correlation value for the language pair en-cs.
The model "RBERT--Multi" has the highest correlation value for the language pair en-de.
Table 5 presents the Pearson correlation on the 2015 COCO Captioning Challenge for various evaluation metrics.
The correlation values for the M1 and M2 measures in the Leic evaluation metric are -0.019∗ and -0.005∗ respectively.
The table contains information on models trained on QQP and models evaluated using metrics not trained on QQP or PAWSQQP.
The highest correlation values among the task-specific and task-agnostic metrics are bolded in the table.
Both "roberta-large" and "roberta-large-mnli" have 24 total layers.
The best layer for "xlnet-large-cased" is 7.
BERTScore has different values for different types of BERT models.
The metric "PARSE-2+Recall+MULT-MAX" has an empty value for the "RBERT" type.
The "SGMM" system performs worse than the "GMM" system across all feature configurations.
Increasing the feature dimension leads to a decrease in WER for both the "GMM" and "SGMM" systems.
The GMM system outperforms the DNN hybrid system in all cases.
The GMM system with MFCC_0+Δ+ΔΔ performs better than the GMM system with MFCC_0(±3)+LDA_STC in all cases.
Table 7 provides the macro and micro F1 results on OntoNotes.
The highest macro F1 score in the "Fine Ma" column is in the "Ultra" category for both "Onto Hy" and "Onto Eu".
The number of instances in the "Train" split is larger than the number of instances in the "Dev" split, which is also larger than the number of instances in the "Test" split.
The number of instances in the "Dev" split is smaller than the number of instances in the "Train" split, which is also smaller than the number of instances in the "Test" split.
The model "multi" has the highest F1 score on both the Dev and Test sets.
The Precision, Recall, and F1 scores for the "AttNER" model are lower on the Test set compared to the Dev set.
MA has the lowest perplexity score among all the models.
HTD has the highest TRR score among all the models.
Table 2 presents the annotation results for different models and their performance in terms of appropriateness, richness, and willingness.
The HTD model outperforms the ERM model in terms of appropriateness, richness, and willingness.
The accuracy of UH-PRHLT (primary) is higher in the test set results compared to the development set results.
The Mean Average Precision (MAP) for UH-PRHLT (contr. 1) is higher in the test set results compared to the development set results.
The "UH-PRHLT (primary)" model performs the best among all the models in the development set.
The "UH-PRHLT (contr. 1)" model achieves the highest F1 score among all the models in the test set.
The precision of BiV-HNN on the Python Testing Set is 0.808.
The recall of the Select-All heuristic baseline on the SQL Testing Set is 1.000.
BiV-HNN has the highest F1 score on both the Python Testing Set and the SQL Testing Set.
BiV-HFF has the highest Precision score on the SQL Testing Set.
The error percentage for WMT using the "NMT de-en + en-de" method is 1.7%.
The error percentage for IWSLT using our method with cosine similarity is 13.8%.
There are three different models in the table: PLSR 50, PLSR 120, and Feature2Vec.
The table includes accuracy scores for three different models: PLSR 50, PLSR 120, and Feature2Vec.
The accuracy score for retrieving the correct concept from the top 1 most similar nearest neighbor is 3.55 for PLSR 50, 3.55 for PLSR 120, and 4.96 for Feature2Vec.
The table shows the frame error rates for different encoder architectures.
The frame error rate for the LSTM 256x3 architecture on the dev set is lower than on the test set.
The table shows the phone error rates for segmental models trained end to end initialized from the two-stage system trained with hinge loss.
The dev score for dropout=0 and loss function=hinge is 19.4.
The test score for dropout=0.5 and loss function=log loss is 19.7.
The cross entropy on the development set is 2.2395 for the LSTM model with the best training performance.
The Phone Error Rate on the development set is 21.4% for the LSTM model with dropout.
Table 8 provides information about forced alignment quality on the test set.
The percentage of correctly positioned phone boundaries within a tolerance of 40ms is 96.7.
lrfr1-tucker & lrfr2-cp has the highest accuracy of 90.3.
RBG + HPCD (combined model) uses a dependency parser in addition to distance, embedding, POS, WordNet, and VerbNet.
The method "lrfr1-Brown" has the highest accuracy of 88.18.
The method "Concatenation" has an accuracy of 86.73.
The method "RCNNs + pre-train, last state" achieves the highest Dev MAP score.
The method "RCNNs + pre-train, last state" achieves the highest Test P@1 score.
The RCNNs + pre-train method achieves the highest Test MAP score among all the methods.
The CNNs method achieves a Dev P@1 score of 58.4.
Table 5 compares different model variants on the test set when question bodies are used or not used.
The MAP score for the "RCNNs, mean-pooling" model variant when only the title is used is 56.0.
The table displays the average IoU of two models with and without attention at T=1 and T=4.
The average IoU increases as the number of direct descriptions increases.
Our model has a higher consistency rate than the baseline model.
Our model has a higher quality rate than the baseline model.
The retrieval performance of DE I2T is higher in the CxC Retrieval task compared to the MS-COCO Retrieval task.
The retrieval performance of MultitaskI2T+T2T is higher in the MS-COCO Retrieval task compared to the CxC Retrieval task.
The F1 score for the "Overall" sub-task with the "IR + BERT" system is 17.5%.
The F1 score for the "Perspective stance" sub-task with the "Human" system is 90.9%.
CtrlGen is preferred 30.0% of the time in terms of IMDb style.
Ours is preferred 36.8% of the time in terms of Yelp content.
Our model achieves the highest Yelp ACC score.
Our model achieves the highest Yelp self-BLEU score.
Table 4 presents the evaluation of rating prediction in terms of Mean Square Error (MSE) and Root Mean Square Error (RMSE).
The MSE and RMSE values for the HFT - K=10 model in the 5-core dataset are bolded.
Table 4 presents the evaluation of rating prediction in terms of Mean Square Error (MSE) and Root Mean Square Error (RMSE).
The MSE and RMSE values for the HFT - K=10 model in the 5-core dataset are bolded.
DMAN (Ensemble) achieves the highest performance on both the SNLI and MultiNLI datasets.
The performance of the models is generally lower on the MultiNLI Mismatched dataset compared to the MultiNLI Matched dataset.
Removing the Sentence Encoder Model from the model results in an accuracy of 87.24.
The accuracy of the model without any ablations (DMAN) is 88.83.
The FB15K dataset has more entities than the FB15K-237 dataset.
The FB15K dataset has more test sets than the FB15K-237 dataset.
The model "DistMult-LiteralE glin" has a higher Hits@1 score than the model "ConvE-LiteralE g" in the Head Prediction task.
The model "TransEA" has the highest MRR score in the Tail Prediction task.
Table 6 presents the results of five variants of s-QuASE{}_{\small QAMR} on the development set of QAMR, including the average exact match (EM) and average F1 scores as evaluation metrics.
Model V outperforms all other models in terms of both average exact match (EM) and average F1 scores.
Table 6 presents the results of five variants of s-QuASE{}_{\small QAMR} on the development set of QAMR, including the average exact match (EM) and average F1 scores as evaluation metrics.
Model V outperforms all other models in terms of both average exact match (EM) and average F1 scores.
The table presents the results of ablation studies for context information on MultiWOZ 2.1 for four different models.
CSFN-DST has the highest joint accuracy among the four models tested in the ablation studies.
The table presents the joint goal accuracy (%) of ablation studies on MultiWOZ 2.1 for two different models: CSFN-DST and (-) No schema graph,  [BOLD] A [ITALIC] G= [BOLD] 1.
The use of BERT improves the joint goal accuracy (%) of the models in the ablation studies on MultiWOZ 2.1.
The accuracy decreases as the dialogue turn increases.
Using schema graph improves the accuracy of the proposed CSFN-DST.
CSFN-DST + BERT outperforms CSFN-DST in terms of F1 score for all gate types.
CSFN-DST + BERT performs better in terms of F1 score for the "DONTCARE" gate compared to the "NONE" gate.
The CSFN-DST model with BERT achieves the highest overall accuracy.
The CSFN-DST model with BERT achieves the highest F1 score for the "restaurant-food" slot.
The evaluation of LSTM-NNLM is done on datasets from the fields of Electronics and Books.
The perplexity values for LSTM-NNLM are provided for different combinations of training data in the Electronics and Books datasets.
The table compares the results of different neural network language models.
The table displays results for different models.
The table includes the training speed (words per second) for each model.
Table 6 examines the learning ability of LSTM-NNLM neural network models.
The dynamic property of the neural network is examined in Table 6, with both "No" and "Yes" options.
The table provides the percentage of interchangeable neighbors per pivot POS for different window sizes and algorithms.
The percentage of interchangeable neighbors decreases as the window size increases for the CBOW algorithm.
The benchmark "WordSim353-R" has a size of 252.
The CBOW algorithm has a relative change of -3% for the benchmark "WordSim353".
The table shows the sarcasm detection performance on the validation set before and after data augmentation is applied.
The T+BiLSTM+NeXtVLAD model has a higher recall on the redditvalid dataset compared to the T+BiLSTM+MaxPool model.
Table 5 shows the sarcasm detection performance according to data augmentation on the twittervalid dataset.
The F1 score for the unlabeled augmentation is 0.8977 and the Precision score is 0.8747.
The table shows the sarcasm detection performance according to the ensemble methods on the twittervalid dataset.
The precision, recall, and F1 scores for the ensemble method with the maximum context on the twittervalid dataset are 0.8558, 0.8182, and 0.8366, respectively.
The model "LASER + LR" performs better than the model "mBert" in all languages.
Increasing the training size leads to improved performance for both the "LASER + LR" and "mBert" models.
The performance of the "ESE on FPGA (ours)" is higher than the performance of the "Matrix Comput." for all configurations listed in the table.
The "ESE on FPGA (ours)" has a higher performance than the "CPU Real Comput." and "GPU Real Comput." for all the metrics listed in the table.
The total number of judgments for the "mono" category is 20.
The total number of judgments for the "poly" category is 24.
The method "fist co-occurrence + first" achieves the highest Rogue-1 Precision and Rogue-1 F1 scores.
The method "Liu et al. (2015)" achieves a higher Rogue-1 Recall score compared to its Rogue-1 Precision and Rogue-1 F1 scores.
Table 2 analyzes the effect of using JAMR-Parser in step-1 and compares the ROGUE scores with gold-standard AMR graphs and AMR graphs generated by JAMR-Parser.
The ROGUE-1 recall score is higher when using gold-standard AMR in step-1 compared to using JAMR-Parser for step-1.
The "jamr" generator consistently outperforms the "gold" generator in terms of the R-2 score.
The "original sentence" parser has better readability compared to the "jamr" and "gold" parsers.
The table shows domain adaptation performance with different adaptation strategies.
The LHUC adaptation strategy performs better than the Base strategy on the IWSLT'15 dataset.
The table shows the average SRL F1 on CoNLL-2005 for sentences where LISA and D&M parses were completely correct or incorrect.
The proportions of L+/D+, L–/D+, L+/D–, and L–/D– are 26%, 12%, 4%, and 56%, respectively.
The "+D&M" model achieves the highest F1 score on the WSJ Test set.
The "+Gold" model outperforms the other models in terms of precision, recall, and F1 scores in the ELMo column.
The F1 score for the "+D&M" model using ELMo on the test set is 83.3.
The precision for the SA model using GloVe on the development set is 82.32.
The table provides parsing (labeled and unlabeled attachment) and POS accuracies for different models on test datasets.
The table includes results for different datasets such as WSJ, Brown, and CoNLL-12.
Table 5 provides information about predicate detection precision, recall, and F1 on the CoNLL-2005 and CoNLL-2012 test sets.
The model LISA achieves higher precision, recall, and F1 scores compared to He et al. (2017) on the WSJ, Brown, and CoNLL-12 datasets.
The table presents subjective preference scores (%) between the LSTM-RNN- and HMM-based SPSS systems for different languages.
The LSTM-based SPSS system outperforms the HMM-based SPSS system in terms of subjective preference scores for certain languages.
The table provides subjective preference scores between LSTM-RNNs using 4-frame bundled inference with data augmentation and single-frame inference for different languages.
Table 2 shows the subjective preference scores between LSTM-RNNs using 4-frame bundled inference with data augmentation (4-frame) and single-frame inference (1-frame).
The subjective preference scores for each language in Table 2 add up to 100%.
Table 2 displays subjective preference scores for different languages.
The "No pref." category has the highest subjective preference scores for each language.
Table 5 represents subjective preference scores (%) between the LSTM-RNN- and HMM-based SPSS systems.
The total latency for the LSTM-based SPSS system is lower than the total latency for the HMM-based SPSS system in all categories.
The languages with the highest subjective preference scores for the Hybrid system are Thai, Turkish, Swedish, Mandarin, and Norwegian.
The languages with the lowest subjective preference scores for the LSTM system are Cantonese, Arabic, Polish, Indonesian, and Danish.
52% of the sentences in the dataset have complications.
24% of the sentences in the dataset have resolutions.
The University of Toronto is a public research university located in Toronto, Ontario, Canada.
The T2T system achieves the highest BLEU score among all the systems.
The T2T system has the second-highest average z-score among all the systems.
As the maximum sentence length increases, the maximum batch size that fits into 11GB memory also increases for all combinations of model size and optimizer.
As the maximum sentence length increases, the percentage of sentences in the train and test data that are longer than a given threshold also increases for all combinations of model size and optimizer.
The table provides statistical information about the different types of tokens in the Google and CLUE dictionaries.
The Google dictionary contains more tokens than the CLUE dictionary.
The "Sparse IB" approach achieves the highest FEVER IOU score of 85.64.
The "Task Only" approach achieves the lowest BoolQ IOU score of 10.39.
"Sparse IB (Us)" achieves the highest FEVER IOU, MultiRC IOU, Movies IOU, BoolQ IOU, and Evidence IOU scores.
The "25% data (Us)" approach achieves the highest FEVER Task, MultiRC Task, and BeerAdvocate Task scores.
The hyperparameter values used for Movie dataset are: π (Sparsity threshold (%)) = 40 and γ (weight on SR) = 0.5.
The hyperparameter values used for the BEER dataset are: π (Sparsity threshold (%)) = 20 and γ (weight on SR) = 0.01.
The table presents the results of the test on performance prediction on test interactions using two different approaches: R2DE and IRT.
The IRT approach has a higher accuracy (0.701) compared to the R2DE approach (0.689).
The table shows the results of the test on performance prediction, using only the questions in DSTEST.
The IRT approach has a higher accuracy score than the R2DE approach.
The F1 scores for multi-modality (T+A) are higher than the F1 scores for text only (T) and audio only (A) in both MOSI and IEMOCAP datasets.
The accuracy for multi-modality (T+A) is higher than the accuracy for text only (T) and audio only (A) in both MOSI and IEMOCAP datasets.
The "Ours-FAF" approach achieves the highest weighted accuracy for both 2-class and 5-class sentiment analysis categories.
The "C-MKL1" approach achieves a higher weighted accuracy than the "GSV-e Vector" approach for the 2-class emotion recognition category.
The approach "Ours-FAF" achieves the highest accuracy and F1 score in both the MOSI and IEMOCAP datasets.
The approach "Ours-HF" achieves a higher F1 score than the approach "Ours-VF" in both the YouTube and EmotiW datasets.
The number of items with a minimum of 20 reviews is higher than the number of items with a minimum of 50 reviews and 100 reviews for all categories.
All Bluetooth speakers in the dataset have at least 50 reviews.
Table 9 provides the percentage of reviews with a minimum of 20, 50, and 100 in each category.
BluetoothSpeakers has the highest percentage of reviews with a minimum of 20, 50, and 100 compared to other categories.
The "TT2" model has a higher test loss compared to the "TT1" model.
The "TT2" model has a smaller number of parameters compared to the "TT1" model.
The models used in the experiment are named "Big", "Big+LR1", "Big+LR2", etc.
The BLEU scores decrease as the rank of the models decreases.
Table 4 shows the impact of the candidate selection method on the development performance on CoNLL and TAC-KBP 2010.
The fine-tuned model on CoNLL achieves a performance of 96.9.
The grammaticality score for the IE (ours) model is 1.84.
The BLEU-1 score for the HLSTM+MSA(GA) model is 0.2588.
The test error rate for the AG dataset using the Char-level CNN model is 9.51%.
The test error rate for the Yelp-full dataset using the ULMFiT model is 29.98%.
Table 5 shows the validation error rates for ULMFiT with a vanilla LM and the AWD-LSTM LM on three different datasets.
The AWD-LSTM LM performs better than the Vanilla LM in terms of validation error rates on all three datasets.
Table 6 shows the validation error rates for ULMFiT with different variations of LM fine-tuning.
The combination of full fine-tuning, discriminative fine-tuning, and slanted triangular learning rate achieves the lowest validation error rates among all variations.
Table 2 provides the evaluation of alignment quality for two different systems: RNNSearch and Mixed RNN.
The Mixed RNN system has a lower Alignment Error Rate (AER) compared to the RNNSearch system.
The table provides the ROT on test sets for the RNNSearch and Mixed RNN systems.
The ROT values for the "ALL" category are lower for both the RNNSearch and Mixed RNN systems.
The model "ZeroShot" performs better than the model "CrossLexRep" on the "Restaurants" domain.
The table presents the RL gains over the mutual information system based on pairwise human judgments in three different settings.
In terms of multi-turn general quality, the RL-win setting outperforms both the RL-lose and Tie settings.
The RL model has a higher average number of simulated turns compared to the Seq2Seq and mutual information models.
The mutual information model has an average number of simulated turns of 3.40.
The number of verbs in the "Matrix tense" column is different for each frame combination.
The number of verbs in the "Frame" column varies depending on the tense category.
The label pair "Justification - Support" has the highest PCC value of 0.742.
The label pair "Directed Hate - Opposition" has the lowest p-KCC value of 0.0433.
Table 4 provides information about the inter-annotator agreements for all the annotation tasks.
The inter-annotator agreement for the relevance task is 0.92.
The SVM classifier has the highest Macro F1 score and Accuracy for all masking methods.
The Precision for the left class is consistently lower than the Precision for the right class and the main class for all masking methods.
The BLSTM model with standard attention (L1) has a lower word error rate (WER) for clean speech compared to the BLSTM model with standard attention.
The streaming model with SpecAugment (F=27, T=100) has a lower word error rate (WER) for other speech compared to the streaming model without SpecAugment.
Table 10 presents the test results of different methods for the Reddit task.
The "IR (query+context)" method performs better on the "Entity Matched" subset compared to the "Whole Test Set" subset in the Reddit task test.
The table shows the results of different methods on the Ubuntu Dialog Corpus.
The MemN2N method with 2-hops achieves the highest test accuracy among the mentioned methods.
The model "BERT_QA_Trigger (best trigger question strategy)" achieves the highest F1 score in trigger detection.
The model "Our BERT FineTune" achieves a Precision of 67.15, Recall of 73.20, and F1 score of 70.04 in trigger detection.
The table presents the evaluation results on unseen argument roles.
The F1 score for the "Argument ID + Classification" column with "Template 3" is 67.79.
The trigger word "verb" has the highest F1 score for trigger detection.
The trigger word "action" has the highest recall score for trigger detection.
Table 2 shows automatic evaluation results for the baseline approach and at each stage of the training pipeline.
The in-domain approach achieved higher scores in the BLEU, ChrF, and Meteor metrics compared to the other approaches.
The table presents the results of an ablation study on different auxiliary prediction modules for sequence tagging.
The removal of the future/past and forward/backward prediction modules negatively impacts the performance of the sequence tagging models.
The CVT + Multi-task + Large method achieves the highest scores in all tasks.
The ELMo + Multi-task method achieves a higher score in the FGN task compared to the CVT + Multi-task method.
Table 2 provides the dev set performance of multi-task CVT with and without producing all-tasks-labeled examples.
The table compares the dev set performance of two models, CVT-MT and w/out all-labeled.
The accuracy for sentiment classification using BERT-pair-QA-M is 93.6.
The F1 score for aspect classification using BERT-pair-NLI-B is 87.5.
The word error rates decrease as the value of "pass" increases for all three types of scaling.
The "three scales" type of scaling results in the lowest word error rates for each value of "pass".
The count of speakers increases from 440 to 447.
The WER decreases from 22.3 to 10.6.
Table 4 compares the number of errors on simulated and resampled test data using 39-dimensional features and 13-dimensional features.
The number of errors is lower for the simulated test data compared to the resampled test data for both 39-dimensional and 13-dimensional features.
The BLEU4 score for I3D(RGB) is 0.417.
The ROUGE-L score for ResNext+I3D(Flow) is 0.561.
Table 4 shows the average cross-lingual transfer performances on the test sets using different adversarial training objectives and settings.
The SelfAtt-Graph model achieves a performance of 64.51 on the en-fr test set.
The adversarially trained model performs better on the en-fr language pair compared to the model trained only on the source language (en).
The sv language pair has the highest UAS%/LAS% scores compared to the other language pairs in the same row.
English paired with French achieves the highest performance in the "Lang. Test Perf. AT" column, with a score of [BOLD] 62.25.
English paired with Russian achieves the lowest performance in the "Average Cross-lingual Perf. MTL" column, with a score of 56.20.
The average distance to other languages is highest for the "la" language.
The performance of multilingual BERT is highest for the "de" language.
The average distance to other languages is highest for the "la" language.
The performance of multilingual BERT is highest for the "de" language.
Table 6 shows the performance on various types using Justice subtypes for training.
The highest hit rate for trigger classification at k=5 is 90.20.
The different settings in the table are labeled as A, B, C, and D.
The Hit@k Trigger Classification performance increases from setting A to setting D.
The phi-LSTM model outperforms the DeepVs and NIC models on all four metrics (B-1, B-2, B-3, B-4) in the Flickr8k dataset.
The DeepVs model performs the worst among the three models in terms of the B-1 metric in the Flickr8k dataset.
The number of sentences in the trained data is the same as the actual data for both the train and test datasets.
The average caption length is higher for the generated captions compared to the actual captions in both the test data and the generated captions.
Table 1 presents the performance of SA-NMT on the development set for different loss functions in terms of BLEU.
Table 2 compares the performance of fast_align and GIZA++ aligners in terms of BLEU on the development set.
GIZA++ performs slightly better than fast_align in terms of BLEU on the development set.
Table 3 compares the BLEU scores for a large-scale translation task using different systems.
SA-NMT performs better than Moses, NMT1, and NMT2 in all test sets.
Table 5 provides a comparison of BLEU scores for different systems on the low-resource translation task using the CSTAR03 development set and the IWSLT04 test set.
SA-NMT performs significantly better than both NMT1 and NMT2 on the low-resource translation task with a significance level of p<0.01.
The table presents the evaluation results of three different methods for content selection.
The "Our abstractive model" performs better than the "Our abstractive model (without anaphora resolution)" and the "Extractive Model (baseline)" in terms of R-2 and R-SU4 scores.
Table 2 shows the CERs of different FR with the same FS for a 4-layers LSTM model.
The CER for FS=3 and FR=3 is 3.81.
As the number of stacked frames increases, the character error rate also increases.
As the number of matching frame retaining increases, the real-time factor decreases.
The WER for the Original domain is higher than the WER for the New domain for all methods.
The Fine-Tuned method has the highest WER for both the Original and New domains.
The average score achieved by the baseline model on the GLUE dataset is 83.28.
The average score achieved by the baseline model on the EnRo dataset is 27.65.
All the architectural variants in Table 2 are trained using the denoising objective.
The "Encoder-decoder" architecture with the "Denoising" objective achieves the highest score on the GLUE benchmark.
Table 4 shows the performance of three different pre-training objectives: Prefix language modeling, BERT-style [devlin2018bert], and Deshuffling.
The BERT-style pre-training objective achieves a performance of 82.96 on the GLUE benchmark.
Table 5 compares the performance of different variants of pre-training objectives, including BERT-style and MASS-style.
The "Drop corrupted tokens" variant outperforms other variants in terms of performance on the SQuAD dataset.
As the number of tokens decreases, the performance on GLUE, CNNDM, SQuAD, SGLUE, EnDe, EnFr, and EnRo also decreases.
Table 10 compares the performance of different fine-tuning methods on various tasks.
The "Gradual unfreezing" method achieves the highest performance on the SGLUE task compared to the other fine-tuning methods.
The table presents the results of four different models: Random, Word+all, Word+sample, and Char+all.
The "Word+all" and "Word+sample" models were trained using only the training data.
The "Char+sample" model achieved the highest F1 score.
The parsing accuracy for Basque using Convex-MST with prior is 40.0.
The average parsing accuracy for all languages using DMV(EM) with a length of ≤ 10 is 41.0.
Table 3 compares recent unsupervised dependency parsing systems on English.
The total number of tweets in the corpus is 5,625,844.
The number of tweets in the Asia region for the "Happy" emotion is 149,129.
Freebase has more triples and paths in the training split compared to WordNet.
Freebase has more entities and relations compared to WordNet.
The experiment results show that the performance of the models improves as more techniques are applied, as evidenced by the decreasing values in the "Test" column.
The "+ LM Fusion" technique achieves the highest performance among all the techniques, as indicated by the bolded value in the "Test" column.
Table 2 provides the results of an ablation test of different masking methods.
The table does not provide definitions for any of the semantic groups.
The "YouCook" and "TACos-M-L" datasets belong to the "Action Description" task.
The "Bilibili" dataset has the highest number of comments among all the datasets.
The "Unified Transformer" model has the highest recall values at 1, 5, and 10 among all the models in the "Video Only" and "Both" categories.
The "Fusional RNN" and "Unified Transformer" models have the lowest "MR" value and highest "MRR" value among all the models in the "Comment Only" and "Both" categories.
Table 5 shows the results of human evaluation metrics on the test set for different models trained and tested on both videos and surrounding comments.
The Fusion model has the highest fluency score among all the models evaluated in Table 5.
The "Humans" outperform all the models on the Uncorrelated and Success Only test sets.
Table 2 shows the BLEU-4 results of NLG modules trained and tested on MultiWOZ.
The Seq-2-Seq Attn NLG module achieves a BLEU-4 score of 58.69 on MultiWOZ.
Table 1 shows the evaluation results of NLU modules trained and tested on MultiWOZ dataset.
The MILU BERT module achieves higher Precision, Recall, and F1-Score values compared to the OneNet and MILU modules.
The hyperparameter count for the speaker LSTM in Model A is 200 and for the listener LSTM is 300.
The total number of speakers in Model A is 708k and in Model B is 670k.
The Linear SVM model has the highest F1 Macro score.
The Multinomial NB model has the lowest Precision score for the "Not Offensive" class.
The Linear SVM model performs the best for the "Not Offensive" class.
The Multinomial NB model and the Linear SVM model have the same performance according to the weighted average F1 score.
The Linear SVM model has the highest F1 Macro score among all the models.
The Multinomial NB model has a lower Precision score for offensive language detection compared to the Linear SVM model.
The Linear SVM model has the highest F1 Macro score.
The Multinomial NB model has a higher precision for offensive language detection compared to the Linear SVM model.
Linear SVM has the highest F1 Macro score among the four models.
Multinomial NB has a higher F1 score for offensive language detection compared to Linear SVM.
The F1 score for the "Not Offensive" class is higher than the F1 score for the "Offensive" class for the "BERT-Base Multilingual Cased" model.
The Precision, Recall, and F1 scores for the "Offensive" class are lower than the weighted averages for all classes for the "2D Convolution with Pooling" model.
The table presents the BLEU scores for EN↔TR translation for two different years and two different systems: baseline and +copied.
The +copied system achieves a higher BLEU score than the baseline system for EN→TR translation in 2017.
The model "SPHRED" has the highest average score among all the models.
The accuracy for SCENE2-A is 99.8% when using the Extrema method.
The Selective-Env model achieves the highest score of [BOLD] 33.63 on the DUC-2004 R-L evaluation metric.
The BI-RNMT model outperforms the Transformer model on the English Gigaword dataset for both the R1 and R2 evaluation metrics.
The BI-RNMT model outperforms the RNMT model in all translation tasks.
The BIFT model achieves the highest translation quality among all the models in all translation tasks.
The BI-RNMT model has a higher BLEU score than the GNMT, Conv, AttIsAll, RNMT, RNMT (R2L), Transformer, Transformer (R2L), Rerank-NMT, and ABD-NMT models.
The BIFT model has the highest BLEU score among all the models listed in the table.
The CNN [ITALIC] I method achieves higher accuracy scores than the SYS method for all three events.
The SYS method has lower macro F1 scores than the SVM method for all three events.
The model with Gumbel-Softmax achieves the highest F1 scores in all categories compared to other models.
The model by Human* Eric et al. (2017) achieves the highest InCar BLEU score compared to other models.
The "w/ Gumble-Softmax" model has the highest consistency score.
The "w/ Gumble-Softmax" model has the highest correctness score in human evaluation.
The F-score values for the "birthplace of" relation are higher for both Wikidata and Alexa using the fastText model compared to the HypeNET model.
The F-score values for the "applies to" relation are higher for both Wikidata and Alexa compared to the "part of" relation.
The table provides notations for BERT, including batch size, number of self-attention heads, number of layers, number of hidden units, feed-forward hidden units, and sequence length.
The table provides the values for the notations in the "Large" version of BERT.
BlockBert with n=3 has a higher activation memory than BlockBert with n=2 for all values of N and b.
BERT has a higher activation memory than BlockBert with n=2 and BlockBert with n=3 for all values of N and b.
RoBERTa-1seq has a shorter training time than BlockBert with n=2 and n=3 for both 512 and 1024 configurations.
BlockBert with n=2 has a lower perplexity than RoBERTa-1seq for both 512 and 1024 configurations.
The hyper-parameter "Warmup steps" has a value of 10K.
The value of the Adam ϵ hyper-parameter is 1e-8.
The table shows the results of an ablation study for multitask learning on the SciERC development set.
The addition of coreference as a task did not improve the performance on the SciERC development set.
The F1 score for span identification in the Best SemEval model is 28.
The F1 score for keyphrase extraction in the SciIE model is 46.0.
The technique "SemRel(M,R)+CMM+CCF" has the highest score among all the techniques.
The technique "SemRel(M,R)+CMM+CCF" has the highest score among all the techniques when combined with the "IR status".
The technique "SemRel(M,R)+CMM+CCF" has the highest P@1 value.
The P@1 value for the "IR status+CMM" technique is 63.8 with a 95% CI ranging from 60.8 to 67.8.
Table 4 provides information about the mean similarity between pre-trained vectors and vectors trained from different sentence selections.
The mean similarity for the "sim-inf" condition is 0.567.
The "c2v feed-forward NN" classifier has the highest accuracy on the test data.
The "FastText" classifier has the highest Spearman's r value.
The sentence "To tell means express something in words." has a higher human score than the sentence "To tell is to let something be known."
The sentence "What can I tell him ?" has the highest classifier score among the given sentences.
The experiment compared different variations of the system based on their disk size, number of parameters, and BLEU scores on newstest2014 and newstest2015.
The Decision-Level Fusion model achieves the highest accuracy in the Business category.
The MRC Database model achieves an accuracy of 82.7% in the General category.
The combination of MRC+MI+PR achieves the highest accuracy for all categories.
The accuracy of the Business category is higher than the accuracy of the Science category, and the accuracy of the Science category is higher than the accuracy of the Sports category.
The "Politics" category has the highest agreement score among all categories.
The "Politics" category has the highest kappa score among all categories.
The table presents binary classification accuracies (%) on basic human annotated datasets for models trained on heuristically labeled data.
The "Overall model" has the highest accuracy among all models in the "Politics" domain.
Table 6 presents the results of different models for entity mention detection and typing on the test set.
SciBERT performs better than other models for entity mention detection and typing.
The models used in the experiments include RBF SVM, Logistic Regression, BiLSTM mat2vec, BiLSTM word2vec, + mat2vec, + bpe, + BERT-base, + SciBERT, BiLSTM BERT-base, BiLSTM SciBERT, BERT-base, SciBERT, and BERT-large.
The model BiLSTM SciBERT achieves the highest F1 score among all the models.
The model "BiLSTM SciBERT" performs the best in terms of macro F1 score on both the dev and test sets.
The addition of BERT-base improves the performance of the model in terms of macro F1 score on the test set.
Table 9 shows the micro-avg. F1 scores of various models in the synthesis procedure dataset.
The model "BiLSTM + all (with SciBERT)" achieves the highest micro-avg. F1 score among all the models in the synthesis procedure dataset.
Table A1 shows the results of the MRQA model on unseen datasets.
The MRQA model performs well on the BioASQ dataset, achieving an EM score of 59.62 and an F1 score of 74.02.
The Ensemble model achieves the highest precision and recall@2 scores on the CovidQA dataset.
The Ensemble model outperforms the MRQA and BioBERT models in terms of recall@2 on the CovidQA dataset.
Table 1 shows the exact match accuracy of MRI on CELEX using different models.
The "lat-region" model achieves the highest accuracy for both 13SIA and rP in the MRI on CELEX task.
The BERT-Large model achieves higher scores on the SQuAD1.1 dataset compared to the SQuAD2.0 dataset.
The XLNet-Large-wikibooks model achieves a score of 77.4 on the RACE dataset.
The detection accuracy of BERT on SIGHAN 2014 is 76.8.
The correction F1 score of SpellGCN on SIGHAN 2015 is 81.3.
Table 3 shows the performance of different methods and baseline models in terms of precision, recall, and F1 score for both character-level and sentence-level detection and correction.
BERT achieves a precision of 87.2, a recall of 92.3, and an F1 score of 84.6 in character-level correction.
Table 5 shows the ablation results for different graph combination methods.
The attentive combination method with \beta=3 achieves a C-F score of 68.2.
The F1 score for the "Tree-kernel+Emb (Combination)" model is 52.3.
The F1 score for the "PM’13 (Brown)" model is 48.3.
The model "+ fcm (ST)" has the highest average F1 score.
The "+Gated orthogonalization" model outperforms the other models on the French WikiBio dataset in terms of BLEU-4, NIST-4, and ROUGE-4 scores.
The "+Gated orthogonalization" model achieves a higher BLEU-4 score compared to the "Basic Seq2Seq" model on the French WikiBio dataset.
The "+Gated orthogonalization" model outperforms the other models in terms of BLEU-4, NIST-4, and ROUGE-4 scores.
The "+Gated orthogonalization" model shows a significant improvement in BLEU-4, NIST-4, and ROUGE-4 scores compared to the "Basic Seq2Seq" model.
Increasing the amount of target domain data improves the BLEU-4 score for the target data in the "Arts" category.
Including the target domain data improves the BLEU-4 score for the target data in the "Sports" category.
The translation quality for CDCM1 using monolingual word embeddings in MT04 is 35.74.
The model "BM25 + QGen" achieves the highest average precision for the BioAsq dataset.
The model "QGen" achieves the highest average nDCG for the NaturalQuestions dataset.
The corpus "jfleg" has the highest number of sentences changed among all the parallel corpora available for gec.
The corpus "fce" has the highest mean number of characters per sentence among all the parallel corpora available for gec.
The number of annotated pairs for Motion – GP-claim is 2,750, for Speech – GP-claim is 3,246, for Sentence – GP-claim is 4,271, and for iDebate claim is 2,164.
The number of positive examples for Motion – GP-claim is 1,265, for Speech – GP-claim is 1,491, for Sentence – GP-claim is 854, and for iDebate claim is 368.
The "+radical +tie +bigram" model performs the best on the PKU dataset.
Among all the models, the DL-Text & Metadata (Interleaved) model achieves the highest AUC, Accuracy, Precision, Recall, and F1 scores for the Hate Dataset.
The DL-Text & Metadata (Naive Train.) model outperforms the DL-Text only model in terms of AUC, Accuracy, Precision, Recall, and F1 scores across all three datasets.
The combination of all metadata features (TF+UF+NF) achieves the highest AUC value of 0.961.
Including the network feature (NF) along with the user feature (UF) does not improve the AUC value compared to using only the user feature (UF).
The error rates for sentiment classification are higher than the error rates for news document classification.
The error rates for the combination of t2t and s2t are lower than the error rates for both t2t and s2t individually.
Table 5 provides baseline results using English unlabeled data and results using unlabeled data from the target domain.
The target domain has lower scores compared to the original domain in all languages.
Table 6 is an ablation study about the translate-train strategies, with results for sentiment classification and news document classification.
The test accuracy for sentiment classification on the English dataset using the amazon-en model is 10.08.
Table 7 shows the effect of the temperature of the translation sampling decoder.
Table 8 provides error rates for cross-domain document classification for different models and training methods.
XLM ft with UDA has lower error rates for cross-domain document classification compared to XLM ft without UDA.
The MaxCosine-LSTM-biEmb-biWay-Ensemble model has the highest test accuracy among all the models.
The "Transformer + Aggregation" model outperforms the other models in terms of salient, fluency, and non-repeated metrics.
Both the "Pointer Generator + Coverage" and "Transformer" models perform better in terms of salient metric compared to the "Pointer Generator" model.
"Aggregation Transformer(attention)" achieves the highest scores for all three ROUGE metrics.
The ROUGE-L score for "rnn-ext + abs + RL + rerank [chen2018fast]" is lower than the ROUGE-L score for "Aggregation Transformer(attention)".
The table includes the results of 7 different models.
The model "(m6)Agg-Transformer(attn 1 layer)" achieves the highest ROUGE-1 score.
Table 6 presents the results of experiments on a Chinese dataset using three baseline models.
The Transformer + Aggregation model achieved the highest ROUGE-1, ROUGE-2, and ROUGE-L scores among the three baseline models.
The table compares the "Rank" of DNN posterior matrix, projected posterior matrix, and RPCA senone posterior matrix.
The rank of the Robust PCA senone posterior matrix is lower than the rank of the projected posterior matrix, which is lower than the rank of the DNN posterior matrix.
Table 2 compares the ASR performance using DNN posteriors and projected posteriors in clean and noisy conditions on the Numbers'95 database.
The WER (%) for the DNN method in clean conditions is 2.6%.
The table provides the WER (Word Error Rate) for three different systems: NT, attention within chunk; NT, look back; and + look ahead.
The chunk size for all three systems mentioned in the table is 10.
Table 4 shows the Word Error Rate (WER) for the NT system with MHA and Chunk.
The WER for the NT system with Chunk is 10.1.
Table 6 provides the WER for NT and NT with an external LM.
The Chunk system has a WER of 8.6, while the No LM with LM system also has a WER of 8.6.
The percentage of source word overlap is higher than the percentage of source subword overlap for all domains.
The BLEU score is higher for the DALI domain compared to the Base domain for all domains.
The table provides examples of word translation from the output of the IT to Medical adaptation task.
The count column represents the number of occurrences of word pairs in the pseudo-in-domain training set.
Table 2 shows the performance of various models, including MFC, LogReg, WP, LSTM, and CNN.
The WP model with POS tagging has higher accuracy than the WP model without POS tagging.
Table 5 shows the QWK scores for baseline models with and without POS embedding.
The highest QWK score for the CNN-CNN-MoT model is 0.500, for the CNN-CNN-MoT-POS model is 0.741, for the CNN-LSTM-ATT model is 0.584, and for the CNN-LSTM-ATT-POS model is 0.612.
The QWK scores for "CNN-CNN-MoT-Feat" are higher than the scores for "CNN-CNN-MoT" in all "Prompts" columns.
The average QWK score for "CNN-LSTM-ATT-Feat" is higher than the average score for "CNN-LSTM-ATT".
The "Weak+Gold" supervision has the highest accuracy and F1 score.
The "Baseline II (Gold)" has higher accuracy and F1 score compared to the "Baseline I (majority in TRAIN)".
The method "SS-SG" has the highest precision and recall values among all the categorization methods.
The precision value for the method "k-NN-SG" is higher than the precision value for the method "LR-SG".
The majority of predictions in the editorial evaluation are for the "female" class.
The editorial evaluation shows that the accuracy of predictions for the "female" class is higher than the accuracy for the "male" class.
The "Copy + Generate" model outperforms all other models in terms of Relevance, Readability, ROUGE-1, ROUGE-2, and ROUGE-L.
The crowdsourced titles do not have ROUGE scores because they are the ground truth.
The Sinkhorn Transformer (32) model achieves the lowest Edit Dist. and the highest EM score among all the models.
The Sinkhorn Transformer (32) model achieves the highest EM score among all the models.
The Sinkhorn Mixture model achieves the lowest perplexity score on the Base setting.
The Transformer model has a higher perplexity score on the Big setting compared to the Sinkhorn Mixture model.
The Sinkhorn Transformer has two different versions with different numbers of parameters and perplexity values.
The Mesh Tensorflow model has the lowest perplexity value among the listed models.
The "Sinkhorn Mixture" model has the lowest "Bytes per char" values among all the models.
The "Local Attention" model has a higher "Bytes per char" value for the "Base" configuration compared to the "Big" configuration.
The Sinkhorn Transformer (256) achieves the best performance in terms of Bpd among all the models.
The Sinkhorn Transformer (256) performs similarly to the Local Attention model in terms of Bpd.
Table 6 presents the experimental results on word and character level document classification on IMDb dataset and SST datasets.
The Sinkhorn (32) model achieves the highest performance on the SST Word dataset.
Sortcut Sinkhorn (2x16) achieves the highest performance on MNLI.
Transformers (vaswani2017attention) achieves a higher performance on SNLI than on MNLI.
The F1 score is highest for the "100% Clean" data, followed by "Noise Channel" and then "50% Clean + 50% Distant" data.
The Precision is highest for the "100% Clean" data, followed by "Noise Channel" and then "50% Clean + 50% Distant" data.
The method "Wikidata" has a precision of 50, recall of 25, and F1 score of 33.
Adding "Nigerian Names" to the "Wikidata" method increases the precision to 55, recall to 39, and F1 score to 46.
The "Transformer big" model achieves the highest BLEU score on the test set.
The "SAC Large (dependency)" model achieves the highest BLEU score on the test set.
The model "Compress Trans" achieves the lowest perplexity on both the dev and test sets.
The model "Trans-XL Large" has a lower perplexity than the model "Trans-XL Base".
The AK-DE-biGRU model outperforms all other models in terms of R2@1, R10@1, R10@3, and R10@5 scores.
The DL2R model performs better than the r-LSTM model in terms of the R2@1 score.
The model "AK-DE-biGRU" achieves the highest values for R10@1, R10@3, and R10@5.
The model "AK-DE-biGRU" with additional settings "w2 v" outperforms the base model "AK-DE-biGRU" in terms of R10@1, R10@3, and R10@5.
The model "VL-BERT" achieves the highest score for "Zero-shot R@1".
The model "InterBERT" performs better than "VL-BERT" in most metrics.
Table 2 is an ablation study of the training techniques conducted on the validation set of VCR.
The combination of MSM+MRM+ITM achieves a higher score in the VCR QA→R task compared to the combination of MLM+MOM+ITM.
The BERT-base model has a higher accuracy score on QNLI and SST-2 compared to the Single Stream model.
InterBERT has a higher F1 score on CoLA and a higher Spearman correlation on STS-B compared to the BERT-base model.
The table presents an ablation study on different negative samples in the contrastive objective on MultiWOZ in the end-to-end evaluation setup.
The table shows the results of three different models: [EMPTY], + Extra, and + Teach.
The "Inform" metric improves for the + Extra model compared to the [EMPTY] model, but decreases for the + Teach model compared to the + Extra model.
The third-place model in the DSTC8 competition has the lowest number of turns in the conversation.
The track 1 winner model in the DSTC8 competition has the highest success rate in the task.
MTE-CQA pairwise has the highest MAP score among all the systems.
Baseline rand has the lowest average recall score among all the systems.
Removing each feature individually has a negative impact on the MAP score.
The MTE-CQA system has higher MAP, AvgRec, and MRR scores compared to the other systems.
The system labeled as "1st" has the highest MAP, AvgRec, and MRR scores.
The average MAP score is 73.54, the average AvgRec score is 84.61, and the average MRR score is 81.54.
Table 6 shows the performance of a neural machine reading comprehension model on a generated corpus.
The F-1 scores for the development and test sets are 88.20 and 87.79, respectively.
The "Human" evaluation method received higher ratings for grammaticality, making sense, and answerability compared to the ContextNQG and CorefNQG methods.
The CorefNQG method has the same average rank as the Human evaluation.
The values in the "τ" column represent different thresholds for the algorithms.
The LPA-TD-Coh algorithm with K = 3 achieves the highest performance in the "politics-rel" category.
The precision of log_reg_bert is 0.846.
The precision of char_cnn is higher than the precision of char_gru.
Table 7 compares the performance of multi-task learning and sequential fine-tuning using different configurations of the BERT-Base model.
The inclusion of the NLI task improves the performance of the BERT-Base model with sequential fine-tuning on the MC160 dataset.
RoBERTa-Large+MMM achieves the highest accuracy on the DREAM dataset.
BERT-Base+MMM shows a significant improvement in accuracy compared to the BERT-Base baseline.
The tasks in the table include Sentiment Analysis, Paraphrase, Span-based QA, NLI, Combination, and Multi-choice QA.
The datasets in the table include SST-2, Yelp, GLUE-Para., SQuAD 1.1, SQuAD 2.0, MRQA, MultiNLI, NLI, GLUE-NLI, GLUE-Para.+NLI, and RACE.
Adding NLI improves the accuracy for all three versions of the RACE dataset.
Adding MAN improves the accuracy for the RACE-H version of the dataset.
RoBERTa-Large+MMM achieves the highest test accuracy for all three RACE datasets.
The table provides an error analysis on the DREAM dataset, showing the percentage of different question types that were wrongly predicted by the BERT-Base baseline model.
The best model achieves an accuracy of 84.8% on questions of the "Matching - Keywords" sub-type.
The language bxr has a higher average POS tagging accuracy than the language am.
The language hsb has a higher POS tagging accuracy than the language am in the CHR11 model.
The average POS tagging accuracy for all languages with higher quality data is 0.77.
The POS tagging accuracy for the Spanish language with higher quality data is 0.80.
Table 1 shows the results of tagging accuracy and attachment score for the latent tree grammar in ListOps.
The forward relaxed method achieves a tagging accuracy of 99.6 and an attachment score of 99.7 in ListOps.
The "SSDMNs" model achieves the highest F1 score on average across all domains.
The "MN+S" model performs better than the "LSTM" model in all domains.
The JointEventEntity model achieves the highest F1 score for Event Trigger Identification.
The StagedMaxEnt model achieves the highest Precision score for Event Argument Identification.
JointEventEntity outperforms all other models in terms of F1 score for both trigger and argument role classification.
JointEventEntity performs better than CNN in terms of F1 score for cross-document event extraction.
The "Average-pooled" adapter training method achieves the highest Bleu score and the lowest Ter score on newstest2013.
The Bleu score increases with the "None" to "Max-pooled" to "Average-pooled" adapter training methods.
The table describes cross-lingual encoder variations in the French→ German language pair in a zero-shot setting with step-wise pre-training.
The model trained on the "Pivot side of parallel" with noisy input achieves a performance of [BOLD] 72.7 on the newstest2013 Ter dataset.
The addition of synthetic data improves the performance of the models on the French→German newstest2012 task.
The combination of a cross-lingual encoder and a pivot adapter improves the performance of the models on the German→Czech newstest2013 task.
The MER (%) test decreases when the decoder LM is applied in the Baseline model.
The MER (%) test is lower when the Baseline + CD & JSD model is used with the decoder LM compared to when it is not used.
The table lists different machine translation methods, including Glossed, Majority Class, I-Matrix, ADD, BAE-cr, BilBOWA, and Binclusion.
The Binclusion method with reduced vocabulary achieves the highest scores in both the "EN→DE" and "DE→EN" translation tasks.
The table provides information about the number of occurrences of different medical conditions in the OpenI and ChestX-ray8 datasets.
ResNet-50 performs better than AlexNet, GoogLeNet, and VGGNet-16 on Atelectasis, Cardiomegaly, Effusion, and Pneumonia.
The performance of CEL and W-CEL is the same on Atelectasis, Cardiomegaly, Effusion, and Pneumonia.
The accuracy for all disease classes decreases as the T(IoBB) threshold increases.
The average false positive number (AFP) for the Mass class is consistently higher than the AFP for other disease classes.
The accuracy of pathology localization decreases as the T(IoU) value increases for all disease classes.
The average false positive number increases as the T(IoU) value increases for all disease classes.
The total number of Atelectasis cases in the ChestX-ray8 dataset is 5,789.
The number of cases for Cardiomegaly is higher in the ChestX-ray14 dataset compared to the ChestX-ray8 dataset.
Table 3 presents the language modeling results on the recipe and CoNLL 2003 datasets.
The bidirectional test score for KALM on the recipe dataset is [BOLD] 1.71.
The "2+2" system performs better than the "Baseline" system in terms of BLEU score for English-German document-level translation.
The "2+1" system performs worse than the "Baseline" system in terms of BLEU score for English-German document-level translation.
Table 4 provides English-German development results comparing different models and ensembles.
The ensemble of 5 save-points and 4 fine-tuned runs achieves a BLEU score of 47.45.
The table shows the BLEU scores for different models and their fine-tuned versions.
The rescore of all L2R models achieves a higher BLEU score than the ensemble of all L2R models.
The table shows the results from individual MarianNMT transformer models, as well as their combinations, for English to Finnish translation.
In the individual runs of the model, the BLEU score for news2017 L2R improved from "Run 1" to "Run 2".
The NMT-HAN model achieves a BLEU score of 35.03 on the sentence-level evaluation.
The selectAttn model achieves a BLEU score of 34.75 on the document-level evaluation.
The entity embedding dimension and auxiliary task weight are the same for emrQA-rel, BoolQ, and emrQA.
The entity embedding dimension for emrQA is 100 and the auxiliary task weight is 0.3.
Table 4 provides precision, recall, and F1-score for logical form prediction on the "emrQA" and "MADE" datasets using the "Exact" and "Relaxed" settings.
The "Relaxed" setting outperforms the "Exact" setting in terms of precision, recall, and F1-score for logical form prediction on both the "emrQA" and "MADE" datasets.
MCA-I-H performs better than other models in the sparse annotation phase in terms of NDCG.
MCA-I-VGH performs better than other models after curriculum fine-tuning in terms of R@10.
When |V|=24, the valid accuracy is 0.801±0.10.
When NL=2, the average ρ71:80 is 0.830±0.02.
The validation set size increases from 0 to 32.
The validation performance increases as the validation set size increases.
The table shows the blind test set performance achieved by different models on WikiHop and HotpotQA datasets.
The BERT-Para (top 5) model achieves the highest performance on both the dev and test sets.
Table 2 describes the blind test set performance achieved by the model on WikiHop and HotpotQA, and it mentions that all published works except DecompRC use annotated supporting facts as extra supervision on HotpotQA.
Roberta-Para (top 5) achieves the highest EM and F1 scores among all the models listed in Table 2.
Table 3 provides information about the downstream QA performance of chains generated by different models on different datasets.
The F1 score for HotpotQA-Hard is lower than the F1 score for HotpotQA.
The f1-score is higher when oversampling (OS) is applied compared to when no oversampling (No OS) is applied.
The precision for non-propaganda class is higher when oversampling (OS) is applied compared to when no oversampling (No OS) is applied.
The diversity metric (DIST-1) decreases as the number of keyphrases increases, except for the "All" row.
The relevance measure (Story-Cloze) increases as the number of keyphrases decreases.
The model "Coverage Loss + ITF" achieves the highest score in the DIST-3 metric.
The model "Seq2Seq" achieves the lowest score in the DIST-1 metric.
The Seq2Seq + ITF model has the highest performance with a score of 55.9.
The Context Concat + ITF model has a higher performance than the Keyphrase Add + ITF model.
Table 2 provides information about the dev set performance and training times for different values of W.
As the value of W increases, the dev PER decreases.
Table 1 shows the performance (measured by %PER) of different methods.
The Self-train method with W=1 achieves the best performance among all the listed methods.
The "Conditional attention" model achieves the highest NDCG score among the single-task models.
The NDCG score improves when transitioning from the single-task models to the multi-task models.
Table 2 compares the prediction performance of CVs using mean squared error (MSE) criteria.
The proposed model with conditional attention has the lowest MSE in the "MSE All" column.
Table 5 compares different GRU models for creative texts and their attribute value interactions.
The NDCG score for the "w2v + attributes" model with the "Vanilla" architecture is 78.03.
Table 6 shows the results of different number of extracted candidates on the test set of Quasar-T.
The table presents ablation results concerning the selection model on the test set of Quasar-T.
The candidates fused representation is the least effective feature when modeling the answer selection procedure.
"Bridge Reasoner + entity linking" achieves the highest Hits@10 score among all the approaches.
"Our Methods" achieve a Hits@10 score of 48.4 on HotpotQA IR.
The "Ours" method has the highest Dev EM and Dev F1 scores among the methods without BERT.
The "CogQA" method has the highest Test EM and Test F1 scores among the methods with BERT.
The Large model has 12 layers, the Medium model has 6 layers, the Small model has 6 layers, and the Tiny model has 2 layers.
The Large and Medium models have a feed-forward hidden dimension of 2048, while the Small and Tiny models have a feed-forward hidden dimension of 1024.
For the ted and wipo domains, no adaptation was performed.
The BLEU development score for the de-en language pair in the ted domain with a large size is 29.25.
Table 10 shows the perplexity of Purvanchal languages and Hindi.
The perplexity values for Maithili are lower with unigram and bigram models compared to trigram models.
The frequency of Bhojpuri [ke] is higher than Maithili [aCi].
The cumulative coverage of Magahi [ke] is the same as the relative frequency of Magahi [ke].
The maximum word length in terms of characters is 61 for Hindi.
The standard deviation of word lengths in terms of characters is 6.83 for Bhojpuri.
The table provides information about different languages.
Maithili has a higher entropy value than Bhojpuri.
The BLEU, CharacTER, chrF3, and BEER scores are consistently higher when BPE is applied after non-final tokens compared to after every token.
The DiSAN method has the highest prediction accuracy for both the Matched and Mismatched datasets.
The Bi-LSTM method has a higher prediction accuracy than the cBoW method for both the Matched and Mismatched datasets.
The "Bi-LSTM with s2t self-attention" model achieves the highest test accuracy of 84.98%.
The "Directional self-attention network (DiSAN)" model achieves a test accuracy of 85.62%.
The "DiSAN" model achieves the highest test accuracy among all the models on the Stanford Sentiment Treebank (SST) dataset.
The "CNN-Tensor" model outperforms the "Bi-LSTM" and "Word Embedding with s2t self-attention" models in terms of test accuracy on the Stanford Sentiment Treebank (SST) dataset.
BERT + RS BM25 + US BM25 (K = 3) performs better than BERT + RS BM25 (K = 3) on all three categories: Easy, Challenge, and Overall.
BERT + RS BM25 + US BM25 (K = 3) has a higher accuracy overall than BERT (no explanation) and BERT + RS BM25 (K = 5) but lower accuracy overall than BERT + RS BM25 (K = 10).
The model "RS + US (Best)" achieves the highest MAP Test and MAP Dev scores among all the models in Table 1.
The approach "BERT re-ranking with inference chains" achieves the highest MAP Test and MAP Dev scores among all the approaches in Table 1.
The table shows the in-topic and cross-topic Recall@k of the ranker used for aspect candidate recommendations.
The ranker performs better in terms of Recall@5 for the in-topic setting compared to the cross-topic setting.
The retrieval approach outperforms the CTRL approach in terms of METEOR and ROUGE-L scores for both the cc and reddit datasets.
The retrieval approach achieves higher METEOR and ROUGE-L scores than the CTRL approach for both the cc and reddit datasets.
Table 6 represents the class correctness of generated arguments.
The F1 score for the "pro" class is higher for the "cc" dataset compared to the "reddit" dataset.
The "Baseline + BERT + Dep (IIR)" method achieves the highest F1 score of [BOLD] 87.54.
The "Sha et al. (2016)" method achieves an F1 score of 77.69.
The table compares the experimental results of four different syntax-aware methods on the CPB1.0 dataset.
The "Baseline + Dep (FIR)" method achieves the highest F1 score on the test set among all the syntax-aware methods compared.
The F1 score for the "Baseline + Dep (IIR)" method is [BOLD] 85.1.
The recall (R) score for the "Baseline + BERT" method is [BOLD] 89.2.
"Hugging Face" is ranked 1st in terms of Hits@1 and F1 scores.
"Hugging Face" has the lowest perplexity score.
The model generates rare words less frequently than humans, as shown in Table 7.
The team "Lost in Conversation" has the highest engagingness score among all the teams, as shown in Table 7.
The team "ADAPT Centre" has the highest percentage of unique responses.
The team "ADAPT Centre" has the highest average number of repeats for bigrams.
The LAS score for the PUD test set is higher when using the training treebank as a proxy compared to using the single or concatenated test sets.
The LAS score for the FicTree treebank is higher than the LAS score for the CLTT treebank.
The accuracy of the model with α=1 is 84.55.
The perplexity of the TTransfo model is 32.12.
"Ours, α=1" achieves the highest accuracy, BLEU score, F1 score, and distance among all models.
TTransfo has the lowest perplexity among all models.
The "Cross-Aligned AE" model achieved a transfer score of 57% in the human evaluation.
The "ARAE, λ(1) b" model achieved a naturalness score of 3.8 in the human evaluation.
Table 1 shows the perplexity of language models trained on different types of data and evaluated on real data or synthetic samples.
The table includes different models for sentiment transfer, including Cross-Aligned AE, AE, ARAE λ(1) a, and ARAE λ(1) b.
ARAE λ(1) b achieves the highest automatic evaluation transfer score among the listed models.
The reconstruction error for both AE and ARAE increases as the number of swaps performed on the original sentence increases.
ARAE consistently has a lower reconstruction error than AE for all values of k.
The table shows the baseline accuracy on the development and test set of MedNLI for different models.
The ESIM model performs better than the BOW and InferSent models in terms of accuracy on the development and test set of MedNLI.
The "fastText[MIMIC-III] (0.8B)" embedding consistently outperforms the baseline "GloVe[CC]" in all three models (BOW, InferSent, ESIM).
The accuracy gain increases as more embeddings are added to the sequence from "GloVe[CC]" to "fastText[BioASQ]" to "fastText[MIMIC-III]".
The method "PALM" has the highest Rouge-L score of 0.498 on the official MARCO leaderboard.
The Rouge-L scores of the methods generally increase from "ConZNet" to "BERT+Multi-Pointer" and then decrease from "Masque" to "PALM".
PALM achieves the highest scores in both Rouge-1 and Rouge-2 metrics.
BERT+LM performs better in Rouge-1 than in Rouge-L.
The "Attention (fr-mb)" system has the highest recall score.
The precision score of the "Segmental DTW Baseline" system is lower than the recall score.
The "Multiling. Bottleneck" feature type has the highest recall values for R@1, R@5, and R@10.
The "Mel-filterbank" feature type has a lower recall value for R@1 compared to the "Multiling. Bottleneck" feature type.
Table 3 provides the purity, homogeneity, and variation of information (VI) scores for the latent discourse roles measured against the human-annotated dialogue acts.
The Topic+Disc model has the highest purity and homogeneity scores among all the models.
Table 2 shows the Cv coherence scores for latent topics produced by different models.
The Topic+Disc model achieves the highest coherence scores among all the models.
Our model achieves the highest accuracy scores for both the TREC and TWT16 datasets.
Our model achieves the highest average F1 scores for both the TREC and TWT16 datasets.
Table 7 shows the linking accuracy of the zero-shot (Z-S) approach on different datasets.
The zero-shot (Z-S) approach achieves higher linking accuracy on the TAC15-Test (es) and TAC15-Test (zh) datasets compared to the Xelms (Z-S w/ prior) approach.
Table 4 provides a summary of relatedness and NED datasets, including their references and number of instances.
The TAC10 dataset has a reference available at http://www.nist.gov/tac/.
The probability of "Gotham_City" is higher than the probability of "Gotham_(magazine)".
The frequency of "Gotham_City" is higher than the frequency of "Gotham_(magazine)".
Table 3 provides the experimental results on semantic relevance detection.
The "Our" model outperforms LR, GBDT, TextCNN, and Transformer in terms of accuracy and F1 score.
Table 2 shows experimental results on phonological relevance detection.
The "Our" model achieves the highest Accuracy and F1 score in phonological relevance detection.
The LSTM model without any attack achieves an accuracy of 88.8% on the IMDB dataset.
The Bi-LSTM model without any attack achieves an accuracy of 92.9% on the AG's News dataset.
The shift in accuracy between SEM and RSE is smaller for the LSTM model compared to the Bi-LSTM and Word-CNN models.
The Word-CNN model has the lowest accuracy after the attack compared to the LSTM and Bi-LSTM models for all three datasets.
The table includes results for both En-De and En-Ru language pairs.
The PredEst-XLM system achieves a higher F1 score on the target language (En-De) compared to the source language (En-Ru).
The table presents the results of document-level submissions for three different models: BERT, MQM (BERT), and MQM (LINBERT).
The method "STR + Schema Label Feat." achieves the highest NDCG scores at all cutoffs.
The NDCG scores at cutoff 15 are higher than the NDCG scores at cutoff 10 for both the "STR" method and the "Schema Label Features" method.
For Task #2, there are more pairs assigned to the "excellent" relevance label than the "poor" relevance label.
For Task #4, there are more pairs assigned to the "good" relevance label than the "off topic" relevance label.
The SDR method with the "T+D" used fields achieves an NDCG@5 score of 0.8920 and a Precision@5 score of 0.4122.
The SLMR method with the "T+D+G" used fields achieves statistically significant improvements over other single and multifield document ranking models, with an NDCG@k score of 0.9293+ and a Precision@k score of 0.5000+.
The performance of the model improves as more fields are used in the training.
The model with "text + data table + generated labels" has the highest performance at NDCG@5.
The "+multi" model outperforms the "baseline" model on all four datasets.
The "+multi" model performs better than the "baseline" model on the PKU dataset.
Table 1 shows the MAE values for 12 regression tasks in QM9 for both Multi-task and Single-task modes.
Table 1 provides the MAE values for different targets in the 12 regression tasks in QM9.
Table 2 compares the performance of multi-task models and single-task models on 12 regression tasks in QM9.
The multi-task pp-GRAT model outperforms the single-task pp-GRAT model in terms of standard mean absolute error (stdMAE).
The table compares the top-1 accuracy obtained by different single-model methods on the current benchmark.
The MT Schwaller et al. (2019) model achieves the highest accuracy among the listed models on the current benchmark.
The systems in the table represent different variations of the baseline system.
The systems in the table represent different variations of the baseline system.
Increasing the alpha value from 0.0 to 0.2 improves the performance on tst2010 for BL, AC, and SUG.
The model fails to converge for all metrics on all datasets when the alpha value is set to 1.0.
The OOV rate for Arabic is 7.3%.
The number of words in the training set for Arabic is 502,938.
The joint diacritic restoration model performs better in terms of Word Error Rate (WER) compared to the base model BASE (Char).
The joint diacritic restoration model with diacritics and syntax performs better in terms of Lexical Error Rate (LER/Lex) compared to the other models.
Table 4 shows the WER performance for different diacritic restoration models when passivation is considered.
Table 8 shows the HNMT official results on the English-Chinese language pair news translation task.
The BLEU score for the English-Chinese translation in the HNMT official results is 23.9.
Table 1 provides information about backtranslated Finnish news data.
The "news2014" dataset contains 1,378,833 sentences in Finnish.
The BLEU score is lower for the "Hybrid OMorFi" row compared to the other rows in the "BLEU None" column.
The BLEU score is higher for the "BPE" rows compared to the "Char" rows in the "BLEU All" column.
The AUC ROC scores for the "age" and "alternateName" relations are the same.
The AUC ROC score for the "parents" relation is 0.74.
Table 7 shows the results comparing models trained without (-w2v) and with word2vec features (+w2v) for different relations.
Models trained with word2vec features (+w2v) generally achieve higher AUC ROC scores compared to models trained without (-w2v) for most relations.
RDN algorithm outperforms RF in terms of AUC ROC score for all relations.
RDN algorithm has higher recall than RF for all relations.
The TensorGCN model performs better than the Merge edges and TensorGCN(intra) models in all four datasets.
The TensorGCN model achieves the highest performance in the R8 dataset.
The model performance is highest when using all three graphs (SeqGraph, SynGraph, SemGraph) for all datasets.
The model performance is consistently higher when using all three graphs (SeqGraph, SynGraph, SemGraph) compared to using a single graph or performing ablation experiments.
Table 6 shows the mean SimScore for Wikipedia using different approaches.
The Two-Step Approach without any restriction for linear structures has a SimScore of 0.304.
Table 1 presents the performance of three different types of classifiers: type-1 SVM, type-2 SVM, and type-2 MLP.
The "+RAML +SwitchOut" method outperforms the "+RAML" method in all three language pairs.
The "+RAML +SwitchOut" method performs better than the "+RAML" method in the "de-en" language pair.
The method "+BT + RAML + SwitchOut" has the highest test BLEU score of 23.76.
The method "+BT + SwitchOut" has a higher test BLEU score of 22.93 compared to the method "+BT" with a test BLEU score of 21.82.
The table provides hyperparameters for three different translation tasks: en-de, de-en, and en-vi.
The "dmodel" parameter has different values for each translation task.
Table 5 compares the average test scores when training on different datasets.
The VETE-BOW model performs the best among all the models on the Pinterest5M dataset.
The BOW-MEAN encoder performs better than the other encoders on the "images2014", "images2015", and "Pin-Test" datasets.
The RNN-GRU encoder performs better than the other encoders on the COCO-Test dataset.
The "Pearson" has the highest average score among all the text encoders.
Table 6 compares the performance of word-level and sentence-level models on different datasets.
The sentence-level model outperforms the word-level model on the COCO-Test dataset.
As the threshold value increases, the SARI, BLEU, GM, and FRE scores decrease.
As the weight value increases, the SARI, BLEU, GM, and FRE scores increase.
The table shows the influence of the multi-perspective matching function on the EM and F1 scores.
As the value of "l" increases, the EM and F1 scores also increase.
Table 2 shows the results on the SQuAD test set.
The MPCM (Ours) model outperforms other models in both the Fine-Grained Gating and Match-LSTM (Boundary) ensembles.
Table 4 shows the results of layer ablation on the dev set.
The F1 score increases as we remove different layers from the model.
The exact measure (EM) score of our proposed method is higher than the F1 score for all labeling rates.
Table 5 shows the nDCG scores of different models.
The SLT model has higher nDCG scores than the SLNT model.
The table compares the performance of different models on the CX-CHR and COV-CTR datasets.
Our model achieves the highest performance on both the CX-CHR and COV-CTR datasets.
Table 2 presents the results of ablation studies for different auxiliary signals on two datasets.
The inclusion of both internal and external auxiliary signals (baseline+IA+EA) leads to the highest scores for CIDER-D, ROUGE-L, BLEU-4, and AUC.
The table shows ablations with varying values of τ on TriviaQA.
The values of τ decrease as we move down the table.
Our training method consistently outperforms the First-Only and MML methods in both the development and test sets for both the NarrativeQA and NaturalQ -open datasets.
The state-of-the-art method achieves a higher F1 score on the TriviaQA dataset compared to the NarrativeQA dataset.
Our method outperforms previous weakly-supervised methods and most of the published fully-supervised methods.
The table represents subsets of the train set on WikiSQL varying in the size of the solution set (|Z|). All subsets contain 10k training examples.
As the size of the solution set (|Z|) increases, the average value also increases.
Table 6 provides information about ablations with varying values of τ on TriviaQA and DROPnum with BERT, as well as the final τ chosen for the main results on each dataset.
The highest performance on TriviaQA is achieved with a τ value of 10k.
Table 6 provides information about ablations with varying values of τ on different datasets.
The batch size for DROPnum with BERT is 14 and the value of τ is 10K.
The "EM" scores decrease as we move down the table.
The "ModFwd1" scores decrease as we move down the table.
The total time decreases as the weight # decreases.
The values in the "ModFwd2" column are higher than the values in the "ModBwd2" column.
The table shows the results for different experiments conducted.
The "All + Creative" experiment achieves higher overall scores compared to the "All" experiment.
The highest score for "Quality of Story" is 3.77 in the "All + C-T" experiment.
The highest score in the "Use Again" column is 1.70 in the "All + Creative" experiment.
TransE(Rule) achieves the highest accuracy among TransE, TransH, and TransR on all three datasets (FB166, FB15K, and WN18).
KALE outperforms TransE, TransH, and TransR in terms of accuracy on all three datasets (FB166, FB15K, and WN18).
Table 4 shows the coefficient of variation of Equation 10: √β in the validation and test sets of PTB.
As the value of λβ increases, the coefficient of variation decreases in the test set.
The learning rate for PTB is 20 and for WikiText-2 is 15.
The dropout rate for [ITALIC] h0 [ITALIC] t is 0.4 for PTB and 0.65 for WikiText-2.
AWD-LSTM-DOC and AWD-LSTM-DOC (fin) have the lowest perplexities in both the Valid and Test sets.
The perplexity values are consistently lower in the Valid set compared to the Test set for all models.
The AWD-LSTM-DOC model performs better in terms of F1 Rerank score compared to the AWD-LSTM model.
Increasing the number of ensembles for the AWD-LSTM-DOC model improves the F1 Rerank score.
MPCN achieves better performance than D-CON (DeepCoNN) on all 24 benchmark datasets.
MPCN achieves better performance than D-ATT (Review-based D-ATT) on all 24 benchmark datasets.
The batch size used in the experiment is 50.
The weights assigned to the entity task for the head and tail entities are both 0.5.
BGRU+EWA achieves a higher PR curve area than BGRU on the original data.
BGRU+EWA achieves a higher PR curve area than BGRU-WLA+EWA on the STP data.
The table shows the P@N values for relation extraction models under different test settings.
The Mintz model has lower P@N values compared to other models.
The average accuracy for the Dirichlet model is higher than the average accuracy for the Logistic Normal model for most languages.
The average accuracy for the Logistic Normal model is higher than the average accuracy for the Dirichlet model for some languages.
The accuracy scores for the Logistic Normal model are higher than the scores for the Dirichlet model.
The accuracy scores for the "n" sound are higher than the scores for the "r" sound.
The "Ours" method generates adversarial examples with higher readability than the JSMA method.
The "Ours" method has a higher human accuracy in generating adversarial examples compared to the JSMA method.
The table compares the performance of different models on multiple datasets.
The TNet-LF model achieves the highest accuracy and F1 score on the Twitter dataset.
The ablation study compares the performance of different models by removing specific components from the ASGCN-DG model.
The KG (transition-based) model has a UAS score of 76 and a LAS score of 93.90.
The MaStackPointer model has a processing speed of 10 sentences per second on CPU.
The UPoS-based encoding performs better than the XPoS-based encoding for Ancient Greek.
The UPoS-based encoding performs better than the XPoS-based encoding for Czech.
Ancient Greek achieves a higher UAS score than Chinese on the UD-CoNLL18 test sets.
Czech achieves a higher LAS score than Finnish on the UD-CoNLL18 test sets.
The SVM-LK classifier performs better than the Ridge and Random Forest classifiers for all combinations of features.
The classification performance is better for the O-ranked records compared to the PRC-ranked records.
SEntNet achieves the highest accuracy on both the dialog bAbI and mDSTC2 datasets.
SEntNet performs better than EntNet on the dialog bAbI dataset.
SEntNet achieves the highest accuracy on the dialog bAbI dataset.
HHCN achieves the highest accuracy on the mDSTC2 dataset.
The table shows human correctness and quality judgments for different combinations of delex, lex, p0, p1, qR, qϕ, g, and b.
The quality judgments for all combinations in the table are equal to 0.
The Slug model and the delexicalized models have 0 errors on the NAME and NEAR attributes.
The Slug model has a customer rating of 67.
The table presents human correctness and quality judgments for delexicalized sentences.
The majority of judgments labeled the quality of delexicalized sentences as equal.
DTLM has the highest average word-level accuracy of 49.0% among the systems in the table.
The average word-level accuracy of all the systems in the table is below 50%.
The table provides word-level accuracy on transliteration for different systems.
The DTLM system has the highest word-level accuracy on transliteration among all the systems.
The system "DTLM" achieves the highest word-level accuracy in all three language pairs.
The system "DTLM" outperforms the system "DTL" in terms of word-level accuracy in the EN-DE language pair.
Table 7 shows the word-level accuracy (in %) on phoneme-to-grapheme conversion with large training sets.
The DTLM model achieves the highest word-level accuracy in both the NetTalk and Brulex datasets.
As the distance increases, the percentage of instances classified as definition-1 decreases.
As the edit distance between terms increases, the number of instances decreases.
As the edit distance between terms increases, the percentage of instances that are considered easier also increases.
The average accuracy of the multi-task models increases as we move from "FS-MTL" to "ASP-MTL*".
The multi-task model "ASP-MTL*" outperforms the single-task models "Single Task BiLSTM" and "Single Task att-BiLSTM" on the "Books" task.
Table 4 provides the average precision of multi-task models with auxiliary tasks.
The average precision for the SSP-MTL model without any additional tasks is 86.2.
Table 2 compares the performance of three different approaches: Raw-caption, Coarse Template, and Fine Template.
The Raw-caption approach has a vocabulary size of 10,979, the Coarse Template approach has a vocabulary size of 3,533, and the Fine Template approach has a vocabulary size of 3,642.
The entity-aware model performs better than the baseline models in terms of BLEU-1, BLEU-2, BLEU-3, BLEU-4, METEOR, ROUGE, CIDEr, and F1 scores.
The BLEU-4 score for the text-summarization model is 6.5.
[ITALIC] C1 has a higher preference than [ITALIC] C∞ in terms of compliance.
[ITALIC] C1 has a higher preference than [ITALIC] C∞+Mem in terms of theme.
The table shows F1 scores of different methods on HANS dataset, targeting different biases: lexical overlap, subsequence overlap, and constituent overlap.
The DRiFt-Hypo method consistently outperforms the MLE method across all biases on the HANS dataset.
The table shows the accuracy of biased classifiers on the SNLI test set and the MNLI development set.
The accuracy of the biased classifier is higher than the majority-class baseline for both SNLI and MNLI.
Table 5 shows the F1 scores of each class on Stress, and DRiFt improves results on class E with some degradation on other classes for DA and ESIM.
The F1 scores for the Negation E class are higher than the F1 scores for the Negation C and Negation N classes for the BERT model.
BERT has the lowest MLE score in the "Negation E" column.
The DRiFt-Hand score for ESIM is 53.9 in the "Overlap N" column.
The number of candidates and the number of answers differ for each relation in Table 1.
Table 2 provides information about four different QA datasets: WebQuestions, TriviaQA, SearchQA, and Quasar-T.
The number of test examples for each dataset is as follows: WebQuestions - 2032, TriviaQA - 10790, SearchQA - 27247, Quasar-T - 3000.
"Our BERT + 1M MLM updates" achieves the highest SQuAD EM score among the models listed in Table 6.
"WKLM" achieves a higher SQuAD F1 score than "Our BERT" without MLM.
SC-GPT achieves the highest Entity F1 score among all the models in the table.
GPT-2 achieves the highest BLEU score among all the models in the table.
RNNsearch-50⋆ has the highest BLEU score on both "All" and "No UNK∘".
The BLEU score on "No UNK∘" is lower than the score on "All" for the Moses model.
Our model performs better than Seq2seq, H&W Hua and Wang (2018), w/o Style, and w/o Passage in terms of BLEU score.
Our model performs better than Seq2seq, H&W Hua and Wang (2018), w/o Style, and w/o Passage in terms of ROUGE score.
Table 2 describes the sentence style distribution for argument and Wikipedia datasets.
29.1% of the arguments have no premise, 62.2% have a premise, and 8.7% have a functional premise.
The ROUGE score is higher for the Normal Wikipedia compared to the Simple Wikipedia.
The BLEU score is higher for the Simple Wikipedia compared to the Normal Wikipedia, and the length is also higher for the Simple Wikipedia.
The table shows the results of human evaluation on argument generation and Wikipedia generation, including ratings for grammaticality, correctness, and content richness.
Our model performs statistically significantly better in terms of grammaticality, correctness, and content richness compared to the variant without style specification.
Table 2 provides an overview of the VerSe dataset divided into motion and non-motion verbs.
The number of images associated with motion verbs in the VerSe dataset is 1812.
The "Greedy decoding" method has the highest score for the "oReact" category in the human validation of gold Atomic.
The "xIntent" score is highest for the "Beam search - 2 beams" decoding method.
As the percentage of training data increases, the perplexity decreases, indicating better performance.
The highest BLEU-2 score is achieved with 10% of the training data.
The table compares the human scores of two different models: ℂ𝕆𝕄𝔼𝕋 and ℂ𝕆𝕄𝔼𝕋 (+ hierarchy meta-tokens).
The table provides the human scores for different categories (oEffect, oReact, oWant, xAttr, xEffect, xIntent, xNeed, xReact, xWant) for both the ℂ𝕆𝕄𝔼𝕋 model and the ℂ𝕆𝕄𝔼𝕋 (+ hierarchy meta-tokens) model.
The model "GRU + Capsnet" has the highest Macro-F1 score among the investigated approaches.
The Macro-F1 score for the model "GRU + Hierarchical Attention" is 0.671.
The GRU (1 layer) + Capsnet model has the highest Macro-F1 score among the different variants of the proposed system.
The LSTM (1 layer) + Capsnet model has a higher Macro-F1 score than the GRU (2 layers) + Capsnet model.
The proposed model achieves the same Precision, Recall, and F1-Score for the micro-average across all emotions in the test set.
The proposed model achieves higher Precision, Recall, and F1-Score for the "Joy" emotion compared to the macro-average values.
Table 2 shows the results of the English→German system using large monolingual data.
The Mix-source big system outperforms the Baseline system in terms of BLEU score in both tst2013 and tst2014.
The "Mix-source (En,De→De,De)" system outperforms the "Baseline (En→De)" system in terms of BLEU score for both tst2013 and tst2014.
The "Mix-multi-source (En,Fr→De,De)" system shows an improvement in BLEU score compared to the "Baseline (En→De)" system for both tst2013 and tst2014.
The DRNN model has higher accuracy than the LSTM model for all values of n.
Increasing the number of non-attractor intervening nouns (n) improves the accuracy of both the DRNN and LSTM models.
Table 1 shows the genre distribution of modern narratives.
"Fine-tuned BERT" achieves the highest micro-F1 score among all the models.
"TF-NRC + SVM" performs better than "TF-IDF + SVM" in terms of micro-F1 score.
The system "Deep Speech" has the lowest WER in all three datasets: Clean, Noisy, and Combined.
The system "Deep Speech" has the lowest WER in the Combined dataset, followed by "Bing Speech".
The error rates for the models are generally higher in the hard subset ("CH") of the Hub5'00 dataset compared to the easy subset ("SWB").
The "Deep Speech SWB + FSH" model performs better than the "Deep Speech SWB" model in all subsets of the Hub5'00 dataset.
Table 5 presents the results of different translation systems on the FR test dataset.
Model H, which is QA-LSTM with attention (avg pooling), has the highest validation and test1 scores.
Model I, which is QA-LSTM/CNN (fcount=4000) with attention, has the highest Test2 score.
Table 2 provides the baseline results of InsuranceQA.
Architecture-II with GESD achieves the highest performance among all the models in the Validation, Test1, and Test2 datasets.
The B2T2 5-Ensemble model achieves a test accuracy of 77.1% in the QA→R task.
The Dual Encoder model achieves a validation accuracy of 45.3% in the QA→R task.
The hyperparameter "Lower the case for input" is set to False.
The maximum sequence length is set to 384, maximum query length is set to 64, and maximum answer length is set to 30.
The table shows the experimental results on 10 probing tasks using different models.
SBERT-WK-base performs better than other models in multiple probing tasks.
The CPU inference time for BERT is 86.89 ms.
The model "MGNER" achieves the highest F1 scores on both ACE-2004 and ACE-2005 datasets.
The model without context achieves a lower F1 score on ACE-2005 compared to the model with context.
The MGNER model outperforms all other models in both the overlapping and non-overlapping categories.
All models show a decrease in performance when dealing with non-overlapping sentences compared to overlapping sentences.
Table 7 provides forward and reverse perplexity (PPL) results for the SNLI and COCO datasets using synthetic sentences of maximum length 15 and 20 respectively.
The forward perplexity (F-PPL) for the SNLI dataset using LATEXT-GAN I is [BOLD] 51.39.
The table compares the performance of different models trained for different epochs and vector dimensionality.
The accuracy of the models trained for one epoch is generally lower than the accuracy of the models trained for three epochs.
The Skip-gram model has the highest accuracy in the Semantic-Syntactic Word Relationship test set.
The RNNLM model has the lowest accuracy in the Semantic-Syntactic Word Relationship test set.
The "Skip-gram + RNNLMs" model has the highest accuracy of 58.9%.
Table 4 provides the per-genre WER (%) for different adaptation methods.
GSPO has the highest recall value among all the categories.
GJOB has the highest precision value among all the categories.
The category "GSPO" has the highest recall value.
The category "GDIP" has the highest precision value.
The "GSPO" category has the highest recall value.
The "GDIP" category has the lowest f-measure value.
The table presents an ablation study on the development set, evaluating the performance of different models by removing specific modules.
The method "DREAM" has the highest accuracy and score on the blind test set on FEVER.
The label for the method "DREAM" is "our approach".
The table includes the results of XLNet and RoBERTa models for evidence selection.
XLNet has a slightly lower accuracy score on the development set compared to RoBERTa.
Table 6 shows the results of different numbers of orthogonal columns for SF.
The highest BLEU score achieved in the different numbers of orthogonal columns for SF is 36.2.
The dropout rate of 0.2 achieves the highest score for the IWSLT setup.
The dropout rate of 0.1 achieves the highest score for the WMT setup.
Table 5 shows the translation accuracy of VNMT models with different numbers of flows in the IWSLT condition.
The MTL3 model outperforms the other models in terms of macro F score for both PHEME 5 events and PHEME 9 events.
The branchLSTM model has the highest accuracy score among all models for PHEME 5 events.
Table 4 provides per event and per-class results for a multi-task learning approach with 3 tasks on PHEME 5 events.
The macro F1 scores for the events vary across the different tasks in the multi-task learning approach with 3 tasks on PHEME 5 events.
The event "prince-toronto" has the highest kurtosis value among all events in the dataset.
The events "ebola-essien" and "prince-toronto" have the same entropy value.
The model with the description "LSTM-OP + 3-layer ReLU" has the lowest CER(%) among all the models listed in the table.
The "2-layer Conv + 2-layer ReLU + LSTM-OP" model has a lower CER(%) value compared to the "3-layer ReLU + LSTM-OP" model.
Table 4 shows the LSTM accuracy for distinct grade bins on the test set for X=40.
The table shows the Pearson correlation (r) of scores for systems produced in two separate data collection runs on Mechanical Turk.
Table 3 shows the raw scores and z scores of systems participating in the TRECVid 2016 VTT task.
Human-b is one of the systems participating in the TRECVid 2016 VTT task, with a raw score of 88.2, a z score of -0.895, and 940 captions produced.
Table 5 shows the correlation of BLEU*, METEOR*, and STS scores for submissions participating in TRECVid 2016 VTT task with human assessment.
The number of submissions participating in TRECVid 2016 VTT task with human assessment was 5.
The proportion of "Yes" responses is higher than the proportion of "No" responses.
The number of "Yes" responses is higher than the number of "No" responses.
The "Paper's authors" category contributed the highest proportion of annotations in the dataset.
The total count of annotations in the dataset is 74.
There are more annotators who specified "Yes" than annotators who specified "No".
The proportion of annotators who specified "Yes" is higher than the proportion of annotators who specified "No".
The proportion of "No information" is higher than the proportion of "Yes for all items".
The count of "Yes for all items" is higher than the count of "Yes for some items".
The F1 score for NCS is consistently higher than the F1 score for CFS in the perturbation combinations study results.
The Precision score for ID 4 is higher than the Precision score for ID 0 in the perturbation combinations study results.
The CB-BBA model achieves the highest precision at 10 (P@10) score among all the models.
The CB-BBA model achieves the highest normalized Discounted Cumulative Gain (nDCG) score among all the models.
The recall values for all labels are higher in Phase I-II compared to Phase-I.
The F1 score for the label "/organization" is higher in Phase I-II compared to Phase-I.
FGET-RR Phase I-II (Contextualized Embeddings) outperforms all other models in terms of Wiki strict performance.
AFET, Attentive, FNET-AllC, FNET-NoM, FNET, NFGEC+LME, and FGET-RR Phase I-II (Contextualized Embeddings) perform better than FIGER and HYENA in terms of OntoNotes mic-F1.
Phase I-II (FGET-RR + ATTN) outperforms the other models in terms of strict, Wiki mac-F1, mic-F1, OntoNotes mac-F1, and mic-F1.
Phase I-II (FGET-RR + PIVOTS) achieves the highest performance for the BBN dataset in terms of strict, mac-F1, and mic-F1.
The mode for all statements in Session 1 is either 3 or 4.
The p-value for all statements in Session 1 is less than 0.05.
There are four different models: seq2seq, seq2BF−, seq2BF+ keyword, and seq2BF+ remaining.
The entropy values for seq2BF+ keyword and seq2BF+ remaining are higher than the entropy values for seq2seq and seq2BF−.
The highest recall at 1 is achieved by the R-LDA-Conv model.
The highest recall at 2 is achieved by the R-LDA-Conv model.
Table 5 shows the zero-shot accuracy on the XNLI test set with more languages using 20K parallel sentences for each language paired with English.
Aligned BERT with 20K parallel sentences achieves the highest accuracy for each language compared to Base BERT.
Table 1 compares the accuracy of different methods on the XNLI test set for multiple languages.
After fine-tuning-based alignment, the accuracy of Word-aligned BERT is higher for Bulgarian and Greek compared to the Translate-Train baseline.
The table shows the zero-shot accuracy on the XNLI test set, where BERT is aligned with varying amounts of parallel data.
The highest accuracy for the Bulgarian language is achieved with 50K sentences.
"Word-aligned BERT (fine-tuned)" performs better than "Base BERT" in terms of word retrieval accuracy for all languages.
"Word-aligned BERT (fine-tuned)" performs better than "Base BERT" and "Aligned fastText + sentence" in terms of word retrieval accuracy in a version of the task where context is not necessary.
Aligned BERT performs better than Base BERT in terms of lexical overlap accuracy.
Aligned BERT performs better than Base BERT in terms of closed-class accuracy.
The QQP → PAWSQQP combination has the highest percentage of paraphrase pairs with higher paraphrase scores than identical pairs.
The QQP → PAWSQQP combination has the highest percentage of non-paraphrase pairs with higher paraphrase scores than identical pairs.
The table shows the percentage (%) of sentence pairs with asymmetrical prediction results for different datasets and models.
The BOW model predicts all identical pairs as non-paraphrases for the MRPC dataset.
The table provides information about the model parameters and results on the character identification task for four different models: SemEv-1st, SemEv-2nd, biLSTM, EntLib, and EntNet.
The table includes the F1 scores for the main task for each of the four models: SemEv-1st, SemEv-2nd, biLSTM, EntLib, and EntNet.
The biLSTM model type has a higher correlation with the main entities compared to all entities.
There are more pairs available for all entities compared to the main entities.
Table 1 provides cross-lingual performance averaged on 5 seeds on the Narabizi test set. Baselines are described in Section 3.
The mBERT+Task POS and mBERT+Task UAS models perform better on Maltese compared to the other models.
Table 6 presents the results of human evaluation for different models.
The BiSET(best template) model achieved the highest scores in both the "Concise" and "Read" evaluations.
The ROUGE-1 scores increase as the type of template becomes more optimal.
The ROUGE-L scores increase as the type of template becomes more optimal.
The "Bi-selective layer" approach achieves the highest scores for all three ROUGE metrics.
The "Concate+multi self-att" approach outperforms the "Concatenation" approach in terms of ROUGE-2 score.
BERT-DK +ARW w/o Contra(⋅) has fewer incorrect examples from contrastive sentences compared to BERT-DK for both the Laptop and Restaurant datasets.
The Restaurant dataset has a higher number of total examples compared to the Laptop dataset.
There are more aspects in the Restaurant category compared to the Laptop category.
The percentage of contrastive sentences is higher in the Restaurant category compared to the Laptop category.
Table 3 provides a summary of the Contrastive Test Set.
There are 78 contrastive sentences for the "Laptop" aspect and 80 contrastive sentences for the "Restaurant" aspect in the Contrastive Test Set.
The ARW scheme improves the performance of the BERT-DK model on the Full Test Set compared to the ARW scheme with manual initial weighting.
The Rest. Acc. metric performs better than the Laptop Acc. metric for the RAMChen et al. (2017) model on the Contrastive Test Set.
The method "Ours (Faster R-CNN, ResNet)" does not have any retrieval results reported in the table.
The method "SCAN + GOT" achieves the highest Recall@1 score in the Sentence Retrieval task.
The method "Ours (Faster R-CNN, ResNet)" does not have any retrieval results reported in the table.
The method "SCAN + GOT" achieves the highest Recall@1 score in the Sentence Retrieval task.
The "+LSTM+WS" method achieves the highest precision at rank 1 among all the methods listed in Table 1.
The "+DeepMatch [ITALIC] tree" method achieves a higher precision at rank 1 than the "+Translation" method.
The "SMN+WS" model achieves the highest Mean Average Precision (MAP) on the Douban Conversation Corpus.
The "SMN+WS" model achieves the highest Precision at 1 (P@1) on the Douban Conversation Corpus.
LXMERT achieves the highest accuracy for all VQA question types.
LXMERT achieves an accuracy of 60.3 for the GQA Open question type.
Method 4 (P10+QA10 + FT) achieves the highest accuracy scores for all three tasks (VQA, GQA, NLVR2).
Method 4 (P10+QA10 + FT) performs better than Method 2 (P20 + FT) on the GQA task.
Table 1 presents accuracy scores for different processing approaches for the newly proposed multimodal features.
The highest value in the "PARENT-T" column is 56.10.
The value in the "params" column is 45.94M for all rows.
The model "(C (3 × 3) / 2) × 2 + ResCNN × 8 + NiN" has the lowest Word Error Rate (WER) among all the models in Table 2.
The model "NiN (from Section 3.2)" has a Word Error Rate (WER) of 12.88.
The model "(L + P / 2 + B + R + C(1×1) + BN + R) × 2 + L" has the lowest WER among the three models.
The model "L × 8" did not converge and diverged during training.
The "seq2seq + deep convolutional (our work)" model has the lowest WER among all the models listed in Table 4.
The "CTC (Graves et al., 2014)" model has the highest WER among all the models listed in Table 4.
The table provides the accuracy and difference values for different message length bins for each classifier.
IBM Watson achieves an accuracy of 98.0% for tweets with a message length greater than 15.
The R2 Training (SE) value for ALL 90% and ELVIS 90% is 0.42.
The R2 Test (SE) value for TOOT is higher than the R2 Test (SE) value for ANNIE.
Our model outperforms both the Bilingual Enc-Dec and Multi-lingual Enc-Dec models in terms of translation performance on both the "test (fr-en)" and "test (de-en)" datasets.
Removing the discriminator or using separate encoders does not significantly affect the translation performance of the model on either the "test (fr-en)" or "test (de-en)" datasets.
"Ours" has a higher accuracy of classifying label y compared to "VFAE (Louizos et al., 2016)".
"Logistic regression" has the highest accuracy of classifying factor s among all the methods.
Table 1 provides information about three different types of argument mining tasks: Claim Sentence, EXPERT Evidence, and STUDY Evidence.
Table 1 provides information about two different argument mining tasks: TE and WikiQA.
The "TE" task in argument mining is a multiclass problem, while the "WikiQA" task is a binary problem.
Table 5 represents a confusion matrix for the best combined model.
The "NomAnaph" category has a count of 395 in the confusion matrix.
BERT achieves the highest "Micro F1" score on the development set among all the models.
BERT + Logistic Regression* achieves the highest "Macro F1" score on the validation split among all the models.
The F1 score for span-level metrics using BERT is 0.815.
The accuracy for token-level metrics using RoBERTa* is 0.952.
Ethics has a higher performance rate than Constitutional Law.
Consumer's Law has a higher performance rate than Civil Procedures.
Table 4 provides information about the model sizes of different models, including TD-LSTM, MemNet (3), IAN, AOA-LSTM, TD-GAT-GloVe (3), TD-GAT-GloVe (4), TD-GAT-GloVe (5), BERT-CLS, TD-GAT-BERT (3), TD-GAT-BERT (4), and TD-GAT-BERT (5).
In Table 4, the model sizes of MemNet (3) and TD-GAT-BERT (3) are marked as [BOLD].
BERT-CLS performs better on the restaurant dataset compared to the laptop dataset.
TD-GAT-BERT (4) performs better on the laptop dataset compared to the restaurant dataset.
Table 3 presents the results of an ablation study that examines the impact of explicit target information.
The GAT-GloVe model achieves a higher performance on the Laptop 4 layer compared to the Laptop 3 layer.
The table compares different feature-based methods and shows their precision at different recall levels.
The table provides precision values for recall levels of 10%, 20%, and 30% for each feature-based method.
Table 2 provides information about the performance improvements observed with different numbers of ceRNN nodes.
The ceRNN (512) + LSTM model achieves a Word Error Rate (WER) of 11.5 on clean data and 18.3 on noisy data.
The "Partial" method achieves higher F1 scores than the "Concat" and "Propagate" methods across all types and splits.
The F1 scores generally increase as the number of types increases in the "Standard♠" models.
The term "schrodinger-operator" has the highest tf-idf value among all the terms.
The term "quark-mass" has a tf-idf value of 2.07 in the "hep-th" section.
The number of unique words appearing at least N times in hep-th titles decreases as N increases.
The 16,105 words with at least two appearances in hep-th titles were used as a training set for Word2vec.
The word model "black-hole" has the most similar theory "rotating" with a value of 0.9277.
The word model "state" has the least similar entropy with a value of -0.1240.
The parent model "French-English" has the highest Bleu score.
The parent model "French-English" is chosen because French and Spanish are similar.
The languages listed in the table are Hausa, Turkish, Uzbek, and Urdu.
The Bleu scores for NMT and SBMT are different for each language.
Table 2 shows the results of our method for improving NMT results for the translation of low-resource languages into English.
The Bleu scores in the "Final" column of Table 2 improve after ensembling 8 models and using unknown word replacement.
The table shows the results of applying our transfer method to re-scoring output n-best lists from the SBMT system, as well as the results when using an RNN LM to re-score the n-best list.
The transfer method performs the best on the Hausa language compared to the other settings.
The table shows the Bleu and perplexity scores of the Uzbek-English model as different parameters are retrained.
As more parameters are retrained in the Uzbek-English model, the perplexity score decreases.
The table presents the results of different transfer settings using parent models trained only on English data.
The "French-English transfer" setting achieves the highest Bleu score among all the transfer settings mentioned in the table.
The Combined method with U+SE has a MAP of [BOLD] 0.62 at 2% error injection.
The Neural method with USE (U) has a Recall@k=10% of [BOLD] 0.38.
The SVM accuracy is highest when training and testing on the same dataset.
The coverage is highest when using a random test set.
The "unique" approach consistently produces data with higher diversity scores compared to the "same" and "random" approaches in all three data collection rounds.
The number of samples collected increases with each data collection round for all three approaches (same, random, unique).
Training on the unique data produces a more robust model, with consistently high performance across test sets.
Training on the unique data leads to higher coverage scores compared to training on the same or random data.
Different pretraining methods are used for spoken word classification.
The accuracy scores for different pretraining methods on the Speech Commands dataset are shown in the table.
The table provides results for spoken word classification on the SPC and LRW datasets.
The accuracy of spoken word classification is highest when there is no introduced noise.
The SPC dataset has two variations: SPC-100% and SPC-10%.
The training split size of the LRW-100% dataset is larger than the training split size of the LRW-10% dataset.
The split duration for the validation and test sets of SPC-100% dataset is 1.90 hours.
The split duration for the train, validation, and test sets of LRW-10% dataset is shorter than the split duration of LRW-100% dataset.
The model HDM outperforms the other models in terms of F1 score, Acc@3, Acc@4, and Acc@≥5 scores.
The model HDM has the highest accuracy score among all models for Acc@≥5.
Table 4 shows the end-to-end performance of all methods on the test dataset.
The F1 score for HDM is [BOLD] 77.43.
There are 5 different models labeled as 1, 2, 3, 4, and ≥5.
The highest accuracy achieved is 100.0.
The F1 score for the GD1 model is 71.03.
The recall (R) score for the Human model is 84.42.
The model "DepthConv dynamically selected kernel (k=3,15)" achieves the highest BLEU score of 36.3.
The model "MUSE" achieves a BLEU score of 36.3.
MUSE has a higher inference speed than Transformer.
MUSE offers a 31% acceleration compared to Transformer.
The PMF method performs worse on the "Musical Instruments" dataset compared to the "Amazon Instant Video" and "Digital Music" datasets.
The ConQAR method achieves the lowest MAE score on the "Musical Instruments" dataset compared to the other datasets.
Table 10 provides the model coefficients, standard errors, and p-values.
The coefficient for the "Scene variation" variable is 4.29.
Table 2 shows the F1-scores (%) for English all-words fine-grained WSD with the utilization of manually labeled training data.
MLab+DisDict+ULab+MFS has the highest F1-score for verbs in the concatenated test sets.
Table 4 shows the F1-scores for noun disambiguation.
The F1-score for the Train-O-Matic method on the SE13 test dataset is 65.8.
The table compares the semantic and syntactic accuracy of four different word embedding models: CBoW, Skip gram, GloVe, and SdfastText.
Increasing the window size (ws) improves the semantic and syntactic accuracy of the word embedding models.
Table 5 lists the optimized parameters for CBoW, SG, and GloVe models.
The optimized parameter "minn char" is set to "02" for the CBoW model and is not applicable for the SG model.
The RF classifier performs better than the SVM classifier in terms of AUC for all the datasets.
The data improvements process results in higher AUC values for all the categories.
"Skip-Gram" with a length of 300 and a corpus size of 300 billion achieves the highest correlation score on the WS-353 task.
"Ling Sparse" with a length of 172,418 achieves the highest accuracy score on the SimLex task.
The accuracy of WordCNN is lower than the accuracy of WordLSTM and BERT across all datasets.
The accuracy of BERT is higher than the accuracy of WordCNN and WordLSTM on the SNLI dataset.
The after-attack accuracy for MR is lower than the after-attack accuracy for AG and SNLI.
The after-attack accuracy (random) for AG is higher than the after-attack accuracy (random) for MR and SNLI.
The accuracy after the attack is lower for SNLI compared to MR, IMDB, and MNLI(m).
The semantic similarity is higher for IMDB compared to MR, SNLI, and MNLI(m).
WordCNN performs better than WordLSTM on the IMDB dataset.
BERT performs better on the SNLI dataset than on the IMDB dataset.
The experiment with the highest Word Error Rate (WER) is "TE-MONO".
The WER values in the "Exp3" column are highlighted in bold.
System (3) + Neural LM performs better than system (2) + Expanded lexicon in terms of WER and NE-WER.
System (6) + Embedding matrix augmentation performs better than system (5) + Embedding matrix augmentation in terms of WER on SG-streets.
The "hybrid ASR (ours)" system has the same LM rescoring value as the "hybrid ASR [maduo2019da]" system.
The "E2E ASR [zzp2019]" system has a higher testman value than the "hybrid ASR [guo2018study]" system.
The model performs better when using 1H regularization compared to RND and EMB regularization methods.
The model performs better when using 1H regularization compared to RND and EMB regularization methods in the Explicit task as well.
The R2 score is higher for triplets with generalization compared to triplets without generalization.
The F1 score is higher for words with generalization compared to words without generalization.
The LARGE VANILLA model has the highest JS score among all the models.
The SMALL HALF model has the lowest SC score among all the models.
The triangle+Ltrans model outperforms the triangle model in ASR.
The cascade model achieves a Mboshi CER of 39.7.
The F-score for the "reverse" configuration in the "our single-task" model is higher than the F-score for the "base" configuration.
The F-score for the "reconstruction+0.5Linv" configuration in the "reconstruction+0.5Linv" model is higher than the F-score for the "reconstruction+0.2Linv" configuration.
The CPM method does not have any performance values in the Weibo dataset columns.
The F-Score Driven method does not have any performance values in the MSRA News dataset columns.
The models RNN, SCRN, MGU, MI-RNN, GRU, LSTM, and Delta-RNN are tested on the IMDB-SW dataset.
The models RNN, SCRN, MGU, MI-RNN, GRU, LSTM, and Delta-RNN have different performance values.
The models "RNN", "SCRN", "MGU", "MI-RNN", "GRU", "LSTM", and "Delta-RNN" have different performance values.
The models "RNN", "SCRN", "MGU", "MI-RNN", "GRU", "LSTM", and "Delta-RNN" have different values for the PTB-SW task.
The average synaptic operations ratio for MFCC (40-dim.) is higher than that for FBANK (40-dim.) and FMLLR.
The number of frames decreases as the Utterance Index increases.
Table 1 presents the PER (%) on the TIMIT development and test sets for various ANN and SNN architectures from the literature, as well as the results achieved by the ANN and SNN models in this work.
The ANN model achieves a lower PER (%) than the SNN model for the FBANK (13-dim.) feature.
The F1 score for predicting the "is_followed_by" relation is higher than the F1 score for predicting the "is_parent_of" relation.
The mAP score for the WS+FT model is higher than the mAP score for the baseline model at IoU=0.5.
The precision for the baseline model is higher than the precision for the WS model at IoU=0.8.
The F1 score for the BiLSTM encoder-decoder model in the 50-class classification task is 0.67.
The F1 score for the HAN model in the 13-class classification task is 0.89.
Some classes in the "13 nest" classification task have multiple included members, while others have only one included member.
The class "proof" has the highest frequency among all the classes in the "13 nest" classification task.
The "No Seg +BERT+BAN [ITALIC] last" approach achieves the highest F1 score.
The "No Seg +BERT+BAN [ITALIC] ave" approach has a higher F1 score than the "Gold Seg" approach.
The "CharLM+BiLSTM+CRF" approach achieves the highest F1 score for the German language.
The F1 scores for the "Spanish" language are higher than the F1 scores for the "German" language for most approaches.
The approach "BERT+BiLSTM+CRF" achieves the highest overall F1 score among all the approaches.
The approach "+BAN [ITALIC] last" outperforms the approach "Char+BiLSTM+CRF" in terms of precision, recall, and F1 score for named entities (NE).
The different embeddings used in the approach are Random Embedding, FastText+BiLSTM+CRF, BAN Embedding [ITALIC] first+BiLSTM+CRF, BAN Embedding [ITALIC] last+BiLSTM+CRF, and BAN Embedding [ITALIC] ave+BiLSTM+CRF.
The F1 scores for the different embeddings are 16.33, 32.18, 32.12, 35.84, and [BOLD] 36.24.
Table 1 displays the average response times in seconds for different topics and stances.
The table represents the ablation study of the search objective for Model HC_title_10 on the headline generation test set.
The Rouge F1 scores for R-1, R-2, and R-L in the full model (f←→LM⋅ fSIM) are 27.52, 10.27, and 24.91 respectively.
Each abbreviation in the "Abbr." column corresponds to a specific code in the "CTRS Code" column.
Each abbreviation in the "Abbr." column has a corresponding score range in the "low/high" column.
The positive GI terms in EmoLex-GI are mostly marked as positively evaluative and the negative terms are mostly marked as negatively evaluative.
The highest score for joy terms in EmoLex-WAL is shown bold.
The intensity of emotions generally decreases as we move from "Intensity no" to "Intensity strong".
The percentage of terms with any emotion is highest in the "Intensity strong" category.
The majority class size associated with each emotion in the pilot set is different.
The majority class size associated is higher than the majority class size evokes in the pilot set.
BERT-base performs better on SWDA than on SNIPS.
The table shows the test set performances of different models.
The LSNM model performs better than the TE and BoW models in terms of precision, recall, and F-score.
The BLSTM (our proposed model) has the highest F1 score for both SE2 and SE3.
htsa3 has a higher F1 score than BLSTM without dropword.
The table compares the NMT BLEU scores for different selection sizes and LM types.
Using LSTM LM type results in higher NMT BLEU scores compared to n-gram LM type.
The method "Gradual FT" achieves the highest scores in all four datasets.
The relevance-based methods consistently outperform the random methods in all four datasets.
The perplexity values decrease as more training data is added and the perplexity on the test set is lower than the perplexity on the validation set for all cases.
The lowest perplexity values are achieved when training on the PAISA' dataset first, then the DP dataset, and finally the DC dataset.
Judge 2 rates the readability higher than Judge 1.
The table shows the mean PageRank and relative increase in MRR for different datasets with respect to the DistMult model.
The relative increase values in the table indicate the change in MRR compared to the DistMult model.
The "Discourse-hierarchy learning" method has the lowest test error rate among all the methods listed in Table 7.
The test error rate for the "Discourse model + RNNLM" method is higher than the test error rate for the "Discourse-hierarchy learning" method.
Table 2 provides information about the size of the compositional model at each level.
The small compositional model has 2 layers and each layer has 128 memory.
Table 2 provides information about the size of the compositional model at each level.
The small compositional model has 2 layers and each layer has 128 memory.
TLT-TS and CATS are supervised text segmentation models.
The performance of the GraphSeg model on the Wiki-727K dataset is not available.
CATS has the lowest performance scores in all three datasets (CS, FI, TR).
CATS has a higher performance score compared to GraphSeg and TLT-TS in the TR dataset.
The "sif" model has the highest clustering purity measure for the Newsgroup dataset with k=3 and C=3.
The "doc2vec" model has the highest clustering purity measure for the Newsgroup dataset with k=8 and C=8.
The table provides Pearson ρ values for STS Benchmark and SICK relatedness, as well as Accuracy%/F1 values for MSR Paraphrase detection.
The Pearson ρ value for GloVe sif in the STS Benchmark is 0.685.
The accuracy obtained through majority polling using our resource is higher than using Bigram before segmentation.
The number of unclassified reviews is higher when using SentiWordNet after segmentation compared to before segmentation.
The table provides information about two datasets: OAG and Gowalla.
The clustering coefficient of OAG is lower than the clustering coefficient of Gowalla.
The table shows the F1-scores of attribute classifiers for three different settings: Speechdata, Blog-age, and Blog-gender.
The attribute classifiers perform better on the validation set sentences for Blog-age and Blog-gender compared to Speechdata.
The A4NT FBsem model has lower performance than other models in terms of both seen classifier and holdout classifiers mean.
The A4NT CycML model achieves the highest performance among all models in terms of the maximum holdout classifiers score.
Table 1 shows the ASR performance with and without front-end (FE) for different systems.
The Google FE system has a lower Word Error Rate (WER) for laughter compared to the IBM FE system.
The unsupervised baselines with additional modifications have higher Bleu scores compared to the unsupervised baseline "copy input".
The supervised baseline "vanilla seq2seq" has a higher Rouge-L score compared to the supervised baseline "E2E challenge Dušek and Jurcıcek (2016)".
The table shows the perplexity (PPL) scores of different models with Brown word clusters, POS tags, and LID.
The model "BrC + POS" has the lowest perplexity score on the evaluation set.
The model with both POS tags and LID has the lowest perplexity on the dev set.
The model with both POS tags and LID has the lowest perplexity on the evaluation set.
Table 2 shows the perplexity values of different models on open vocabulary language modeling.
The perplexity values of the models decrease from "Dev Vanilla LM" to "Test NKLM" to "Test LRLM" for all base models.
Table 6 shows the perplexity values of different models on the WikiFacts dataset.
The LRLM model performs worse than the NKLM model on the test set.
The "MD" tag has the highest mean value among all the tags.
The maximum value for the "DL" tag is 6.33.
The total number of subjects for each trait is the sum of the corresponding values in the "Gender Male" and "Gender Female" columns.
The total number of subjects for each trait is the sum of the corresponding values in the "Classes High" and "Classes Low" columns.
Table 8 shows the number of subjects per trait in the Essays corpus.
There are 1271 subjects with a "yes" label and 1196 subjects with a "no" label for the trait "Openness" in the Essays corpus.
The table shows the average running time per epoch on the TMC dataset under different values of v and k.
As the value of v increases, the average running time per epoch on the TMC dataset also increases.
The table shows retrieval precisions under different values of v with k fixed to be 10 on the Reuters dataset.
Table 1 shows the best β and γ values for an adversarial shared private model on different sets of tasks.
The best β and γ values for an adversarial shared private model on the SNLI and Multi-NLI tasks are 0.005 and 0.001, respectively.
The "NOHIER" system has a higher accuracy on the test set compared to the "RNNLM" system.
The "VAE" system has a higher accuracy compared to the "RNNLM+Role" system on the validation set.
The table shows the results of automatic evaluation for different models used in the experiment.
The GPMM model has a lower recall for Item Keywords compared to the Original model.
The proposed model performs better than the original model in terms of attractiveness.
The proposed model performs better than the original model in terms of informativeness.
The model with the highest BLEU score is the Proposal model.
The model with the highest recall for the combination of k and q is the Proposal model.
The "Proposal" model achieves the highest recall(k+q) value among all the models.
The optimal hyperparameters for the LSTM, BiLSTM, and C-BiLSTM models are the same for "Embd. Size", "MaxLen", "Batch Size", "LSTM cells", "Optimizer", and "Learning rate".
The C-BiLSTM model has a filter size of 128, while the LSTM and BiLSTM models do not have a specified filter size.
The C-BiLSTM model outperforms other models in detecting Help queries based on the F1 score.
The XGBoost model has a lower precision than the SVM, CNN, LSTM, and BiLSTM models.
As the similarity threshold decreases, the precision increases while the recall decreases.
The highest F1 score is achieved at a similarity threshold of 0.75.
The SQuAD-es dataset contains 87,595 examples out of a total of 87,599 examples from the original SQuAD v1.1 dataset.
The average context length for the SQuAD-es dataset is 140 tokens.
The percentage of Type I errors is 7% of the total errors.
The number of Type II errors is 68 out of 227 instances.
The models used in the experiment are LSTM, HRED, VHRED, LSH-RkNN, LSH-RMM, and LSH-RLSTM.
The Extrema score for LSH-RkNN is 0.34±0.15.
The model "LSH-RLSTM" has the highest average score compared to other models.
The models "LSH-RkNN" and "LSH-RMM" have higher scores in the "Extrema" evaluation compared to other models.
LSH-RkNN and LSH-RMM have the highest average scores among all models.
VHRED has the highest score among all models.
Table 3 shows the human evaluation scores on the Depression dataset for different models.
The LSH-RkNN model has a diverse score of 80% on the Depression dataset.
Table 3 shows the impact of candidate pool size on the performance of the model.
The maximum F1 score for the short answer on the training set is 0.5857.
The F1 score for the long answer on the test set is 0.715.
Table 2 shows the performance on FriendsPersona with 3 different formats for different traits.
RoBERTa has the highest performance score among all the models for the FriendsPersona dataset.
The RoBERTa model outperforms other models in terms of accuracy for all personality traits (AGR, CON, EXT, OPN, NEU).
The LIWC (2016) model performs the best in terms of accuracy for the AGR personality trait.
The table shows the results (% WER) on the AMI dataset with various filter initializations.
The Mel initialization achieved a WER of 30.6 on the evaluation set.
Table 5 shows the results (% WER) of different adaptation methods from AMI to individual speakers in PF-STAR for 8 epochs.
The adaptation method "LHUC1" has 800 parameters.
Table 9 shows the BLEU scores achieved with only Coursera parallel data extracted by different similarity measures.
The MT+CS method using similarity measure B16 achieves the highest BLEU scores for both Japanese to English and English to Japanese translations.
The ASPEC dataset has 1.0M sentence pairs in the training set, 1,790 sentence pairs in the development set, and 1,812 sentence pairs in the test set.
The TED dataset has 223k sentence pairs in the training set, 1,354 sentence pairs in the development set, and 1,194 sentence pairs in the test set.
The BLEU score for corpus A is 4.1.
The BLEU score for corpus A after translation to AT is 15.0.
Table 7 shows the BLEU scores for different test sets.
The BLEU score for the Ja→En translation is higher than the BLEU score for the En→Ja translation for all test sets.
The BLEU scores decrease from Iteration 1 to Iteration 2 in the A→AT→ATC translation direction.
The En→Ja translation direction has higher BLEU scores than the Ja→En translation direction in both Iteration 1 and Iteration 2.
Increasing the vocabulary size from 32k to 200k improves the F1 scores for all languages in the "Joint Multi" setting.
Adding adapters improves the F1 scores for all languages in the "Mono Trans" setting.
MLDoc en has a higher accuracy than MLDoc fr, MLDoc es, MLDoc de, MLDoc ru, and MLDoc zh.
MLDoc de has a higher accuracy than MLDoc en, MLDoc fr, MLDoc es, MLDoc ru, and MLDoc zh.
Table 4 presents the semantic and syntactic probing results of a monolingual model and monolingual models transferred to English.
The monolingual models transferred to English perform better than the monolingual model in most languages in the "Reflexive anaphora" task.
The "Simple" category has the highest accuracy for subject-verb agreement across all languages.
The "In a sentential complement" category has the lowest accuracy for reflexive anaphora across all languages.
The table provides MOS Naturalness scores for three models: GT, TC2, and GS-TC2.
The MOS Naturalness score for the GT model is higher than the scores for the TC2 and GS-TC2 models.
The "Pitch cosine" metric performs better in terms of prosody transfer in the "GS-TC2" condition compared to the "TC2" condition.
The "RMS DTW" metric shows consistent performance in terms of prosody transfer between the "TC2" and "GS-TC2" conditions.
The F1 scores for the "Sentiment (3)" task are similar across all models.
The "gaze" model performs better than the "baseline" model on the "NER" task.
The F1-score for the "gaze + EEG" type aggregation experiment is the highest among all the experiments.
The "corpus" dataset has the highest percentage of unknown tokens.
The accuracy for NER is 87.34.
The accuracy for the FFD freq aux task is [BOLD] 91.87.
The VQS method outperforms other methods in all categories.
The performance of all methods is similar, with only slight differences in scores.
Table 4 compares the results of different segmentation mask resolutions for supervised attention in VQA.
The highest values in each row are marked in bold in the "All" column.
Table 5 shows the comparison results of different language embeddings for VQS.
DeconvNet (L) has the highest performance among the three language embeddings.
The new obtained low dimensional embeddings and the concatenation of the original raw vector with the new transformed one perform better than the raw embeddings on the FastText type of embedding.
The concatenated embeddings perform worse than the raw embeddings on the GloVe Wiki type of embedding.
The FastText + New embeddings improve the accuracy of logistic regression based classifiers on various datasets compared to the FastText only embeddings.
The FastText + New embeddings show a significant improvement in accuracy (+11.3%) on the MDSD books dataset compared to the FastText only embeddings.
The table shows the performance of different models on the MELD dataset.
The table provides the average F1 score for each model on the MELD dataset.
The table provides information about two conversation emotion datasets: IEMOCAP and MELD.
The average length of conversations in the MELD dataset is lower than in the IEMOCAP dataset.
The BiF-BiAGRU model achieves the highest average F1 score among all emotion classes in the IEMOCAP dataset.
The scLSTM model achieves the highest accuracy for the "neutral" emotion class in the IEMOCAP dataset.
UniF-BiAGRU achieves the highest accuracy, F1 score, and mF1 score among all the models tested on IEMOCAP and MELD datasets.
UniF-BiAGRU achieves a higher accuracy than UniF-Soft and BiF-Soft on the IEMOCAP dataset.
The system "Ours" has higher ratings for both grammaticality and naturalness compared to the system "serban-EtAl:2016:P16-1".
The system "Ours" has a higher rating for naturalness compared to the system "serban-EtAl:2016:P16-1".
As the number of transformer layers increases, the performance ACC also increases.
As the number of transformer layers decreases, the inference speed QPS increases.
The Original BERT Model has an inference speed of 52 QPS.
The MKDM model has a performance accuracy of 77.18%.
Table 8 compares the performance of Original BERT and TS-MKDM on the DeepQA and CommQA-Labeled datasets.
TS-MKDM outperforms Original BERT in terms of accuracy on the DeepQA dataset.
The models being compared in Table 1 are RNN, B-RNN, B-RNN-A, DD-RNN, LSTM-4, and GRNN-5.
DD-RNN has the highest accuracy for location prediction among all the models compared in Table 1.
The GloVe+Emo2Vec representation outperforms the previous state-of-the-art results on the SS-Twitter dataset.
The SVM model performs better on the "tube_tablet" dataset compared to the "tube_auto" dataset.
The "Natural language" annotation has the highest number of annotations.
The "Regular expressions" annotation has the highest evaluation score.
L3 consistently outperforms alternative learning methods based on multitask learning, meta-learning, and meta-learning jointly trained to predict descriptions (Meta+Joint).
L3 performs better on both the previously-used and novel visual concepts compared to the other models.
The model "L3" performs the best in terms of string editing, with the highest values in both the validation and test sets.
The "Meta" model performs better than the "Meta+Joint" model in terms of string editing, with higher values in both the validation and test sets.
Table 14 provides the percentages of NEG-88-NAT with a true continuation given a higher probability than false for BERTBASE and BERTLARGE models.
BERTBASE model has a higher percentage (87.5%) for negative natural true continuation than for affirmative natural true continuation (62.5%).
The table presents the word prediction accuracies for BERTBASE and BERTLARGE models using different perturbation techniques and values of k.
The BERTLARGE model performs better than the BERTBASE model in word prediction accuracy, regardless of the value of k.
Table 8 shows the word prediction accuracies for ROLE-88 with and without sentence perturbations.
The accuracies for the "Orig" perturbation are lower than the accuracies for the "Obj" and "Sub" perturbations in the "BERTLARGE [ITALIC] k=1" row.
BERTLARGE has a higher percentage of ROLE-88 items with good completion assigned higher probability than role reversal compared to BERTBASE.
The percentage of ROLE-88 items with good completion assigned higher probability than role reversal for BERTBASE is 31.8%.
For both BERTBASE and BERTLARGE models, the percentage of NEG-88-SIMP items with true completion assigned higher probability than false is 100%.
There are no negative values in the "Negative" column for both BERTBASE and BERTLARGE models.
Flair Embeddings outperform Static Embeddings in the Task of Topic Classification.
BERT Language Models perform better than Flair Embeddings in the Task of NER.
BERTeus achieves the highest Micro F1 score among all the models in Table 2.
BERTeus achieves the highest Macro F1 score among all the models in Table 2.
"BERTeus" achieves the highest micro F1 score among all the models in the table.
"BERTeus" achieves the highest Macro F1 score among all the models in the table.
The Flair-BMC embedding achieves the highest word accuracy among all the embedding models.
The BERTeus model has the highest word accuracy among all the BERT language models.
BERTeus achieves the highest F1 score among all the models.
Flair-BMC has a higher precision than Flair-official.
The table shows the adversarial success and machine-vs-random accuracy for three different models: Iter1 (standard), random, and RL.
The proposed model (RL) has a higher adversarial success rate (0.088) compared to Iter1 (standard) (0.058) and random (0.056).
The greedy decoding strategy has an AdverSuc score of 0.935.
The stochastic greedy decoding strategy has an AdverSuc score of 0.058.
RL performs better than Iter2 in pairwise human judgments.
Iter2 performs worse than Iter1 in pairwise human judgments.
The table presents the results achieved while predicting Task dimension communicative functions in the DialogBank for different scenarios.
Transfer learning improves the performance of predicting Task dimension communicative functions in the DialogBank compared to the base model in all scenarios.
Table 1 contains questions related to genes and questions related to chemicals.
Each question in Table 1 has multiple example answers.
The "Att-BiLSTM-CNN" model achieves the highest F1 score among all the models on the OntoNotes 5.0 dataset.
The "Cross-BiLSTM-CNN" model achieves the highest F1 score among all the models on the WNUT 2017 dataset.
As the number of microphones increases, the %WER and %SAWER decrease.
The %WER decreases as the number of microphones increases.
The "CNC output" row has lower values for "Misses", "FAlarms", and "SpkrErr" compared to the "Avg. by channel" row.
The "CNC output" row has a lower value for "DER" compared to the "Avg. by channel" row.
The method "SinABS (H+LSD)" achieves the highest F1 score in the R1 task.
The method "Concat (H+LSD)" achieves the highest F1 score in the RL task.
Table 4 compares the robustness of different attention designs on the noisy dataset.
The attention design "Uniform (H+LSD)" performs better than "Referee (H)" in terms of F1 score.
The F1 score for the joint model at hop 0 is 29.95.
Removing the CNN component at hop 1 decreases the F1 score by 2.45.
The table provides results from the PSD semantic parsing task for BERT{}_{\tt BASE} and BERT{}_{\tt LARGE} models.
The in-domain performance of BERT{}_{\tt BASE} and BERT{}_{\tt LARGE} models is higher than their out-of-domain performance.
The table presents the results from the PSD semantic parsing task using two different BERT models.
The average embedding method performs better than the last embedding method in the out-of-domain scenario.
The combination of Baseline + BERT achieves the highest scores in all the metrics (DM, PAS, PSD, AVG) compared to other models.
The addition of BERT to the baseline model leads to improvements in all the metrics (DM, PAS, PSD, AVG).
The Baseline + BERT model achieves the highest scores for all metrics (DM, PAS, PSD, AVG) among all models.
Adding BERT to the baseline model leads to an improvement in scores for all metrics (DM, PAS, PSD, AVG).
The table presents test results for semantic dependency parsing in Chinese using unlabeled and labeled dependency F1 scores as evaluation metrics.
The labeled dependency F1 score for the "Baseline + BERT" model on the "TEXT" dataset is 80.41.
The Seq2seq model outperforms the CRF model in terms of precision, recall, and F1 score in the Teacher-forcing Pitch task.
The Songwriter model achieves a higher BLEU score than the Seq2seq and CRF models in the Sampling task.
The table presents the human evaluation results in blind-review mode for three different models: Seq2seq, Songwriter, and Human.
The Songwriter model received the highest overall score among the three models.
The percentage of annotated labels agreeing with the majority label is highest for the "zh" language.
The percentage of annotated labels agreeing with the majority label is lowest for the "it" language.
The table shows the XCOPA accuracy scores of different transfer variants that adapt to out-of-sample languages.
The table presents the XCOPA accuracy scores of different transfer variants using the XLM-R model.
The "GeoMMmulti" method achieves the highest average precision@1 score among all languages.
The "Adv-Refine" method achieves a precision@1 score of 0.4 for the en-fi language pair.
"GeoMM" and "GeoMMmulti" have the highest scores in the "en-de" and "en-zh" language pairs.
"Grave et al. (2018)" has the highest scores in the "en-es" and "en-fr" language pairs.
GeoMM has the highest Pearson correlation coefficient for the SemEval 2017 cross-lingual word similarity task.
Luminoso_run2 has a higher correlation coefficient than NASARI, Procrustes, MSF, and Joulin et al. (2018) for the en-es language pair in the SemEval 2017 cross-lingual word similarity task.
The method "GeoMM" has the lowest accuracy on the monolingual word analogy task.
The "Original English embeddings" and "Procrustes" methods have the same accuracy on the monolingual word analogy task.
GeoMMsemi outperforms RCSLS in most language pairs.
On average, GeoMMsemi achieves a higher precision@1 than RCSLS.
The table shows F1 scores on three different datasets: WoZ2.0, DSTC2, and MultiWoZ.
As the value of ϵ decreases, the number of search steps decreases.
Different values of ϵ do not significantly affect the overall performance.
The LogicalFactChecker program performs better on the simple test set compared to other models.
The Table-BERT model achieves an accuracy of 66.0% on the validation set.
Table 2 presents the results of ablation studies on the development set and the test set for three different models: LogicalFactChecker, -w/o Graph Mask, and -w/o Compositionality.
The label accuracy of the LogicalFactChecker model is 71.83% on the development set and 71.69% on the test set.
Table 4 provides an error analysis of potential false positive entries.
The table includes mappings that are not present in LEXCONN.
There are 371 French DCs in total in the Europarl corpus.
The number of French DCs with a frequency greater than 50 is 309.
The recognition accuracy for the baseline case in the Hillenbrand Database is 75.2%.
The recognition accuracy for the MAE with adjusted outliers in the Peterson & Barney Database is 85.6%.
The table provides specifications for front end signal processing used in the experiments of speech recognition.
The default value for the "Window Length" parameter is 20ms.
The table illustrates the speech recognition performance using different normalization methods.
The maximum length of sentence pairs ranges from ≤4 to ≥10.
The performance of LSTM (o.g.) is higher than the performance of LSTM (no o.g.) for all maximum lengths.
The maximum similarity score achieved using paragram-phrase-XXL embeddings on the STS 2015 Average dataset is 80.2.
The 50th percentile similarity score achieved using paragram-phrase-XXL embeddings on the SMT dataset is 31.8.
The table shows results from supervised training of each compositional architecture on similarity, entailment, and sentiment tasks.
The LSTM with original gates performs better on the binary sentiment (SST) task compared to other compositional architectures.
The table shows the results from supervised training on similarity, entailment, and sentiment tasks, with the sentence representations fixed to the paragram-phrase model.
The paragram-phrase 2400 model achieves the highest score of 84.94 on the similarity task (SICK).
The table shows the precision, recall, and F1 scores for sentence-level classification in different categories.
The average F1 score across all categories is 0.289.
The average Precision, Recall, and F1 for sentence-level classification of the subjectivity-adjusted corpus are approximately 0.36.
The Precision and F1 scores for the "CONF" category in sentence-level classification of the subjectivity-adjusted corpus are 0.315 and 0.220, respectively.
The biLSTM+Multitask system achieves the lowest score in the nbow+ setting.
The SVMcs system achieves a higher score than the SVMovr system.
The "Full EA-EMR" method outperforms the baselines on the SimpleQuestions dataset.
The "w/o Alignment but keep" method performs better than the baselines on the FewRel dataset.
The table compares the performance of different methods for selecting data for EMR.
The "+ K-Means" method performs better than the "EMR Only" method for the FewRel task.
The score for the word similarity task using GlobalSim with the Huang:2012 model and the ws353 dataset is 71.3.
The score for the word similarity task using GlobalSim with the Word2Vec Skip-gram model and the rw dataset is 32.64.
Table 1 provides information on five benchmark datasets: ws353, SimLex, scws, rw, and men.
The ws353 dataset contains 353 word pairs, the SimLex dataset contains 999 word pairs, the scws dataset contains 2003 word pairs, the rw dataset contains 2034 word pairs, and the men dataset contains 3000 word pairs.
The model "neelakantan:2014" has the highest values for both AvgSim and AvgSimC.
The model "\textsc [ITALIC] mswe−1200" has the same values for both AvgSim and AvgSimC.
Table 4 provides accuracies for the word analogy task.
The result of Pennington2014 is higher than the result of Word2Vec Skip-gram.
The precision for 3CA using SG-Wiki Google is 0.151.
The mean average precision (MAP) for the Trans model using GloVe-CC DV is 0.387.
The mean error measurement of the "v1 + v2 → v1" row is lower than the mean error measurement of the "v1 + v2 → v2" row.
The median error measurement of the "v1 + v2" row is lower than the median error measurement of the "v2" row.
FastText (Max) performs the best among all algorithms in identifying helpful sentences for a given query.
Tf and BERT (Fine-tuned) have the same performance in identifying at least one helpful sentence at K=2.
The Seq2Seq (copy) algorithm outperforms all other algorithms in terms of BLEU-4, Rouge-1, Rouge-2, and Rouge-L scores.
Human judgment performs better than all algorithms in terms of BLEU-4, Rouge-1, Rouge-2, and Rouge-L scores.
Table 1 provides information about the amount of pairs per entailment type, the amount of unique rumorous claims used for creating the pairs, and the amount of unique tweets discussing these claims.
The number of unique tweets discussing rumorous claims in the "gwings" dataset is 461.
The highest accuracy achieved with the BERT model is with the "Orig. (19k)" training data.
The highest accuracy achieved with the SVM model is with the "Orig. (3.4k) & Rev." training data.
The accuracy of BERT on NLI is higher when using the original dataset with both revised premise (RP) and revised hypothesis (RH) compared to using only the revised premise (RP) or revised hypothesis (RH) separately.
The accuracy of BERT on NLI is higher when using a larger training set (500 k) compared to using the original dataset (8.3 k).
The table shows the accuracy of a Bi-LSTM classifier trained on different datasets.
BERT has the highest accuracy among all models for differentiating between original and revised data in the IMDb and SNLI/RH datasets.
The number of pictures with at least one comment decreases as the number of judgments increases.
The total number of comments is highest for pictures with at least one judgment and decreases as the number of judgments increases.
The total count of triggers for females is 377, for males is 318, for mixed is 76, and for nobody is 252.
The number of triggers for females in the "Pose" category is 114, for males is 99, for mixed is 11, and for nobody is 5.
Females are more likely to trigger a comment on pictures compared to males.
Pictures with "Nobody" are less likely to trigger a comment compared to pictures with other subject types.
The table shows the results of our model, as well as the results of the best classifier CUUI (A) and the deep context model (B), in comparison to each other.
Our model outperforms both the best classifier CUUI (A) and the deep context model (B) in terms of article, verb form, noun number, and subjective agreement errors.
The table shows the results of a neural classification model, with and without attention, on CoNLL-2014 data for different types of errors.
The neural classification model with attention performs better than the baseline model for all types of errors.
The table shows the overall performance of different systems, including "The public SMT", "Wang et al. (2017)", "Our neural model", "Ji et al. (2017)", "Rozovskaya and Roth (2016)", and "Our system".
Among the listed systems, "Our neural model" and "Our system" have the highest F0.5 scores.
The "IHM" target has the lowest WER for all training models.
The "IHM + SDM" training model has the lowest WER for both "IHM" and "SDM" targets.
The table presents the Word Error Rates (WERs) for models trained on hidden vectors produced by an FHVAE, with different feature representations.
Name Entity Recognition has the lowest average entropy value among all the NLP tasks.
Doc2VecC has the lowest error rate among all the models on the Imdb dataset.
The error rate (including test) for Skip-thought Vectors is not provided in the table.
The different models used for document representation in the experiment are BOW, DEA, Word2Vec + AVG, Word2Vec + IDF, ParagraphVectors, and Doc2VecC.
The classification error for the "h=1000" configuration is 31.13 for BOW, 31.78 for DEA, 32.06 for Word2Vec + AVG, 33.02 for Word2Vec + IDF, and [BOLD] 30.24 for Doc2VecC.
Table 6 provides test set results on the SICK semantic relatedness task using various methods.
The Bidirectional LSTM method outperforms all other methods in terms of Pearson's γ.
The success rate (SR) is higher for the seen validation set compared to the unseen validation set in the Full Model.
The trajectory length (TL) decreases when the action-aware module (AA) is removed from the full model.
The proposed object-and-action aware model (OAAM) mainly improves performance on the Unseen scenarios, while the nearest point loss (NPL) mainly improves performance on the Seen scenarios.
The proposed method improves the success weighted by dynamic time warping (SDTW) metric for both Seen and Unseen scenarios.
NMT outperforms PBMT in terms of perplexity scores.
The word alignments obtained after translating the test set with PBMT and NMT systems for the EN→CS language direction are more similar to the monotone alignment than to the word alignments of the reference translation.
The word alignments obtained for the NMT system's translation for the DE→EN language direction are more similar to the word alignments of the reference translation than to the monotone alignment.
The Partial Us model performs better than the Partial Baseline model in terms of Precision, Recall, and F1 scores for the "victim" field.
The Strict Baseline model performs better than the Strict Us model in terms of Precision, Recall, and F1 scores for the "Time" field.
The hyperparameter "Embeddings feature (ϕl) size" has a value of 20.
The hyperparameter "Training patience" has a value of 10.
The size of the argument (F [ITALIC] A) is 150.
The memory limit for training is a document size of 600.
The F1 scores for the "victim Name" field are higher in the "Strict Us" column compared to the "Strict Baseline" column.
The F1 scores for the "City" location are higher in the "Strict Us" column compared to the "Partial Us" column.
The "Full BEM" achieves the highest "Dev F1" score among the different model ablations.
The "Frozen Context Encoder" performs better in terms of "Dev F1" compared to the "Frozen Gloss Encoder".
The F1-score for BEM is higher than the F1-score for WordNet S1, MFS, BERT-base, HCAN, EWISE, GLU, LMMS, SVC, and GlossBERT.
The F1-score for BEM is higher than the F1-score for WordNet S1, MFS, BERT-base, HCAN, EWISE, GLU, LMMS, SVC, and GlossBERT in the "Concatenation of all Datasets Nouns" column.
The table shows the F1-score (%) on the MFS, LFS, and zero-shot subsets of the ALL evaluation set.
The F1-score for zero-shot words using the BERT-base system is 84.9% and for zero-shot senses is 53.6%.
The F1 score of BERT-large is lower than that of BERT-base, RoBERTa-base, and RoBERTa-large.
RoBERTa-large achieves the highest F1 score among all the pretrained models.
DCFEE-O has higher F1 scores for all event types and the average F1 scores on the single-event and multi-event sets compared to DCFEE-M.
GreedyDec has higher F1 scores for the event type "EU" on the single-event and multi-event sets compared to the other models.
BERT generally performs the strongest in clean settings.
The CDAE is better at handling noise without adversarial training than BERT.
Table 6 shows the results of the Ensemble of CDAE and BERT and compares its performance against combined noise.
The ensemble of CDAE and BERT performs the strongest against combined noise among all of their methods.
Table 6 shows the top 20 topics of user's interests.
The table provides specific sub-topics related to each topic.
The total number of individuals in each racial group is equal to the total number of males and females combined.
The total number of males is equal to the sum of males in each racial group.
The z value for "anger" is higher than the z values for "anxiety" and "sadness" in the "Affective attributes" section.
The value of μ(male) for "causation" is the same as the value of μ(male) for "certainty" in the "Cognitive attributes" section.
The table shows the results of three different methods: LSTM, TDLSTM, and TCLSTM.
The TDLSTM method has a higher Macro F1 O score compared to the LSTM method.
The TDParse+ model performs better than the TDParse model on all datasets.
The TCLSTM model performs better than the average performance across all models.
The table shows the relative change in metrics as a result of the server trained and federated emoji prediction models.
The experiments in Table 2 are categorized by different values of B and K, as well as different optimization methods.
The accuracy@1 and AUC values for the "Best server trained" experiment are 0.239 and 0.898, respectively.
The vocabulary size of the MLB dataset is larger than that of the RotoWire dataset.
The average summary length of the RotoWire dataset is smaller than that of the MLB dataset.
The ENT dataset has the highest performance in content selection (CS) compared to the other datasets.
Table 3 shows the ablation results on RotoWire (RW) and MLB development set using relation generation (RG) count and precision, content selection (CS) precision and recall, content ordering (CO) in normalized Damerau-Levenshtein distance, and BLEU.
The BLEU score for RotoWire (RW) with the "+Hier" ablation is 19.02.
The table provides test performance results for three types of tasks: WTC, Standard Task, and Adversarial Task.
The adversarial model performs better than the standard models on Adversarial Task 2.
The PER(%) obtained with the UniDir LSTM architecture is 17.1.
The MFCC BiDir architecture achieves a PER(%) of 15.7.
The UniDir LSTM model has a WER of 32.5%.
The UniTwin GRU model has a WER of 29.6%.
The UniTwin model performs better than the UniDir model on CHiME.
The BiDir model performs better than the UniDir model on LibriSpeech.
ProGraph achieves the highest micro-average score among all models in the sentence-level task.
Pro-Global achieves the highest score in category 1 among all models in the sentence-level task.
Pro-Graph has a precision of 67.30% and a recall of 55.80%.
Pro-Struct has an F1 score of 53.75.
Table 2 presents the results of different variations of the ProGraph model on a document-level task.
The variation of ProGraph without the entire graph has the lowest F1 score among all the variations.
The Turbo parser has a higher error reduction percentage than the RBG parser for both POS and UAS metrics.
The RBG parser performs better than the Turbo parser in terms of UAS and LAS metrics.
Table IV provides information about the number of positive and negative predictive features per message before the reported message.
The number of positive predictive features is consistently higher than the number of negative predictive features per message before the reported message.
The accuracy of the prediction increases when sentiment features are added to the text features.
The number of features increases when sentiment features are added to the text features.
The RNN model outperforms the Regression model in terms of accuracy and F1 score.
Increasing the number of features from 190 to 260 improves the performance of the models.
The table shows the hours of data selected by automatic budget decision for different domains.
The table shows the Word Error Rate (WER) in percentage for two different models: "PLP" and "PLP+BN".
The model "PLP+BN" performs better than the model "PLP" in terms of Word Error Rate (WER).
Table 2 provides the Word Error Rate (WER) in percentage for different domains using domain-specific acoustic models with PLP features.
The domain with the lowest WER across all acoustic models is "RS" with a WER of 16.9%.
The WER for the "PLP features" method is 34.9%.
The total WER for the "PLP features" method is 34.9%.
The table shows the perplexity of word class language models for four different languages: Bengali, Tamil, Turkish, and Zulu.
The hybrid method performs better in terms of perplexity compared to the in-domain baseline and Twitter baseline for all languages.
Bengali has the smallest vocabulary size among the languages in the table.
Zulu has the highest number of tokens among the languages in the table.
The table presents the results of Turkish LM interpolation for three different models: Babel, Babel + Twitter, and Word + Class.
The Babel + Twitter model performs better than the Babel model in terms of word trigram and class trigram.
The table compares the queue ranking strategy LM perplexity for 150, 300, and 500 users.
The "B-AON" model outperforms the "AON" model in both the NIPS and SIND datasets.
The "B-TSort" model performs better than the "B-AON" model in the NIPS dataset with a window size of 3.
The DMN model performs better than the MemNN model on the bAbI dataset.
The accuracy of the MemNN model on the bAbI dataset is 93.3%.
The DMN model has the highest test accuracy among all the models in Table 3.
The Sogaard model has a higher test accuracy than the SVMTool model in Table 3.
The accuracy of the tasks increases as the number of passes of the episodic memory module increases.
The accuracy for task 8 is higher than the accuracy for the sentiment task.
The table provides the lexicon size for each emotion category.
The table provides the classification accuracy for each emotion category in the forward process.
The system "Tint" has a speed of 9,000.
The system "Tanl (DeSR)" has a LAS score of 89.88.
The accuracy of the models generally increases from the Q-type baseline to the CNN+LSTM+RN model, with the highest accuracy achieved by the CNN+GRU+CBN model.
The CNN+GRU+CBN model outperforms all other models in terms of comparing numbers.
For the Reddit dataset, the EI model is used with an emotion weight of 25.
The table provides performance results for multilingual sentiment analysis in Arabic, German, Portuguese, Russian, and Swedish.
The B4MSA model achieved an F1 score of 0.642 and an accuracy of 0.799 when using 100% of the multilingual set of parameters.
The total number of instances in the SemEval'15 dataset is 1,963.
The total number of instances in the Arabic dataset is 2,000.
The table provides information about different sentiment analysis datasets and their performance scores.
The SENTIPOLC '14 dataset is in Italian, has four classes, and achieved a score of 0.677 with a rank of 2 out of 14.
Table 4 displays the performance of B4MSA on cross-validation and gold standard for different datasets and parameter searches.
The cross-validation score for the Multilingual Parameters Random Search on the SENTIPOLC '14 dataset is 0.678256.
The Jaccard score increases as the dataset size increases.
The False Pos. rate is higher than the False Neg. rate for all dataset sizes.
The NN model consistently outperforms the Freq and Regr models in terms of Jaccard score for all three datasets.
The NN model consistently has lower False Neg. values than the Freq and Regr models for all three datasets.
The table presents results for three different methods (Freq., Regr., NN) for the D3large dataset using the intersection metric.
The intersection metric values for the Freq., Regr., and NN methods are 0.319, 0.278, and 0.398, respectively.
Topics such as Actions or Intentions, Antecipation and Socialising, Health, Movies and TV, Relationships and Friendship, Shopping, and Voting are more prevalent in São Paulo compared to Rio de Janeiro.
Rio de Janeiro has the highest percentage of tweets with specific places.
New York City has the highest number of tweets with specific places.
New York City has the highest percentage of hashtags in total tweets among the four cities.
London has the highest percentage of user mentions in total tweets among the four cities.
The F1-score for the combination of BoW + BoE is 0.8548.
The ERNIE 2.0 BASE model performs better on the DRCD task compared to the ERNIE 2.0 LARGE model.
The BERT BASE model performs better on the XNLI task compared to the ERNIE 2.0 BASE model.
The WER for Exp1, Exp2, and Exp4 is higher than the WER for LWF and Best FT.
The WER for Exp4 is lower than the WER for GU-MONO.
The WER for Experiment 1 is lower than the WER for Experiment 2.
The WER for Experiment 3 is lower than the WER for Experiment 1.
Table 7 shows the aggregated results over datasets for combinations of lexica in Bi-LSTM.
The Jaccard accuracy for the combination of "combi+vae" is [BOLD] 0.272.
Table 1 shows the F1 scores for different models on sentence-level and role-level in SLU accuracy on DSTC 4.
The F1 score for sentence-level and role-level is the same for the "Decay-Function-Free Time-Aware Attention" model.
The table shows the SLU accuracy of possible combinations of the proposed methods in role-level attention with different history representations.
The F1 scores are higher when both intent and distance vectors are used compared to using only intent vectors.
The model "FastText" generally outperforms the model "AWE-S" on word similarity datasets.
The "Our model (full)" variant achieves the highest F1@1 score for both Twitter and Weibo.
The "Seq2seq (post only)" variant achieves the lowest F1@1 score for both Twitter and Weibo.
Our model outperforms all other models in terms of F1@1, F1@5, MAP, and RG-1 on the Twitter dataset.
Our model performs better than Seq2Seq in terms of F1@1, F1@5, MAP, and RG-1 on the Weibo dataset.
The "Our model" outperforms all other models in terms of ROUGE-1 F1 scores for both the Twitter and Weibo datasets.
Including conversation data in the classification task leads to lower ROUGE-1 F1 scores compared to using only post data.
SWEM-concat has the highest accuracy on WikiQA.
LSTM has the same accuracy for MultiNLI Matched and MultiNLI Mismatched.
The SWEM-aver model achieves a test accuracy of 93.59% on the DBpedia dataset.
The fastText (bigram) model achieves a test accuracy of 72.3% on the Yahoo! Answers dataset.
Table 6 shows the test accuracy for an LSTM model trained on original and shuffled training sets for three different datasets: Yahoo, Yelp P., and SNLI.
The test accuracy for the LSTM model trained on the original Yahoo dataset is 72.78.
Table 8 shows the test accuracies of different models on sentence classifications.
Table 11 shows the test accuracy of SWEM on the Yahoo dataset with a wide range of word embedding dimensions.
The highest test accuracy for SWEM on the Yahoo dataset is 73.31 with a word embedding dimension of 1000.
The accuracy of mBERT in-language training is the same for both in-language and zero-shot scenarios.
MMTE performs better than mBERT in the zero-shot scenario for the German language.
The mNMT model performs slightly worse than the baseline model on the "ur-en" and "en-bg" language pairs.
The mNMT model performs better than the baseline model on the "mr-en" language pair, but worse on the "en-es" language pair.
The zero-shot performance of MMTE on the intent classification dataset is higher for the Spanish language compared to the other languages.
The in-language performance of mBERT on the intent classification dataset is higher for the English language compared to the other languages.
The MMTE model performs better than the mBERT model on the "ar" POS tagging dataset.
The "hi" POS tagging dataset performs better than the "mr" POS tagging dataset.
Table 10 shows the effect of the number of languages used for mNMT training on downstream zero-shot performance.
The F1 score for POS 102x102 is higher than the accuracy for XNLI 102x102.
Table 11 shows the effect of <2xx> token on zero-shot cross-lingual performance, with XNLI numbers representing accuracies and POS numbers representing F1 scores.
Our model outperforms most competitors in terms of MRR, R@1, R@5, R@10, and Mean.
The table presents the quantitative evaluation on the VisDial-Q dataset with the VisDial-Q evaluation protocol.
The mean value decreases as we move from SF-QI to SF-QIH to Ours (w/o iter) to Ours (const. graph) to Ours (full, 3 iter).
The method labeled as "[BOLD] Ours [ITALIC] (3 iter)" outperforms the other methods in terms of MRR, R@1, R@5, R@10, and Mean values.
The method labeled as "[ITALIC] const. graph." performs worse than the other methods in terms of MRR, R@1, R@5, R@10, and Mean values.
The Seq2Seq model with attention and bag-of-words outperforms the Seq2Seq model with attention only on the Ch-En machine translation task.
The Mixed RNN model achieves higher performance than the PKI model on the Ch-En machine translation task.
The character-level accuracy on the LCQMC dataset is higher than on the BQ dataset.
The word-level accuracy on the LCQMC dataset is higher than on the BQ dataset.
The SVM model has the highest accuracy among all the models in Table 2.
The Baseline BiLSTM model performs better than the Baseline BottomUp ConTree LSTM model in terms of accuracy.
The "BiConTree" model achieves the highest test set accuracies for all four sentiment classification tasks.
The "BiLSTM" model achieves an accuracy of 83.3% on the 5-class Phrase sentiment classification task.
There are four different head lexicalization methods: L, R, A, and G.
The highest accuracy among the four head lexicalization methods is 53.5%.
The F1 score improves as we move from the "Baseline (zhu2013fast)" model to the "ConTree" model to the "ConTree+Lex" model to the "Our 8-best Oracle" model.
The "Our 8-best Oracle" model outperforms the "ConTree" model in terms of F1 score.
The Good-Turing smoothing method achieves higher accuracy than the Knezer-Ney smoothing method for all n-gram models.
The 2-gram model achieves a higher F1-Score than the 3-gram and 4-gram models for both the Good-Turing and Knezer-Ney smoothing methods.
Table 1 compares different re-ranking methods against the QL baseline on ERR@20 and nDCG@20 metrics for Trec Web Track 2012-14.
The rouge score for the "BertS2S" model is 38.42.
The faithfulness score for the "Faith." model is 31.27.
The Spearman's correlation coefficient for Entailment is higher with faithful annotations compared to factual annotations.
The Spearman's correlation coefficient for QA is higher with faithful annotations compared to factual annotations.
The "Gold" model has the highest Fleiss' Kappa score for the factuality assessment.
The "TConvS2S" model has the highest Fleiss' Kappa score for the hallucination assessment.
The metric "Entailment" has a higher correlation coefficient with the "Faithful" annotation compared to the "Factual" annotation.
The metric "Repetition" has a higher correlation coefficient with the "Factual" annotation compared to the "Faithful" annotation.
The SVM model has an accuracy of 0.423 for the "Market" feature type in the classification task.
The "doc2vec" feature type has an MSE of 0.00140 for the GK regression task.
The "Gold" method does not involve any compression, while all other methods involve compression.
The "SLAHAN w/ syn" method achieves the highest compression ratio, followed by the "Base" method.
Glove ELMo is used with all the features in the development data.
BERT is used with the first and third features in the development data.
The "SLAHAN w/ syn" tagger achieves the highest F1 score among all the taggers.
The "SLAHAN w/o syn" tagger has the highest ΔC value among all the taggers.
The compression ratio for the "SLAHAN w/ syn" method is lower than the compression ratio for the "Gold" method.
The change in compression ratio for the "SLAHAN w/ syn" method is lower than the change in compression ratio for the "Gold" method.
Table 2 represents the "Self talk" AMT human evaluation for two datasets, DAQUAR and coco-VQA.
The "s2s+copy" model performs better than the "s2s" model in terms of Rouge-1 F1 score.
The "NN-SE" model outperforms the "LEAD1" model in terms of Rouge-L R score.
The dataset "Psg2Sum" has a different granularity than the other datasets in the table.
The dataset "DUC2002 (task 1)" has a smaller corpus size compared to the other datasets in the table.
As the minimum overlap threshold value λ increases, the percentage of "Good" decreases.
The addition of NLM improves the classification accuracies for both NTN and TransE models.
The addition of NLM improves the classification accuracy for the "HasInstance" relationship type.
Table 2 presents the results of an Analogy Test, comparing single and joint objective embeddings.
The embedding method "NLM + GD" achieves the highest LAS score in all five web domains and their average.
The embedding method "NLM + GD" performs better than the embedding method "NLM + NTN" in the "Weblogs" domain.
The performance ratings given by users range from "Very Poor" to "Excellent".
There are 6 users who rated the performance as "Very Poor", 22 users who rated it as "Poor", 22 users who rated it as "Fair", 29 users who rated it as "Good", and 4 users who rated it as "Excellent".
The table provides NLU F1 scores for the ILLC-IER dataset and user study dialogues.
Table 7 provides Bleu scores for WMT15 English→French.
The Bleu score is higher for "+ RTT APE only fr-orig" compared to "our NBT".
Microsoft has a higher Bleu score for the WMT18 submission compared to Cambridge.
Microsoft has a higher Bleu score for "+ APE only de-orig" compared to Cambridge.
The table shows the average Bleu scores for WMT18 English→German newstest2014-2017.
The average Bleu score for the "our bitext" without any post-editing is 27.7 for orig-de and 33.1 for orig-en.
Different methods were used to improve the average Bleu scores for the "our bitext" and "our NBT" datasets.
The original German translations have higher average Bleu scores compared to the original English translations.
The method "Ours (TN)" outperforms all other methods in terms of all metrics on the NELL-995, Amazon Beauty, and Amazon Cellphones datasets.
The NELL-995 dataset has higher MRR scores compared to the Amazon Beauty and Amazon Cellphones datasets for all methods.
Table 2 shows the POS predictive accuracy over the AAVE dataset, stratified over three domains, and includes the macro-average accuracy.
The POS predictive accuracy for the "adv" model is higher for subtitles compared to lyrics.
Table 1 shows the POS prediction accuracy stratified by sex and age using the Trustpilot test set.
The absolute difference in accuracy between the two sexes is 0.2 and between the two age groups is 1.5.
Table 3 provides information on sentiment F1-score and accuracy of discriminators for three private attributes.
The discriminator for the "loc" attribute has the highest accuracy among all discriminators.
Table 3 shows the unsupervised sentence retrieval results on BUCC.
MARGE performs better than other unsupervised models in unsupervised sentence retrieval on BUCC.
The translation score from German to Dutch is 11.6.
The translation score from Italian to Romanian is 9.8.
MARGE outperforms other unsupervised models in all languages.
The average performance across all models is higher in de and fr compared to ru and zh.
The table shows different models used for MLSum, including Extractive Oracle, Lead 3, Pointer-Generator, M-BERT, MARGE-NEWS, and MARGE.
The ROUGE-L score for MARGE-NEWS in the German language is 25.91.
The ratio of unchanged words in the target sentence to the source sentence is highest for CoNLL-2013 and lowest for JFLEG.
Lang-8 has the highest number of tokens in the target sentence.
Finetuning the model improves the precision and recall values.
Using the denoising auto-encoder improves the precision and recall values.
The 4-GRU model has the highest R@10 score.
The [merkx2019language] model has the highest median rank.
The Encoder has a larger vocabulary size than the Decoder in the Word-based representation.
Table 1 shows the results of different algorithms used for grammaticality tests.
The "Random" algorithm has the highest ROUGE-1, ROUGE-2, and ROUGE-SU4 scores.
The "All" algorithm has lower ROUGE-1, ROUGE-2, and ROUGE-SU4 scores compared to the "First" algorithm.
On average, each worker made 57.5 submissions.
Each gold was reformulated 5 times.
The domain classification accuracy between the original dataset and the reformulated query is similar.
The slot-filling accuracy for the reformulated query is higher than that for the original dataset.
The F1 score for the "Gold + MTurk" dataset is higher than the F1 score for the "Gold" dataset.
The Bleu score for the "Gold + MTurk" dataset is lower than the Bleu score for the "Gold" dataset.
The backtrans-good approach achieves a higher BLEU score than the baseline approach for English→French translation on the test-07 dataset.
The natural approach achieves a higher BEER score than the baseline approach for English→German translation on the newstest-14 dataset.
The Baseline model achieves a BLEU score of 21.36 on the English→French test-07 dataset.
The copy-marked + noise model achieves a CTER score of 55.72 on the English→German newstest-14 dataset.
The deep-fusion model outperforms the baseline model on the English→French test-07 dataset.
The copy-marked + noise + GANs model achieves higher BLEU score than the deep-fusion model on the English→German test-07 dataset.
The model "This work" outperforms other models in terms of MAP and MRR scores on the QASent dataset.
The models "cicero2016attentive (Attention-based CNN)" and "miao2015neural (Attention-based LSTM)" perform better than the models "yang2015wikiqa (2-gram CNN)" and "yin2015abcnn (Attention-based CNN)" in terms of MAP and MRR scores on the QASent dataset.
The "cicero2016attentive (Attention-based CNN)" model has the highest MRR score on the QASent dataset.
The model in this work has a higher MAP score than the "severyn2015learning (CNN only)" model on the QASent dataset.
The model "ji2013discriminative" achieves the highest accuracy and F1 score among all the models.
The F1 score is not reported for the models "he2015multi (without POS embeddings)" and "he2015multi (without Para. embeddings)".
The Rouge scores for all models are lower than the Rouge score for the original text.
The BLEU scores for the S4+gv+bce model are higher than the BLEU scores for the S4 model.
The model correctly chose to copy a word 5359 times out of the total 5616 times when the ground truth alignment indicated a copy action.
The model correctly chose to change a word 227 times out of the total 333 times when the ground truth alignment indicated a change action.
Table 2 shows the performance of the classifier on the annotated test set at the end of each iteration of the up-sampling procedure.
The precision values increase from iteration 1 to iteration 2, and then decrease from iteration 2 to iteration 3.
The BLEU score for "key → 1 BL-1" is higher for CVAE-D compared to CVAE-D-P.
The BLEU score for "1&2 → 3 BL-1" is higher for B&T compared to B&T-P.
The "B&T" model outperforms the "B&T-P" model in terms of the similarity between the first two poem lines and the last two poem lines.
The "CVAE-D-P" model is rated higher in terms of consistency in human evaluation compared to the "CVAE-D" model.
Different models are used for the Restaurant and Laptop aspects.
The F1 score for the Restaurant Aspect in SemEval-1 is 84.01.
Table 3 shows the impact of different components on sentiment analysis for restaurant and laptop aspects.
The RNCRF+F model outperforms all other models in terms of accuracy for sentiment analysis on restaurant and laptop aspects.
The values in the "λ 100" column are consistently higher than the values in the "λ 0" column.
The highest value in each row is marked as [BOLD].
The table provides evaluation results of different character-based English-Japanese NMT models.
The model with fine-tuning (τ=1.0) has a lower BLEU score compared to the Flexible Attention (τ=1.0) model.
The table provides evaluation results on English-Japanese and German-English translation tasks.
As the data size increases, both the Transformer and Fine-tuning methods show improvement in performance.
With a data size of 900K, the Fine-tuning method performs better than the Transformer method.
The table shows the results of different models on WMT14 English-German translation task.
The table shows the results on WMT14 English-German with rate-scheduled learning using different values of ηlm.
The BLEU score is higher for ηlm=0.01 compared to ηlm=0.
The BLEU scores for the different layers of BERT on the WMT14 English-German dataset are similar, ranging from 28.4 to 29.2.
The "Last Hidden" layer of BERT performs worse than the other layers on the WMT14 English-German dataset.
The number of physical objects annotated with an agreement level of ≥ 0.8 is 2989.
The total number of physical objects annotated with an agreement level of ≥ 0.6 is 4089.
The table presents an ablation study of different training objectives on the TACRED dataset.
The training objective L (ours) achieves the highest F1 score among the different objectives.
BERT-base (frozen) achieves the highest F1 score on TACRED.
NERO has a higher recall than CBOW-GloVe on SemEval.
The objective of the table is to conduct an ablation study of different soft-matching models for Nero on the TACRED dataset.
The precision, recall, and F1 score for the BERT-base (frozen) model are 45.6, 47.6, and 46.5, respectively.
Table 6 shows the effect of training the BiDAF Single model on the original training data alone and augmenting the data with raw AddSent examples.
The performance of the BiDAF Single model is lower when trained on the augmented data with raw AddSent examples compared to the original training data.
Table 2 presents the results of adversarial evaluation on the Match-LSTM and BiDAF systems.
The BiDAF Ensemble system outperforms the Match Single, Match Ens., and BiDAF Single systems in all cases.
Table 4 provides human evaluation results on adversarial examples.
Human accuracy drops to 79.5 on the AddSent adversarial examples.
The "AddAny" targeted model has no evaluation results for the "Model under Evaluation" columns.
The performance of ML Ens. is higher than ML Single in the "Model under Evaluation" columns.
The average number of attributes per turn is higher for Property dialogue turns compared to Room and Agent dialogue turns.
The average number of sentences per turn is higher for Property dialogue turns compared to Room and Agent dialogue turns.
The table shows the names of different models used in the experiment.
The Matching Transformer-CFA model achieves the highest performance in terms of Recall@k, MRR, and MR.
When all three modalities (Text, Vision, and Audio) are used, the performance of the Matching Transformer model is improved, as indicated by higher Recall@1, Recall@5, Recall@10, MR, and MRR values.
When only the Vision modality is used, the performance of the Matching Transformer model is better in terms of Recall@1, Recall@5, Recall@10, MR, and MRR values compared to when only the Text or Audio modality is used.
The Matching Transformer model has a higher correctness score compared to the other models.
The Matching Transformer model has a higher relevance score compared to the other models.
"Character 4-grams" has the highest accuracy among all the features.
The "Oracle" feature has the highest accuracy among all the features.
The table shows the classification results on the dataset using various ensemble fusion methods to combine individual classifiers.
The "Oracle" method achieves an accuracy of 91.6% on the dataset.
The F1-score for the "Hate" class is 0.45.
The recall value for the average of all classes is 0.80.
The F-measure performance is highest for experiment 5 with a context size of 5 on the SMS (large) dataset.
The F-measure performance is highest for a context size of 4 on the SMS (small) dataset.
The "2-skip-1-gram + String feature" method has the highest F-measure score among all the methods.
The "2-skip-1-gram + String feature" method has a higher precision score than the "2-skip-1-gram" method.
The "Constr (ϵ=0.8)" encoder has the lowest retention rate.
The "Constr (ϵ=0.2)" decoder has the highest accuracy.
The number of training instances for the language "fin" is 34,181.
The average phone error rate for the language "mah" is 10.1 with a confidence interval of ±0.11.
The backness feature has a positive correlation with the number of readings.
The vowel pair /i/ and /o/ has a negative correlation with the number of readings.
The table shows the differences in Alpha agreement measures for different sentiment classes.
The values for the "Spanish" dataset in the "Alphaint(−,+)" column do not support the thesis that sentiment classes are ordered.
Table 1 shows the Rouge results on MedStar Georgetown University Hospital's development and test sets.
The UMLS PG model performs better than the other models on Test RG-1.
Table 4 shows the results of Word Embedding Association Tests (WEAT) for GloVe and the Universal Encoder, comparing different sets of target words.
The effect size reported for the comparison between Instruments and Weapons in terms of Pleasant vs Unpleasant is 1.53, indicating a large effect size.
Table 2 shows the performance of different models on transfer tasks.
Table 3 shows the WER on training data during 100 iterations of extended Baum-Welch using two different recognition methods.
The WER decreases as the number of iterations increases for Method A.
Table 1 shows the Word Error Rate (WER) on training data during 100 iterations of extended Baum-Welch using two different recognition methods.
The WER decreases as the number of iterations increases for Method A.
As the number of iterations increases, the Word Error Rate (WER) decreases.
The Minimum Likelihood Estimation (MLE) has a lower Word Error Rate (WER) compared to the other iterations.
Table 1 shows the perplexity of different models on the Yelp and BillionWord datasets.
The perplexity of the NLM+KN5 model on the BillionWord dataset is [BOLD] 47.472.
The proposed model with pre-training has the highest accuracy among all the methods.
The proposed model with pre-training has a higher accuracy than the proposed model without pre-training.
Among all models, "Neural Snowball (CNN)" has the highest F1 score.
"Fine-tuning (BERT)" has a higher precision than "Relational Siamese Network (BERT)".
The table provides the WER(%) on AMI-IHM and AMI-SDM using adapted CNNs.
Table II shows the WER(%) for different re-parametrization functions for LHUC transforms on TED tst2010. The unadapted baseline WER is 15.0%.
The values in the "max(0, r)" column are all equal to or greater than 0.
The table provides the WER (%) for different sampling strategies and SAT-LHUC training on the TED tst2013 dataset.
The WER (%) for sampling strategies per speaker is higher for the SI model compared to the SD model.
The "Our system" model performs differently on tst2011 and tst2013.
The WER for the "DNN (sMBR) + HUB4 + WSJ" model is 15.7 on tst2013.
The table shows the WER(%) on AMI-IHM and one-shot adaptation for three different models: CNN, +LHUC, and +LHUC.one-shot.
The +LHUC and +LHUC.one-shot models have higher WER(%) on the eval dataset compared to the CNN model.
The model "+LHUC" performs better than the model "DNN" on both the "eval2000 SWB" and "eval2000 CHE" datasets.
The model "++SAT-LHUC" performs better than the model "+fMLLR" on the "eval2000 TOTAL" dataset.
The table presents results on the Aurora 4 Multi-condition DNN model.
The AVG column represents the average value of columns A, B, C, and D.
The performance of the STCKA model decreases as the value of λ increases.
The STCKA model with λ=0.25 achieves the highest performance on the Weibo dataset.
The table shows the impact of different embedding tuning methods on the model's performance.
The STCKA-non-static method achieves the highest performance on the Weibo dataset.
The number of echographic examples remaining decreases as the value of α decreases.
The Word Error Rates (WER) for LibriSpeech dev-clean and BNC subset increase as the value of α decreases.
The WikiHop dataset has a larger training set compared to the MedHop dataset.
The WikiHop dataset has a larger number of instances compared to the MedHop dataset.
Table 4 compares the accuracy of three baseline models (Document-cue, Maj. candidate, and TF-IDF) on the WikiHop dataset before and after filtering.
After filtering, the accuracy of all baseline models (Document-cue, Maj. candidate, and TF-IDF) decreases on the WikiHop dataset.
The BiDAF model achieves the highest test accuracies for both the WikiHop and MedHop datasets in the standard setup.
The test accuracy for the WikiHop dataset in the masked setup using the "Majority-candidate-per-query-type" model is 58.4%.
Table 6 shows the test accuracy comparison when only using documents leading to the correct answer (gold chain).
The test accuracy for BiDAF mask is 100.0.
Table 7 shows the test accuracy (masked) when only documents containing answer candidates are given for different models.
The test accuracy for BiDAF on the WikiHop test is 54.5.
The table displays the average accuracy, log-loss, and AUROC for bills using legislative events post introduction for two different feature sets: "combined" and "combined+act".
The "combined+act" feature set performs better in terms of average accuracy compared to the "combined" feature set for bills using legislative events post introduction.
The combined feature set has higher average accuracy, log-loss, and AUROC than the other feature sets.
The "just_spon" feature set has a higher accuracy average than the "just_txt" and "no_txt_spon" feature sets.
The semantic search model outperforms the Lucene baseline model in terms of DCG@10 scores for all queries.
The Lucene baseline model performs better than the semantic search model for the query "California fire" in terms of DCG@3, DCG@5, and DCG@10 scores.
The table provides the error rates for "Total Error," "- Trigger/NIL," "- Sibling Sub-types," and "- Other."
The table shows the percentage change in error rates for each category compared to the previous column.
The "BiDAF + Memory Network" model has the highest MC Attention score of 68.5.
The "BiDAF + Self-Attention" model has the highest BLEU score of 27.4.
The MacNet model improves the BLEU scores for both WMT14 En→De and WMT15 De→En compared to the baseline and other ablation experiments.
Adding both the encoding layer and modeling layer to the baseline model improves the BLEU scores for both WMT14 En→De and WMT15 De→En.
The average rank of words in the "Homoglyph Neural" category is higher than the average rank of words in the "Neural" category.
The average rank of words in the "Human" category is higher than the average rank of words in the "Neural" category.
"depccg + MRF" has a higher accuracy than "depccg".
"jigg" has a higher recall than "depccg".
"depccg + MRF" has a higher accuracy than "depccg".
"jigg" has a higher recall than "depccg".
The "Score" values for both CommonsenseQA and OpenBookQA datasets are less than 100.00.
The "Novelty" values for both CommonsenseQA and OpenBookQA datasets are greater than 50.00.
The "Summarization Random" method has lower ROUGE-1, ROUGE-2, and ROUGE-L scores compared to other summarization methods.
The "MilNet+MATE+MT+RD" method has the highest ROUGE-1, ROUGE-2, and ROUGE-L scores among all the summarization methods.
The word similarity score for the "compositional" model on the "SYN" task is [BOLD] 0.735.
The word similarity score for the "compositional+positional" model on the "MIXED" task is [BOLD] 80.5.
The "compositional+positional" embedding has the highest Spearman's correlation among the different embeddings.
The "word2vec" embedding has a higher Spearman's correlation than the "Mitchell08vector-basedmodels" embedding.
The LSTM model with augmented data (s) outperforms the LSTM model with unmodified data (¯¯¯s) in terms of BLEU, NIST, METEOR, and ROUGE scores.
The CNN model with augmented data (s) outperforms the CNN model with unmodified data (¯¯¯s) in terms of BLEU, NIST, METEOR, and ROUGE scores.
As the number of slots in the corresponding MR increases, the average number of sentences in the reference utterance also increases.
The proportion of MRs with 3 slots is higher than the proportion of MRs with 4 slots.
The BLEU scores for the TV dataset are higher than the BLEU scores for the Laptop dataset.
The slot error rates for the LSTM model are higher than the slot error rates for the SCLSTM model.
CNN has the highest PR AUC value among all the models.
SAME has the highest F1 score among all the models.
The accuracy for the semantic parser in the Basketball domain using the CNN model is 0.452.
The accuracy for the semantic parser in the Calendar domain using the Surprise model is 0.679.
The CNN model performs the best in identifying domain-adjacent instances in the Basketball category.
The Surprise model performs the best in identifying domain-adjacent instances in the Publications category.
The Surprise model performs better in the Basketball domain compared to other domains in terms of AUC score.
The PreTrained model performs worse in the Calendar domain compared to other domains in terms of AUC score.
The "Reuters-8" dataset has 8 classes and the "20-Newsgroup" dataset has 20 classes.
The "Sexual predator detection" task has 6588 terms when a reduced vocabulary is used.
The F1 score for the Joint RNN Model in boundary detection on persuasive essays is 0.873.
The F1-B score for the Bi-LSTM-CRF (with knowing argumentative status) model in boundary detection on persuasive essays is 0.976.
The DDTM model has the lowest perplexity among all the models.
There are three different models evaluated in the table.
The average BLEU scores for RNNSearch (16K), phraseNet gate (16K), and phraseNet softmax (16K) are 14.97, 16.47, and 15.42, respectively.
Mitsuku performs better than Meena and Human in terms of the "Interesting" quality.
Meena performs better than Mitsuku and Human in terms of the "Overall" quality.
The qualities "Interesting", "Relevant", "Fluent", "Coherent", "Likeable", and "Understanding" are the most important qualities for predicting the overall impression at both turn-level and dialog-level.
The quality "Relevant" is the most important quality for predicting the overall impression.
SyncMap has the highest computation time among SyncMap, PARSER, Word2vec, and MRIL.
PARSER has the lowest computation time among SyncMap, PARSER, Word2vec, and MRIL.
Models 4A, 4B, 4D, and 4E do not use joint CTC-CE alignments, while model 4C uses joint CTC-CE alignments.
Model 4E achieves the lowest word error rate (WER) of 11.7% on the SWB dataset and 20.2% on the CH dataset among the models listed in Table 4.
The WER for the 2A UniLSTM model is higher than the WER for the 2B BiLSTM model.
The WER for the 2C UniLSTM distilled from model is not provided, but the WER for the 1× BiLSTM (2B) model is provided.
The sharing policy for self-attention in the San model is uniform across all attention heads.
The baseline model achieves a BLEU score of 27.52.
The "Random Forests" model has the lowest mean squared error for the "Nonverbal" feature.
The mean squared error for all features is the same for all models.
The denotation accuracy of Seq2Seq on the ATIS dataset is 85.2%.
The denotation accuracy of Inan et al. (2019) on the Restaurants domain is 91.0%.
The "+ Denoising" system achieves the highest Bleu scores for de-en, en-de, and fr-en translations.
Artetxe et al. (2018) only provide translation results for the fr-en language pair.
The DNC model performs better than the EntNet model in terms of the number of failed tasks.
The DNC model has a lower mean error than the EntNet model.
The EntNet model has the lowest mean error among all models.
The "EntNet (simple)" model achieves the highest accuracy on the CBT test set.
The NSE Adaptive Computation model achieves the highest accuracy among the multi-pass models on the CBT test set.
The EntNet model has the lowest number of failed tasks among the three models.
The EntNet model has the lowest mean error rate among the three models.
The MemN2N model failed on 11 tasks out of the 20 bAbI tasks.
The EntNet model has a higher mean error (29.6) compared to the MemN2N model (13.9) on the bAbI tasks.
The MemN2N model failed on 11 tasks out of the 20 bAbI tasks.
The EntNet model has a higher mean error (29.6) compared to the MemN2N model (13.9) on the bAbI tasks.
Our approach achieves an accuracy of 0.831.
Our approach correctly identifies 441.6 positive instances.
The "Stack+POS" model has the highest F1 score among all the models.
The "Stack+POS" model has a recall score of 0.689.
The "Joint large" model has the highest F1 scores for both aspects and opinions compared to other models.
The F1 score for the "Aspect only" model is higher than the F1 score for the "Klinger2014" model.
The F1 score for aspect-opinion relation extraction using our approach is 0.81.
The F1 score for aspect-opinion relation extraction using Klinger2014 is 0.65.
The accuracy of Google Translate is 73%.
The accuracy of version 2 of the system is 83.2%.
The table provides n-gram counts for version 2 of a system and 1/40% of the WMT08 News Commentary dataset.
The count of 2-grams in version 2 of the system is 8,324.
The DRRD model outperforms the baseline models in terms of accuracy and F1 score for both Rumour and Non-Rumour classes.
The CAMI model performs better than the other models in terms of precision, recall, and F1 score for the Non-Rumour class.
The WER for the AT/S (λ=0.25) adaptation method is 12.49.
The WERR for the Seq T/S unsupervised adaptation method is -0.5.
SesameBERT achieves the highest dev set accuracy in the Second-to-Last Hidden layer.
BLUR-BERT has lower dev set accuracy compared to other approaches in all layers.
BERT-PR achieves the highest MAP score on the WikipassageQA dataset.
GM (noise) achieves a P@5 score of 20.70 on the InsuranceQA_v2 dataset.
The "GM" method has the highest AUC score for the WikipassageQA dataset.
Table 8 shows the Pearson correlation (%) for the STS benchmark test set.
The LSTMGaussian model achieved the highest Pearson correlation (%) on the STS benchmark test set.
The WLO method has the highest accuracy in both the Full and Length norm. conditions.
The accuracy of the Majority baseline is 54.6%.
The percentage of cases where the hypothesis has larger entropy than the premise is 39.6% for the GloVe model in the Contradiction category.
The percentage of cases where the hypothesis has larger entropy than the premise is 78.3% for the ELMo model in the Entailment category.
The table presents the accuracies and class-wise precision, recall, and F-Measure of predicting orientation using different features.
The recall for the "All-main." category is 1.00.
Left-wing publishers have a higher average number of links per article compared to right-wing publishers.
The total number of false fact-checking results is higher than the total number of true and mix fact-checking results combined.
Table 2 presents the accuracies of predicting hyperpartisanship vs mainstream under different training sets.
The precision for predicting all articles as hyperpartisan is 0.49.
The recall for predicting the main class is 0.74.
The table compares the performance of predicting fake news using two different types of classifiers, the "Generic classifier" and the "Orientation-specific classifier".
The "All-fake" classifier has a low precision for predicting fake news and a low recall for predicting real news.
The precision for predicting satire is 1.00, indicating that all instances predicted as satire are actually satire.
The class-wise precision for satire in the Rubin et al. experiment is 0.90, which is higher than the precision for predicting satire in the All-sat. experiment.
The embedding size for the RTE task is 100.
The regularization value for the MQA task is 5 [ITALIC] E−5.
The "three stacked LC-LSTMs" model achieves the highest P@1(5) and P@1(10) scores among all the models.
The "TC-LSTMs (Single Direction)" model achieves lower P@1(5) and P@1(10) scores compared to the "three stacked LC-LSTMs" model.
The "bi-LSTM with attention" model has the highest performance in terms of accuracy, precision, F1 score, and AUC among all the neural network models.
Adding the "title" to the input improves the performance of the neural network models.
Different combinations of features were used for the logistic regression models.
The logistic regression model achieves the highest accuracy when it includes both the title and username as input contents.
The best logistic regression model has a higher recall than the character baseline model.
The average score ensemble model has a higher accuracy than the best neural network model.
Our model with pre-trained word embeddings achieves the highest BLEU 1 score among all the systems.
The DirectIn system achieves a BLEU 4 score of 11.18.
Our method performs significantly better than H&S in terms of average rank.
Our method outperforms H&S in terms of the best percentage.
The "Ours + paragraph" method performs the best in terms of BLEU-4 score among all the methods.
The "Ours + paragraph" method performs the best in terms of METEOR score among all the methods for questions generated with paragraphs.
UBS outperforms other methods on the Precision@1 metric in both the Semantic Analogies and Syntactic Analogies tasks.
UBS outperforms other methods on the similarity task for the WS353 dataset.
The model "XLNet-Base" has the highest accuracy on Task 4A of SemEval 2017.
The model "Bert-Large" has an accuracy of 0.687±0.015 on Task 4A of SemEval 2017.
The models listed in Table 2 are ordered based on their accuracy on the Yelp dataset.
The number of parameters increases as we move from left to right in Table 2.
The number of utterances decreases from the training set to the development set and then to the test set.
The number of frames decreases from the training set to the development set and then to the test set.
The average length of training sentences tokenized with the "Byte 256" vocabulary is higher than the average length of training sentences tokenized with the "BBPE 1K" vocabulary.
The average length of training sentences tokenized with the "BBPE 11K" vocabulary is 89.
There are three transformer models used in the experiments: Tflores, Tbase, and Tbig.
Tbig has the highest number of parameters among the three transformer models.
Table 6 shows the Ja-En test BLEU scores for different models and datasets.
The BPE 16K+ model performs better than the Byte+ and BBPE 4K+ models in terms of Ja-En test BLEU scores.
The Tuned + Template Synthetic model achieves a ROUGE-1 CS score of [ITALIC] 39.12.
The Summarizing with Template Model achieves a ROUGE-2 ENGR score of [ITALIC] 11.2.
The performance of the "Oracle" system is better when trained on Channels 1-4 compared to when trained on Channel 4 only.
The "Oracle" system has a lower Word Error Rate (WER) than the "Baseline" and "First pass" systems.
The Italian corpus contains 174,940 utterances.
The French corpus has a duration of 157 hours.
The values in the second column ([ITALIC] λ) are increasing in a geometric progression with a common ratio of 2.0.
The lowest value in the third column ([ITALIC] λ) is [BOLD] 68.3.
Table 4 shows the results of adaptation on Italian data with f=2, λ=2.0.
As the number of hours decreases, the Word Error Rate (WER) increases.
Table 5 shows the results of adaptation on French data with f=2, λ=2.0.
As the number of hours decreases, the Word Error Rate (WER) increases.
The table provides accuracy breakdowns for two different methods: "Training AC-Seq2Seq" and "Collaborative-only baseline".
The "Training Seq2Seq" method performs better than the "Collaborative-only baseline" method on the MTD limit test set.
The table provides accuracy breakdown by output length for two different training methods: Training AC-Seq2Seq and Training Seq2Seq.
The accuracy for the MTD limit without a model is 0.601 for output length 1.
Table 6 presents the results of an ablation study on four different model variants.
The sparsity levels in the table range from 70% to 40%.
The table presents the performance of subnetworks at the highest sparsity for which IMP finds winning tickets on each task.
The performance of the subnetworks with IMP pruning and pre-trained weights is higher than the performance of the full BERTBASE model on MNLI, QQP, and STS-B tasks.
Adjectives have higher values than Nouns in both the ANT and SYN categories.
Verbs have lower values than Adjectives in both the ANT and SYN categories.
Table 1 displays the accuracy in percentage when combining the ℓ2-norm of weights and embeddings (Experiment I).
The highest accuracy of 64.00% is achieved when the value of λW is 3⋅10−4.
The bold numbers in the table represent the highest accuracies for each combination of λW and λembed.
As the value of p increases, the accuracy decreases.
FastSpeech.v2 has the lowest CER among all the methods.
Tacotron2.v3 has the lowest insertion rate among all the methods.
CKRL (LT) performs better on FB15K-N2 compared to FB15K-N1 and FB15K-N3.
CKRL (LT+PP) performs better on FB15K-N1 compared to FB15K-N3.
Table 2 provides evaluation results on entity prediction.
CKRL (LT) outperforms TransE in terms of Hits@10% for FB15K-N1, achieving 45.5 for the "Raw" metric and 61.8 for the "Filter" metric.
The table shows the results of different models used in the experiment.
The F-score when using an oracle threshold is higher than the F-score when using thresholds optimized on pseudo-labels.
The "Dec Auto 256 + Prob" model with Giza EN-DE alignment has the highest F-score when using the oracle threshold.
The comparison between {door} and {curtain} with attribute words {girlish} and {boyish} has an inconclusive outcome.
The comparison between {dog} and {cat} with attribute words {actress} and {actor} has a large negative effect size.
The Transformer method outperforms the other methods in terms of ROUGE-1, ROUGE-2, and ROUGE-L scores.
The Bert Abstractive method generates summaries with a lower average number of duplicates compared to the other methods.
The table shows the test accuracy over MTL-16 datasets.
The average test accuracy across all datasets is 86.34%.
The "Multi-Scale Transformer" model has the highest accuracy of [BOLD] 51.9.
The "Emb + self-att" model has the lowest accuracy of 48.9.
The "Residual encoders" model has the highest accuracy of 86.0.
The "Multi-Scale Transformer" model has a higher accuracy of [BOLD] 85.9 compared to the "Transformer" model.
The "Korean RoBERTa (large)" model achieves the highest performance on both KorNLI and KorSTS.
The "Korean RoBERTa (large)" model performs better on KorNLI compared to the "Korean RoBERTa (base)" model.
The average number of tokens per utterance is higher in the train dataset compared to the dev dataset.
The average number of tokens per sentence for subject facts is lower in the dev dataset compared to the test dataset.
The performance of the model in the "Q → AR" task is [BOLD] 57.76.
The performance of the model in the second-stage pre-training is [BOLD] 77.03.
The table presents the results of experiments on two-stage pre-training for VCR using three different settings: Triplet, Pair, and Pair-biattn.
The Pair-biattn setting achieves the highest performance in both the dev and test-P evaluations.
The first axis in the local chart of the word "notebook" corresponds to different types of books for writing a note.
The second axis in the local chart of the word "power" corresponds to physical energy.
The "LM-Rev" model has a higher perplexity than the "LM" model for all granularities.
The "LM-All (scratch)" model has a lower perplexity than the "ILM (scratch)" model for all granularities.
The ILM model achieves a higher score than all of the other models.
The score increases from BERT to SA to LM to ILM.
Table 3 shows the document infilling perplexity (or language modeling) of ILM and various baselines initialized either from scratch or from a pre-trained checkpoint across three datasets.
The perplexity for ILM initialized from scratch is 30.8 for Sto dataset, 45.3 for Abs dataset, and 30.6 for Lyr dataset.
The performance of the model without visual grounding is lower than the performance of the model with grounding using human-based cues on both the VQA-CPv2 Train and VQA-CPv2 Test datasets.
The performance of our regularization method is lower on the VQAv2 Val dataset but higher on the VQA-CPv2 Train dataset compared to the model with grounding using variable random cues.
The table presents the p-values from the Welch's t-tests and the percentage of overlap between the predictions for different variants of HINT and SCR.
The overlap percentage between the predictions of the Default variant and the Baseline is 83.6%.
The "SCR" method performs better than the "HINT" method across all grounding types.
The "Ours100%" method has a higher accuracy than the "Ours1%" method.
For the subcategory "Lexical Overlap" and the heuristic "subject/object_swap", MT-DNN large performs better than BERT large.
For the subcategory "Subseq" and the heuristic "PP_on_subject", BERT base performs better than other models.
BERT large and MT-DNN large have the highest accuracy scores among all the models.
BERT large has a higher accuracy score than BERT base.
BERT large and MT-DNN large have the highest accuracy scores among all the models.
BERT large and MT-DNN large have higher accuracy scores than BERT base and MT-DNN base.
Our model outperforms poesio-etal-2019-crowdsourced in terms of average F1 score for both "Singletons included" and "Singletons excluded" cases.
Our model achieves a higher F1 score than poesio-etal-2019-crowdsourced on the CEAF ϕ4 metric for both "Singletons included" and "Singletons excluded" cases.
Table 1 provides the hyperparameters for the models.
The BERT embedding size is 1024 and the last 4 layers are used.
The F1 scores for the "Our model" in the "Contextual Embeddings" row are consistently higher than the F1 scores for all other models.
The F1 scores for the "Our model" in the "Fine-tuned on BERT" row are higher than the F1 scores for the "Pre-trained" and "Contextual Embeddings" models.
The F1 score is higher for the "LARGE" type of BERT compared to the "BASE" type.
The F1 score is highest for the "LARGE" type with 18 layers.
The best overall performance is achieved after fine-tuning on all tiers.
Performance on each tier-specific subset is best on the last tier used for training.
The tiers in Table 1 are selected based on the difficulty level of the games, with tier 1 being the simplest and tier 6 being the most difficult.
The maximum scores achieved in the games increase as the difficulty level of the tiers increases, with tier 6 having the highest maximum score.
Table 4 provides a breakdown of information conversion strategies of tier-4 to tier-6 on both Test 1 and Test 2 with the best model after curriculum learning on tier-6, without the final fine-tuning.
The "go-room" method performs the best for both Test 1 and Test 2.
Table 7 provides performance stratified by question turns on the development set.
The "CD-Seq2Seq" model outperforms the "SyntaxSQL-con" model when the question turn is greater than or equal to 4.
The table shows the performance of two models, CD-Seq2Seq and SyntaxSQL-con, stratified by question difficulty.
The performance of CD-Seq2Seq decreases as the question difficulty increases.
The table provides the hyper-parameter settings for the extraction model.
The extraction model has 3 labels: 'O', 'B-software', and 'I-software'.
The table shows the results of different methods used for transfer and active learning in the restaurant genre.
When training on the target dataset alone, the model achieves perfect precision, recall, and F1 scores in the restaurant genre.
The table shows the results of three different methods for transfer learning: "Train on Source", "+Adaptation", and "Train on Target".
The "+Adaptation" method has a higher F1 score on the DBLP-Scholar dataset compared to the "Train on Source" and "Train on Target" methods.
SVM outperforms DL on both DBLP-ACM and DBLP-Scholar datasets.
Increasing the training size from 7,417 to 30,000 improves the performance of DL on the Cora dataset.
The combination of Probase and list-wise ranker achieves the highest semantic similarity score at 3.
The combination of Probase and pair-wise ranker achieves the highest F1 score at 3.
Table 2 presents an end-to-end comparison of different methods on the test set.
The DS(list-wise) method achieves the highest scores for all evaluation metrics in Table 2.
There is a significant difference between the groups in the model based on the ANOVA table.
The sum of squares for the corrected total is 0.38481745 based on the ANOVA table.
The Factorization factor has a significant effect on the similarity task.
The Interaction factor has the highest mean square value among all the factors.
The ANOVA table provides information on the sources of variation in the Analogy task results.
The Model source has a significantly higher F value compared to the Error source in the ANOVA table.
All values in the table are statistically significant.
The highest correlation value in the table is 0.215.
The Rouge-1 score for "Rnn+Attn" is higher than the scores for "Trans" and "Rnn".
The Rouge-L score for "Rnn+Attn" is higher than the scores for "Trans" and "Rnn".
The correlations between the extractive score and model performance are stronger than for the Wikipedia corpus for all models.
The "Trans" model achieves the highest Rouge-1 and R.-L scores.
The Rnn+Sc model performs the best on all metrics.
The Rnn+Sc model has the highest homogeneity score among all the models.
The table compares the F1 scores of different datasets to previously reported scores.
Our F1 scores are higher than the previously reported scores for most of the datasets.
The average change in F1 scores for each fine-tuned model across all test sets is negative.
The model fine-tuned on NQ achieved the highest F1 score on SQuAD.
Table 6 shows the F1 scores of answered questions as models are fine-tuned on increasingly noisy data.
The F1 scores of answered questions decrease as the percentage of random labels increases for each dataset.
Table 7 shows the F1 scores on test sets with shuffled context sentences for different datasets.
The F1 scores decrease, but not dramatically, on test sets with shuffled context sentences for all datasets.
Table 8 shows the F1 scores for different datasets on test sets with incomplete input.
Table 9 shows the F1 scores on test sets where a filler word is added to the question.
The F1 scores slightly decrease on test sets where a filler word is added to the question.
Table 10 provides information about the performance of different models on test sets with negative questions.
Table 11 provides information about the fraction of questions in the training set that include "n't" or "never" and how many of those questions are impossible.
85% of the questions in the SQuAD dataset that include "n't" or "never" are possible.
The model "BBC-0.2" has the highest rank among all the models.
The model "Marx-0.1" has the lowest diversity score among all the models.
Table 2 shows the information-coverage scores for different sources.
The information-coverage score for BBC News is higher when lambda is 0.3 compared to when lambda is 0.1 and 0.2.
The table provides topic-diversity scores for BBC News and Factiva-Marx for three different values of λ (0.1, 0.2, and 0.3).
Factiva-Marx has higher topic-diversity scores than BBC News for all three values of λ (0.1, 0.2, and 0.3).
The scholar model performs better when using covariates on the IMDB dataset compared to the other datasets.
The logistic regression model performs better on the IMDB dataset compared to the other datasets.
The "scholar + w.v." and "scholar" models have the highest values for NPMI (ext.) among all the models listed in Table 1.
The "SAGE" model has the highest value for sparsity among all the models listed in Table 1.
The table shows the performance of various models on the 20 newsgroups dataset with 20 topics and a 2,000-word vocabulary.
The SAGE model has the highest sparsity value among all the models in the table.
Table 5 presents the performance of various models on the Yahoo answers dataset with 250 topics and a 5,000-word vocabulary.
The "scholar - b.g." and "scholar + w.v." models have higher external NPMI values compared to the other models.
Table 1 shows the open-world entity prediction results on DBPedia50k and DBPedia500k.
The ConMask model outperforms other models in terms of Mean Rank (MR) and Mean Reciprocal Rank (MRR) on both DBPedia50k and DBPedia500k datasets.
TransE performs poorly on DBPedia500k Head with a Mean Rank (MR) of 10034.
ConMask achieves a low HITS@10 score of 0.20 on FB15k Tail.
Table 1 provides information about the number of collected sentences and words in the JParaCrawl corpus.
The filtered version of the JParaCrawl corpus has 8,763,995 sentences and 196,084,272 words.
ASPEC has the highest number of sentences and words compared to the other datasets.
Table 3 provides information about the number of sentences and words in the English side of the training sets.
Table 6 provides information about the time required to train an NMT model for English-to-Japanese.
The time required to train an NMT model without pre-training is higher for bigger datasets compared to smaller datasets.
The "Xavier" weight initialization method is used for all models.
The minibatch size for the "Different captions" model is 68.
The "Different captions" row has a different activation function compared to the other rows.
The "Different captions" row has a smaller minibatch size compared to the other rows.
The average reward for agents trained in the same-image games on the test set is 72.
The average reward for agents on the noise test set is 95.
The table shows the accuracy of the TRADE and SUMBT models on the full MultiWOZ 2.1 dataset (test set), with and without synthesized data.
The table compares the accuracy of the TRADE and SUMBT models on the full MultiWOZ 2.1 dataset (test set) with and without synthesized data.
The accuracy of the SUMBT model with zero-shot learning using the Dialogue-Model based data synthesis is 52.8% for the Attraction Slot.
The accuracy of the TRADE model with zero-shot learning is 74.0% for the Train Joint.
MatchPyramid has the highest accuracy among all the neural text matching models.
ARC I has a higher accuracy than ARC II.
The "Two-channel lexical and semantic text matching" model has the highest accuracy among the three models.
The "Single-channel semantic text matching" model has a higher accuracy than the "Single-channel lexical match" model.
The average agreement for the "Connective" category is 57.0%.
The percentage of examples with connectives is higher than the percentage of all examples for the "Single / pair", "Anaphor", and "Connective" categories.
The final rater selection was determined by taking the majority of raters' evaluations.
The understandability of the text after splitting is higher for the "Yes" rater selection compared to the "No majority" and "No" rater selections.
DfSport outperforms DfWiki on all discourse phenomena.
DfSport performs better than DfWiki on most discourse phenomena.
The connective "because" has the highest accuracy for both DfSport and DfWiki.
The top 3 connectives for DfSport are "and, but, ⟨other⟩" and for DfWiki are "and, ⟨other⟩, but".
Table 10 shows the fusion results on WebSplit, measured by SARI and the F1 scores that compose it.
The SARI score for the fusion method "DfWiki + WebSplit" is [BOLD] 44.2.
The table compares different transformation functions with ComplEx-OWE-300 on the FB15k-237-OWE dataset without target filtering.
The "Affine" transformation function has higher MRR Filt., MRR Raw, HITS@ 1, HITS@ 3, and HITS@ 10 values compared to the other transformation functions.
The "ConMask" model outperforms other models on both DBPedia50k and FB15k-237-OWE datasets.
The "Cmplx-OWE-300" model performs better than other models on the FB20k dataset.
The "DistMult-OWE" model achieves the highest HITS@10 score among all the models.
The "ComplEx-OWE" model outperforms the "TransE-OWE" model in terms of HITS@3 score.
The confusion matrix in Table 8 shows the distribution of instances across different levels (A2, B1, B2, C1, C2).
Level B2 has the highest number of instances in the confusion matrix in Table 8.
The number of documents in the modified corpus decreases as the age group increases.
The number of documents in the modified corpus is lower than the number of documents in the original corpus for all age groups.
The classification model has 5 levels.
In the C2 category, there are no instances classified as levels 1, 2, 3, or 4.
The linear SVM model has the highest accuracy score among all the mapping functions.
The polynomial regression model has the highest Pearson correlation coefficient (PCC) score among all the mapping functions.
Table 7 provides results of domain adaptation from native to language testing data.
EasyAdapt achieves a pairwise ACC of 0.933 and a PCC of 0.905.
The accuracy of self-training is higher than the accuracy of L2 data only.
The Pearson correlation coefficient of self-training is higher than the Pearson correlation coefficient of L2 data only.
Tabela 1 shows the evaluation results of different model configurations.
The LSTM várias camadas + reg L2 + features model configuration has a similarity score of 0.23.
The table shows the BLEU-4 scores (%) of Dep2str, + tagCNN, and + tagCNN_dep systems on the MT04 and MT05 datasets.
The + tagCNN_dep system has a higher average BLEU-4 score than both the Dep2str and + tagCNN systems.
The Dep2Str system has the lowest average BLEU-4 score among all the systems.
The inCNN-8-pooling model has the highest BLEU-4 score among the inCNN models with different pooling strategies.
The model used for fine-tuning is "Seq2Seq-text".
As the percentage of train_asr.enc data increases, the BLEU score increases.
The "\modelname{}multi-only" model achieves the highest F1 score on the ACE05 dataset.
The "\modelname{}full" model outperforms the "\modelname{}bi-only" model on the CTS subset of the ACE05 test set.
The models in the table are categorized into non-ensemble models and ensemble models.
The performance of the \modelname{}full model is higher than the performance of the \modelname{}bi-only and \modelname{}multi-only models.
LE-expert has the highest values for P@5, P@10, P@20, NDCG@5, NDCG@10, NDCG@20, and MAP among all the measures.
JointHyp has the highest value for MAP among all the measures.
The "+FT(OPUS)" adaptation method consistently performs better than the other methods across all four TED datasets.
The "+FT(OPUS)" adaptation method has the highest average performance across all four TED datasets.
The "+Parallel" system performs better than the "ID" system on all ted-11, ted-12, ted-13, and ted-14 datasets.
The "+NNJM(FT)" system has the highest average score among all the systems.
Freezing the embeddings during fine-tuning results in a decrease in the average performance of the systems.
Fine-tuning 30% of the embeddings improves the performance of the system.
LR-LSTM achieves the highest performance on the TREC dataset.
CNN-non-static achieves the highest performance on the TREC dataset.
Capsule-B achieves the highest F1 score on the Reuters-Multi-label dataset among all the models.
Capsule-B achieves the highest precision on the Reuters-Full dataset among all the models.
KNV+RNNV (Ours) performs better than KNV on both morphs and characters.
KNV+RNNV (Ours) performs better than KNV on characters.
The table displays length rates for frequent words and out-of-vocabulary words in the Arabic and Finnish datasets.
The OOV rate for single-character subwords is lower than the OOV rate for multi-character subwords in both the Arabic and Finnish datasets.
The "+ Copy" model achieves the highest ROUGEL score for Arabic.
The "Ours" model achieves a higher BLEU 2 score than the "IRext" model for Esperanto.
The generative accuracy of GRU and LSTM with k=10 is the same.
LSTM has a lower number of epochs compared to GRU.
The table compares the performance of the PRE. and JOINT models in captioning tasks.
The JOINT model achieves higher performance than the PRE. model in captioning tasks for Multi30K images in English.
The TextRank (undirected) method has the highest average score for keyword extraction.
The tf-idf method has the highest standard deviation per user for keyword extraction.
Table 2 shows the average precision on the dev set for different configurations of stacked layers and fully connected layers using classifier-based embeddings.
The table provides results for different models including MFCCs + DTW, Corr. autoencoder + DTW, Classifier CNN, Siamese CNN, Classifier LSTM, and Siamese LSTM.
The table provides the average precision values for each model including MFCCs + DTW, Corr. autoencoder + DTW, Classifier CNN, Siamese CNN, Classifier LSTM, and Siamese LSTM.
The "Prior Name" model has the lowest BPE PPL and the highest ROUGE-L score.
The "Prior Tech" model has the highest Distinct-1 percentage.
The models in Table 2 are sorted by average performance.
SkipFlow LSTM* (Tensor) is the best performing model in all datasets/prompts.
The class distribution for HC and KM is different for each class.
The class distribution for svcR is the same for all three classes.
The error rate decreases from column (A) to column (D).
Column (C) and column (D) both use the "max. Margin" loss function.
The table compares the performance of three different models for dependency parsing on the PTB dev set.
The s-mtl(2) model outperforms the other models in terms of both UAS and LAS on the PTB dev set.
The "d-mtl-aux" model achieves the highest Dependency Parsing UAS score for each language.
The "d-mtl-aux" model achieves the highest Constituency Parsing F1 score for each language.
The performance of the TransResNet architecture is improved when using the multimodal combiner (MMC) for all three datasets: COCO, Flickr30k (Fl30k), and Image Chat (IC).
The combination of ResNeXt-IG-3.5B features with MMC and Faster R-CNN features improves the performance of the TransResNet architecture for all three datasets: COCO, Flickr30k (Fl30k), and Image Chat (IC).
As the size of the COCO training set increases, the accuracy on the COCO test set also increases.
Multi-task learning improves the accuracy on the COCO test set compared to single-task learning.
There are two models with the RBF Kernel and two models with the Linear Kernel in Table 6.
The model with the [ITALIC] pos−1 feature has the highest DEV MAP score in Table 6.
The GR baseline model outperforms the Sim. and TK models in terms of MAP score on both the DEV and TEST datasets.
The TK + pos-1 model achieves higher AvgRec scores than the Sim. + pos-1 model on both the DEV and TEST datasets.
The Reinforced Ranker-Reader (R3) model achieves the highest F1 score in the Quasar-T dataset.
The Single Reader (SR) model achieves a higher F1 score than the Simple Ranker-Reader (SR2) model in the SQuADOPEN dataset.
The ranker from R3 performs better than the ranker from SR2.
The addition of the ranker from SR2 improves the performance of the single reader model.
The "auto" lexicon achieves the highest BLEU score in the KFTT dataset.
The "hyb" lexicon achieves the highest NIST score in the KFTT dataset.
The "auto-bias" system outperforms the "attn" system in terms of BTEC BLEU, BTEC NIST, and BTEC RECALL.
The "hyb-bias" system performs better than the "attn" system in terms of KFTT BLEU.
The table represents the results of the BTEC experiment.
The BLEU bias scores for the "auto", "man", and "hyb" lexicons are higher than the other lexicons.
Category A has more blow-up words than category B.
Category D has a longer average reading time than category C.
The table presents the results of different methods for training and fine-tuning.
The accuracy of the "CNN+GRU+FiLM" method decreases after fine-tuning.
The models listed in Table 1 are ordered based on their overall accuracy in descending order.
The model "CNN+GRU+FiLM" outperforms all other models in terms of comparing attributes.
The "CNN+GRU+FiLM" model achieves the highest test accuracy after fine-tuning on CLEVR-Humans data.
Fine-tuning on CLEVR-Humans data improves the test accuracy of all models.
The answer to the question "Are there as many gray things as cyan things?" is "Yes".
The answer to the question "Are there fewer gray things than cyan things?" is "Yes".
Table 3 shows the influence of context filtering on disambiguation in terms of F-score.
The F-score is higher in the "Full TWSI w2v" column compared to the "Balanced TWSI JBT" column.
The FA-NMT model outperforms the NMT model in translating from German to English in both the wmt16 and wmt17 datasets.
The SA-NMT model performs better than the FA-NMT model in translating from English to German and English to Russian in the wmt17 dataset.
The proposed method achieves the highest scores in all metrics compared to the other methods.
The scores in the "B4" metric increase from "SrcOnly" to "TgtOnly" to "All" to "FineTune" to "Dual" to "Proposed".
The proposed method achieves the highest performance on B1.
The FineTune method achieves a performance of 26.2 on the C domain.
The "FineTune" method outperforms the "All" method in domain adaptation from Flickr30K to MSCOCO dataset.
The "Dual" method achieves the highest performance in domain adaptation from Flickr30K to MSCOCO dataset.
Table 2 provides information about the logical form coverage of Dnat by Don for different values of D in the Scholar and GeoQuery datasets.
The logical form coverage of Dnat by Don decreases as the value of D decreases in both the Scholar and GeoQuery datasets.
The table shows the results for De-En and Cs-En translation tasks on the WMT'15 dataset.
Deep Fusion achieves the highest scores in the De-En and Cs-En translation tasks on the WMT'15 dataset.
The NMT+LM (Deep) model outperforms the previous best models on all the test sets.
The NMT model performs worse than the previous best models on all the test sets.
Each model in Table 4 uses different combinations of information for relation extraction.
The CR-CNN model with word position embeddings achieves the highest F1 score of 54.1.
The table presents the results of Named Entity Recognition on the proposed corpus using the Bi-LSTM and CRF models.
The model's performance improves when augmentation is used in both training and inference compared to when no augmentation is used or when augmentation is only used in training.
The model's performance is consistently better in the uncased setting compared to the cased setting, regardless of the use of augmentation.
The supervised model achieves a BLEU score of 38.38 on the Multi30k-Task1 en-fr dataset.
Our model achieves a BLEU score of 14.31 on the WMT fr-en dataset after the 3rd iteration.
The highest scores in the ablation study are achieved for the en-fr and fr-en language pairs.
The scores decrease when λcd is set to 0 in the "Without pretraining" condition.
The model "KN" has the highest perplexity score on the LTCB test set.
The model "WD-SRNN [M*200]-600-80k" has the lowest perplexity score on the LTCB test set.
The FNN model performs better than the WI-SRNN model.
The Deep RNN model has the same performance as the LSTM model.
Our Framework outperforms Vision-Only in both Shape and Style tasks for all the objects.
Adding extra parameters improves the performance of Our Framework for all the objects.
The recall percentage is higher for the test set compared to the validation set for all the different configurations.
Table 6 provides information about the number of training, validation, and test queries generated for different query structures.
The number of training queries for 1p and others in FB15k is the same.
The average H@3 score for gqe on FB15k is 0.228.
The average H@3 score for gqe-double on FB15k-237 is 0.23.
The table compares the performance of three different methods on the simple link prediction task on three datasets.
Among the three methods, query2box-1p achieves the highest performance on the FB15k dataset in terms of H@3.
T-CNN achieves the highest accuracy among all the models for both the fine-grained and binary tasks.
LSTMN and 2-layer LSTMN have the same accuracy for the binary task, but LSTMN has a slightly higher accuracy for the fine-grained task.
Table 1 displays the language model perplexity on the Penn Treebank for various models with a memory size of 300.
The perplexity of the LSTMN model decreases as the number of layers increases from 1 to 3.
As the hidden unit size of the LSTMN deep fusion model increases, the model size and accuracy also increase.
Increasing the hidden unit size of the LSTMN shallow fusion model does not always lead to an increase in accuracy.
Increasing the context window size leads to a decrease in WER.
The WER is higher for the "test other" data compared to the "test clean" data.
The GFLOPS decreases as the reduction factor increases.
The model accuracy is not affected by the kernel size.
ContextNet has fewer parameters than TDNN.
ContextNet has a lower Word Error Rate (WER) than TDNN.
Table 9 provides the parameters of a neural CRF sentence alignment model.
The neural CRF sentence alignment model has 768 hidden units and 12 layers.
"Infersent" and "ESIM" perform better than "BERT [ITALIC] embedding" and "BERT [ITALIC] finetune" on Task 1.
"Our CRF Aligner + gold ParaAlign" performs better than "Infersent" on Task 2.
Table 10 shows the thresholds used in paragraph alignment Algorithm 2 for Newsela data.
The thresholds in the paragraph alignment Algorithm 2 for Newsela data are sorted in increasing order.
The Lambada dataset was evaluated using different models.
The LSTM-1024 (our implem.) has the lowest test score among all the models.
The Neural cache model has the highest test score among all the models.
Table 6 shows the performance (sensitivity/specificity) of the SVM classifier trained over 14 days of smartphone/social media features (FEAT) compared against 3 naïve baselines.
The sensitivity for positive sentiment is 64.02% and the specificity for negative sentiment is 95.07%.
In LOIOCV, the accuracy for positive unique instances is higher than in MIXED, while the accuracy for negative unique instances is lower than in MIXED.
In LOUOCV, the accuracy for wellbeing personal instances is higher than in MIXED, while the accuracy for stress unique instances is lower than in MIXED.
Table 5 shows the impact of EM-style optimization on domain curriculum for deep models.
The Test Bleu score for the P8 model with EM-style optimization is 37.3 on IWSLT15.
Co-curriculum improves the performance on the IWSLT15 test dataset compared to the WMT14 test dataset for all curriculum types.
The difference in performance between the Cco-Cdomain curriculum and the Cco-Ctrue curriculum is smaller for the IWSLT15 test dataset compared to the WMT14 test dataset for all curriculum types.
Retraining with Cco results in higher Test Bleu scores than fine-tuning with Cco for both IWSLT15 and WMT14 datasets.
Retraining with Cco consistently outperforms fine-tuning with Cco in terms of Test Bleu scores for both IWSLT15 and WMT14 datasets.
Table 8 compares the performance of different retraining methods on two different test datasets, IWSLT15 and WMT14.
Models retrained with curriculum learning consistently outperform models retrained with a static, top selection on both the IWSLT15 and WMT14 test datasets.
The table shows the overall training time improvements for two methods: Baseline and PC+Grad+CG Speedups.
PC+Grad+CG Speedups method accessed less total data points compared to the Baseline method.
The table provides information about the total CG runtime for different quasi-Newton PC schemes.
The table compares different quasi-Newton PC schemes, including noPC, PC-16, PC-32, and PC-64.
The table shows the overall training time improvements for two methods: Baseline and PC+Grad+CG Speedups.
The PC+Grad+CG Speedups method reduces the total training time compared to the Baseline method.
The MinAvgOut model has the lowest average of Plausibility and Content Richness among all the models.
The MinAvgOut+RL model has higher Richness, Plausibility, and Average scores compared to the MinAvgOut model.
MinAvgOut has the highest average iAUC compared to other models.
The iAUC-1 value for LSTM (attn) is 0.310.
The CRAFT model without the Context Encoder achieves a higher F1 score on Reddit CMV compared to the Awry model.
The Cumul. BoW model achieves a higher recall on Wikipedia Talk Pages compared to the BoW model.
The method in "This work" achieves the highest F1 score on GENIA and the highest recall on ACE among all the methods.
The method using the "Discrete" approach achieves the lowest F1 score on both GENIA and ACE.
Table 4 shows the McNemar significance test results of MarkerEB against MarkerB.
The ensembling performs better when the p-value is within the range [0.005,0.05) compared to when the p-value is less than 0.005.
The cs-en language pair has a high correlation between BLEU and DA at the system level.
The de-en language pair has a moderate correlation between BLEU and DA at the system level.
The accuracy for the "ar" language when the swapped paraphrases share the same POS tag is 52.6%.
The accuracy for the "zh" language when the swapped paraphrases have different POS tags is 53.4%.
The model has a higher accuracy on the "zh" language compared to the "ar" language in the MNLI-1 test set.
The model performs better than the majority class in all languages in the MNLI test sets.
The total number of context sentences is highest in the "20-30" range.
The quality of the Bayes W emnlp method on the IMDb task is 83.62.
There are 17 non-constant gates in the LSTM layers for the IMDb task.
The "Translation Accuracy" values for "ben-hin" and "hin-mal" are the same for both "default (stack, tl=20,ss=100)" and "Cube Pruning" decoding methods.
The "Relative Decoding Time" for "Word-level" decoding is faster than the other decoding methods for all language pairs.
The translation accuracy for the "default (stack, tl=20,ss=100)" and "tl=10" settings is the same for the "ben-hin" translation.
The decoding time for the "tl=5" setting is the lowest for the "hin-mal" translation.
The F1 score for the HSLN-RNN model in the full model configuration is 92.6.
The F1 score for the HSLN-CNN model without attention-based pooling is 91.7.
The table compares the performance of different word embeddings for the HSLN-RNN model trained on the PubMed 20k dataset.
The F1-score for the HSLN-RNN model trained on the PubMed 20k dataset using the Word2vec-wiki+P.M. word embeddings is 92.6.
The English language performs better than the German language in both translation directions.
The English language performs better than the German language in the I → T translation direction.
Fine-tuning the original De+COCO model gives the highest scores for German I → T and German T → I.
The highest scores for German Sum are obtained when using the filtered version of the pseudopair set (+ pseudo 75%).
The recall for translation retrieval from English to German using the De + COCO model is 70.7%.
The recall for translation retrieval from English to German using the En + De + c2c model is 90.6%.
Table 6 provides a performance comparison on WMT14 En-De data and Transformer-base architecture, including both BLEU scores and the number of trainable parameters inside each position encoder.
The model architecture "FLOATER" achieves a BLEU score of 28.57 and has 526.3K trainable parameters.
Table 2 shows the experimental results of various position encoders on the machine translation task.
The FLOATER score is higher when the position encoder is only at the input block compared to other position encoders.
Table 5 provides experiment results on the SQuAD benchmark using the RoBERTa-large model.
The RoBERTa model achieves an F1 score of 94.6 on SQuAD 1.1.
The table compares the performance of different models on two datasets.
The WDec model performs the best among all the models on both the NYT29 and NYT24 datasets.
The combo model has the highest F1 score on NYT29.
The Single model has an F1 score of 0.607 on NYT29.
Table 6 shows the percentage errors for wrong ordering and entity mismatch for different models on the NYT29 and NYT24 datasets.
The HRL model has a percentage error of 0.2 for wrong ordering on the NYT29 dataset.
The table shows the evaluation scores for different models based on their relevance to a given video and grammatical correctness.
The GroundTruth model has the highest relevance score among all the models evaluated.
Table 4 shows the effect of increasing the number of languages on the zero-shot performance of multilingual models.
The zero-shot performance for the language pairs De→Fr, Be→Ru, Yi→De, Fr→Zh, Hi→Fi, and Ru→Fi is 11.15, 36.28, 8.97, 15.07, 2.98, and 6.02, respectively.
The SDM model improves the performance of the basic additive models.
The smoothed vector addition baseline performs better than the CBOW model.
Table 2 shows the results for the Vector Addition Baseline, Smoothed Vector Addition Baseline, LSTM, and the Structured Distributional Model (SDM) on the RELPRON development set.
The R&al. model achieves a Mean Average Precision score of 0.50 for the word combination "head noun+verb+arg".
As the dimension of the vectors increases, the correlation values for both the Additive and SDM models also increase.
The SDM model consistently outperforms the Additive model in terms of correlation values.
The performance of the SDM model is consistently lower on the subject relative clauses compared to the object relative clauses.
The performance of the SDM model is consistently higher on the object relative clauses compared to the subject relative clauses.
The recall and precision values for the Model-AVG in the ensemble model are 0.520 and 0.495, respectively.
The ARI and precision values for the Vote-AVG in the ensemble model are 0.783 and 0.535, respectively.
The recall values for "Ours", "-Pre-train", and "-Switch" are different.
The MRR value for "Ours" is higher than the MRR values for "-Pre-train" and "-Switch".
The "Ours" model has a higher recall@1 score compared to the "-Switch" and "-Pre-train" models.
The "Ours" model has a higher MRR score compared to the "-Disentangle" model.
The ARI score for "Ours" is higher than the ARI score for "baseline".
The precision score for "Ours" is higher than the precision score for "-features".
The accuracy of the model trained on 50K accepted/unaccepted data is 0.6966.
The precision of the model tested on 25K accepted/unaccepted data is 0.4658.
The training dataset consists of 50,000 accepted samples and 50,000 unaccepted samples.
The accuracy of the model is higher when the test dataset consists of 50,000 accepted samples and 50,000 unaccepted samples compared to when the test dataset consists of 25,000 accepted samples and 75,000 unaccepted samples.
Table 1 shows the LSTM perplexities for different models including Full Softmax (SM), Sample-Based SM (Sample-SM), Hierarchical-SM (HSM), Adaptive-SM, NCE, and ECOC-NSP.
The perplexity of the Rand-Hierarchical-SM model on the PTB validation dataset is 94.31.
Table 4 presents the results of an ablation study of the proposed model on the test set.
The proposed model without WEIGHT performs worse in terms of BLEU-1 compared to the SARG model.
The majority baseline performs better on IMDB→MR than on IMDB→QC.
The EH□O□ setting performs better on SNLI→SICK than on SNLI→MSRP.
There are more candidate edges for Pro-U than for Pro-R.
More edges are added after filtering predictions for Pro-U than for Pro-R.
The features in Table 2 are ranked based on their R2 values in descending order.
The feature "Number of words" has the highest R2 value among all the features in Table 2.
The setups in the table represent different thresholds applied to templates in MaltParser with the arc-eager transition system.
Increasing the threshold applied to templates in MaltParser with the arc-eager transition system leads to a higher percentage of word reduction.
The BIST Parser was tested on both the dev and test datasets.
The LAS score for the MaltParser Covington is higher for the baseline compared to the M2,387−87 setup.
The larger 345M parameter GPT2 model has a lower perplexity (PPL) than the 117M parameter GPT model.
The larger 345M parameter GPT2 model has a higher classification accuracy than the 117M parameter GPT model.
The model "Pseudo-Self" has the highest CIDEr score among all the models in the table.
The model "Context-Attn" has a higher B4 score than the model "Repr-Trans".
Our best model achieves an accuracy of 84.3% on the MRDA dataset.
The majority class achieves an accuracy of 59.1% on the SwDA dataset.
The RNN-Input-Attention (LSTM) model achieves the highest accuracy on the MRDA dataset.
The Max model achieves the lowest accuracy on the SwDA dataset.
TACoS has more clips than YouCook.
MSR-VTT has more videos than TGIF.
Table 6 shows the semantic parser accuracy on MPII-MD.
The semantic parser accuracy for the task "Roles" on MPII-MD is 0.7.
SMT with our sense-labels performs better than SMT with our text-labels in terms of METEOR score on MPII-MD.
As the version changes from IDT 30 to IDT 100 to Combi 100, the METEOR score decreases for both SMT with our sense-labels and SMT with our text-labels on MPII-MD.
The table compares the word error rate (WER) of different versions of the system on the SWB part of the Hub5'00 testset.
The system achieves a lower word error rate (WER) on the SWB-1 (300h) dataset with speaker adaptation.
The ensemble composition varies for each row in the table.
The precision and recall values vary for each classifier and ensemble across different test sets.
On average, the ensemble has higher precision and recall values compared to the classifier.
Table 5 shows the performance of different models on RSDD's validation set with different post selection strategies and values of npost.
The F1 score for the latest post selection strategy with npost=1500 is higher than the F1 score for the earliest post selection strategy with npost=1500.
The first method uses categorical cross entropy as the loss function, while the second method uses mean squared error (MSE) as the loss function.
The first method uses 25 convolution filters, while the second method uses 150 convolution filters.
The "User model - CNN-R" method has the highest precision and F1 score among all the methods.
The "User model - CNN-E" method has the highest recall among all the methods.
Categorical Cross Ent. achieves the highest Non-green F1 and Flagged F1 scores among all the methods.
Class Metric (Ordinal) achieves the highest Urgent Acc. score among all the methods.
The Class Metric (Ordinal) performs better than the Flagged F1.
The Categorical Cross Ent. performs worse than the All Acc.
Table 7 compares the performance of different methods for self-harm risk assessment on the ReachOut CLPsych '17 test set.
The Class Metric (Ordinal) method achieves higher F1 scores for flagged accuracy, urgent F1, and all accuracy compared to the other methods.
The MPR for the "None" intervention in Round 1 is 93.1%.
The gap between the "Images" intervention and the "None" intervention at the final round is 0.4%.
The weighted-voting system achieves the highest accuracy among all the combined systems in the top 3 best results.
The Siamese-4 system achieves an accuracy of 70.2% in the second-best result among all the individual systems.
The ablation test for α=1 and τ fixed to 0.1 achieves a value of 0.51 for the Request goal.
The ablation test for α fixed to 1 and τ=0 achieves a value of 0.14 for the value goal.
The performance of the models is higher when using a bilingual corpus compared to using a monolingual corpus (see Table 2).
The "Variational Attention" model achieves the highest BLEU score among the different algorithms on the IWSLT test set.
The "Actor-Critic Bahdanau et al. (2016)" model outperforms the "BSO Wiseman and Rush (2016)" model in terms of BLEU score on the IWSLT test set.
Word2Vec (word) embedding achieves the highest accuracy for PT-BR syntactic analogies.
Sense2Vec (sense) embedding outperforms FastText (word) embedding for PT-EU semantic analogies.
As the number of objects increases, the performance of the "Ours SGMN" method improves.
The "w/o transfer" method performs better on the validation and test splits compared to the "w/o norm", "max merge", and "min merge" methods.
"Ours SGMN" outperforms all other methods on both "Split val" and "Split test".
The performance of the methods generally improves as the number of objects increases.
CMRIN* performs better on RefCOCO testA than the other holistic models.
Ours SGMN* performs better on RefCOCO+ testB than the other structured models.
The table shows the BLEU score on IWSLT 2014 German-English translation for different models.
The Rel-4 model achieves the highest BLEU score among the models.
Table 4 shows the BLEU scores on newstest2014 for WMT English-German translation for different models.
The Back-translation model achieves a BLEU score of 35.00 on newstest2014 for WMT English-German translation.
The table presents the BLEU score for the L2R model in multi-agent training on a large-scale corpus of Chinese-English translation.
The L2R+R2L+Enc+Rel model performs better than the L2R(baseline) model in multi-agent training on a large-scale corpus of Chinese-English translation.
Table 6 provides the accuracy of subject-verb agreement (SVA) and word sense disambiguation (WSD) for different models.
The "Enc" model has the highest accuracy for word sense disambiguation (WSD) among all the models.
The accuracy of the audio-level discriminator with score is 69.01%.
The F-Score of the text-level discriminator with score (regression) is 0.888.
The "Regression" model has a higher linear correlation than the "Classification" model.
The "Regression" model has a lower mean absolute error than the "Classification" model.
Table 2 provides information about the perplexity scores of different language models on both the development and evaluation sets.
TheanoLM 500+1500+1500 has a lower perplexity score on the evaluation set compared to TheanoLM 100+300+300 for both Finnish and English.
The F1 score for the "Curious Cat" dataset is 70.61 when using the CNN-BiLSTM + EA + emoji model.
The macro F1 score for the "ask.fm" dataset is 83.56 when using the CNN-BiLSTM + RA + emoji model.
The proportion of unique words in topic summaries obtained with different strategies decreases as the number of topics increases.
The PCM FREX strategy consistently obtains a higher proportion of unique words compared to the LDA FREX and LDA FREQ strategies for all numbers of topics.
Our model has a higher DKL value than the "DCVAE + KLA" model.
Our model has a higher I qϕ(z, x) value than the "DCVAE + KLA" model.
The VNMT model achieves a BLEU score of 30.35 on the WMT14 De-En benchmark.
Our model achieves a BLEU score of 34.97 on the WMT16 En-Ro benchmark.
The table shows the results of an ablation study on translation quality for two different models: "De–En (3.9M)" and "Ro–En (608K)".
The results of the ablation study show that using both models ("De–En (3.9M)" and "Ro–En (608K)") yields higher translation quality compared to using only the Bag-of-Words (BoW) model.
Our method (PretRand) achieves the highest token-level accuracy on the Test dataset.
The ARK method achieves a token-level accuracy of 94.6% on the TweeBank Dev dataset.
The "Enc-Dec" model performs the best in terms of the Youtube2Text METEOR⋆ metric.
The "+ Both" model has the lowest perplexity score on the Montreal DVS dataset.
The table shows the performances of different image caption generation models in the Microsoft COCO Image Captioning Challenge.
The models in the table are sorted according to their scores in the "Human M1" column.
The BLEU scores obtained by the Blue-Fringe clustering algorithm are higher for α values of 10−2 and 10−5 compared to other α values.
The α value of 10−2 corresponds to the highest BLEU score obtained by the Blue-Fringe clustering algorithm.
Table 4 shows the BLEU scores obtained by the k-medoids clustering method for different sizes of the subset of non-terminals and for different numbers of clusters.
The BLEU scores for k=2, k=3, and k=4 are 0.5975, 0.5991, and 0.5952, respectively.
The sentence "A casting director at the time told Scott that he had wished that he’d met him a week before; he was casting for the “G.I. Joe” cartoon." has the highest rank of 3.
The sentence "An animal that cares for its young but shows no other sociality traits is said to be “subsocial”." has the label "✗".
The "Look-ahead LM (65K) [this work]" system outperforms other end-to-end ASR systems in terms of both "dev93" and "eval92" scores.
Table 2 provides automatic evaluation results (BLEU) for different translation systems.
The trigger extraction performance is higher on the MIMIC dataset compared to the UW dataset.
The labeled argument extraction performance is generally higher on the MIMIC dataset compared to the UW dataset.
The combination of both regions and instances leads to the highest scores for BLEU@4, METEOR, and CIDEr-D.
The presence of both regions and instances improves the BLEU@4 score.
Table 1 shows the performance (%) of different methods on the COCO Karpathy test split.
The GCN-LSTM+HIP method achieves the highest Cross-Entropy Loss and CIDEr-D Score Optimization scores among all the methods.
GCN-LSTM+HIP achieves the highest scores for all evaluation metrics at both c5 and c40.
GCN-LSTM+HIP has a higher BLEU@1 score than GCN-LSTM, RFNet, Up-Down, LSTM-A, and SCST.
HIP performs better than Up-Down in terms of precision, recall, and F1 score for the "C" class.
HIP performs better than Up-Down in terms of precision, recall, and F1 score for the "O" class.
The "Major" category has the highest total count for all three translation systems: NMT, SMT, and RBMT.
The "Minor & Major" category has the highest total count for all three translation systems: NMT, SMT, and RBMT.
The BLEU scores of the pruned models decrease as the models are increasingly pruned, but the BLEU score of the 60% pruned and retrained model is slightly higher than the 60% pruned model without retraining.
The 70% pruned model has the lowest BLEU score among all the pruned models.
The table provides language expert annotations on 175 anomalies proposed by the text length system and the uncommon character system for data in fields POS and MAIN.
The total number of errors and non-errors for each system and field can be calculated by summing up the values in the "Real Error" and "No Error" columns.
The table shows the accuracy (F1 score) of Citation Need classification models on Featured Article vs individual expert editor annotations on the same set of Featured Articles.
The "RNN+ Sa" model performs better in classifying Citation Need when there is a citation compared to when there is no citation, with an average F1 score of 0.904.
The pre-trained model has a higher F1 score compared to the model trained without pre-training.
The pre-trained model has a higher precision compared to the model trained without pre-training.
Our model outperforms Seq2Seq, PG, and KVMN in user attribute extraction, achieving the highest scores in all metrics.
The human evaluation score for our model is statistically significant and higher than all other models in user attribute extraction.
The Entity Generator outperforms the Predicate Classifier in terms of accuracy and F1 score.
The Predicate Classifier has a lower accuracy than the Entity Generator.
TURL achieves the highest precision at all ranks compared to the other methods.
TURL achieves a higher precision at rank 1 compared to Exact, H2H, and H2V.
The "TURL + fine-tuning" method achieves the highest F1 score among all the methods.
The recall (R) for the "TURL + fine-tuning" method is 58.
The F1 score for the "TURL + fine-tuning" method in our testing is 68.
The precision for the "Wikidata Lookup (Oracle)" method in our testing is 82.
The table presents the results of different methods used for model evaluation on the column type annotation task.
The "TURL + fine-tuning" method achieves the highest F1 score among all the methods evaluated.
The table presents the performance of different methods on 5 selected types.
The method "TURL + fine-tuning" achieves the highest F1 score on the "person" type.
TURL + fine-tuning achieves the highest F1, P, and R scores.
The BERT-based method achieves an F1 score of 90.94.
Table 8 shows the model evaluation results on the row population task for different methods.
The "TURL + fine-tuning" method achieves a MAP of 40.92 and a Recall of 48.31 on the row population task.
The table shows the F1-scores of ablation experiments on BiDTreeCRF.
The "Full model" has the highest accuracy among the different subsets of features in Experiment 1.
The "Character n-grams" subset has a higher accuracy than the "Word n-grams" subset in Experiment 1.
The model "Combined (ours + RNN)" has the highest accuracy among all the models listed.
The model "HIER,ENCPLOTEND,ATT" has a higher accuracy than the model "RNN".
The PRA method achieves a MAP% of 38.85 on WN18RR and 34.33 on FB15k-237.
The APRD method achieves a MAP% of 84.91 on WN18RR and 57.35 on FB15k-237.
Table 3 shows the MAP% of three models using different levels of abstraction for entities.
The Attention model performs better on WN18RR and FB15k-237.
Table 5 compares the MAP% of the APRD model to two SOTA embedding methods on the WN18RR and FB15k-237 datasets.
The APRD model achieves a higher MAP% on the WN18RR dataset compared to the FB15k-237 dataset.
The "CompTreeNTN" model achieves the highest accuracy on the train, dev, and test sets.
The "CompTreeNTN" model performs better than the "Attention LSTM" model on the dev set.
The model "Small FF, 64 dim" has an accuracy of 94.24%.
The model "Small FF, 64 dim" has a size of 846KB.
BERT has a higher average performance than ERNIE2 and RoBERTa.
ERNIE2 performs better than T5 on QNLI.
The "Back-translation (median)" method has a higher performance than the "EDA (median)" method on the CoLA task.
The "EDA (best)" method has a higher performance than the "Back-translation (best)" method on the STS-B task.
The performance on CoLA is higher when using the BERT Training Data compared to the Target Training Data.
The performance on STS-B is higher when using the BERT Training Data compared to the Target Training Data.
The table presents classification results for the Concept-Project Matching task.
The "topic_science" category has the highest fscore value.
The category "Electronics" has the highest value in the "KL-MoE" column.
The category "Cell Phones" has the highest value in the "EM-MoE-S" column.
The highest value in the "KL-MoE" column is marked as [BOLD] for each category.
The values in the "EM-MoE" column are consistently higher than the values in the "MoE" column for each category.
The AUC scores for the "s-MoE" model are lower than the AUC scores for the "m-MoE" and "m-MoE-S" models in all categories.
The AUC scores for the "m-MoE-S" model are higher than the AUC scores for the "m-MoE" model in all categories.
The F1 scores for non-shared objects are consistently lower than the F1 scores for shared objects.
The F1 scores for strict discreteness are higher than the F1 scores for non-strict discreteness.
The MAC model achieves the highest accuracy among the MLP, MAC, and NS-CL models.
Figure 7 displays an example image-question pair from the VQS dataset and the corresponding execution trace of NS-CL, as well as the results on the VQS test set.
The NS-CL model performs better than the NS-VQA model in all categories.
The NS-CL model outperforms the NS-VQA model in terms of overall performance.
The K40c GPU has a global memory of 11520 MB and an L2 cache size of 1.57 MB.
The K40c GPU has 15 multiprocessors and 192 cores per multiprocessor.
The MetaMap method has the highest precision compared to other methods.
The "Ours" method has the highest recall compared to other methods.
The system "DynamConvWu et al. (2019)" achieves the highest BLEU score among all the systems listed in the table.
The addition of "RGSE (ours)" improves the BLEU score of the base Transformer model.
The accuracy of the Refined Ask Your Neurons architecture on the subset all is 24.48% and on the subset single word is 26.67%.
The WUPS@0.9 of the SAN (2, CNN) architecture on the subset all is 35.10% and on the subset single word is 68.60%.
Table 10 shows the results on the VQA validation set for the "Question-only" model and analyzes the CNN question encoders with different filter lengths.
The accuracy of the "multi view" model is higher for "k=3" than for "k=4".
The metric used for evaluating the CosmosQA and MCScript2.0 datasets is "Accuracy", while for the MCTACO dataset, the metric used is "Exact Match (EM)/F1".
The CosmosQA and MCScript2.0 datasets are both related to "Relevance Ranking", while the MCTACO dataset is related to "Pairwise Text Classification".
The table provides the development and test results of different models on CosmosQA, MCScript 2.0, and MCTACO datasets.
ALICE outperforms other models on both the development and test sets for CosmosQA, MCScript 2.0, and MCTACO datasets.
MN(+AS) has higher Macro-F1 scores on the LAPTOP dataset compared to MN.
TNet-ATT(+AS) has higher accuracy on the REST dataset compared to TNet.
RoBERTa∗ has a higher F-1 score than RoBERTa.
KEPLER-Wiki has higher precision, recall, and F-1 scores compared to KEPLER-KE.
Table 5 provides the precision, recall, and F-1 results on TACRED for different models.
The KEPLER-Wiki model achieves a precision of 72.8% on TACRED.
The KEPLER-Wiki model achieves the highest accuracy in the 5-way 1-shot setting on the FewRel 1.0 dataset.
The RoBERTa-based Prototypical Networks model achieves an accuracy of 80.68% in the 5-way 1-shot setting on the FewRel 1.0 dataset.
The UFET model has the lowest F-1 score among all the models.
Table 8 shows the results of the three models (RoBERTa, RoBERTa∗, and KEPLER) on different GLUE tasks.
The GVT model has the lowest PPL score among all the models.
The SVT model has the highest diversity scores for Dist-1, Dist-2, and Dist-3.
ALBERTbase (SA+CA) performs worse than ALBERTbase (SA) and ALBERTbase (SA) + DUMA (CA) on the dev and test sets.
Adding co-attention (CA) to ALBERTbase (SA) improves the performance on the dev and test sets.
The table compares the performance of three different models: ALBERTbase, +DUMA, and +TB-DUMA.
The +TB-DUMA model performs better than the ALBERTbase and +DUMA models on the dev set.
The "+DUMA" model performs the best among all the models on the DREAM dataset.
The "+DCMN" model performs the worst among all the models on the DREAM dataset.
The table presents the results of two different models, ALBERTbase and BERTbase, on the DREAM dataset.
The DUMA method improves the performance of both ALBERTbase and BERTbase models on the DREAM dataset.
The proposed system performs better on the Django dataset compared to the other systems in terms of both BLEU score and xxAcc score.
The Attention system performs better on the HS dataset compared to the other systems in terms of both BLEU score and xxAcc score.
The table provides information about the success rate and average dialog length for the rule-based and RL-based dialog policies.
The RL-based dialog policy has a higher success rate and a lower average dialog length compared to the rule-based dialog policy.
The RF value for the BeamConv architecture with Top k value of 1 is 10.
The PostEdit Dec. SWB value for the WEmb architecture with Top k value of 1 is 17.3.
When using the CTC loss criterion, the system performs better on the SWB subset of Eval2000 than on the CH subset.
When using "Seed 1", the system performs worse on the CH subset when both loss criteria are used compared to when they are not used.
The KGSF model performs better than the other models in terms of both Fluency and Informativeness.
The improvements of the KGSF model in Fluency and Informativeness are statistically significant compared with the best baseline.
KGSF outperforms all other models in terms of Dist-2, Dist-3, and Dist-4 scores.
KGSF has a higher item ratio compared to other models.
The table presents results for different existing NMT techniques.
The word-based model achieves a score of 29.4.
The "mvFLSTMp" models in the table have projection dimensions of 128, 256, and 512.
The model with id 09 has a total number of trainable parameters of 44.8 million.
The mvFLSTMp model shows higher average WER reduction compared to the LSTM, FLSTM, and mvFLSTM models.
The mvFLSTMp model achieves higher WER reduction compared to the LSTM, FLSTM, and mvFLSTM models.
The method "MCB+Att." has the highest accuracy for all the question types.
The average accuracy of the method "Concat+Att." is 51.2%.
The GOSS+LOSS method with RandomForest achieves the highest F1-score on the Honeypot Dataset.
The LOSS method with SVM has a higher precision than the GOSS method with SVM on the Weibo Dataset.
Table 6 provides a summary of ablation analysis.
The accuracy of the full model is 75.18%.
The accuracy of the model fine-tuned on original cs.NI data is 65.22%.
The accuracy of the model fine-tuned on noisy and augmented cs.NI data is 67.99%.
The table shows accuracy scores for two categories of models: word-based models and character-based models.
The accuracy score for English input in the character-based models category is 0.94.
Table 1 shows the different layers of the word-based convolutional model.
The kernel size used in the first layer of the word-based convolutional model is 3.
The table provides information about the size of three different datasets: Soft-labelled English, Soft-labelled Arabic, and Machine-translated Arabic.
The table provides information about the number of sentences for each dataset: Soft-labelled English has 49,296 sentences, Soft-labelled Arabic has 11,466 sentences, and Machine-translated Arabic has 3,931 sentences.
BERTLARGE achieves the highest accuracy across all categories in the SST dataset.
BERTBASE achieves an accuracy of 94.0% in the SST-2 All category.
Table 5 provides the averaged accuracy and MAE for all strategies, including the annealed λ strategy, for Uent and Usoft.
The Uent Accuracy for the "tos" strategy is higher than the Usoft Accuracy for the same strategy.
The MAE values for the "STAT", "MOT", and "INT" categories are higher compared to the other categories.
The MAE value for the "DEC" category is the lowest among all the categories.
The unit types used for character level language modeling on Penn TreeBank include GRU, LSTM, RNN, CRNN, VCRNN, and VCGRU.
The bits per character (bpc) values for character level language modeling on Penn TreeBank range from 1.42 to 1.47.
VCRNN-1000 has the lowest bits per bit value among all the models in the table.
The table provides information about the performance of different models for character level language modeling on Penn TreeBank.
The bpc value decreases as the model size increases in the "Character level Text8 RNN-d" column.
As the dropout rate increases, the number of parameters decreases in the "VCGRU-1024" row.
The Curriculum setting achieves higher Precision, Recall, and F1 scores for Wiki-KBP compared to the Joint (w/o curriculum) and Separate (w/o joint) settings.
The Separate (w/o joint) setting achieves higher Precision, Recall, and F1 scores for BioInfer compared to the Joint (w/o curriculum) and Curriculum settings.
The Meta-Model (CGS10) has the highest accuracy of 76.1.
The Lex-HMM+LM model finishes in half the time compared to the PYP-1HMM+LM approximation.
The "Full Ensemble" model achieves the highest performance on the "RetEdit Test" dataset.
The "Templates+MC" model achieves the highest performance on the "Monte Carlo Pipeline" dataset.
The "Full Ensemble" model has the lowest perplexity score.
The "Full Ensemble" model has the highest BLEU score.
As we move from model #1 to model #4, the CER decreases.
As we move from model #1 to model #4, the relative error reduction for CVER increases.
The table shows the fact prediction results (MAP) of different methods on two datasets - FB15K-237 and NELL-995.
RL performs the best in terms of fact prediction results (MAP) on the FB15K-237 dataset.
FB15K-237 RL achieves the highest MAP score on the FB15K-237 dataset.
FB15K-237 RL achieves the highest MAP score on the "filmLanguage" task.
The table shows the average length of different types of sentences in the Training, Dev, and Test sets.
Team BUT-FIT outperforms Team CN-HIT-IT.NLP+ in terms of both BLEU score and human evaluation.
Solomon outperforms SWAGex, UI, and TMLab* in terms of both BLEU score and human evaluation.
The CER decreases as more tasks are added to the training chain.
The SFP decreases as more tasks and models are added to the training chain.
Table 2 shows the performance of an end-to-end SLU system with beam-search decoding on the MEDIA test.
The CVER decreases as more components are added to the training chain in the end-to-end SLU system.
The performance of the end-to-end SLU system improves when "ASR" and "SFP+" are included in the training chain.
Table 4 provides a comparison between different systems based on their CER and CVER values.
The DNN-HMM hybrid system with uncertainty decoding using arithmetic (UD arithm) achieves lower WER scores than the system without uncertainty decoding (No UD) for both SimData and RealData.
The DNN-HMM hybrid system with uncertainty decoding using arithmetic (UD arithm) achieves lower WER scores than the system with uncertainty decoding using weight (UD weight) for both SimData and RealData.
Table 1 shows WER scores for the REVERB challenge evaluation test set using different feature types.
The WER scores for the "Far" condition are consistently higher than the WER scores for the "Near" condition.
The evaluation results in Table 6 are based on different types of questions.
Table 7 shows the evaluation results (accuracy %) of different models on JEC-QA.
Skilled Humans perform better than Unskilled Humans in all question types.
The lowest score in the "KD-Q" column is in the "All Miss" row with a value of 21.05.
The "1d-conv" model outperforms the "Dense Baseline" model in terms of accuracy, average precision, average recall, and F1 score.
The "1d-conv" model has a higher accuracy score than the "Dense Baseline" model.
Our Stack-Propagation model outperforms the baseline models in all SLU performance categories on both SNIPS and ATIS datasets.
The gate-mechanism model performs the worst among all the baseline models in all SLU performance categories on both SNIPS and ATIS datasets.
Our model achieves the highest F1 score on the SNIPS Slot filling task compared to all other models.
Our model achieves the highest accuracy on the SNIPS Intent detection task compared to all other models.
Our model + BERT performs as well as the BERT SLU Chen et al. (2019) model on both SNIPS and ATIS datasets.
The BERT SLU Chen et al. (2019) model outperforms our model in terms of overall accuracy on the SNIPS dataset.
The model with static + unigram performs better on the Web domain than the model with static + local cache.
The model trained on the News domain performs better on the Commentary domain than the model trained on the Wiki domain.
The "Web" dataset has the highest computational time for the static model, local cache, and unbounded cache.
The local cache has lower computational time than the unbounded cache for all datasets.
The performance of the models improves as the number of layers in the LSTM increases.
The bi-directional LSTM with 3 layers achieves the lowest phoneme error rate among all the models.
The genre "Newscast" has the highest VSM AUC value.
The weighted average AUC value for meta-features is 0.824.
The F1 score increases as we move down the table, indicating an improvement in performance.
The study "Stefan:20" uses the BERT approach and achieves the highest F1 score of 95.40.
mBERT has 12 hidden layers.
XLMR-l has a vocabulary size of 250,002.
The uTODCARRL system outperforms the uTODMAPO system in terms of CamRest Ent. F1.
The uTODCARRL system achieves higher BLEU scores than the uTODMAPO system on DSTC2.
The CARRL approach has the highest accuracy among all the weakly supervised approaches on DSTC2.
The MAPO approach achieves higher total test rewards than the other weakly supervised approaches on CamRest.
The uTODCARRL system has the highest performance in terms of CamRest Ent. F1 and DSTC2 Ent. F1.
The uTODCARRL system has the highest performance in terms of CamRest BLEU and DSTC2 BLEU.
Table 5 compares the performance of different WaveNODE models with different dilations.
Table 1 provides information on different models, including the number of parameters, CLL, and MOS score.
The table presents evaluations of the model "WaveNODE" with different types of norm layers.
The CLL values for the models with norm layers "Actnorm", "MBN", and "None" are 4.497, 4.460, and 4.457 respectively.
The "Multitask Training +QPtr" model achieves the highest decaScore of 571.7.
The "Single-task Training +CAtt" model achieves scores of 25.1 on the "CNN/DM" dataset and 28.5 on the "QA-ZRE" dataset.
The table presents evaluation results of different models on DCASE 2018 Task1b.
The "+ NLE-RTSL adaptation" model achieves the highest accuracy on Dev C.
The CRF classifier achieves the highest F1 score among all the classifiers.
The Random Forest classifier has the lowest recall score among all the classifiers.
The CRF classifier has the highest F1 score among all the classifiers.
The SVM classifier has the highest F1 score among all the classifiers.
The CRF classifier has the highest Precision among all the classifiers.
Table 3 presents the results of the state-of-the-art baseline classifier.
The F1 score for the state-of-the-art baseline classifier is 0.113.
The model "+Deep-RNN-LM" has the highest recall on the development set with a recall of 40.2.
Table 3 shows the results of multiple baseline runs, average, and ensemble for the CoNLL benchmark.
The ensemble model achieves a CoNLL Dev score of 19.3 and a CoNLL Test score of 52.5.
The table shows different models used for GEC-specific adaptations.
The table provides the development set results for each model.
Table 5 shows the results for a model type +Tied-Emb trained with edit-weighted MLE and chosen Λ.
The precision decreases as Λ increases from 1 to 3.
The table shows the results of the best logistic regression (LR) and LSTM models on sentences with a single location and multiple locations.
The AUC score for the LR - Mask (n-gram + POS) model is 0.916 for sentences with a single location and 0.907 for sentences with multiple locations.
The table presents the performance breakdown of LR - Mask (n-gram + POS) and LSTM - Final models on different aspects.
LR - Mask (n-gram + POS) performs better than LSTM - Final in terms of price aspect.
The table compares the performance of Word2Vec and SPhrase(NU) on the Conll2003Eng and Wikigold datasets.
The average F1 score of Word2Vec on Conll2003Eng is 83.82±0.3831 and on Wikigold is 55.49±0.4708.
The values in the "gamma NPN" column are consistently lower than the values in the "Gaussian NPN" and "Poisson NPN" columns for all three datasets.
The values in the "VAE" column are consistently higher than the values in the "SAE" and "SDAE" columns for all three datasets.
The "gamma NPN" method consistently outperforms the "Gaussian NPN" and "Poisson NPN" methods in terms of AUC across all three datasets.
As the percentage of training data increases, the LM and MTL values decrease for both CS and RU.
The Δ values decrease as the number of characters increases for both CS and RU.
The language models perform better on the morphological data compared to the LM data.
The SK data achieves the lowest BPC value among all the combinations.
Table 6 presents the results on the Multilingual Wikipedia Corpus (MWC) in bits per character (BPC), trained on the vocabulary from Mielke and Eisner (2019).
The "MTL" approach achieves the best results in terms of bits per character (BPC) for all languages in the Multilingual Wikipedia Corpus (MWC).
Table 2 provides the performance of different models on multiple datasets: CADEC Random, CADEC Custom, PsyTAR Random, PsyTAR Custom, and SMM4H Official.
BERT with TF-IDF (max) performs better than BERT without TF-IDF (max) on all datasets: CADEC Random, CADEC Custom, PsyTAR Random, PsyTAR Custom, and SMM4H Official.
The CNN model achieved the highest F1 Macro score of 0.69.
The Precision, Recall, and F1 scores for the TIN class are 0.89, 1.00, and 0.94 respectively.
The BiLSTM model has the highest F1 score among all models.
The CNN model has the highest F1 Macro score.
The CNN model has the highest F1 score among all the models.
All models have a precision of 0.00 for the "IND" class.
CorefQA + SpanBERT-large achieves the state-of-the-art performance on all metrics including F1 scores on Masculine and Feminine examples, a Bias factor (F / M), and the Overall F1 score.
CorefQA + SpanBERT-large has the highest Bias factor (F / M) compared to other models.
Table 3 shows the performance of the Model (Comp-Clip +LM +LC) on the QNLI corpus with different numbers of clusters.
The MRR scores for 8, 16, and 1 clusters are the top scores in the "Pointwise Learning MRR" column.
The performance of the Transformer model trained synchronously decreases as the learning rate increases.
The performance of the RNN model trained asynchronously is consistently lower than the performance of the Transformer model trained synchronously.
When the gradients alternate between -1 and 2, the Adam optimizer takes 6 steps before the parameter has the correct sign.
The scaled values of [ITALIC] gt are 0.5, 1.5, 0.5, 1.5, 0.5, and 1.5.
Table 4 shows the effect of global accumulation on translation quality for different language pairs on development and test sets, measured with BLEU score.
The BLEU score for the "EN → FI 2018" translation task using the "Trans. + synchronous SGD" model is 14.03.
Table 5 shows the performance of the asynchronous Transformer on English to German with 4x Global accumulations (GA) across different learning rates on the development set measured with BLEU score.
The BLEU score for the asynchronous communication with 4x GA decreases as the learning rate increases.
The method used for sentiment classification in all datasets is "RIPPLES".
The accuracy of sentiment classification is higher for the "Clean" dataset compared to other datasets.
Table 5 shows the effects of different hyperparameter changes on the accuracy of the model.
Changing the learning rate to 5e-5 results in an accuracy of 65.0.
Table 8 shows the results of different ablations on the SST dataset using embedding surgery and different settings.
The clean accuracy of the "ES Only" setting is higher than the clean accuracy of the "BadNet + ES" setting.
Table 12 provides sentiment classification results for different settings in XLNet.
The sentiment classification accuracy for most settings in XLNet is above 90%.
Table 13 shows the sentiment classification results for the SST dataset using different settings.
The RIPPLES model achieves an accuracy of [BOLD] 86.6 in the "Clean" setting.
AtheneMLP achieves the highest recall, precision, and F1m scores among all the models.
The "extendedESIM" labeling method achieves the highest F1 score in the claim validation results.
The "BertEmb" labeling method achieves the highest precision score in the claim validation results.
The table represents the confusion matrix for stance detection using the AtheneMLP model.
The AtheneMLP model predicted "support" stance when the actual stance was "no stance" 531 times.
The "Fine-tune" dataset has the highest UAS and LAS scores among all the datasets.
The "UD-EWT" dataset has a higher UAS score, while the "Tweebank+" dataset has a higher LAS score.
The inter-annotator agreement between the two annotators for JN, NN, and VN is higher than 0.5.
The correlation between ML-Paraphrase and all of the ML dataset is higher for JN and VN compared to NN.
The "paragram WS" model with a dimensionality of 25 has a similarity score of 0.56* on the SimLex-999 word similarity task.
The "+ constraints" model with a dimensionality of 25 has a higher similarity score of 0.58* on the SimLex-999 word similarity task compared to the other models.
The "paragram" word vectors with "25" dimension have an accuracy of [BOLD] 80.9%.
The "skip-gram" word vectors have accuracies of "77.0%" and "79.6%" for "25" and "50" dimensions respectively.
Table IV shows the change in different attention weights with epochs for Seq2Seq + conv + POS (weighting).
The attention weight on bigrams increases from 2.69 to 3.65 as the epochs progress.
Table 1 provides information about the hyperparameters used to train Recurrent and Transformer models.
Both Recurrent M-NMT and Transformer M-NMT have a lower total error count compared to the bilingual reference model (NMT).
Both Recurrent Δ NMT and Transformer Δ NMT have a lower reordering error count compared to the bilingual reference model (NMT).
There are three different models used in the feature ablation experiment on the TVC validation set.
The MMT (ResNet+I3D) model achieves the highest CIDEr-D score in the feature ablation experiment on the TVC validation set.
The XML model with both ResNet and I3D video features achieves the highest IoU=0.7 score among all models.
The XML model with both ResNet and I3D video features achieves the highest R@100 score among all models.
Table 11 provides captioning results on the TVC test-public set.
MMT (video+sub) achieves the highest scores in the B@4, METEOR, Rouge-L, and CIDEr-D metrics.
The table provides information about the split distribution in the TVC dataset.
On average, each moment in the TVC dataset has 2 descriptions.
The table provides sentiment classification accuracies on two movie review datasets.
The sentiment classification accuracy is higher for the "Socher et al." baseline compared to the "Pang & Lee" baseline for the "D1. Lexicon" approach.
The "Cosine" distance has a higher WER (%) on both the dev and test sets compared to the "PLDA" distance.
The "Same" gender-selection has a lower WER (%) on both the dev and test sets compared to the "Random" gender-selection.
The method "Transformer" outperforms all other methods in terms of R1, R2, and RL scores.
The "Lead-1" method outperforms the "Random-1" method in terms of R1, R2, and RL scores.
Table 1 provides detailed subjective results for the SPOKE task.
The scores for different gender combinations in the F-F, F-M, M-F, M-M, and ALL columns are provided in the "N10" row.
The CNN-LSTM model outperforms the Word N-grams-SVM model in terms of segment-level accuracy on the PAN 2012 dataset.
The Syntactic CNN-LSTM and LSTM-LSTM models achieve perfect document-level accuracy on the PAN 2012 dataset.
The table provides test results for models trained with original data and data containing "_NOTA".
The recall at 10% for the model trained with original data is 56.12%.
The "+LogReg (Logits)" method achieves the highest F1 score for the "Selection Model (original data)".
The F1 score for the "GF110" column is consistently higher than the F1 score for the "NF110" column in the "Direct Predict" row.
The F1 scores for R, N, and G increase as the number of candidate responses increases for both "Direct Predict" and "+LogReg" models.
The "+LogReg" model has a higher average F1 score compared to the "Direct Predict" model for all numbers of candidate responses.
The optimizer used in the experiment is Adam.
The maximum sentence length used in the experiment is 59.
The RnnLM model does not have information for the D and R columns.
The Vamp/N model has the lowest perplexity value among the three models.
The table shows the performance of different models on the Yahoo/Yelp corpora.
Table 4 provides information on common cases of disagreement for anaphoric, pleonastic, and event reference pronouns.
The ON-LSTM-2 model achieves the highest F1 scores for both WSJ10 and PTB23 datasets.
The LSTM-1,2 model achieves the highest F1 scores for both WSJ10 and PTB23 datasets among all the LSTM models.
Our Model performs better than the other models on most languages in the SPMRL experiment.
Our Model performs better than the kitaev-klein-2018-constituency model on Hungarian in the SPMRL experiment.
"Our Model" has the highest F1 score among all the models in the table.
All models have the same F1 score for "Top-Down Inference".
The parsing speed of "Our model" is 130.2 sents/sec.
The "Our model" has the highest parsing speed among all the models.
The kitaev-etal-2019-multilingual model performs better than our model on most languages in the SPMRL experiment.
Our model outperforms the kitaev-etal-2019-multilingual model on both Polish and Swedish in the SPMRL experiment.
The VAE SP-SP method outperforms the baseline method in terms of mean Mel-cepstral distortion for all combinations of SF1-TF1, SF1-TM1, SM1-TF1, and SM1-TM1.
The proposed CDVAE MCC-MCC method achieves the lowest mean Mel-cepstral distortion among all combinations of SP-SP, SP-MCC, MCC-SP, and MCC-MCC.
The "Dual-MFA*" method has the highest validation score of 59.82.
The "MFA-MUL*" and "MFA-Norm*" methods have the same validation score of 59.18.
The Dual-MFA method achieves the highest score on the Test-dev Open-Ended evaluation.
The MLB method achieves the highest score on the Test-std Open-Ended evaluation.
The WUPS score for all methods is higher when using a similarity threshold of 0.9 compared to a similarity threshold of 0.0.
The Dual-MFA method outperforms all other methods in terms of WUPS scores for all categories.
The combination of ConceptNet and Wikipedia as knowledge sources leads to the highest dev accuracy.
Using ConceptNet and Wikipedia as knowledge sources improves the dev accuracy compared to not utilizing any knowledge sources.
Our model achieves an accuracy of 79.3% on the development dataset in Group 4.
SGN-lite achieves an accuracy of 57.1% on the test dataset in Group 1.
The ERAC algorithm has the highest BLEU score of 29.36 on the IWSTL 2014 dataset for MT.
The Q(BLEU) algorithm has a BLEU score of 28.3 on the IWSTL 2014 dataset for MT.
VAML achieves the highest mean values in both "MT (w/ input feeding)" and "Image Captioning" categories.
ERAC achieves the lowest minimum value in the "MT (w/ input feeding)" category.
The table shows the average validation BLEU scores for ERAC with different values of λvar and β.
The average validation BLEU score for ERAC with β=1 is 27.44.
The optimizer used for MT with input feeding is SGD, while the optimizer used for Image Captioning is Adam.
The value of λMLE for Joint Training is 0.1.
The F-1 score for the "Query" category is lower than the F-1 scores for the "DiaAct" and "Offer" categories.
The overall F-1 score is higher than 90%.
The accuracy for the "Gaussian, γ=0.05" kernel is 68.0%.
The highest accuracy for the rude classes is 26.4% for the "Gaussian, γ=0.05" kernel.
The accuracy for the "Gaussian, γ=0.05" kernel is lower than the accuracy for the "Gaussian, γ=0.5" kernel.
The accuracy for the "Polynomial, degree 1" kernel is 0 for both regular and smoothed accuracy.
The classification in Table 6 is based on two categories: "Verbal Insults" and "Other".
The table represents the Video Scene-aware Dialog Dataset on Charades.
The number of words in the training set is 1,163,969 and the number of words in the validation set is 138,314.
The table shows the distribution of positive and negative data in the training and testing sets.
The total number of data points in the training set is 1200 and in the testing set is 400.
Table 2 presents the main results of different models on the DL-PS data.
The LSTM-CRF model outperforms all other models in terms of F1 score on the DL-PS data.
The "Fusional PT" system has a higher accuracy than the "Agglutinative DE" system.
The table shows the results of different models on the "description" subset of MS-MARCO.
The "Best Passage" model performs better than the "Passage Ranking" model on the "description" subset of MS-MARCO.
The BLEU-4 score for MPQG+R is [BOLD] 13.98.
The ROUGE-L score for Du, Shao, and Cardie (2017) is 39.75.
Table 5 shows the upper bound of ensemble performance.
The LAS score is higher without punctuation compared to with punctuation for the "Tree" ensemble.
The table shows the LAS scores for different dependency labels for four parsers: BistG, BistT, MST, and Malt.
The BistG parser achieves the highest LAS score of 47.83 for the "dep" dependency label.
POS tag "P" achieves the highest UAS score in the MST parser.
The BistG parser outperforms the BistT parser in terms of LAS score for all POS tags.
The CNN-Max + GloVe model achieves the highest performance on fMRI (S) data.
The CNN-Max + GloVe model achieves the second highest performance on fMRI (D) data.
BERT-PT (best) achieves the highest F1 scores for aspect sentiment classification in both the Laptop and Rest16 domains.
BAT (Ours) achieves the highest F1 scores for aspect sentiment classification in both the Laptop and Rest16 domains.
BERT-PT (best) achieves the highest accuracy on the Laptop domain in aspect sentiment classification.
BAT (Ours) achieves the highest accuracy on the Rest14 domain in aspect sentiment classification.
The table shows the impact of the auxiliary image classification training objective on the metrics P, R, F, and Acc.
The auxiliary image classification training objective improves the F-score compared to not using the objective.
Glyce+BiMPM outperforms BiMPM in terms of F-score in the LCQMC dataset.
Glyce+BERT achieves higher precision than BERT in the NLPCC-DBQA dataset.
The BERT model outperforms the LSTM model in terms of accuracy on all three datasets.
The addition of Glyce to the BERT model improves the accuracy on all three datasets.
Table 8 shows the impact of different training strategies on performance metrics.
BERT-glyce-joint strategy achieves a higher F-score than the only BERT strategy.
Transformers achieve the highest F1 score among all the strategies.
BiLSMTs have the highest recall score among all the strategies.
The table shows the impact of the auxiliary image classification training objective.
The F-score for the Vanilla-CNN model is 87.4.
The BLEU score for the whole dataset in the "End2End" system for En-It is 21.5.
The difference in BLEU score between the Correct and Wrong datasets in the "Cascade+Tag" system for En-Fr is 3.6.
The accuracy score for feminine word forms in the "Cascade+Tag" system for the En-It language pair is 44.7.
The accuracy score for all wrong predictions in the "End2End" system for the En-Fr language pair is 27.0.
Table 6 shows the accuracies of models trained on PQL3 and tested on PQL2, with and without UHop. The maximum length of relation paths for models without UHop is set to 3.
The HR-BiLSTM model without UHop has an accuracy of 32.18% and the ABWIM model without UHop has an accuracy of 32.94%.
The error type distribution for the WQ dataset with 1-hop reasoning and the HR-BiLSTM model is 17.46%.
The error type distribution for the PQ+ dataset with 3-hop reasoning and the ABWIM model is 1.15%.
The model "G.S.(Topic)+RoBERTa" achieves the highest R-L score among all the models.
The model "G.S.(Discourse)+RoBERTa" achieves the highest R-2 score among all the models.
The model "GraphSum+R.B." has the highest rating among all the models.
The model "FT" has a negative rating.
Table 6 shows the evaluation results on the WikiSum test set with sentence-level ROUGE-L value.
GraphSum+RoBERTa achieves the highest sentence-level ROUGE-L value among RoBERTa+FT and GraphSum.
The phrase "please post this as" has the highest value of 2.59 in the "4 gram" column.
The phrase "post if you agree" has the highest value of 67 in the "rg" column.
The addition of the "+ Context Gate" decreases the alignment quality for both the GroundHog and GroundHog-Coverage systems.
The GroundHog system has higher alignment quality scores compared to the GroundHog-Coverage system.
Table 6 shows the correlation between context gate weight and improvement of translation performance.
Sentences with a length of 30 or more have higher values for BLEU, Adequacy, and Fluency metrics compared to sentences with a length of less than 30.
The "ATTOrderNet" model performs better on "NSF abstracts" than on "arXiv abstracts" based on PMR.
The "HierarchicalATTNet" model performs better on "ROCStory" than on "NIPS abstracts" based on τ.
The "RankTxNet ListMLE" method has the highest accuracy for predicting the last sentence in both the SIND and arXiv datasets.
The accuracy of predicting the last sentence is consistently higher than the accuracy of predicting the first sentence for all methods.
Table 2 provides decoding error rates for TIMIT and Iban corpora.
The tri-phone model outperforms the mono-phone model in terms of decoding error rates.
The table compares the performance of two models, "From scratch" and "Pre-train ord."
The ROUGE-1 scores for both models are higher when the summary length is 275b compared to when the summary length is 75b.
The "Ours" model outperforms all other models in terms of accuracy on the Accidents data.
The "Ours" model outperforms all other models in terms of accuracy on the Earthquakes data.
The proposed model achieves the highest accuracy among all the models for all three datasets.
The proposed model achieves the highest τ value among all the models for all three datasets.
The table compares different models in terms of their performance on extractive summarization.
The "Supervised" and "+ Uni-ST" models have higher correlation values (SICK r) in comparison to the other models.
The VotingClassifier and Naive classifiers have the same macro F1 score of 0.637.
The MLP Classifier has the highest macro F1 score of 0.668.
The table shows the results of an ablation test on the PTB dataset, where different variations of the full model were evaluated.
The CME model with bootstrapping outperforms all other models and baselines in terms of accuracy for all datasets.
The CME model with bootstrapping achieves the highest accuracy for all datasets, with 100% accuracy for the Battig dataset, 99% accuracy for the DOTA-single dataset, 95% accuracy for the DOTA-mult dataset, and 98% accuracy for the DOTA-all dataset.
Table 1 presents the results of analogical reasoning, given as percent accuracy, for different methods.
The CME model gives the best result on semantic analogies and higher overall accuracy than all other models.
Table 4 provides the results of semantic parsing before and after argument type identification on the GEO and ATIS datasets.
Argument type identification resulted in improved accuracy on both the GEO and ATIS datasets.
The table compares the F1 scores for punctuation using models trained on reference transcripts.
The F1 score for punctuation using the MuSe-ASR model with 3-best output is 69.0.
The MuSe model achieves the highest F1 scores for punctuation prediction (streaming).
The LSTM model has a lower F1 score for predicting noun phrases compared to the BERT and MuSe models.
The ATT_RA+BAG_ATT model achieves the highest AUC values for both the CNN and PCNN models.
The AUC value for the ATT_BL‡ model is higher than the AUC value for the ATT_BL† model.
The models listed in the table are arranged in ascending order based on their AUC values.
The model "PCNN+ATT_RA+BAG_ATT" has the highest AUC value among all the models listed in the table.
The mean inter-bag attention weight increases as the number of sentences in a bag increases.
Bags with 5 or more sentences have a higher mean inter-bag attention weight compared to bags with fewer sentences.
The transfer F1 score for translating from Russian to English is 0.62.
The transfer classification accuracy for translating from English to Russian is 0.58.
In the table, the "Target" language is the same as the "Expanded" language for each row.
The "Expanded F1" values are lower than the "Only target F1" values for each row in the table.
The S2S-DM model achieves the highest scores for both DE2EN and EN2DE translations.
The Transformer model performs better than the Softmax-Q and RAML models for DE2EN translation.
The model "S2S-DM" achieves the highest scores in CIDEr, BLEU, and MET metrics.
The model "Att2in SCST" achieves the highest CIDEr score among all models.
The table presents results on Level 3 using the two segment representation approaches for both User and System speakers.
The accuracy of the System speaker is higher than the accuracy of the User speaker.
The label "Nulo (Nil)" has the highest count in the Level 2 Label column.
The label "Tipo Viaje (Trip Type)" has the highest count in the Level 3 User column.
The table shows the accuracy results of three different approaches on Level 1 using the two segment representation.
The standard deviation of the accuracy results increases as the window size of the Convolutional (CNN) approach increases.
Table 5 shows the accuracy results on Level 1 using context information from n preceding segments.
The accuracy increases as the value of n increases.
Table 7 shows the results on Level 2 using two different segment representation approaches.
The average accuracy for the CNN with window size [1,3] is 71.58 with a standard deviation of 0.33.
The table shows the results on Level 2 using two segment representation approaches.
The accuracy mean values increase as the value of "n" increases.
The accuracy of the model increases as the value of n increases.
The F1 score is consistent across different values of n.
The table presents results on Level 2 using two segment representation approaches for two speakers: "User" and "System".
The F1 score for the "System" speaker is higher than the F1 score for the "User" speaker in the results on Level 2 using the two segment representation approaches.
The table shows the results on Level 3 using three different segment representation approaches: RNN, CNN w = [1,3], and CNN w = [3,5].
The accuracy for all three levels (0, 1, and 2) is above 96%.
The accuracy for Level 3 using the two segment representation approaches is consistently high, with values ranging from 96.17 to 96.29.
The F1 score for Level 3 using the two segment representation approaches is consistently low, with values ranging from 0.06 to 0.11.
The accuracy mean for Level 3 using the two segment representation approaches is consistently high.
The F1 score standard deviation for Level 3 using the two segment representation approaches is consistently low.
Table 16 shows the results achieved on the combination of Levels 1 and 2 using three different approaches: Hierarchical, Single-Label, and Martínez-Hinarejos et al. (2008).
Table 16 provides the MR (Mean Recall) values for the three different approaches: Hierarchical, Single-Label, and Martínez-Hinarejos et al. (2008).
The "Hierarchical" approach achieves an MR m score of 92.34.
The "Martínez-Hinarejos et al. (2008)" approach achieves an MR m score and MR s score of 89.70.
The coverage for the question "Treatment Risks" is 80% and the recall score is 0.90.
The table shows the results for multimodal models and vote-based baselines.
The Late Fusion model has the highest F1 score among all the models.
The "HireNet" model achieves the highest F1 scores for both Text and Audio modalities.
The "Non-sequential" model has the lowest performance in terms of Precision, Recall, and F1 scores for all three modalities.
Table 2 provides part-of-speech tagging accuracy scores for three different models: Multilingual BERT, BERTje850, and BERTje.
BERTje850 achieves the highest part-of-speech tagging accuracy score on the UD-LassySmall Test dataset among the three models mentioned in Table 2.
"Dist. Const. + MMI (Beam=200)" has the highest value in the "distinct-2" column.
"TA-Seq2Seq(Beam=10)" has the lowest average length.
The "TA-10" model has the highest percentage of "No" responses among all the models.
The "DC-MMI200" model has the highest percentage of "Yes" responses among all the models.
The table compares the performance of three different models: DC-10, DC-MMI10, and DC-MMI200.
The results suggest that all three models are considered plausible based on the responses.
Table 2 shows the accuracy for the Gysseling subcorpora.
The Sent-LM model has an accuracy of 62.1 on the Unknown subcorpus.
Table 1 shows the average accuracy across historical languages for different models.
The Sent-LM model has the highest accuracy for both the Full and Ambiguous datasets.
Table 3 provides the average accuracy across morphologically related standard languages, categorized into different types.
The Sent-LM model achieves the highest accuracy for Type 2 languages.
The Sent-LM model performs better than the Sent model on all probing tasks, as indicated by the higher accuracy values for all categories.
The Majority baseline performs worse than both the Sent and Sent-LM models on all probing tasks, as indicated by the lower accuracy values for all categories.
The baseline model performs better on SQuADOPEN compared to CuratedTrec.
Removing the length features improves the performance on WebQuestions.
Table 2 compares the exact matches of RankQA to DrQA as a natural baseline without re-ranking and state-of-the-art systems for neural QA.
RankQA achieves higher exact matches than DrQA for all datasets.
Table 1 presents the mean Pearson correlations over monolingual and cross-lingual tracks of the SemEval 2017 STS dataset.
The monolingual track has lower mean Pearson correlations compared to the cross-lingual track for the LC and LC IDF measures.
The OT (Outlier Threshold) value for both "Es→En" and "Tr→En" is the lowest among all the language pairs.
The performance of the translation model is better for the "Ar→En" language pair compared to the "Ar" language pair.
AdaBoost has the lowest AUC score among the three classifiers on the Census dataset.
XGBoost has the highest AUC score among the three classifiers on the Occupation dataset.
The table presents the results of four different methods for phrase localization on Flickr30K Entities using Fast-RCNN features.
Method (d) λ1=2, λ2=0.1, λ3=0.1 achieves the highest mAP score for phrase localization on Flickr30K Entities using Fast-RCNN features.
The table presents the results of different models on a subset of the Winograd Schema Challenge.
The plausibility score for the SP pair "(eat, meal)" is 10.00.
The plausibility score for the SP pair "(hate, investment)" is 4.00.
The plausibility scores in the Plausibility column indicate the likelihood of the SP pairs being plausible.
The SP pairs (singer, sing) and (law, permit) are more likely to be plausible based on their higher plausibility scores.
The plausibility scores in the table decrease as the SP pair becomes less related or less logical.
The plausibility scores in the table range from 0.75 to 9.77.
The table shows the Plausibility scores for different SP Pairs.
The Plausibility score for the SP Pair "(lift, heavy  [ITALIC] object)" is 9.17.
The plausibility scores in the table range from 0.56 to 10.00.
The table shows different combinations of subject and action words.
The "BERT+CNN+MLP (Our model)" method achieves the highest F1 score among all the methods.
The "BERT+CRF" method achieves the lowest F1 score among all the methods.
As the beam size increases, the likelihood BLEU score decreases.
The +Reconstruction BLEU score is higher than the likelihood BLEU score for all beam sizes.
Table 2 shows the correlation between reconstruction score and translation adequacy (and fluency).
Evaluator1 has a higher adequacy score compared to Evaluator2.
The "RNNSearch+Reconstruction" model performs better than the "RNNSearch" model in all test sets.
The "RNNSearch+Reconstruction" model achieves the highest BLEU score among all the models.
Table 5 shows the contributions of reconstruction from parameter training and reranking of candidates in testing.
The scores in the "Beam 100" column increase as we move from "×" to "✓" in the "Rec. used in Training" and "Rec. used in Testing" columns.
The test scores of the models increase as additional mechanisms (Cov., Ctx., and Rec.) are added.
The addition of the "Rec." mechanism improves the test scores of the models.
None of the MT systems for the "en-de" language pair have a reverse BLEU score greater than its forward score.
For the "et-en" language pair, 7.1% of the MT systems perform better in the reverse direction than in the forward direction.
The average difference in reverse and forward DA scores for the "en-fi" language pair is 9.47.
CTRN performs the best in terms of TRAIN MAP among all the neural baselines.
CTRN performs the best in terms of TRAIN MRR among all the neural baselines.
The model "CTRN (This paper)" has the highest P@1 and MRR scores compared to other models in the table.
The model "QRNN" has a higher P@1 score compared to the model "AP-BiLSTM".
CTRN (This paper) achieves the highest precision at 1 (P@1) and mean average precision (MAP) scores among all the models in the QatarLiving dataset.
CTRN (This paper) outperforms the AI-CNN model in terms of precision at 1 (P@1) and mean average precision (MAP) scores in the QatarLiving dataset.
"CTRN (TRAIN-ALL)" has the highest MAP and MRR scores among all published works on the TREC QA dataset.
"CTRN (TRAIN-ALL)" has a higher MRR score than "He and Lin (2016) - MP-CNN" on the TREC QA dataset.
The "DeepParaphrase Architecture" model has a higher accuracy and F1 score compared to the "Pair-wise Features" model.
The "AugDeepParaphrase" model has a higher accuracy compared to the "SentMod Architecture" model.
The "DeepParaphrase Architecture" model has the highest F1-score.
The "AugDeepParaphrase" model has the highest precision.
The model "AugDeepParaphrase" achieves the highest F1-score of 0.751.
The model "Zhao and Lan (2015)" achieves the highest precision of 0.767.
The AugDeepParaphrase model achieves the highest accuracy and F1 score among all the models.
The ARC-II model outperforms the ARC-I model in terms of accuracy and F1 score.
The sentences classified as claims have a higher average α value compared to the sentences classified as non-claims.
There are more sentences annotated as claims compared to sentences annotated as non-claims.
Table 1 represents the review count of ZhihuLive-DB.
The mean review count of ZhihuLive-DB is 119.
The F1 score for entity extraction is higher for the Netflix data compared to the CB data.
The F1 score for entity extraction is higher for the Session model with missing label compared to the Baseline 1.
ISI-Syntax achieves the highest BLEU score on the NIST-2009 test set.
JHU-Syntax achieves a percentage performance of 99.00% on the NIST-2009 test set.
The F1 score for English (PE18) is higher than the F1 score for German (PE18).
The precision for German (PE18) is higher than the precision for English (PE18).
The table provides information about the performance of the ADU model on English and German languages.
The ADU model performs better on English language compared to German language.
The table shows the accuracy of relation type classification models for English and German.
The XGBoost model performs the best in relation type classification for both English and German.
Our generated bird images have a higher mean Inception score compared to the competitor's generated bird images.
Our generated flower images have a lower standard deviation of Inception scores compared to the competitor's generated flower images.
Our model performs better than the comparison model in terms of BLEU-4 score for bird captions.
Our model performs better than the comparison model in terms of ROUGE score for flower captions.
The p-values for both bird and flower captions indicate a statistically significant difference between our method and the comparison method in terms of faithfulness.
Our method generates more faithful captions compared to the comparison method for both bird and flower.
The table shows the automatic evaluation results of generated captions using metrics BLEU-4, ROUGE, Meteor, and CIDEr.
Our model outperforms the comparison model in terms of all metrics (BLEU-4, ROUGE, Meteor, and CIDEr) for the bird category.
The Support Vector Machine (SVM) model has the highest AUC value among all the models in Table 1.
The Stacked Ensemble model has the lowest Brier Loss value among all the models in Table 1.
The symbol "t" in Table 1 represents the text max length, which is 300.
The symbol "e" in Table 1 represents the embedding size, which is 218.
The system prediction wins in fewer cases than the human summary.
Approximately half of the test examples have equal quality between the system summary and the human-written summary.
Our model outperforms the extractive baselines and the Pointer-Generator models in terms of ROUGE-1, ROUGE-2, and ROUGE-L scores.
Our model achieves the highest ROUGE-1, ROUGE-2, and ROUGE-L scores among all the systems.
The ROUGE-1 score for our model is 35.02.
The ROUGE-L score for LexRank is 14.60.
The error analysis on the 100 sampled dev examples from the Stanford dataset includes categories such as "Good Summary", "Missing Critical Info.", "Inaccurate/Spurious Info.", "Redundant", and "Ungrammatical".
The error analysis shows that 63% of the sampled dev examples have a good summary, 24% are missing critical information, 8% have inaccurate/spurious information, 4% are redundant, and 6% are ungrammatical.
The table compares the performance of two configurations: GRU and StackGRU.
The SymAcc (%) values for each variant in Table 2 are different.
The BLEU (%) values for each variant in Table 2 are different.
The table provides the number of training and testing examples for each dataset.
The table provides the number of classes for each dataset.
The UST (Easy) strategy achieves an average performance of 91.00 on the Dbpedia dataset.
The Classic ST (Uniform) strategy achieves a performance of 78.97 on the IMDB dataset.
The "+match+demotion" combination shows the highest precision, recall, and F1 score for the "Politicians" dataset.
The "+demotion" combination shows the highest precision, recall, and F1 score for the "Public Figures" dataset.
The "+match+demotion" technique improves the F1 score for the model trained on Politicians Training Data.
The "+match" technique improves the Recall score for the model trained on Public Figures Training Data.
The table shows the results obtained from the TempEval-2 test set.
TERSEO achieved a value of 0.98 in the TempEval-2 test set.
SVM-RBF achieves the highest classification accuracy on the Cinderella dataset.
The CNE classifier achieves a classification accuracy of 67 on the Cinderella dataset.
Table 4 presents the results of QA-based evaluation on NYT, CNN/DM, and TTNews, including different methods and their scores on each dataset.
The Oracle method achieves scores of 49.0*, 53.9*, and 60.0* on NYT, CNN/DM, and TTNews datasets, respectively.
Table 10 displays the Euclidean distances between partial input matrices and the normal training matrix.
The Euclidean distance between the partial input matrix without a verb and the normal training matrix is 8.9100. The Euclidean distance between the partial input matrix without a noun and the normal training matrix is 0.9450. The Euclidean distance between the partial input matrix without a location and the normal training matrix is 0.6736.
The table provides hyperparameter tuning ranges for all tasks except Enwik8.
The hyperparameter "input_embedding_ratio" has a low value of 0.0 and a high value of 2.0.
The mLSTM model has a slightly lower perplexity than the LSTM model in the PTB ablation study.
The "learning_rate" parameter has a range from 0.001 to 0.004.
The "mogrifier_rounds (r)" parameter has a range from 0 to 6.
Table 2 shows the automatic evaluation of treatment ranking models using Spearman's rho.
BoW has a higher Precision score than USE in multi-label classification.
BoW has a higher Recall score than USE in multi-label classification.
The accuracy of predicting regular food and location parameters is 100% for both regular test cases with p=0 and p=0.1.
The accuracy of predicting OOV food parameters is higher for the regular test case with p=0.1 compared to the regular test case with p=0.
The table provides information about different experiments conducted on the Books dataset.
The distance of a 1,000 words chapter from a 10-chapter virtual book is higher than the distance from a 10-chapter authentic book.
As the threshold decreases, both the Baseline and Our method Precision, Recall, and F1 scores increase.
Our method consistently outperforms the Baseline in terms of F1 scores.
The CorefNMN model outperforms all other models in R@5, R@10, and mean rank.
The NMN model has the best MRR.
The CorefNMN model has the highest accuracy on the MNIST Dialog dataset.
The AMEM model has an accuracy of 96.4 on the MNIST Dialog dataset.
CorefNMN has the best retrieval performance among all the models.
The retrieval performance of CorefNMN (ResNet-152) is better than CorefNMN.
The CorefNMN model outperforms all other models across all metrics, except for NDGC, where the neural module baseline (NMN) performs better.
The CorefNMN model has a lower mean rank compared to the other models.
The performance of the model improves when using the data augmentation technique "cutout".
The dataset size increases when using the data augmentation techniques "cutout", "dropout", and "both".
The ANP-Replace method performs better than the CNN+RNN baseline in terms of Rouge L, Meteor, and Cide r metrics.
The SentiCap method has a higher fraction of images for which at least two AMT workers agree that it is the more negative sentence compared to the ANP-Scoring method.
Table 3 shows the crowd-sourced evaluation on content overlap for summary vs. sentence level on the validation set.
The F1 score for stable sentences is higher than the F1 score for greedy sentences.
The "Ours (NLG)" model outperforms other models in terms of natural language metrics in the MIMIC-CXR dataset.
The "Ours (full)" model performs better than other models in terms of the "Natural Language BLEU-4" and "Natural Language BLEU-3" metrics in the Open-I dataset.
The label "Pleural Effusion" has the highest precision score of 0.735.
The precision score for the label "Fracture" is 0.000.
"Oracle (Multi)" has the highest F-score values among all methods (R1, R2, RL) in Table 2.
The F-score values for all three methods (R1, R2, RL) are higher for "bart-wcep DynE-5" compared to "bart-wcep DynE1" in Table 2.
The F-score for the MultiNews Transformer method is higher than the F-score for the MultiNews Hi-Map method.
The F-score for the bart-MultiNews DynE-1 method is higher than the F-score for the bart-MultiNews DynE-5 method.
The F-score for PG-MMR with SentAttn is higher than the F-score for PG-MMR with SummRec and PG-MMR with Cosine (default) for all three R values.
The F-score increases as the DynE value increases for bart-cnn-dm for all three R values.
Table 9 provides the test perplexity for phrase prediction for two different alignment models.
The "Topic-Aware" model has a lower test perplexity compared to the "Baseline" model for both 50 and 100 topics.
Incorporating a topic model in the equation model improves the perplexity metric.
The TE-LSTM (Ours) model has the best performance in terms of both perplexity and syntax error rate among all the equation models.
The meProp algorithm with k=20 outperforms the MLP algorithm with h=20 in terms of Test UAS in the Parsing task.
The meProp algorithm with k=20 outperforms the MLP algorithm with h=20 in terms of Test Acc in the MNIST task.
The meProp model with k=20 has a higher backprop time and a slightly higher test UAS than the MLP model with h=500 in the Parsing (AdaGrad) task.
The correlation between WER and TER is higher than the correlation between MR-WER and TER.
The correlation between WER and WERd is higher than the correlation between MR-WER and WERd.
The MR-WER value after normalization is 25.3%.
The WER value for ref2 after normalization is 42.9%, and the TER value for ref2 after normalization is 38.7%.
The Word Error Rate (WERd) increases as the maximum edit distance (ED) increases for all references.
The Word Error Rate (WERd) decreases as the maximum edit distance (ED) decreases for "ref3".
Table 1 provides information about OOVs on the dev and test sets.
The BLM system achieves the highest MTR score on the NewsTest dataset.
The BLM+BWE10 system achieves the lowest TER score on the NewsTest dataset.
The alignment quality between English and European Union languages has a correlation of -0.76.
The syntactic distance between English and Spanish has a correlation of 0.40.
UniLM-v2 achieves the highest scores for ROUGE-2/L, BLEU-3/4, METEOR, and SPICE metrics.
BART achieves the highest score for the Coverage metric.
The "combine-QT" model outperforms all other models in terms of "MR", "CR", "SUBJ", "MPQA", "TREC", "MSRP (Acc)", "MSRP (F1)", "SICK r", "SICK ρ", and "SICK MSE".
The "uni-QT" and "bi-QT" models have the same dimensionality of 2400.
As the embedding size increases, the performance also increases.
As the embedding size increases, the training time also increases.
The training set contains 18,531 questions and 38,586 QA pairs.
The average number of tokens per question is 70.97 in the test set.
The CsrQA model has the highest accuracy at the top-3, top-5, top-30, and MRR metrics compared to other systems.
The "Ratings" method performs better than the "Random" method in terms of accuracy at top-3, top-5, top-30, and MRR metrics.
Table 6 shows the test set performance (Acc@3 in %) of ablation systems on questions with different candidate answer space sizes.
The candidate space size is divided into two categories: <=1000 and >1000.
Table 7 shows the performance of different systems, including the CsrQA model, on a task measured using human judgments and gold-reference data.
The CsrQA model has an accuracy of 19.79% for CR and 22.92% for CS.
The "CrQA (CL) Task Emb." method achieves the highest accuracy at the top-3 and top-5 positions, as well as the highest MRR among all the methods.
The "CrQA (CL) Doc2Vec Emb." method achieves a slightly higher accuracy at the top-30 position compared to the "CrQA (No CL)" method.
Table 2 provides a summary of the improvements and degradations caused by neural reranking.
The total improvement percentage caused by neural reranking is 78%.
The rerank system improves the BLEU score for the "en-ja" language pair compared to the base system.
The rerank system outperforms the base system in terms of HUMAN score for the "ja-en" language pair.
The average accuracy improvement or deterioration for each category between WMT18 and WMT19 is shown in Table 2.
The accuracy for the "Verb tense/aspect/mood" category improved by 5.1% between WMT18 and WMT19.
The system with the highest accuracy in each category is indicated by boldface in the "average (categories)" row.
The system with the highest accuracy in the "Verb tense/aspect/mood" category is indicated by boldface in the corresponding column.
The system with the highest forward BLEU score is "MMT-production-system" with a score of 51.8.
The system with the smallest difference between forward and reverse BLEU scores is "Facebook-FAIR" with a delta of 0.4.
The BT model performs better than the OP model in all language directions.
The BT model performs better on the reverse portion of the test set compared to the direct portion in all language directions.
The table shows the concept detection performance on compound word queries.
The concept "qEcAzAd" has the highest MCC values for both non-experts and experts.
The accuracy of the test data increases as the length of the data increases.
The accuracy of the unigram chimera decreases as the length of the data increases.
Game D has a smaller lexicon size compared to the other games.
Game C is balanced, with an equal number of distractors on each side.
The average number of lemmas in the projects from the MathComp family is 11,266.
The average number of tokens per project in the MathComp family is 2,235,082.
The overall performance of the base system in the CoNLL16 Test is 60.4.
The overall performance of the system with frame-chain SemLM features and additional conditional probability and frame embedding features in the CoNLL16 Blind Test is 53.8.
LB consistently produces the lowest perplexities for both frame-chain and entity-centered SemLMs.
The perplexities for SemLMs are generally lower when using Gold PropBank Data with Frame Chains compared to using Gold Ontonotes Data with Coref Chains.
For both "Gold PropBank Data with Frame Chains" and "Gold Ontonotes Data with Coref Chains", FC performs better than FC-FM in terms of MRR.
LB performs better than other methods for both frame-chain and entity-centered SemLMs.
Table 5 shows the results of co-reference resolution with entity-centered SemLM features on the ACE04 and CoNLL12 datasets.
The addition of entity-centered SemLM features with log-bilinear model and conditional probability and frame embedding features improves the performance of the system on co-reference resolution tasks.
The table includes the performance metrics of three different algorithms: Up-Down + LSTM, Up-Down + Transf, and Up-Down + ObjRel Transf.
The Up-Down + ObjRel Transf algorithm has the highest CIDEr-D score and BLEU-4 score compared to Up-Down + LSTM and Up-Down + Transf algorithms on the test split.
The model trained with "geometric attention" has the highest CIDEr-D score among all the positional encoding methods.
The model trained with "positional encoding (ordered left-to-right)" has a slightly higher CIDEr-D score than the model trained with "positional encoding (ordered top-to-bottom)".
The "Government operations" topic has the highest percentage among all the topics.
The keywords associated with the "Education" topic are "education", "area", and "loan".
The table compares the running time per iteration of the CGS, Word, and Refine methods on datasets of different sizes.
The ratio of running time for the Refine method to the CGS method decreases as the number of documents increases.
The objective value decreases as we move from the "Basic" algorithm to the "Word" algorithm and then to the "Word+Refine" algorithm.
The values for the "Basic", "Word", and "Word+Refine" algorithms in the "SynthA" column are all lower than the corresponding values in the "SynthB" column.
The average number of words per document in the Annotated Enron Subject Line Corpus is 75.
The Gigaword News Headline dataset has 3,799,588 training documents, 394,622 validation documents, and 381,197 testing documents.
Our system outperforms both random selection, see2017get, original subject, and human annotation based on the overall evaluation scores.
Our system is rated as more informative compared to random selection, see2017get, original subject, and human annotation.
The table provides correlation analysis between the automatic scores and the human evaluation for the ESQE, ROUGE-1 F1, METEOR, and Inter-Rater Agreement metrics.
The Inter-Rater Agreement metric shows a higher correlation with the human evaluation compared to the Pearson's r and Spearman's ρ values for all metrics.
The table provides different types of feature templates used to score formulas.
The table includes feature templates related to Proximity, Familiarity, Compatibility, and Similarity.
Table 3 provides the experimental results of the larger LSTM model with roughly 63 million parameters.
The WER for DMA-I in the "ST without H" system is 12.70.
The T5-Large model achieves the highest BLEU score among all the models.
The T5-Large model achieves the highest METEOR score among all the models.
T5-3B achieves the highest scores in Overall BLEU, Overall PARENT, Overall Subset BLEU, Overall Subset PARENT, Nonoverlap Subset BLEU, and Nonoverlap Subset PARENT on the ToTTo test set.
BERT-to-BERT achieves a higher Nonoverlap Subset BLEU score than the Content Planner and Pointer-Generator models on the ToTTo test set.
Table 5 shows the results on Multiwoz.
T5-Base has the highest BLEU score among all the models.
The "Posterior" method generally performs better than the "Teacher" method in zero-shot transfer on the UD POS tagging datasets.
The "Posterior" method shows consistent improvement in performance compared to the "Teacher" method, while the "Posterior+Top-K" method shows mixed results.
Table 7 provides a summary of order-preserving lexicon translation.
The order-preserving lexicon translation using lexicons with manual selection (OGM) has a higher value than the order-preserving lexicon translation (OG) in the "no2sw" row.
Table 4 provides information on the effects of adding family labels on BLEU scores with respect to increasing addition of language families.
The BLEU scores increase as the addition of language families increases.
The UNIF model performs better than the CODEnn and SCS models on the Java-50 benchmark.
The UNIF model performs better than the CODEnn and SCS models on the Android-287 benchmark.
The table shows the results of ablation tests on the Yelp dataset for different combinations of objectives.
The combination of [ITALIC] JVAE, [ITALIC] Jmul(s), [ITALIC] Jadv(s), [ITALIC] Jmul(c), and [ITALIC] Jadv(c) objectives achieves the highest accuracy in the ablation tests on the Yelp dataset.
The "Manipulated" attention type results in higher accuracy for "Bigram Flip", "Sequence Copy", "Sequence Reverse", and "En → De MT BLEU" compared to the other attention types.
The "Uniform" attention type results in lower accuracy for "Bigram Flip", "Sequence Copy", "Sequence Reverse", and "En → De MT BLEU" compared to the other attention types.
The table shows the accuracy of various classification models, along with their attention mass, on impermissible tokens I, with varying values of the loss coefficient λ.
The BERT (mean) model performs well on Occupation Prediction, Gender Identity Classification, and Ref. Letters Accuracy.
MHAM achieves the highest scores in all three metrics: sBleu, cBleu, and Rouge-L.
The KL model has a lower score in sBleu compared to ALK.
There are two models listed in the table: CNN and RNN.
The average accuracy for the "+ label" condition is highlighted in bold.
The ST model has the lowest word error rate among all the models.
The ST model has the lowest phoneme error rate among all the models.
The proportion of word errors produced by the Dense model when shared with the Sigmoid model is 0.789.
The proportion of word errors produced by the Sigmoid model when shared with the ST model is 0.827.
Table 2 shows the EER(%) results of three SV systems.
The Deep feature system with LDA scoring has the lowest EER% for both C(4-4) and C(40-4) among the three SV systems.
The "LightConv + CGC Encoder (Ours)" model achieves the highest BLEU-4 score of 35.21.
The "LightConv + CGC Encoder (Ours)" model has the same number of parameters as the "LightConv" model, which is 38.15M.
Table 2 presents the results of single model ablations on the development set.
The DCN model with a pool size of 16 HMN achieves the highest performance on the development set.
The F1 score for EASDRL in extracting exclusive action names is 56.19.
The F1 score for BLCC-8 in extracting exclusive action arguments is 13.45.
The number of labeled texts for WHS is higher than CT.
The action name rate for WHS is higher than WHG.
EASDRL achieves the highest F1 scores for all types of action names and action arguments among all the methods.
BLCC-8 achieves the highest F1 score for extracting action names with the WHG label among all the methods.
Table 1 compares the performance of GloVe, PCA, and word2vec on different datasets and benchmarks.
The cosine similarity for the PMC Articles dataset using the original word2vec algorithm is 0.75.
Cui2vec outperforms Choi et al. (claims) in detecting comorbidity relationships in the NDFRT dataset.
Cui2vec has a higher Spearman correlation with human assessments of concept similarity in the Semantic Type dataset compared to Choi et al. (notes).
The model "D16-1130" achieves the highest F1 score for the "Exp." category.
All models in the table achieve an F1 score higher than 50% for the "Cont." category.
Table 1 shows the accuracy (%) on the test sets of the PDTB-Lin and PDTB-Ji settings for multi-class classification.
The "Ours" model has the highest accuracy on both PDTB-Lin and PDTB-Ji settings.
The table compares the performance of two methods, SAN and BiDAF, on DuoRC dataset.
Adding AdaMRC to both SAN and BiDAF improves the performance on DuoRC dataset.
AdaMRC achieves the highest EM/F1 scores compared to the baseline models on all three datasets.
Adding ground truth questions to AdaMRC improves its performance in terms of EM/F1 scores on all three datasets.
The method "AdaMRC + BERTBASE" achieves a higher EM/F1 score than the method "BERTBASE".
The performance of both SAN and AdaMRC + SAN models improves as the labeling ratio increases in the semi-supervised domain adaptation experiment.
The AdaMRC + SAN model achieves better performance than the SAN model in the semi-supervised domain adaptation experiment with varied labeling ratio.
The macro-F1 score for the relation classification model trained on the Original Set with curriculum learning in the 4th round is 79.04±0.29.
The macro-Precision score for the relation classification model trained on the Original Set with curriculum learning in the 6th round is 79.76±0.42.
As the noise ratio increases, the set scale decreases.
As the set scale decreases, the noise ratio increases.
As the token scale increases, the BLEU scores also increase.
The LR scores for the "normal" token scale are lower than the LR scores for the other token scales.
The total number of pairs in the training set for the En-It language pair is 241,618.
The number of pairs with a length ratio between (1.2,∞) for the En-De language pair is 847.
Table 6 provides the results of a manual evaluation on En-It (large data) ranking translation quality.
The p-value for both Baseline and Len-Tok is less than 0.05.
Table 2 compares training methods on PENN Treebank (arc-standard and arc-eager) and the German part of SPMRL-2014 (swap-standard).
The RL-R training method achieves a UAS score of 92.2.
RL-O has a lower total loss than RL-R.
The error proportion decreases as we move from SL to RL-M.
The validation accuracy for the "N-V-Adj-Adv-Pro" class using the PTB-norm cbow model is 66.33.
The validation accuracy for the "sg-pl" class using the WSJ-norm LM emb model is 96.31.
The "Mesoscopic (all features)" approach has the highest ARI and Accuracy scores among the different network approaches.
The "Mesoscopic (Matching Index)" approach has lower ARI and Accuracy scores compared to the "Mesoscopic (Clustering)" approach.
Table 5 provides the higher-order RSI accuracies of various methods on three datasets.
TFBA performs better than HardClust for Shootings and NYT Sports datasets.
The table provides details of the dimensions of tensors constructed for each dataset used in the experiments.
The shape of X1 for the Shootings dataset is 3365×1295×50.
The table provides PGR values for the CBOW model trained on different corpora.
The CBOW model trained on the Wiki 1.6B corpus has a PGR value of 100 for the "syn" aspect.
The strategy "syn" has the highest number of wins if the iteration is stopped when the specified task peaks.
The strategy "loss" has the lowest number of wins if the iteration is stopped when the specified task peaks.
TopicCoRank{assign} outperforms TopicRank and KEA++ in terms of Precision (P), Recall (R), and F-score (F) in the Linguistics, Information Science, and Archaeology domains.
TopicCoRank{assign} achieves the highest F-score (F) among all methods in the Archaeology domain.
The F1 scores for the "using SLK" and "using FLK" encoding strategies are higher than the F1 score for the "no lexicon" strategy.
The recall scores for the "using SLK" and "using FLK" encoding strategies are higher than the recall score for the "no lexicon" strategy.
BERT achieves a higher F1 score on the OntoNotes4 dataset compared to the Resume dataset.
SLK-NER achieves higher precision and recall on the OntoNotes4 dataset compared to the other models.
The total number of questions in the evaluation data is 388.
The "urank" method achieves the highest Rouge-1 score of 48.5 on the DUC 2002 dataset.
The "nn-se" method achieves the highest Rouge-2 score of 8.3 on the DailyMail dataset.
The "nn-abs" model has the highest mean rank among all the models.
The "nn-se" model is ranked 3rd among all the models.
The "ours" captioner achieves a 4.11% improvement in terms of cross entropy loss SPICE compared to BUTP.
The "ours" captioner achieves a cross entropy loss CIDEr of 110.2.
The "ours-single" method achieves the highest accuracy in all four categories: Test-standard Yes/No, Test-standard Num, Test-standard Other, and Test-standard All.
The "ours-ensemble-10" method achieves a higher accuracy than the "ours-single" method in all four categories: Test-standard Yes/No, Test-standard Num, Test-standard Other, and Test-standard All.
Table 3 presents the evaluation results of semantic connection modeling in the test-standard split.
The models "ours (w/ o SC)" and "ours (w SC)" outperform the "BUTD" model in terms of accuracy in the test-standard split.
The performance of CCO-GRU decreases as the number of models increases.
The maximum length of the input sequence is the same for all three training setups.
Table 1 provides information about the training setups for different tasks.
Table 2 provides information about Hits@1 for the WebQuestionsSP and MetaQA datasets, as well as Hits@1 and Hits@10 for two KB completion tasks.
The Hits@1 for RSF on the WebQuestionsSP dataset is 52.7.
The table provides results for different models on the NELL-995 dataset.
RSF (Ours) achieves a Hits@1 score of 64.1 and a Hits@10 score of 82.4 on the NELL-995 dataset.
The table presents the averaged macro-F1 scores for Word Sense Disambiguation (WSD) using three different models: SGNS (averaged), ELMo (averaged), and ELMo (target).
The ELMo (target) model achieves the highest macro-F1 score for WSD in the Russian language.
The lemma-based models show less variation in F1 scores compared to the token-based models for all target words.
The target word "круп" has a lower F1 score compared to the target words "акция" and "крона".
Table 6 shows F1 scores for excluded target words from RUSSE'18.
The values in the "STD" column represent the standard deviation.
As the number of slots in the corresponding MR increases, the average number of sentences in the reference utterance also increases.
The highest proportion of MRs has 5 slots, followed by 6 slots.
The main model of the MTL + LEL + LTN + main preds feats combination achieves the highest performance on the FNC task.
The MTL + LEL + LTN + semi combination achieves a performance of 44.28 on the Topic-2 task.
Table 6 provides the error analysis of LTN with and without semi-supervised learning for all tasks.
The category "SECOND-WEA4" has the highest precision score.
The category "SECOND-WEA1" has the highest recall score.
The BERT model achieves an accuracy, F1, precision, and recall of 66.9%, 65.8%, 65.9%, and 66.2% respectively.
Table 4 provides results of the full model comparing word vectors derived from Google News and Wikipedia on development sets for each corpus.
The performance of the full model using word vectors derived from Google News is 87.5 on the ACE corpus.
Table 3 compares different approaches using different sets of information for ACE, CoNLL, and WP datasets.
Using the full set of information (six features) gives better performance than using only topic information or information derived from the mention and target entity title for ACE, CoNLL, and WP datasets.
The SimLex correlation for Glove vectors with transformation X is 36.9.
The accuracy for the Religion task using GC vectors with transformation B is 68.6.
The table shows the hyperparameters for learning sparse overcomplete vectors tuned on the WS-353 task.
The GC representation has the highest sparsity percentage among all the representations.
Table 5 displays the accuracy of three human annotators on the word intrusion task, along with the average inter-annotator agreement and Fleiss' κ.
The inter-annotator agreement κ for annotator X is 0.40, while the inter-annotator agreement κ for annotator A is 0.45.
Supervised MUSE achieves the highest precision at rank 1 for both the En-Fr and Fr-En language pairs.
BilLex(PS+PD+PI) achieves the highest precision at rank 1 for both the En-Fr and Fr-En language pairs.
The Supervised MUSE model outperforms the other models in terms of word translation accuracy for both En-Es and Es-En translations.
The Unsupervised MUSE model achieves the highest word translation accuracy among all the models for all language pairs.
The texts in the first and second rows have a low valence score.
The valence scores in the table reflect the emotional tone of the paragraphs, with higher scores indicating positive or supportive content and lower scores indicating negative or hateful content.
The paragraphs with the highest and lowest valence scores in the table are from different time periods.
The HMM-TDNN model has a lower Word Error Rate (WER%) than the HMM-GMM model.
The HMM-TDNN model performs better than the HMM-GMM model in terms of word recognition without transcription errors.
Table 5 shows the OpenTag results on disjoint split.
The F-score for the Disjoint Split is 82.4%.
Table 4 shows the performance comparison of different models on attribute value extraction for different product profiles and datasets.
OpenTag performs better than other state-of-the-art NER systems based on BiLSTM-CRF.
The BERT+SAN model achieves the highest F1 score on the REST dataset.
The LSTM-CRF model achieves an F1 score of 54.71 on the LAPTOP dataset.
Table 1 provides the mean precision at one (P@1) for LAMA and LAMA-UHN on the TREx and GooglRE subsets.
The precision at one (P@1) for LAMA-UHN is higher than for LAMA in both the TREx and GooglRE subsets.
The table provides information about different corpora, such as Google-RE, T-REx, ConceptNet, and SQuAD.
The BERT-kNN model performs better than BERT-base, BERT-large, and kNN models across the set of evaluation corpora.
The table shows the backward perplexity (bw PPL) on the source for three different models: Interleave, no S-T, and no S-C.
The backward perplexity (bw PPL) for the Interleave model is 253.24, for the no S-T model is 482.25, and for the no S-C model is 453.78.
The table shows results on the ELI5 dataset using different models.
The "Multi + Sequential" model achieves the highest R-2 score and the lowest PPL score among all the models.
Table 8 shows the backward PPL scores of different models for the question generation task.
The Interleave model has the lowest backward PPL score among all the models.
The coverage model performs worse than the base model.
The "Ours" model has better alignment quality than the "Base" model.
The initialization of decoder states with "Source Summarization" leads to a slightly higher BLEU score compared to initializing with an "All-Zero Vector".
The NMT decoder does not fully utilize the information from source summarization, resulting in a lower BLEU score.
The table provides results for different systems and architectures.
The "Ours" system performs better than the "Base" and "Coverage" systems for both De-En and En-De translation tasks.
The TWA verification accuracy for LM is lower than for CCA.
Var4 performs better than Var5 in terms of Hits@10.
LM performs better than CCA in terms of Mean for the WK3l-15k En-Fr dataset.
Var5 achieves the highest accuracy for WK3l-120k En.
Var4 and Var5 have higher accuracy scores for WK3l-120k Fr compared to Var1 and Var3.
The "Hits@10" metric is provided for En-Fr, Fr-En, En-De, and De-En languages in Table 13.
The highest values in the "Hits@10" metric are achieved in the Var4 row for En-Fr, Fr-En, En-De, and De-En languages in Table 13.
Table 13 shows the accuracy of triple-wise alignment verification for two language combinations: En&Fr and En&De.
The accuracy of triple-wise alignment verification for the En&Fr language combination is [BOLD] 97.46.
The table displays the performances of different systems for noun phrase recall and aggregated F1 scores on the Eve dataset.
The DB-PCFG (D2K15) system shows the highest noun phrase recall and aggregated F1 scores on the Eve dataset.
The system "DB-PCFG" has the highest precision, recall, and F1 scores among all the competing systems.
The "Right-branching" baseline system has higher precision, recall, and F1 scores compared to the "UHHMM-F" system.
The "DB-PCFG (D2K15)" system achieves the highest precision, recall, and F1 scores on the WSJ10test dataset.
The "Right-branching" system has the lowest F1 score on the WSJ20test dataset.
The precision scores for all systems are higher on WSJ10 compared to WSJ40.
The recall scores for all systems are higher on WSJ10 compared to WSJ40.
Fixed BART performs better than the baseline on WMT'16 RO-EN augmented with back-translation data.
Tuned BART improves the performance over the baseline on WMT'16 RO-EN augmented with back-translation data.
The "multi-query" type achieves the highest BLEU score on the test dataset.
The "multi-head local" type achieves the lowest perplexity score.
The Expert segmentation has a higher precision than the Naive segmentation.
The Naive segmentation has a higher recall than the Expert segmentation.
The "TA-SEQ2SEQ" model has the lowest speed among all the models.
The "DOM-SEQ2SEQ" model has the fastest speed among all the models.
The models evaluated in the table are SEQ2SEQ, CVAE, LAED, TA-SEQ2SEQ, DOM-SEQ2SEQ, AdaND (with context para.), AdaND (with topic para.), and AdaND (with both).
The relevance scores for the models range from 69.60% to 75.59%.
The accuracy of the toponym extraction approach decreases as the granularity level increases.
The accuracy of the toponym extraction approach is higher for the "Country" granularity level than for the "City" granularity level.
The EDLPS model outperforms all other models on all metrics.
The EDLP model has the highest CIDEr score among all models.
Table 1 provides information about hyperparameters.
The dropout rate is 0.4.
The average rating for "Our System" is 3.43.
The ratings for "Our System" range from 2.83 to 3.90.
Model (d) LSTM with context information has a higher accuracy than Model (c) Long Short-term Memory.
Model (h) Smoothing with context information has a higher accuracy than Model (g) Sharpening.
Model (a) Oracle and Tf-idf Sorting have the same evaluation results for key term extraction.
The Smoothing technique has a higher evaluation result than the Sharpening technique for the Neural Attention Model in key term extraction.
Table 4 provides coverage and correlation coefficients for different datasets.
Table 1 shows the inter-annotator agreement scores for different datasets.
The exact match and F1 scores are higher for the grounded datasets compared to the free datasets.
The system "elcoref-qa∗" achieves the highest scores for both Wiki and QuAC datasets.
The system "elcoref-qa∗" has a higher F1 score compared to the other systems.
Table 1 shows different classes of features in the EAT-vector format.
Each feature in Table 1 is assigned a unique index number.
For the target "vidaciudad", it took 6 iterations to converge.
For the target "Target manzana", the final residual set constitutes 6.46% of the dataset.
BPE achieves the highest F1 score on the AskUbuntu dataset.
Recast has the highest average F1 score among all the platforms.
As the number of questions in the "Opinion Split" increases, the number of correct answers also increases.
As the number of questions in the "Opinion Split" increases, the average confidence level also increases.
The ablation study investigates the effect of different types of attention on the performance metrics.
Increasing the number of attention heads leads to a decrease in the average number of steps.
As the value of λ decreases, the average attention time steps increase.
The highest BLEU-4 score is achieved when λ is 0.
ELMo embeddings with smoothing have the highest Annotation F-2 score compared to other embedding sources.
BERT embeddings without smoothing have a higher Annotation Precision compared to BERT embeddings with smoothing.
Table 6 shows the performance variation across different numbers of labels per post.
The Fmacro score decreases as the number of labels per post increases.
The proposed method "s(wl(ELMo), wl(GloVe), wl(Ling), tBERT)" achieves the highest values for all metrics compared to other proposed methods.
The baseline method "BERT-biLSTM-Attention" achieves lower values for all metrics compared to other baseline methods.
The total number of annotations for the Hollande - FH group of annotators is 5286.
The aspect correction rate for the C.S. (IR) domain in the Hollande - FH group of annotators is .034.
There were 30 corrections made using the contents-based approach for the candidate Hollande.
There were 25 corrections made using the committee-based approach for the candidate Sarkozy.
Table 4 represents the human evaluation of a trained agent.
The "State2Seq (Ours)" model has an average success rate of 0.778, average reward of 53.88, and average turns of 11.88.
The "State2Seq" model has the highest average success rate in both the Movie Booking and Restaurant Reservation tasks.
The "Agenda" model has the highest average number of turns in the Restaurant Reservation task.
Table 5 provides an analysis of agent policy performance during training on the movie domain.
The Agenda model has the highest success rate, average reward, and average turns compared to the other models.
The Char+Bi+dom model achieves the highest GLEU Test score among all the models.
The GLEU Test score of the SMT+BLEU model is 38.31.
Table 6 displays the M2 scores on the CoNLL-2013 dev set for the Word+Bi model.
As the bias parameter increases, the precision values decrease.
Table 6 shows the performance of ablated models in the one-to-many setting (3-action model).
The performance of NeuMATCH in terms of HM-1 clips and HM-2 clips is higher than the performance of the ablated model with no history.
Table 3 describes the ablation test results on NYT10 and NYT11.
The "HMT" method achieves higher precision, recall, and F1 scores on both NYT10 and NYT11 compared to the "W/o EE Task" method.
The HRL method achieves the highest precision, recall, and F1 score on both NYT10 and NYT11.
The HMT method achieves the highest F1 score on both NYT10 and NYT11.
Our model achieves a sentiment accuracy of 0.851 on the SST-full dataset.
Our model achieves a sentiment accuracy of 0.701 on the Lexicon dataset.
Table 3 shows the performances of different models on the test set of the CTB dataset.
The F-score of the "CNN+Pooling+Highway" model is higher than the F-scores of other models.
The "Quark + SR-MRS (Ours)" model achieves the highest scores for Answer EM and Answer F1.
The "HGN (RoBERTa) + SR-MRS" model achieves the highest score for Joint EM.
The Quark (Ours) model achieves the highest Answer EM score among all the QA models in Table 1.
The HGN (RoBERTa) model achieves the highest Support F1 score among all the QA models in Table 1.
Table 3 presents an ablation study on sentence selection in the distractor setting.
The Oracle model does not consider any context and achieves a Sup F1 score of 3.
Table 1 presents the results of multiclass relation identification using different models.
The model trained using conditional training achieves the highest accuracy and Macro F1 scores among all the models.
Conditional training results in a significantly higher accuracy compared to the baseline.
Joint training does not significantly improve the accuracy compared to the baseline.
The perplexity of the "1. RNNLM" model is higher than the perplexity of the "3. DrLM" model.
The dimension K of the "1. RNNLM" model is the same as the dimension K of the "3. DrLM" model.
Table 3 describes the effect of future context on WER performance.
As the duration of future context increases, the WER performance decreases.
Our system achieves a lower WER than both Baseline 1 and Baseline 2.
Our system has a higher number of parameters than both Baseline 1 and Baseline 2.
Making changes to the TDS model leads to an increase in the WER values.
The WER values are higher for the "vid-noisy" dataset compared to the "vid-clean" dataset.
The models in the table were trained with 960 hours of Librispeech data.
The "BLSTM" model achieved a loss of 4.3 on the "dev-clean" data and a loss of 4.8 on the "test-clean" data.
The model "AE-SCL-SR" outperforms all other models in sentiment classification accuracy for most source→target combinations.
The model "AE-SCL-SR" performs better than the model "MSDA-DAN" in sentiment classification accuracy for the D→B source→target combination.
The sentiment classification accuracy for the Blitzer:07 task is 0.768∗+⋄ when using AE-SCL-SR with the B→E adaptation.
The sentiment classification accuracy for the Blitzer:07 task is 0.85 when using MSDA-DAN with the E→K adaptation.
Table 2 provides the performance (WER) of different models on the WSJ dataset using only the acoustic training data.
The ASG model with zero LM decoding has a higher WER on the nov93dev dataset compared to the nov92 dataset.
The table compares the WER performance of different models on WSJ.
The table shows the mean squared error (MSE) loss of stacked LSTMs and the RAE model for different embedding sizes.
The MSE loss decreases as the embedding size increases for both the LSTM and RAE models.
The models listed in the table are classifiers used for sentiment analysis on the SST-5 and SST-2 datasets.
Table 8 compares the performance of the SQuAD-T-pretrained model (SQuAD-T-Y) and the SQuAD-pretrained model (SQuAD-Y) on WikiQA.
The SQuAD-T-pretrained model (SQuAD-T-Y) performs worse than the SQuAD-pretrained model (SQuAD-Y) in all categories.
Table 1 provides detailed results for the baseline system on the ES development set.
The baseline system classified 43 instances as "NEU" in the ES development set.
The macro average precision, recall, and F1 score are all around 47%.
The accuracy of the baseline system on the ES development set is 61.10%.
The cnn-trad-pool2 model uses two types of layers: "conv" and "softmax".
The number of filters used in the "conv" layers of the cnn-trad-pool2 model is 8.
MSCQG (M) performs better than MSQGGPT2 (B) in all criteria.
MSQGGPT2 (B) performs better than the Oracle (O) in overall evaluation.
The MSCQGself-critic PG+ SCR+ H model outperforms all other models in the "Search-Engine Augmented" category for all IR metrics.
The MSCQGself-critic PG+ SCR+ H model achieves the highest Out-Sample IR MRR score among all models.
The table presents accuracy (%) with increasing word-level dropout across four datasets: MR, Subj, CR, and SST.
The Baseline model achieves the highest accuracy for the MR dataset with a word dropout rate of 0.
The baseline accuracy for the MR dataset is 67.5% and the accuracy for the CR dataset is 61.0%.
The accuracy for the MR dataset with dropout β=0.5 is 71.0% and the accuracy for the CR dataset is 62.1%.
Table 6 provides the number of rare RTPs (Recoverable True Positives) compared to the total number of RTPs found in the candidate term lists of each ATE method.
The Basic, ComboBasic, LP, NTM, PU ATE method has 143 rare RTPs and 2,090 total RTPs in the ACLv2 dataset.
The number of ground truth terms in the GENIA dataset is 33,396.
The maximum Recoverable True Positives for the ACLv2 dataset using the ATE methods TFIDF, CValue, RAKE, Weirdness, Relevance, GlossEx, and χ2 is 1,976.
The logZ mean is higher for the CE trained models compared to the L2 trained models for both the CCG supertagging and Machine Translation tasks.
The accuracy/BLEU score is higher for the CCG supertagging task compared to the Machine Translation task.
The average accuracy for BERT non-OOV is higher than BERT OOV for all languages.
The accuracy for BERT non-OOV is higher than BERT OOV by 6.3% on average.
The table provides the performance of various models on the test set of Weibo NER.
The models "BERT_oov" and "BERT_zh" have the highest F1 scores among all the models.
The table shows the sequence classification performance on WCL based on 10-fold cross-validation with the same data splits.
Our method achieves the highest precision, recall, and F1 score compared to other methods.
The proposed model "Ours" outperforms the baselines (DefMiner, LSTM-CRF, and GCDT) in terms of WCL F1, W00 F1, Textbook F1, and Contract F1.
The addition of BERT to the proposed model "Ours" improves the performance in terms of WCL F1, W00 F1, Textbook F1, and Contract F1.
The performance of the Full - DPP model decreases as the number of layers for GCN increases for both Textbook and Contract metrics.
The Full Model outperforms the Full - DPP (2-layer GCN) in terms of Textbook and Contract metrics.
Table 6 shows the per class performance on the Textbook and Contract datasets of DEFT corpora.
The precision for the "B-Term" class in the Textbook dataset is 68.1.
The "Our Model" outperforms the "Single Task" model on all the mentioned tasks.
The "Our Model" performs worse on the "Multi-Objective IMDB" task compared to the "Multi-Objective RN" task.
According to BERT, the words "Where" and "were" have higher prominence compared to the reference labels.
According to BERT, the word "feel" has higher prominence compared to the reference labels.
BERT predicted the words "left," "other," and "straight" with high prominence compared to the reference labels.
BERT predicted the word "up" differently compared to the reference label.
BERT predicts the word "next" and "concealed" to have higher prominence compared to the reference labels.
BERT predicts the word "he" to have lower prominence compared to the reference labels.
Table 2 shows the DERs on two-speaker CALLHOME for three different models: x-vector+AHC, EEND, and SC-EEND.
Table 2 shows the DERs on two-speaker CALLHOME for three different training methods: PIT, Greedy+TF, and PIT+TF.
The table shows the results of three different models: EEND, SC-EEND, and SC-EEND.
The Diarization Error Rate (DER) decreases as the number of speakers increases for all models.
The highest estimated score is in the "Estimated 3" column for the "Reference 4" row.
The estimated scores decrease as the reference number increases.
The highest estimated value for 2 is 130, for 3 is 54, for 4 is 3, for 5 is 0, and for 6 is 0.
The highest estimated value for Estimated 2 is 130, for Estimated 3 is 54, for Estimated 4 is 3, for Estimated 5 is 0, and for Estimated 6 is 0.
Table 2 shows the effect of different sibling orders on Smatch weighted, Smatch core, and Smatch ordinary scores.
The combined order has the highest Smatch ordinary score among all the sibling orders.
The hyper-parameters settings for the "Sentence Encoder" component are: Transformer layers = 4, lemma embedding size = 200, POS tag embedding size = 32, NER tag embedding size = 16.
The hyper-parameters settings for the "char-level CNN" component are: number of filters = 256, width of filters = 3, char embedding size = 32, final hidden size = 128.
The "Our Model: CCRF+SVR" performs the best among all the models in terms of MSE values for both "Only Title" and "Title & Text" categories.
The inclusion of "Users" in the "Aggregated Model: SVR" leads to lower MSE values compared to the "News Source + All Topics + Language" model for both "Only Title" and "Title & Text" categories.
The MSE of our model, User SVR, is significantly lower than the MSE of the Latent Factor Models (LFM), Simple LFM, Experience-based LFM, and Text-based LFM.
The Text-based LFM model has a higher MSE compared to the Experience-based LFM and Simple LFM models.
Table 8 provides NDCG scores for ranking trustworthy sources.
The CCRF model has the highest NDCG score among the listed models.
The CCRF model has the highest NDCG score among the three models.
The Member Ratings model has a higher NDCG score than the Experience LFM model.
Table 10 shows the correlation between various factors.
The correlation between Insightful, Fairness, Style, Responsibility, and Balance with Article Credibility Rating is provided in Table 10.
Based on the results in Table 2, there is a significant preference for "T-Enc" over "T-Base" with a p-value of 1.07e-84.
Based on the results in Table 2, there is no significant preference between "T-Enc" and "T-Dec" with a p-value of 0.256.
The MCD value for T-Base with input conditioning is 12.89.
The MCD value for T-Enc-Dec with attention conditioning is 13.51.
The table presents results on downstream tasks using different models.
The WordShuffle model performs better than the WordDrop model on the TREC task.
The number of distinct tokens in the generated responses for the "Ours [ITALIC] S [BOLD] I+ [BOLD] R" training data is 183.
The BLEU-1 score for the generated responses in the "Ours [ITALIC] S [BOLD] I+ [BOLD] R" training data is 9.4.
The scores for the utterance-response pairs in Table 2 range from 0.00 to 11.72.
The average score for the human responses is higher than the average score for the system responses in Table 2.
Csáky et al. (2019) used SRC and TRG scoring methods, while Junczys-Dowmunt (2018) used a different method.
The Spearmans r value for "Ours SI+R" is 0.3751 and the p-value is 4.4x10^-8.
The scoring method "Ours S+I+R" has the highest correlation coefficient with human judgments among all the scoring methods.
Table 7 provides correlation results with human judgments for different scoring methods.
Table 5 shows the conTest results, including the Spearman's correlation between a set's class and each metric score.
The "absHDS" metric has a higher correlation with the set's class compared to the other metrics in the same row.
The VNMT system performs better than both Moses and GroundHog in terms of BLEU scores.
The BLEU score for the VNMT system is significantly higher than that of GroundHog on the MT04 test set.
The table provides BLEU scores on the new dataset for two different systems: GroundHog and VNMT.
VNMT shows significant improvements in BLEU scores on the new dataset compared to GroundHog.
Table 1 compares different types of query suggestion generators across different sources.
The query suggestion generator performs better when using the "expanded" source compared to using the "raw" source.
The model "NP-MSSG.300D.6K" achieves the highest value in the "FUZZY-B-CUBED" metric.
The model "AdaGram.300D α = 0.15" achieves the highest value in the "FUZZY-NMI" metric.
The AdaGram.300D. α = 0.15 model achieves the highest ARI scores for all three datasets (SE-2007, SE-2010, SE-2013) compared to the other models.
The MSSG.300D.30K model has the lowest ARI score among all the models for the SE-2013 dataset.
Table 8 shows the Spearman's rank correlation results for the contextual similarity task on the SCWS dataset.
The Skip-Gram.900D model achieves a maximum similarity score of 68.4 on the SCWS dataset.
The PTB dataset is used for the language modeling task.
Fine-tuning improves the perplexity of the Pruning W[i] (r=400) model.
The accuracy of Semi-NMF with W_h is 87.24 for r=10.
The best accuracy (average) for Semi-NMF with W_h is 88.47.
The average labeled attachment score for parsing models with deep contextualized word representations is higher than the average labeled attachment score for parsing models without deep contextualized word representations.
The labeled attachment score for parsing models with deep contextualized word representations is higher for Turkish compared to Greek.
The table shows the P@1 (%) of target document retrieval on the UN corpus using different models.
The forward direction in the en-ru language pair has the highest average P@1 among all language pairs.
The BiDE+AM model achieves higher precision at N (P@N) values for all language pairs compared to the other models.
The BiDE model achieves higher precision at N (P@N) values for all language pairs compared to the DE model.
Table 2 shows the P@1 (%) on the UN corpus from forward search and backward search.
The backward search has a P@1 (%) of 90.6 for the en-es language pair.
Table 3 provides BLEU scores on WMT testing sets of NMT models trained on original UN pairs (Oracle) and two versions of mined UN corpora at the sentence level.
The "Mined backward" method achieves higher BLEU scores than the "Oracle" method for both the en-fr and en-es language pairs.
The model performs better on the "fr-en" language pair compared to the "de-en", "ru-en", and "zh-en" language pairs.
The BERT model performs better on the "fr-en" and "de-en" language pairs compared to the "ru-en" and "zh-en" language pairs.
TFN performs better than the state-of-the-art approaches in terms of Binary Acc(%), Binary F1, 5-class Acc(%), and Regression MAE.
The human performance is higher than all the other approaches in terms of Binary Acc(%), Binary F1, 5-class Acc(%), and Regression MAE.
The combination of Skip-gram and RNNMEs has an accuracy of 58.9.
The combination of LdTreeLSTM with a hidden size of 400 has an accuracy of 60.67.
S-LSTM achieves the highest UAS and LAS scores among all the parsers.
LdTreeLSTM achieves the highest UAS and LAS scores among all the parsers during the development phase.
The table shows the results from sub-task A in English using different models.
Fast-BiLSTM with ε=100 achieves the highest F1 macro score on the OLID dataset.
The "Learned-BiLSTM (60 Epochs)" model achieves the highest Macro F1 score in sub-task B.
All models in sub-task B are trained on the OLID dataset.
The table provides recall, precision, and F1 score by class for the "Learned BiLSTM EN" and "AUX-Fast-BiLSTM DA" models in sub-task B.
The "Gender (GloVe) T:0.5" model performs significantly better than the "rand" model, with a +160.5% difference in performance.
The "Gender (GloVe) NN" model achieves a performance of 0.519.
BERT embeddings achieve higher gender-occupation neutrality scores compared to GloVe and ELMo embeddings.
A higher threshold (T:0.7) for gender-occupation neutrality results in higher scores compared to a lower threshold (T:0.5) for all embeddings.
The values in the "gen." column represent the gender bias of the template parameters, with higher values indicating a stronger gender association.
The verb and the object in each row have a strong association.
Table 5 shows the religion-polarity neutrality scores for models using GloVe and ELMo embeddings.
The neutrality score for the GloVe embedding model at a threshold of 0.7 is 0.636.
The template parameter "dishonest" has the largest entailment value of 0.98 with the GloVe model.
The template parameter "potato" has an adherent value of 0.97 with the GloVe model.
The values in the "2nd", "3rd", and "4th" columns of the table are all smaller than the values in the "cosine" column.
The value in the "cosine" column for the "Gendered" dataset is larger than the values in the "2nd", "3rd", and "4th" columns.
The "proj" method performs better than the "rand" method with a difference of +19.8% for the threshold of 0.5.
The "proj" method performs better than the "rand" method with a difference of +82.5% for the threshold of 0.7.
The performance difference between the "Gender (ELMo Layer 1) T:0.7" and "rand" conditions is -2.6%.
The performance difference between the "Gender (ELMo Layer 1) T:0.5" and "proj" conditions is +17.3%.
The accuracy on the dev set for SNLI after debiasing gender on all layers using ELMo is 88.77%.
The accuracy on the test set for SNLI after debiasing religion on layer 1 using ELMo is 88.30%.
The "hybrider (BERT-large-uncased, τ=0.7)" model performs the best on the Dev In-Table and Dev Total subsets.
The "hybrider (BERT-base-uncased, τ=0.8)" model performs the best on the Test In-Passage subset.
The "Wseq-S (ours)" model has the highest validity score among all the models.
The "Wseq (ours)" model has the lowest fluency rank among all the models.
Table 2 provides automatic evaluation metrics on the WikiSQL test set for generated adversarial questions.
The Seq2seq-based model "Wseq (ours)" has the highest BLEU score compared to other Seq2seq-based models.
The "HMNs" model outperforms other models in all categories on the Key-Value Retrieval dataset.
The "SEQ2SEQ" model performs poorly in the "Weather Ent. F1" category on the Key-Value Retrieval dataset.
Table 4 shows the correlation between human ranking and corpus- and sentence-level metrics.
The correlation between the I-m. metric and human ranking at the corpus level is -0.055∗.
The F1 scores for SISG(cj) and SISG(cjh4) are higher than the F1 score for SISG(c).
The embeddings used for SISG(cj) and SISG(cjh3) are different from the embeddings used for SISG(c) and SISG(cjh4).
The "SISG(cjh4)" embedding has the highest BLEU 1, BLEU 2, BLEU 3, and BLEU 4 scores among all the embeddings.
The "SISG(cjh4)" embedding has the lowest perplexity score among all the embeddings.
The General LM model performs better than the Discriminative Model on the test set.
The table presents the relationship between model accuracy and insertion type based on a sample of 50 correct and 50 incorrect predictions from each model.
The General LM exhibits a bias in accuracy by insertion type, while the discriminative model does not show this bias.
The size of the embedding layer increases as the number of channels (M) increases.
The reconstruction loss increases as the size of the codebook (K) increases.
As the size of the embedding layer decreases, the reconstruction loss decreases.
As the size of the embedding layer increases, the reconstruction loss increases.
The table shows the accuracy for predicted lemmas (bases and derivations) on shared and split lexicons.
The accuracy of the model "biLSTM+CTX+BS+POS" on the shared lexicon is 0.89.
Table 1 presents the results of different methods on the WebQuestions test set, categorized into SP-based and IR-based methods.
Our Method (BAMnet) with a topic entity predictor achieves an F1 score of [BOLD] 0.518 on the WebQuestions test set.
Table 2 shows the ablation results on the WebQuestions test set, where different components of a method are removed.
Removing the two-layered bidirectional attention, kb-aware attention (+self-attn), importance module, enhancing module, generalization module, joint type matching, topic entity delexicalization, and constraint delexicalization decreases the Macro F1 score on the WebQuestions test set.
The "Ensemble" method has the highest accuracy among all the methods.
The "KDMN" method performs better than the "KDMN-NoKG" method.
HIN and HIN-SR achieve the highest accuracy scores across all three datasets.
BERT, BERT(head+tail), HAN, CAHAN, HSSC, and SAHSSC achieve accuracy scores higher than 74.2 on the Sports & Outdoors dataset.
The table shows the results of an ablation study of the proposed model, where different components are removed to analyze their impact on performance.
Removing certain components from the proposed model leads to a decrease in accuracy for both the "Toys & Games" and "Sports & Outdoors" domains.
The SimLex and RuSimLex999 scores are higher for the Araneum corpus compared to the RNC corpus.
The RuSSE sets (HJ, WS353-Sim, WS353-rel) scores are higher for the Araneum corpus compared to the RNC corpus.
The correlation between adjectives in the Araneum dataset is higher than in the RNC dataset.
The Nouns subset has more pairs than the Verbs subset.
The accuracy of the "Context-based model (n=2 utts.)" is 74.37%.
The accuracy of the most common class is 31.50%.
The table provides information about the alignment error rate (AER), wall-clock time, and phrase table size for different alignment models on the KFTT corpus.
The alignment model "Hieralign (σθ=1)" has the lowest phrase table size among all the alignment models.
Hieralign with σθ=1 achieves a higher BLEU score than the fast_align baseline in the en-ja translation task.
GIZA++ achieves a higher RIBES score than the fast_align baseline in the en-de translation task.
Table 2 compares the performance of two models, DS-SF and bLSTM, trained with CTC and proposed CTC modification.
Increasing the beam size from 100 to 2000 improves the performance of both DS-SF and bLSTM models.
The performance of the bLSTM models decreases as the number of hidden units increases in terms of both Argmax and LM decoding with beam 2000.
The model with 1024 hidden units and 5 layers performs better than the model with 1024 hidden units and 7 layers in terms of both Argmax and LM decoding with beam 100.
The model "Image + Event + Place + PG + EP Loss" achieves the highest accuracy among all models without using any textual input during test time.
The model "Image + Event + Place + PG + EP Loss" achieves a BLEU-2 score of [BOLD] 9.71 without using any textual input during test time.
The "Image + Event + Place + PG" modality has a higher average human score compared to the "Event + Place" modality.
The "Image + Event + Place + PG + EP Loss" modality has a lower human score compared to the "Image + Event + Place + PG" modality.
Table 2 provides ¯F1 scores from a closed set experiment with 36 authors, considering different vocabulary sizes and numbers of sentences.
As the vocabulary size increases, the ¯F1 scores increase for all numbers of sentences considered in the closed set experiment with 36 authors.
The table shows the average F1 scores for different document lengths in the test set while keeping the document length fixed at 50 in the training set.
The F1 score is consistently 1.00 for the "Whole Book" document length in the test set.
Fine-tuning the model using 5cmBERTBASE improves the average test accuracy compared to the baseline.
The 5cmBERTBASE model has significantly fewer parameters compared to the BERTBASE model.
Table 1 provides results on GLUE test sets scored using the GLUE evaluation server.
Fixing the adapter size to 64 leads to an overall score of 79.6.
The Char model performs better than the Word model on both test sets.
Test set TS1 has higher accuracy than test set TS0 for both Char and Word models.
Our system outperforms the supervised system in the out-of-domain evaluation for the CoNLL organization entity type.
The CogCompNLP system trained on OntoNotes achieves an F1 score of 98.4% for the OntoNotes person entity type.
Zoe (ours) achieves a higher F1typema score than ELMoNN and WikifierTyper.
Zoe (ours) achieves a higher F1typemi score than ELMoNN, WikifierTyper, and OTyper.
The table shows the classification accuracy with topic-demoting methods on the TOEFL dataset.
The "alt-lo" method achieves higher classification accuracy than other methods in both the in-domain and out-of-domain scenarios.
The "Seq2Seq Q + D to A" model is used for all the experiments in Table 4.
The "Seq2Seq Q + D to A Graph" model with the "Web" input achieves the highest ROUGE-1 score.
The "Multi-task Graph" model achieves the highest ROUGE-1, ROUGE-2, and ROUGE-L scores among all the models.
The "Q + D to A, MMR" model has an average input length of 850.
The multi-encoder approach achieves a higher BLEU score than the one-to-one approach for the Es-En translation on the UN6WAY data.
The multi-encoder approach outperforms the one-to-one approach in terms of BLEU score for the Ar-En translation on the UN6WAY data.
Table 3 provides an overview of results on the development and test set of FOBIE for the RBS and SciIE.
The F1 score for SciIE on the test set is 69.04.
The model "Adaptive Inputs (Baevski & Auli, 2018)" has 16 layers and 247M parameters.
The model "Adaptive Inputs + LayerDrop" with 40 layers has 423M parameters.
The "Dynamic Conv (Wu et al., 2019)" model achieves the highest BLEU score of 35.2 on the IWSLT test set.
The "Transformer + LayerDrop" model performs slightly better with a BLEU score of 34.5 compared to the "Transformer (Wu et al., 2019)" model on the IWSLT test set.
Table 7 compares the performance of BERT and RoBERTa models with and without distillation and LayerDrop on different datasets.
The RoBERTa + LayerDrop + more data model achieves higher performance than other models on the MNLI-m, MRPC, QNLI, and SST-2 datasets.
Finetuning the model after pruning with LayerDrop reduces the model's validation perplexity.
Applying LayerDrop improves the model's performance in terms of validation perplexity.
Table 4 presents an ablation study on the development set, evaluating different models and their LA. F1 and SA. F1 scores.
The "Short answer prediction" module does not have a SA. F1 score listed in the table.
Table 3 shows the influences of graph layer numbers on the development set.
The F1 scores for both LA and SA generally increase as the number of graph layers increases, except for the 4-layer model where the SA.F1 score decreases slightly.
The percentage of long answer case 1 is higher than the percentage of short answer case 1 for BERT-base+Model-I.
The percentage of short answer is higher than the percentage of long answer case 3 for BERT-syn+Model-III.
The recall-at-1 for LSTM-DE is higher for the Ubuntu dataset when using the full dataset compared to using only 1% of the dataset.
The table represents the accuracy of different parsing models built on the two top words of the stack (UAS, LAS) on the development set.
The UAS accuracy for the parsing model built on the "FORM" feature is 56.55.
Table 2 shows the accuracy of different parsing models on the development set using different combinations of features.
The combination of baseline, best supertag, and supertag distribution features achieves the highest UAS and LAS scores.
The table shows the performance (F1) on TriviaQA and MRQA dev sets for varying lengths of context (NC) and background (NB).
The F1 performance on TriviaQA and MRQA dev sets increases as the length of the context (NC) decreases.
The table shows the F1 scores on TriviaQA and MRQA dev sets for different combinations of pretraining and finetuning.
The highest F1 score on the MRQA dev set is achieved when both pretraining and finetuning are done using the TEK dataset.
The method "BiDAF" outperforms the method "Classifier" in both the "Distant Supervision Dev" and "Distant Supervision Test" domains.
The "web" domain outperforms the "Wiki" domain in both the "Distant Supervision Dev" and "Distant Supervision Test" evaluations for both the "Classifier" and "BiDAF" methods.
The NMT model achieves the highest cumulative reward.
The "translate' by copying source" method has the lowest cumulative reward.
The F1 score for BERT-CUR BASE is 0.67.
The Recall score for AllPrevUtterances is 1.00.
The term "condition" is strongly associated with mentions of mental illness.
The term "treatment" is more strongly associated with mentions of mental illness compared to other terms in the same row.
The toxicity scores decrease as the sentences become less specific about a particular group of people.
Sentences that mention specific disabilities have higher toxicity scores compared to sentences that mention general terms.
The system "PG-MMR" has the highest rankings in both the "1st" and "2nd" positions.
The system "PG-Original" has the highest score in fluency.
The table shows the percentages of summary n-grams (or the entire sentences) that appear in the multi-document input for four different systems.
The PG-MMR system has the highest percentage of 3-grams appearing in the multi-document input.
The test accuracy increases as we move from model (1) to model (11).
The kappa agreement also increases as we move from model (1) to model (11).
The word-error-rate for the Original model with a segment length of 16 seconds is 13.3.
The word-error-rate for the MILK model with overlapping inference and a chunk size of 8 seconds is 17.3.
The "Phone/Fax" category has the highest fraction among all PHI tags.
The precision scores for all PHI categories are lower for the DEDUCE rule-based tagger compared to the BiLSTM-CRF model.
The recall scores for the "Date" and "Phone/Fax" PHI categories are lower for the DEDUCE rule-based tagger compared to the BiLSTM-CRF model.
LSTM-ATT-CRF performs better than LSTM-CNN-CRF on the aspect specific opinion expression task.
LSTM-CRF achieves higher precision than LSTM-ATT on the aspect specific opinion expression task.
LSTM-ATTN-CRF achieves the highest F-score among all the models in Table 1.
semi-CRF achieves the highest recall score among all the models in Table 1.
The table presents the MAP comparison of different deep real-valued representation learning methods on the Flickr30K dataset.
The DeViSE method achieves a sentence retrieval R@1 score of 4.5 and a sentence retrieval Med r score of 6.7.
The table compares the Mean Average Precision (MAP) of different features on the Wiki dataset.
The MAP values for the "Image" queries are lower than the MAP values for the "Text" queries when using the SIFT/LDA feature extraction technique.
The table shows the MAP comparison of different shallow real-valued representation learning methods on the Wiki dataset in the three-modality case.
The table compares different shallow real-valued representation learning methods based on their performance in different modalities.
The table shows the PIE extraction performance of the four systems on the test set.
The precision, recall, and F1-score of the "Exact-1Word" variant are 92.66, 59.88, and 72.75, respectively.
Table 11 provides the PIE extraction performance of the combined output of a string-based and a parser-based system on the development set.
The precision, recall, and F1-score for the "Parsing-InContext ∪ Exact-CS-0Words" combination are 89.18, 93.93, and 91.50, respectively.
Table 1 provides evaluation results for the colors task and the birds task in reference games.
When the model human is in a listener role, the translation based on belief matching outperforms the machine translation baseline.
When the model human is in the speaker role, translation based on belief matching outperforms both random and machine translation baselines.
When the model human is in the listener role, the score for as listener R is higher than the score for as listener H*.
The proposed MMVED-TMP model outperforms other baseline methods in terms of nMSE-TMP.
The proposed MMVED-TMP model achieves the highest SRC among all the baseline methods.
The missing modality does not significantly affect the nMSE-TMP performance of the MMVED-TMP model.
The missing modality in "V + A + S" does not affect the performance of the MMVED-TMP model under the nMSE and SRC criteria.
The nDCG@20 Coor-Ascent values for "Desc" and "Desc, keywords" are higher than the values for "Title" and "Narr".
The average length for "Narr" queries is higher than the average length for "Title", "Desc", and "Desc, keywords" queries.
The number of human subjects in the current evaluation is higher than in the preliminary evaluation.
The mean total success percentage decreased in the current evaluation compared to the preliminary evaluation.
The table presents the results of different experiments labeled as E2, E3, E4, and E5.
The table compares the results of rescoring WER with different settings for the first-pass maximum beam size.
The table presents the results of four different experiments labeled as B0, B1, E1, and E2.
Experiment E1 used Beam Search as the decoding method.
The percentage of change between Conventional and Two-pass methods is 13.2%.
The Conventional method outperforms the Two-pass method in 48 cases.
The table compares the average precision (AP) for three different methods with different loss.
The method using both explicit and implicit classification loss achieves the highest average precision (AP) among the three methods.
Our model outperforms both Lin et al., 2016 and Wu et al., 2019 in terms of precision at 100, 200, and 300, as well as average precision.
Our model has the highest mean precision compared to Lin et al., 2016 and Wu et al., 2019.
The table compares the performance of three methods: BERT, Relation_BERT, and Relation_BERT+Transition_Loss.
Relation_BERT+Transition_Loss achieves the highest P@1000 value among the three methods.
The system "globally_normalized" achieves the highest UAS and LAS scores on the German CoNLL'09 dataset.
The system "This work: ensemble, N=20, MST" performs better on the CTB dataset than on the German CoNLL'09 dataset.
The UAS and LAS scores improve as the value of N increases in the "ensemble, N=MST" models.
The UEM score is not available for the "globally_normalized" model, but it improves as the value of N increases in the "ensemble, N=MST" models.
The emotion "brutality" has a slightly lower intensity than the emotion "outraged" in terms of the emotion "anger".
The emotion "sohappy" is more intense than the emotion "violence" in terms of the emotion "joy".
The Spearman correlation coefficients for all emotions (anger, fear, joy, sadness) in the NRC Affect Intensity Lexicon are above 0.9.
The Pearson correlation coefficients for all emotions (anger, fear, joy, sadness) in the NRC Affect Intensity Lexicon are above 0.9.
Table 3 provides the results of multi-domain experiments.
The accuracy of P on the source dataset is higher when trained with 500 training elements from the target domain compared to when trained with 0 training elements from the target domain.
F [ITALIC] cnn has the highest accuracy among all the features.
The accuracy of the models trained with 500 target domain data points is higher than the accuracy of the models trained with 0 target domain data points.
The accuracy of the BNN Arabic Sentiment task using DANN [ITALIC] LR fastText is 54.6.
The accuracy of the CLS-10 task using S only fastText is 62.1.
The pooling method "KMaxPooling" consistently achieves the lowest testing error across different depths.
The testing error decreases as the depth increases for the pooling method "Convolution".
As the depth increases, the number of conv blocks also increases.
As the depth increases, the number of parameters also increases.
Our model performs better than all the baselines in the Movies, Music, and Books categories.
Our model performs better than all the baselines in the Electronics category.
Table 2 compares the performance of four different models on the prediction task.
Our model has the lowest Mean Squared Error (MSE) value for the "Books" category compared to the other models.
The F1-score for GoT is consistently high across different test domains.
The MAP for GoT is consistently high across different test domains.
The method "TiFi" outperforms the methods "Pasca 2018 (Pasca, 2018)" and "Ponzetto & Strube 2011 (Ponzetto and Strube, 2011)" in terms of precision, recall, and F1-score for both the LoTR and GoT universes.
The method "Ponzetto & Strube 2011 (Ponzetto and Strube, 2011)" achieves a recall score of 1.0 for both the LoTR and GoT universes.
Table 3 provides information about the cleaning process for cross-domain categories.
The table presents the results of three different methods: HyperVec, HEAD, and TiFi.
The F1-scores for GoT are higher than the F1-scores for LoTR for all methods.
TiFi outperforms HyperVec and HEAD in terms of F1-score for both proper-name edges and concept edges in both LoTR and GoT universes.
The performance of all three methods is better for proper-name edges in the GoT universe compared to the LoTR universe.
Our system outperforms the PTB system in terms of relation F1 score.
The "DCCL" method has the highest score in the "MR" column.
The "IIQ-32" method has a higher score than the "IIQ-64" and "IIQ-128" methods in the "CR" column.
The method "IIQ-32" outperforms the baseline method "GloVe" on most word similarity tasks.
The method "IIQ-128" performs the best on the "TR9856" word similarity task.
The dataset contains a total of 358,091 unique users and the average number of additional activities per user is 59.52.
The dataset includes a total of 560,526,633 additional tweets and the average number of additional tweets per user is 1,565.
There were a total of 29,494 queries.
On average, there were 11.37 valid tweets per query.
The ACR scores for fullA are consistently lower than the ACR scores for fullT.
The keval scores for rand are consistently lower than the keval scores for fullT.
The CAE-RNN-LC model outperforms the CAE-RNN model in terms of AP (%) on development data for all languages.
The CAE-RNN-LC model achieves the highest AP (%) on development data for the Hausa language.
Table VIII displays the accuracy of linear classifiers predicting the language identity of triphones from the development data of five languages.
The CAE-RNN model achieves an accuracy of 66.8% when predicting the language identity of triphones from the development data of five languages using the RU+CS+FR feature set.
The ablation study evaluates different models on Weibo, Twitter15, and Twitter16 datasets.
GLAN outperforms w/o LRE and w/o GRE in terms of accuracy on Weibo, Twitter15, and Twitter16 datasets.
The table shows the test macro F1 scores for models with and without the gating mechanism.
The F1 scores for AskMen, AskWomen, and Politics are higher when the model uses the gating mechanism.
Table 4 provides the text gate values relative to low karma modes for the categories AskMen, AskWomen, and Politics.
The text gate values for the categories AskMen and AskWomen are the same, and the text gate value for Politics is slightly lower.
The FRR decreases as the layer increases in the GRU (CRNN) attention-based model with soft attention.
The number of parameters increases as the layer increases in the GRU (CRNN) attention-based model with soft attention.
The "GRU soft attention" model has the lowest FRR (%) among all the models listed in the table.
The entity class "Adverse drug reaction" has the highest number of mentions and tokens in the provided training data.
The entity class "Animal" has an average of 1 token per mention in the provided training data.
Table 2 provides statistics for different subsets of the WikiSQL test set based on the number of shots.
The number of questions in the WikiSQL test set decreases as the number of shots increases.
There are more samples in the "Case-Wrong" category compared to the "Case-Correct" category.
Table 4 displays the predicate accuracy of different methods on the SemEval dataset.
The T2T method has the highest accuracy among all the methods on the SemEval dataset.
T2T outperforms MLE, SeqGAN, PG, and NW in terms of BLEU-3 score on the WebNLG dataset.
T2T achieves higher METEOR score on the SKE dataset compared to MLE, SeqGAN, PG, and NW.
T2T achieves the highest scores in both "Grammar" and "Correctness" in the human evaluation on the WebNLG dataset.
MLE outperforms CoT, PG, SeqGAN, and NW in terms of grammar in the human evaluation on the WebNLG dataset.
The difference in BLEU scores between GCN and MGCN+SUM increases as the number of triples increases.
The BLEU score increases as the number of triples increases.
BERTLARGE performs better than BERTBASE in the CoLA task.
RoBERTaBASE performs better than BERTBASE in the SST-2 task.
AP-CNN achieves the highest MAP and MRR scores among all the systems in Table 5.
QA-CNN outperforms QA-biLSTM in terms of both MAP and MRR scores in Table 5.
The table lists the hyperparameter names used in the neural network models.
The word embedding size for the AP-CNN and AP-biLSTM models is 100.
AP-CNN has the highest accuracy on the Dev set.
QA-CNN has the lowest accuracy on the Test1 set.
The system "AP-CNN" has the highest MAP and MRR scores among all the systems.
The "Wang & Nyberg (2015)" system has higher MAP and MRR scores than the "QA-biLSTM" system.
RetrieveNRefine++ has a higher win rate than Memory Network.
There is a statistically significant difference between the performance of RetrieveNRefine++ and Seq2Seq.
Using the true label as a retrieval method results in the lowest perplexity on the ConvAI2 task test set.
Using the true label as a retrieval method performs better in terms of perplexity compared to using the true label's neighbor or a memory network on the ConvAI2 task test set.
The percentage of connectives in generated texts that are consistent with human annotation increases as the number of annotators agreeing on the relation increases.
The percentage of connectives in generated texts that are consistent with human annotation is higher for fine-tuned texts compared to organic texts.
The Fine-tuned predicted model performs better than the Fine-tuned GPT-2 model for the "≥ 4" relation type.
The Organic predicted model performs better than the Organic GPT-2 model for the "≥ 3" relation type.
The "Prevalent (ours)" agent performs worse than the "Press" agent in terms of the Validation Seen NE score.
The "Prevalent (ours)" agent performs worse than the "Press" agent in terms of the Test Unseen TL score.
The "Prevalent (Ours)" agent has the highest goal progress on the Validation Unseen Oracle setting.
The "Shortest Path Agent" has the highest goal progress on the Test Unseen Mixed setting.
Table 3 provides the results of different agents in the HANNA test.
The agent with "perfect assistance" performs better in the seen environment than in the unseen environment.
The table compares the performance of two methods, "Two-stage" and "Feature-based", in an ablation study on R2R.
Table 8 provides a comparison with related works.
The table compares the performance of two different models, "Press" and "VLP".
The utterance "recommend a popular movie" is associated with the "GetPopularMoviesIntent" intent.
The confidence score for the "recommend a popular movie" utterance and the "GetPopularMoviesIntent" intent is 0.239.
Table 5 shows the results on artificial noisy inputs for different noise fractions.
Our method achieves the highest accuracy among all methods for all noise fractions.
The highest scores for all metrics are achieved when using the loss configuration "\mathcal{L}_{aut}+\mathcal{L}_{adv}".
The method "Ours*" using the loss configuration "\mathcal{L}_{aut}+\mathcal{L}_{adv}" achieves the highest score on the MT03 metric.
The loss configurations "\mathcal{L}_{mixup}" and "\mathcal{L}_{aut}" have higher scores on the English-German newstest14 compared to the English-German newstest13.
The method "Ours" with the loss configuration "\mathcal{L}_{aut}+\mathcal{L}_{adv}" has the highest scores in all four translation tasks.
The combination of clean loss and adversarial loss has the highest loss across all values of alpha.
The "Didi Callcenter - 1K" dataset has a speaking style of "Spontaneous".
The "Open Mandarin" dataset has 1150 hours of data.
The CER for HKUST dataset is higher than the CER for AISHELL dataset.
The RERR for Didi Dictation dataset is higher than the RERR for Didi Callcenter dataset.
The table shows the performance metrics (CER and RERR) for different pre-training objectives on the Didi Callcenter dataset.
The combination of MPC and APC pre-training objectives achieves the lowest CER and RERR on the Didi Callcenter dataset.
The table shows results on HKUST and AISHELL with different knowledge transfer methods.
The combination of target data adaption and layer-wise discriminative training achieves a CER value of 20.8 and a RERR value of 3.26.
The average relative WER change when adding auxiliary phoneme and language-adversarial objectives is -2.9%.
The word error rate for Aymara is 34.2%.
The word error rate for SB Quechua without auxiliary training objectives is 13.8%.
On average, 100-lang performs 6.0% worse than the next best method.
The performance of 100-lang is 79.2 and it is 8.2% lower than the next best method.
In Table 1, the value of β is always equal to the value of γ for each item.
In Table 1, the value of [ITALIC] s⇝ [ITALIC] f is always 0.
Table 1 shows the F1 scores of different systems on the CoNLL'02 and CoNLL'03 datasets in multiple languages.
The BERT-ML system achieves an F1 score of [BOLD] 87.9 in English and [BOLD] 91.1 in Dutch.
BERT-ML (this work) achieves the highest F1 score for English and Chinese, while BERT-SL (this work) achieves the highest F1 score for Arabic.
Arabic has the highest average number of BERT wordpieces per token, followed by English and then Chinese.
The BERT-ML3 system achieves the highest zero-shot F1 score for Dutch among the listed languages in Table 3.
The LI system performs better in zero-shot F1 score for Spanish compared to other systems listed in Table 3.
Table 4 provides the results of a cased multilingual zero-shot on CONLL development data, with different numbers of frozen layers during training.
The performance of the model on the German language decreases as the number of frozen layers increases.
The performance of the model decreases as the number of frozen layers increases for all languages.
The performance of the model is consistently higher for English than for Dutch across all number of frozen layers.
Table 6 provides CoNLL test results using ML, LI, CL, CL+LI, and different frozen layers.
The "ML" model with 8 frozen layers achieves the highest performance on the Dutch language.
Table 2 provides data distribution across the Training-Development-Test splits for different datasets and sub-tasks.
For Sub-Task 1 of the Humicroedit dataset, there are 9652 samples in the training set, 2419 samples in the development set, and 3024 samples in the test set.
The RMSE score for the masked-LM fine-tuning on the Humicroedit dataset using BERT (base-uncased) is 0.533.
The accuracy score for the masked-LM fine-tuning on the FunLines dataset without using ALBERT (base-v2) is 0.541.
The NMT (Transformer) model achieves a BLEU score of 17.16 on the en-de language pair.
The PBSMT (Iter. n) model achieves a BLEU score of 27.20 on the fr-en language pair.
Table 3 shows the fully unsupervised results of different translation methods on 8 language pairs.
The combination of PBSMT and NMT performs better in the ru→ en language pair with a BLEU score of 16.62 in the fifth iteration of back-translation.
The user study consists of 405 distinct questions.
The average success rate in the user study is 78.4%.
Table 9 compares the effect of user feedback on correctness for two different training examples.
User feedback leads to an increase in correctness and MRR.
Table 4 provides a manual comparison of the reasoning required to answer each question-answer pair on a random sample of 192 examples from each dataset.
The Reddit and Amazon datasets have more examples requiring world knowledge to resolve lexical variation compared to the New York Times dataset.
The "WEB + c_w_n + m_s" model achieves the highest F-measure values for DL, FR, and DM.
The "WEB + c_w_n + m_s" model outperforms the "Baseline" model in terms of F-measure values for DL, FR, and DM.
The hyperparameter "ϵ" represents the threshold for overall subsampling and its value is 10^-5.
The hyperparameter "d" represents the dimension of word embeddings and its value is 100.
The table shows the results for three different tasks: (1), (2), and (3).
Task (1) has a higher percentage of wins for section-agnostic compared to task (2) and task (3).
The symmetry violation ratios for the gold alignments with GPLA, CAMR, and JAMR, as well as the alignments between JAMR and GPLA, CAMR and GPLA, and CAMR and JAMR, are all less than 10%.
The average symmetry violation ratio for all the AMR metrics is 0.1%.
Table 1 provides information about the symmetry violation ratio and mean symmetry violation of AMR metrics.
The symmetry violation ratio for the worst-case scenario is 81.3% and the mean symmetry violation is 0.2 in pp.
The table presents the human evaluation of accuracies and grammaticality scores for the Amazon and IMDB datasets.
The average grammar score for the original sentences is higher than the average grammar score for the TF and BAE-R+I sentences in both the Amazon and IMDB datasets.
Table 5 shows the performance of different models on three datasets: MR, Subj, and TREC.
The BERT model achieves the highest performance on the MR dataset among all the models.
The table shows the distribution of texts across genres in LCMC and ZCTC datasets.
The total number of texts in LCMC dataset is the same as the total number of texts in ZCTC dataset.
The strategies used in the experiments are "Baseline", "MNMT→Bi (RapAdapt)", "MNMT→Bi (SDE)", "Many ↔ Many", "Data-Augment", "MNMT→Bi (DirAdapt)", and "MNMT→Bi (DynAdapt)".
The Δ (DynAdapt-Baseline) values in the "Ours" row are all higher than the corresponding values in the "Baseline" column.
Neubig & Hu used the Select-one strategy for the LRL→en ZST task.
The Ours model outperformed the Neubig & Hu model for the az language in the LRL→en ZST task.
The table shows the BLEU scores for zero-shot translation (ZST) using models trained with different data-selection criteria.
The baseline BLEU score for non zero-shot translation (ZST) is 16.32.
The dimension of BERT embeddings is either 768 or 1024.
The Flair model has a lower correlation coefficient than the ELMo model.
BERT has the highest score among all models in the "Original" column.
ELMo-1024 has the highest score among all models in the "Masked" column.
Table 5 shows the F1 scores for different models with 1:1 oversampling.
The kappa values for "Thinking error", "Situation", and "Emotion" are all above 0.5.
There is no missing data or empty cells in the "Concept" column.
The system "Yih et al. (2013) – LCLR" achieves the highest MRR score among the published models.
The "bigram + count" model trained on the "TRAIN-ALL" dataset achieves a higher MRR score than the same model trained on the "TRAIN" dataset.
The "bigram + count" model trained on the "TRAIN" dataset achieves the highest MAP and MRR values.
The "bigram + count" model trained on the "TRAIN-ALL" dataset achieves the highest MAP and MRR values.
The "Restaurant" data in the table is sourced from Yelp.
The average length of the "Movie" training data is 668.
The knowledge base contains information about expressions related to customer service and customer care.
The confidence level for the "expressedBy" predicate with the object "customer service" is higher than the confidence level for the "expressedBy" predicate with the object "customer care".
The semantic feature group achieves a higher accuracy than the lexical feature group.
The combined feature set achieves a lower accuracy than the semantic feature group.
The V-measure score for the Oracle categories is higher than the V-measure score for the Automatic categorization.
The homogeneity score for the Category "Hom." is higher than the homogeneity score for the Category "Compl.".
The NVRNN-BoW model predicts the bag of words less accurately compared to the NVRNN model.
The v-VAE model predicts the bag of words less accurately compared to the G-VAE model.
The KL value is 6.52 when no annealing is used for the 3-layer model.
The NLL value is 125 when no annealing is used for the 1-layer model.
The v-NVDM (Ours) model with a dimension of 25 achieves a test set perplexity of [BOLD] 793 on the 20NG dataset.
The G-NVDM model achieves a test set perplexity of 550 on the RCV1 dataset.
The LAS score for the predicted treebank is lower than the LAS score for the gold treebank for Ancient_Greek.
The LAS score for the predicted treebank is higher than the LAS score for the gold treebank for Italian.
Table 2 provides information about other concepts in a résumé.
The concept "Army & Security" appears 42 times in the résumés.
When the number of attention heads is 1, the performance on Fake-English is 77.4.
When the number of attention heads is 3, the difference in performance between Fake-English and Russian is 14.2.
B-BERT performs better on XNLI when the language pair includes English.
B-BERT performs better on NER when the language pair includes English.
As the permutation amount increases, the accuracy of XNLI and the F{}_{1}-Score of NER decrease.
When the permutation amount is 1.0, the F{}_{1}-Score of NER is 2.9 (\pm1.9).
As the depth of the B-BERT architecture increases, the number of parameters also increases.
As the depth of the B-BERT architecture increases, the difference in performance between Fake-English and Russian decreases.
The table shows the test accuracy of textual entailment data with premise and hypothesis in different languages using the XNLI test set.
The test accuracy for the "enfake-target" combination is higher than the test accuracy for the "enfake-es" combination.
The Syn (R+V) model achieves the highest score on the GLUE benchmark.
The Syn (D) model achieves the lowest score on the CoLA task.
The Synthesizer model with Random + Vanilla configuration achieves a BLEU score of 28.47 on the NMT (BLEU) EnDe task.
The Transformer model achieves a BLEU score of 38.21 on the LM (PPL) LM1B task.
"Synthesizer (D+V)" and "Synthesizer (R+V)" outperform other models in most metrics in the Summarization task.
"Synthesizer (D+V)" achieves a higher Conversation Interaction Diversity rate than "Synthesizer (R+V)".
"Syn (R+V)" performs better than other models in most tasks.
All models have low performance in the RTE task.
Table 7 provides ablations of the model on the dev set, specifically focusing on the importance of entity abstraction and message passing (K=2).
RoBERTa achieves the highest average score across all tasks in Table 5.
XLNet achieves the highest score on the SST task in Table 5.
The table compares the performance of static and dynamic masking for BERT\textscbase on SQuAD 2.0, MNLI-m, and SST-2.
The dynamic masking approach outperforms the static masking approach on SQuAD 2.0 and SST-2.
As the batch size increases, the perplexity decreases for both MNLI-m and SST-2.
The learning rate is tuned for each setting in the table.
The "NMN+LSTM" model performs better than the "VIS+LSTM" model on the test-dev set.
The "NMN+LSTM" model performs better than the "VIS+LSTM" model on the number-related questions in the test-dev set.
The table provides BLEU scores for different languages in the xx → en and en → xx translation directions.
The BLEU score for the language pair xx → en is 15.9 for the language "et".
Table 2 shows the impact of CNN vs. LSTM sentence models on the baseline network for Task C.
The CNN Train + ED model has higher MAP and MRR scores compared to the LSTM and CNN models.
The SVM classifier has the highest precision score among the different classifiers in the first version of the training set.
All classifiers in the first version of the training set have the same accuracy score.
Table 16 shows the evaluation results of different classifiers on the most recent version of the training set.
SVM achieves the highest precision, recall, F1, and accuracy scores among all the classifiers in the most recent version of the training set.
Table 3 shows the results of using different stop ASM detection metrics with the initial ASMs.
The accuracy of the model improves as different stop ASM detection metrics are used with the initial ASMs.
The accuracy for the GloVe representation on the semantic task is 79.2.
The accuracy for the Transformer representation on the syntactic task is 87.1.
Table 6 shows the accuracy % on 5 relations in BMASS with the greatest absolute difference in word performance vs entity performance.
The entity performance is higher than the word performance for all 5 relations in BMASS.
The Sabbir2017 (+MetaMap, UMLS) method has a higher accuracy than the Sabbir2017 (entities; +MetaMap) method.
The proposed method includes different variations such as Entities, Definitions (joint words), Entities+Definitions, and Oracle (Entities—Definitions).
The accuracy of "MPME (entities; +graph structure)" is higher than the accuracy of "Wikipedia".
The accuracy increases when adding mentions to the entity embeddings, as seen in the progression from "Wikipedia" to "Wikipedia + mentions" to "Gigaword".
The accuracy of "MPME (entities; +graph structure)" is higher than the accuracy of "Wikipedia".
The accuracy increases when adding mentions to the entity embeddings, as seen in the progression from "Wikipedia" to "Wikipedia + mentions" to "Gigaword".
The table provides information about different models including CNN, OctCNN, and MultiOctCNN.
The OctCNN and MultiOctCNN models utilize OctConv or MultiOctConv.
The table mentions two models: Baseline and BERT-Emotion.
BERT-Emotion performs better than the Baseline model on the Dangerous dataset in terms of accuracy.
The precision at the top 1 for the "Procrustes (CSLS)" method is 63.7 in Italian queries.
The precision at the top 10 for the "Cr5 (CSLS)" method is 58.0 in English queries.
The method "Dinu et al. (Dinu et al., 2014)" achieves the highest precision at k on sentence retrieval for Italian queries.
The method "AdvRefine (CSLS) (Conneau et al., 2017)" achieves a precision of 83.1 for English queries at position 10.
The MinAge F score for "This work: QABasedMinMaxAgeExtractor" is higher than the MinAge F score for "—WO Non-factualSentFilter".
The MaxAge P score for "This work: QABasedMinMaxAgeExtractor" is higher than the MaxAge P score for "—WO MinMaxAgeQA".
The table shows the automatic evaluation results of different models used for the transformation from non-ironic sentences to ironic sentences.
The "MultiProjectedAttentionRNN" model has the highest "f1_macro" score among all the models.
The "ProjectedAttentionRNN" model has a "harassment_f1" score of 0.694082.
The table shows the class distribution of the dataset.
The F1 score for our model variant at level 1 on the Electronics category is 28.81.
The Recall score for the Cluster + Medoid baseline on the Books category is 12.53.
The F1 score of our variants on the Toys category is 29.26 (±1.87).
The ROUGE-L score of the Medoid-Recall baseline on the Camera category is 9.52 (±0.83).
The F1 score for the "C + Med-F1" model is lower than the F1 score for the "Med-F1" model.
The F1 score for the "top-Recall" metric is lower than the F1 score for the "level1-Recall" metric.
The Seq2seq model is used for different attributes and different numbers of dialogs in millions.
The perplexity decreases as the number of dialogs increases for the Switchboard attribute.
The F(Ut,DAt−1,t−2) model has the highest accuracy among all the models in the Reddit validation set.
The F(Ut) model has a higher accuracy than the F(DAt−1,t−2) model in the Reddit validation set.
Table 2 presents the dialog-acts prediction accuracy for classifiers trained on the validation set of different datasets.
The Frames dataset has the highest dialog-acts prediction accuracy among the Reddit and Switchboard datasets.
The Seq2Seq+Attr model achieves the lowest perplexity score on the Reddit validation set.
The Seq2Seq+Attr model outperforms the Seq2Seq model in terms of the Greedy metric on the Reddit validation set.
The table presents experimental results on two datasets, D1 and D2.
The highest ROUGE-1 score achieved in the experiments is 46.11 for D1 and 44.36 for D2.
D1 has a higher accuracy than D2 for all models except for the No-Content setting.
D2 has a longer duration than D1 for all models.
The precision, recall, and F-measure values vary for different categories in the optometry dataset.
The precision, recall, and F-measure values in the "All" row represent the average performance across all categories in the optometry dataset.
The precision, recall, and F-measure values vary for each category in the dataset.
The precision, recall, and F-measure values for the "All" category are calculated as an average of the values for each individual category.
The MRC-DST model achieves the highest accuracy of 0.9858.
The RoBERTa model achieves an accuracy of 0.9378.
The average intent accuracy is higher in the development dataset compared to the test dataset.
The F1-score for requested slots is higher in the test dataset compared to the development dataset.
The table shows the slot tagging F1-score for span-based slots.
The F1-score for the test dataset is slightly lower than the F1-score for the development dataset.
WD-DST outperforms RoBERTa in terms of accuracy for both the Average Boolean Slot and Text-based Slot.
Table 5 shows the results of WD-DST on the development dataset.
Table 5 shows the results on the French XNLI dataset.
XLM-R large has an accuracy of 85.2 on the French XNLI dataset.
FlauBERT large achieves the highest accuracy scores for all three categories: Books, DVD, and Music.
MultiFiT achieves the lowest accuracy scores for all three categories: Books, DVD, and Music.
CamemBERT achieves the highest accuracy among all the models listed in Table 4.
The results reported in [yang2019pawsx] for ESIM† and mBERT† are identical.
Table 6 shows the results of different models for constituency parsing and POS tagging.
Table 7 provides the results of dependency parsing for different models.
CamemBERT has a higher UAS score than mBERT.
CamemBERT has the highest F1 score on the Verb Disambiguation Task.
FlauBERT large has a higher F1 score than FlauBERT base on the Verb Disambiguation Task.
The table presents F1 scores (%) on the Noun Disambiguation Task for different models.
The ensemble predictions of FlauBERT large have an F1 score of 57.85% on the Noun Disambiguation Task.
RoBERTa Baseline performs better than BERT Baseline, as it has a higher F1 score.
The RoBERTa - Sentence - System 2 model achieves the highest F1 score among all the models.
BERT - BIOE has the highest F1 Score among all the models in Table 1.
The Precision of BERT - BIOE is lower than its Recall.
The "I-DP" method achieves the highest accuracy scores on the Reddit, CS-En, and CS-Ja test sets.
The accuracy scores for all methods improve when using the optimal train sets compared to the initial train sets.
The table compares the accuracy results of different methods on Reddit, English, and Japanese Customer Service test sets.
The optimal train set outperforms the initial train set on the English Customer Service test set.
After dataset augmentation, the number of positive samples increased significantly while the number of negative samples remained the same. 
The ratio of positive to negative samples became balanced after dataset augmentation. 
The method "GRU_1_1 + RF" has the lowest Cross-Entropy loss value among all the methods.
The method "GRU_3_2 + Dropout + AdaBoost" has a lower Cross-Entropy loss value than the method "GRU_3_2 + Dropout + RF".
The prediction accuracy increases as the selected layer indices Ls decrease for both the Amazon and Taxonomy datasets.
The prediction accuracy is higher when the selected layer indices Ls is {4,8,12} compared to when it is {6,12} for the Amazon dataset.
The "Human" model has the lowest mean rank among all the models.
The "BERT" model is ranked second most frequently among all the models.
The model "BERT" achieves the highest scores in all three metrics (R-1, R-2, R-L) on the CNNDM test set.
Among the Hibert models, "Hibert [ITALIC] S" achieves the highest scores in all three metrics (R-1, R-2, R-L) on the CNNDM test set.
Among the various models, "Hibert S" achieves the highest scores for R-1, R-2, and R-L.
The "Hibert M" model outperforms the "BERT" model in terms of R-1, R-2, and R-L scores.
The metric "τ" has a higher value for System Level WMT12 than for System Level WMT13.
The metric "ρ" has a higher value for Segment Level WMT12 than for Segment Level WMT13.
The EmoAtt model with a dropout rate of 0.8 and GloVe Twitter 50 embeddings performs better than the Baseline model on the development set for the Sadness emotion.
The Baseline model with GloVe Twitter 50 embeddings performs better than the EmoAtt model on the test set for the Joy emotion.
BERTLARGE+SRL performs better than BERTLARGE on both SNLI Dev and SQuAD 2.0 EM.
SemBERTLARGE has the highest F1 score on the SQuAD 2.0 dataset.
The accuracy values for the different numbers (1, 2, 3, 4, 5) are very similar.
The accuracy values increase slightly from number 1 to number 3, and then decrease slightly from number 3 to number 5.
Table 6 shows the results of four different experiments: "parallel 10k", "+ 20k WoW", "+ 10k WoW + 10k COPY", and "+ 10k WoW + 10k BT".
The "+ 10k WoW + 10k COPY" experiment shows higher performance in the "es-en" language pair compared to the "de-en" language pair.
The number of spans decreases as the depth increases.
The percentage of the three most frequent categories decreases as the depth increases.
The table shows NER results on NNE using different methods.
The precision, recall, and F1 score for the "BiLSTM-CRF-TOP" method are 89.9, 38.0, and 53.5, respectively.
The word "idea" has a frequency of 10 in the "philosophy" category.
The word "boy" has a frequency of 12 in the "book" category.
The "Copy object" method performs the best in the NWE task.
The "Frobenius outer" method outperforms the "Copy object" method in the KS14 task.
The method "KS14 addition" achieves the highest score for the GS11 dataset.
There is no statistically significant difference between the scores of "KS14 addition" and "NWE addition".
The F-Score of the neural word embeddings lemmatized model for the SWDA multiplication task is 0.53.
The gold material prediction count is 710.
The gold process prediction count is 708.
The F1 results increase as we move from (1) Stackers to (3) Stackers+CNNs+AB-LSTMs.
The F1 results in the "T" column are consistently higher than the corresponding values in the "M" and "P" columns.
MUDE has the highest accuracy in all types of noise.
MUDE has the highest accuracy in word deletion.
The MUDE method has the highest accuracy for all types of noise (INT, DEL, INS, SUB) compared to the Enchant and scRNN methods.
The table provides a performance comparison of different methods in terms of accuracy for different types of noise.
Table 5 presents the test performance for different types of noise added to the training data.
The mean test performance across all noise types is 94.13.
see2017get outperforms other models in terms of Rouge 2 and Rouge L scores.
Scratchpad achieves state-of-the-art performance for end-to-end models on summarization without Reinforcement Learning on ROUGE.
The "Transformer (6 layer)" model outperforms the "MIXER" and "AC + LL" models on the IWSLT14 De→En task.
The "Layer-Coord (14 layer)" model performs better than the "Transformer (6 layer)" model on the IWSLT15 En→Vi task.
Table 7 shows the human evaluation results of four different models: GLTran, TNCLS, CLS+MS, and CLS+MT.
The model CLS+MT performs the best in terms of the En2Zh IF score.
Table 5 provides experimental results on different versions of datasets for En2Zh and Zh2En tasks.
The F1-Score increases as more features are combined.
The F1-Score for using all features is 0.76.
The MCC classifier has the highest F1-Score in the "F1-Score leave-one-out" column.
The PSt classifier has the highest F1-Score in the "F1-Score Full" column.
LSTM-1 has a higher accuracy on the training set compared to LSTM-4.
LSTM-4 has a higher number of epochs compared to LSTM-1.
FastText performs better than Word2Vec Skipgram and Word2Vec CBOW on the test set.
FastText is trained for more epochs than Word2Vec Skipgram and Word2Vec CBOW.
The model was trained using LSTM-4 architecture with word dimensions of 100, 200, and 300.
The model was trained for 9 epochs with 100 dimensions, 16 epochs with 200 dimensions, and 12 epochs with 300 dimensions.
Bot 1 and bot 2 perform better than bot 3 based on the total score.
Bot 1, bot 2, and bot 5 have a higher median number of turns in engagement compared to other bots.
The table shows the correlation of evaluation metrics with user ratings.
The median duration of engagement is lower for Frequent-Users and Engagement Evaluators compared to Users.
Table 4 provides the correlation values between user ratings and frequent-user ratings.
The correlation between user ratings and frequent-user ratings is higher for frequent-user ratings (0.70) compared to user ratings (0.66).
Table 5 presents the correlation of the regression model with user ratings.
The GBDT algorithm has an RMSE of 1.340.
Table 4 shows the mean reciprocal rank of held-out context words for different datasets and models.
The addition of context improves the mean reciprocal rank of held-out context words for the Shakespeare dataset.
The scores for Con Ours and Att Ours are higher than the scores for Con DRG and Att DRG on the Politeness dataset.
The scores for Con Ours and Att Ours are higher than the scores for Con DRG and Att DRG on the Yelp dataset.
The OURS model achieves the highest accuracy on the Gender dataset with a value of [BOLD] 82.21.
The OURS model achieves the highest ROUGE score on the Political dataset with a value of [BOLD] 77.51.
The DRG model has the highest accuracy among the three models on the Yelp dataset.
The OURS model has the highest ROUGE score among the three models on the Captions dataset.
Table 5 compares different tagger variants for the Yelp and Captions datasets.
The Replace-Tagger variant has the highest accuracy for the Yelp dataset.
The table presents the number of events and relations in eventKG-g:event_kg.
There are 163,977 events with known time in the KGs.
The MEED model achieves the lowest perplexity score on Validation Set 1.
Table 4 shows the accuracy of a Naive Bayes classifier with word overlap (WO) and additional factuality-mismatch (WO+FM) features.
The accuracy of the Naive Bayes classifier with additional factuality-mismatch (WO+FM) features is higher for the English experimental dataset compared to the German dataset.
The precision scores for English to Italian translation are higher than the precision scores for Italian to English translation in the IBFA + CSLS model.
The precision scores for English to Italian translation are higher in the SVD + CSLS model compared to the SVD model.
IBFA outperforms other methods in all translation directions and at all evaluation points.
IBFA has the highest performance in the Italian to English translation direction at the first evaluation point.
The first row of the table represents monolingual FastText vectors in English, while the rest of the rows represent bilingual embeddings.
The highest correlation score for each STS task in English is marked with [BOLD].
Table 6 shows the Precision @1 when aligning English, French, and Italian embeddings to a common space.
SVD performs worse than MBFA in aligning English, French, and Italian embeddings to a common space.
Table 1 provides information about feeding shuffled messages to the trained Receiver and the average percentage accuracy across 10 random shufflings.
When both attention weights are 0, the accuracy is 0.
The accuracy of the Receivers is higher when symbols in all positions but one are shuffled across the data-set compared to when symbols in a single position are shuffled or when symbols within each message are shuffled.
Table 2 provides information on different models and their corresponding BLEU scores and FLOPs in the WMT16 dataset.
The FLOPs (Floating Point Operations) for the models mentioned in Table 2 vary, with values ranging from 0.02 to 0.242.
The "TensorCoder-8conv" model has the lowest valid loss on PTB.
The "Tensorized Transformer ma2019a" model has the highest attention GFLOPs on Wiki-103.
Table 2 shows the performance of different variations of the NAT model in terms of BLEU score and FLOPs.
The Transformer-8head model achieves the highest BLEU score among all the models listed in Table 2.
The "Ours" models outperform the "Tree-to-string" and "LVNMT" models in terms of Bp score.
The "Ours" models outperform the "Tree-to-string" and "LVNMT" models in terms of Bleu score.
Table 2 provides single system results in terms of (Ter-Bleu)/2 on an 11 million set.
The UGRU system achieves a (Ter-Bleu)/2 score of 12.31 on the MT08 Web dataset.
The "Ours U GRU" system outperforms the "LVNMT" system in terms of precision, recall, and F1 score.
The "Ours U GRU+ U Sub" system performs better in terms of F1 score compared to the "Ours U GRU" system.
The table provides performance metrics for different models on the thread reconstruction task.
The "Grid-CNN" model achieves the highest edge-level accuracy score among all the models.
